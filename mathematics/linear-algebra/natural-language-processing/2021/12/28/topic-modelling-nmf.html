<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Topic Modelling using Non-negative Matrix Factorization (NMF)</h1><p class="page-description">Singular Value Decomposition (SVD) is a method from Linear Algebra widley used accross science and engineering. In this article we will introduce the concept and show how it can be used for Topic Modelling in Natural Language Processing (NLP).</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-12-28T00:00:00-06:00" itemprop="datePublished">
        Dec 28, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      12 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#mathematics">mathematics</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#linear-algebra">linear-algebra</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#natural-language-processing">natural-language-processing</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          
          
          
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Introduction">Introduction </a></li>
<li class="toc-entry toc-h2"><a href="#Dataset">Dataset </a></li>
<li class="toc-entry toc-h2"><a href="#Non-negative-Matrix-Factorization-(NMF)">Non-negative Matrix Factorization (NMF) </a></li>
<li class="toc-entry toc-h2"><a href="#NMF-using-Gradient-Descent">NMF using Gradient Descent </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Applying-SGD-to-NMF">Applying SGD to NMF </a></li>
<li class="toc-entry toc-h3"><a href="#Comparing-Approaches">Comparing Approaches </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Conclusion">Conclusion </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-12-28-topic-modelling-nmf.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h2>
<p><strong>Non-negative Matrix Factorization (NMF)</strong> is a method from Linear Algebra that is used in a wide range of applications in science and engineering, similar to <a href="https://livingdatalab.com/mathematics/linear-algebra/natural-language-processing/2021/12/27/topic-modelling-svd.html">Singular Value Decomopistion (SVD) which I covered in an earlier article</a>. It can be used for tasks such as missing data imputation, audio signal processing and bioinformatics.</p>
<p><strong>Topic modeling</strong> is an unsupervised machine learning technique used in Natural Language Processing (NLP) that’s capable of scanning a set of texts, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of documents.</p>
<p>In this article we will will use NMF to perform topic modelling.</p>
<p>This article is based in large part on the material from the <a href="https://github.com/fastai/numerical-linear-algebra/blob/master/README.md">fastai linear algebra course</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Dataset">
<a class="anchor" href="#Dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset<a class="anchor-link" href="#Dataset"> </a>
</h2>
<p>We will use the <a href="https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups">20 Newsgroups</a> dataset which consists of 20,000 messages taken from 20 different newsgroups from the Usenet bulletin board service, which pre-dates the world-wide-web and websites. We will look at a subset of 4 of these newsgroup categories:</p>
<ul>
<li>rec.motorcycles</li>
<li>talk.politics.mideast</li>
<li>sci.med</li>
<li>sci.crypt</li>
</ul>
<p>We will now get this data.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">categories</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'rec.motorcycles'</span><span class="p">,</span> <span class="s1">'talk.politics.mideast'</span><span class="p">,</span> <span class="s1">'sci.med'</span><span class="p">,</span> <span class="s1">'sci.crypt'</span><span class="p">]</span>
<span class="n">remove</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'headers'</span><span class="p">,</span> <span class="s1">'footers'</span><span class="p">,</span> <span class="s1">'quotes'</span><span class="p">)</span>
<span class="n">newsgroups_train</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'train'</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span> <span class="n">remove</span><span class="o">=</span><span class="n">remove</span><span class="p">)</span>
<span class="n">newsgroups_test</span> <span class="o">=</span> <span class="n">fetch_20newsgroups</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="s1">'test'</span><span class="p">,</span> <span class="n">categories</span><span class="o">=</span><span class="n">categories</span><span class="p">,</span> <span class="n">remove</span><span class="o">=</span><span class="n">remove</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's check how many posts this gives us in total</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">filenames</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">newsgroups_train</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((2351,), (2351,))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's print the first few lines of 3 of the posts to see what the text looks like</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)[:</span><span class="mi">3</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
I am not an expert in the cryptography science, but some basic things
seem evident to me, things which this Clinton Clipper do not address.
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)[:</span><span class="mi">3</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Does the Bates method work?  I first heard about it in this newsgroup 
several years ago, and I have just got hold of a book, "How to improve your
sight - simple daily drills in relaxation", by Margaret D. Corbett, 
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">)[:</span><span class="mi">3</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
Suggest McQuires #1 plastic polish.  It will help somewhat but nothing 
will remove deep scratches without making it worse than it already is.
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can also get the newsgroup category for each from the 'target_names' attribute</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">target_names</span><span class="p">)[</span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">target</span><span class="p">[:</span><span class="mi">3</span><span class="p">]]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array(['sci.crypt', 'sci.med', 'sci.med'], dtype='&lt;U21')</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To use this text dataset for topic modelling we will need to convert this into a <strong>document-term</strong> matrix. This is a matrix where the rows will correspond to to each of the newsgroup posts (a 'document' conceptually) and the columns will be for each of the words that exists in all posts (a 'term' conceptually). The values of the matrix will be the count of the number of words that exists for a particular post for each post/word combination in the matrix.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/document-term-matrix.png" alt="" title="An example Document-Term matrix"></p>
<p>This method of converting text into a count of the words in the text matrix, without regard for anything else (such as order, context etc) is called a <strong>bag of words</strong> model. We can create this matrix using a <em>CountVectoriser()</em> function.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s1">'english'</span><span class="p">)</span>
<span class="n">vectors</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">todense</span><span class="p">()</span> <span class="c1"># (documents, vocab)</span>
<span class="n">vectors</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(2351, 32291)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see this matrix has the same number of rows as we have posts (2351) and we must have 32,291 unique words accross all posts which is the number of columns we have.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">newsgroups_train</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="n">vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>2351 (2351, 32291)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we print the matrix, its just an array of counts for each of the words in each post</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">vectors</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>matrix([[0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0],
        ...,
        [0, 0, 0, ..., 0, 0, 0],
        [0, 2, 0, ..., 0, 0, 0],
        [0, 0, 0, ..., 0, 0, 0]])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This matrix does not actually contain the names of the words, so it will be helpful for us to extract these as well to create a vocabulary of terms used in the matrix. We can extract these using <em>get_feature_names()</em></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
<span class="n">vocab</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(32291,)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">vocab</span><span class="p">[:</span><span class="mi">32000</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array(['00', '000', '0000', ..., 'yarn', 'yarvin', 'yashir'], dtype='&lt;U79')</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>While we have the newsgroup categories here, we will not actually use them for our topic modelling exercise, where we want to create topics independantly based on the posts alone, but we would hope these will correspond to the newsgroup categories in some way, indeed this would be a good check that the topic modelling is working.</p>
<p>Now we have our Document-Term matrix and the vocabulary, we are now ready to use Singular Value Decompostion.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Non-negative-Matrix-Factorization-(NMF)">
<a class="anchor" href="#Non-negative-Matrix-Factorization-(NMF)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Non-negative Matrix Factorization (NMF)<a class="anchor-link" href="#Non-negative-Matrix-Factorization-(NMF)"> </a>
</h2>
<p>NMF is a method of matrix decomposition, so for a given matrix A we can convert it into 2 other matrices: W and H. Also A most have non-negative values, and as such W and H will also have non-negative values.</p>
<p><img src="https://github.com/pranath/blog/raw/master/images/nmf.png" alt="" title="Non-negative Matrix Factorization"></p>
<p>K is a value we choose in advance, in the case of our intention here K will repesent the number of topics we want to create for our topic model of the newsgroup posts.</p>
<p>So if we assume in the original matrix A for our exercise, N are the documents/posts and M are the words in our Document-Term matrix, each of these matricies represents the following:</p>
<ul>
<li>W: <strong>Feature Matrix</strong> this has M rows for words and K columns for the topics, and indicates which words characterise which topics.</li>
<li>H: <strong>Coefficient Matrix</strong> this has K rows for topics, and N columns for documents/posts, and indicates which topics best describe which documents/posts.</li>
</ul>
<p>So one reason NMF can be more popular to use, is due to that fact that the factors it produces are always positive and so are more easily interpretable. Consider for example with <a href="https://livingdatalab.com/mathematics/linear-algebra/natural-language-processing/2021/12/27/topic-modelling-svd.html">SVD</a> we could produce factors that indicated negative values for topics - what would that mean to say a text has 'negative indications for the topic of bikes' ?</p>
<p>Another difference with SVD is that NMF is not an exact decompostion - which means if we multiply W and H matrices we won't get back our original matrix A exactly.</p>
<p>So we can peform NMF on our Document-Term matrix using the sklearn <em>decomposition</em> module.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="c1"># Define constants and functions</span>
<span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="n">vectors</span><span class="o">.</span><span class="n">shape</span>
<span class="n">d</span><span class="o">=</span><span class="mi">10</span>  <span class="c1"># num topics</span>
<span class="n">num_top_words</span><span class="o">=</span><span class="mi">8</span>

<span class="k">def</span> <span class="nf">show_topics</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">top_words</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="p">[</span><span class="n">vocab</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">t</span><span class="p">)[:</span><span class="o">-</span><span class="n">num_top_words</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="n">topic_words</span> <span class="o">=</span> <span class="p">([</span><span class="n">top_words</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">a</span><span class="p">])</span>
    <span class="k">return</span> <span class="p">[</span><span class="s1">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">topic_words</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="c1"># Calculate NMF</span>
<span class="o">%</span><span class="n">time</span> <span class="n">clf</span> <span class="o">=</span> <span class="n">decomposition</span><span class="o">.</span><span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">d</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CPU times: user 29 µs, sys: 0 ns, total: 29 µs
Wall time: 34.3 µs
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can notice here this has run extremely fast taking just 19.6 microseconds. If we recall in an <a href="https://livingdatalab.com/mathematics/linear-algebra/natural-language-processing/2021/12/27/topic-modelling-svd.html">earlier article for the same dataset when we performed one of the fastest versions of SVD Randomised/Trucated SVD this took 20 seconds</a>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="c1"># Extract W and H matrices</span>
<span class="n">W1</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">vectors</span><span class="p">)</span>
<span class="n">H1</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">components_</span>
<span class="c1"># Show topics from H matrix</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">'Top 10 topics, described by top words in each topic'</span><span class="p">)</span>
<span class="n">show_topics</span><span class="p">(</span><span class="n">H1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Top 10 topics, described by top words in each topic
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>['db mov bh si cs byte al bl',
 'people said didn know don went just like',
 'privacy internet pub eff email information computer electronic',
 'health 1993 use hiv medical 10 20 number',
 'turkish jews turkey armenian jewish nazis ottoman war',
 'anonymous anonymity posting internet anon service people users',
 'key encryption des chip ripem use keys used',
 'edu com cs david ca uk org john',
 'dod rec denizens motorcycle motorcycles doom like terrible',
 'version machines contact type edu pc comments ftp']</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So if you recall our original news group categories were:</p>
<ul>
<li>rec.motorcycles</li>
<li>talk.politics.mideast</li>
<li>sci.med</li>
<li>sci.crypt</li>
</ul>
<p>We can see that the topics discovered correspond fairly well to these, bar a few anomalies.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="c1"># Show dimensions of matrices</span>
<span class="nb">print</span><span class="p">(</span><span class="n">W1</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">H1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(2351, 10) (10, 32291)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The shapes of the matrices also make sense. Given our original matrix A was 2351 rows for posts and 32291 columns for words, and we requested 10 topics this NMF has returned:</p>
<ul>
<li>Matrix W with 2351 rows for posts and 10 columns for topics</li>
<li>Matrix H with 10 rows for topics and 32291 columns for words</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="NMF-using-Gradient-Descent">
<a class="anchor" href="#NMF-using-Gradient-Descent" aria-hidden="true"><span class="octicon octicon-link"></span></a>NMF using Gradient Descent<a class="anchor-link" href="#NMF-using-Gradient-Descent"> </a>
</h2>
<p>So in the method just used, we performed NMF using a built in library function from Sklearn. One of the obvious benefits of using this is that it runs extremely fast. However, in order to create this function it took many years of research and expertise in this area. Using this function also means we are limited, if we want to do something slightly different, we can't really change it.</p>
<p>Alternatively, we can use a very different method to calculate the NMF matrices using <strong>Gradient Descent</strong>.</p>
<p>The basic process of Gradient Descent is as follows:</p>
<ol>
<li>Randomly choose some weights to start</li>
<li>Loop:<ul>
<li>Use weights to calculate a prediction </li>
<li>Calculate the loss (loss is a measure of the difference between the prediction and what we want)</li>
<li>Calculate the derivative of the loss</li>
<li>Update the weights using this derivative to tell us how much to change them</li>
</ul>
</li>
<li>Repeat step 2 lots of times. Eventually we end up with some decent weights</li>
</ol>
<p>In our case, the weights would be the values of the matrices we want to calculate for NMF which are the values of W and H.</p>
<p>In <strong>Stocastic Gradient Decent (SGD)</strong> we evaluate our loss function on just a sample of our data (sometimes called a mini-batch). We would get different loss values on different samples of the data, so this is why it is stochastic. It turns out that this is still an effective way to optimize, and it's much more efficient.</p>
<p><a href="https://livingdatalab.com/deep-learning-theory/2021/06/13/optimisation-methods-for-deep-learning.html">SGD is also a key technique used in Deep Learning which I have covered in an earlier article</a>.</p>
<h4 id="Applying-SGD-to-NMF">
<a class="anchor" href="#Applying-SGD-to-NMF" aria-hidden="true"><span class="octicon octicon-link"></span></a>Applying SGD to NMF<a class="anchor-link" href="#Applying-SGD-to-NMF"> </a>
</h4>
<p>The <a href="https://mathworld.wolfram.com/FrobeniusNorm.html">Frobenius norm</a> is a way to measure how different two matrices are. We can use this to calculate the loss by multipling W and H together to create a matrix, and then calculating the Frobenius norm between this matrix and our original matrix A to give us our loss value.</p>
<p><strong>Goal</strong>: Decompose $A\;(m \times n)$ into $$A \approx WH$$ where $W\;(m \times k)$ and $H\;(k \times n)$, $W,\;H\;&gt;=\;0$, and we've minimized the Frobenius norm of $A-WH$.</p>
<p><strong>Approach</strong>: We will pick random positive $W$ &amp; $H$, and then use SGD to optimize.</p>
<p>We will also make use of the <a href="https://pytorch.org/">Pytorch</a> library for these calculations for 2 key reasons:</p>
<ul>
<li>It facilitates calculations on the GPU which enables matrix calculations to be run in parallel and therefore much faster</li>
<li>Pytorch has the <em>autograd</em> functionality which will automatically calculate the derivatives of functions for us and thereby give us the gradients that we need for the process in a convenient way</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="c1"># Define constants and functions required</span>
<span class="n">lam</span><span class="o">=</span><span class="mf">1e6</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="c1"># Create W and H matrices</span>
<span class="n">pW</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">tc</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">d</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pH</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">tc</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">n</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">pW</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">abs_</span><span class="p">()</span>
<span class="n">pH</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span><span class="o">.</span><span class="n">abs_</span><span class="p">()</span>
<span class="c1"># Define report</span>
<span class="k">def</span> <span class="nf">report</span><span class="p">():</span>
    <span class="n">W</span><span class="p">,</span><span class="n">H</span> <span class="o">=</span> <span class="n">pW</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">pH</span><span class="o">.</span><span class="n">data</span>
    <span class="nb">print</span><span class="p">((</span><span class="n">A</span><span class="o">-</span><span class="n">pW</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">pH</span><span class="p">))</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">W</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">H</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="p">(</span><span class="n">W</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="p">(</span><span class="n">H</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="c1"># Define penalty - encourage positive and low loss values</span>
<span class="k">def</span> <span class="nf">penalty</span><span class="p">(</span><span class="n">P</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">((</span><span class="n">P</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">tc</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">)</span><span class="o">*</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">0.</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># Define penalise - for both W and H matrices we want to improve</span>
<span class="k">def</span> <span class="nf">penalize</span><span class="p">():</span> <span class="k">return</span> <span class="n">penalty</span><span class="p">(</span><span class="n">pW</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span> <span class="n">penalty</span><span class="p">(</span><span class="n">pH</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1"># Define loss - Calculate the Frobenius norm between Matrix A and Matrices W x H</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">():</span> <span class="k">return</span> <span class="p">(</span><span class="n">A</span><span class="o">-</span><span class="n">pW</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">pH</span><span class="p">))</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">penalize</span><span class="p">()</span><span class="o">*</span><span class="n">lam</span>
<span class="c1"># Define optimiser to update weights using gradients</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">pW</span><span class="p">,</span><span class="n">pH</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span><span class="mf">0.9</span><span class="p">))</span>
<span class="c1"># Load our original matrix A onto the GPU</span>
<span class="n">t_vectors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">t_vectors</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Create and run the Stocastic Gradient Descent process</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="c1"># For 1000 cycles</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span> 
    <span class="c1"># Clear the previous gradients</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># Calculate the loss i.e. the Frobenius norm between Matrix A and Matrices W x H</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">()</span>
    <span class="c1"># Calculate the gradients</span>
    <span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="c1"># Update the values of Matrices W x H using the gradients</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="c1"># Every 100 cycles print a report of progress</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">99</span><span class="p">:</span> 
        <span class="n">report</span><span class="p">()</span>
        <span class="n">lr</span> <span class="o">*=</span> <span class="mf">0.9</span>     <span class="c1"># learning rate annealling</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>47.2258186340332 tensor(-0.0010, device='cuda:0') tensor(-0.0023, device='cuda:0') tensor(1013, device='cuda:0') tensor(42676, device='cuda:0')
46.8864631652832 tensor(-0.0008, device='cuda:0') tensor(-0.0027, device='cuda:0') tensor(1424, device='cuda:0') tensor(53463, device='cuda:0')
46.73139572143555 tensor(-0.0004, device='cuda:0') tensor(-0.0031, device='cuda:0') tensor(929, device='cuda:0') tensor(53453, device='cuda:0')
46.66544723510742 tensor(-0.0004, device='cuda:0') tensor(-0.0020, device='cuda:0') tensor(736, device='cuda:0') tensor(54012, device='cuda:0')
46.620338439941406 tensor(-0.0006, device='cuda:0') tensor(-0.0018, device='cuda:0') tensor(631, device='cuda:0') tensor(56201, device='cuda:0')
46.586158752441406 tensor(-0.0003, device='cuda:0') tensor(-0.0018, device='cuda:0') tensor(595, device='cuda:0') tensor(56632, device='cuda:0')
46.576072692871094 tensor(-0.0003, device='cuda:0') tensor(-0.0019, device='cuda:0') tensor(585, device='cuda:0') tensor(54036, device='cuda:0')
46.573974609375 tensor(-0.0003, device='cuda:0') tensor(-0.0018, device='cuda:0') tensor(578, device='cuda:0') tensor(53401, device='cuda:0')
46.573814392089844 tensor(-0.0003, device='cuda:0') tensor(-0.0017, device='cuda:0') tensor(667, device='cuda:0') tensor(52781, device='cuda:0')
46.573760986328125 tensor(-0.0003, device='cuda:0') tensor(-0.0019, device='cuda:0') tensor(662, device='cuda:0') tensor(52658, device='cuda:0')
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="c1"># Show topics discovered</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">pH</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">show_topics</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>['msg don people know just food think like',
 'clipper chip phone crypto phones government nsa secure',
 'armenian armenians turkish genocide armenia turks turkey people',
 'jews adam jewish land shostack das harvard arabs',
 'com edu pgp mail faq rsa list ripem',
 'israel israeli lebanese arab lebanon peace israelis arabs',
 'key keys bit chip serial bits 80 number',
 'encryption government technology law privacy enforcement administration use',
 'geb dsl cadre chastity n3jxp pitt intellect shameful',
 'bike bikes ride motorcycle riding dod dog good']</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So if you recall our original news group categories were:</p>
<ul>
<li>rec.motorcycles</li>
<li>talk.politics.mideast</li>
<li>sci.med</li>
<li>sci.crypt</li>
</ul>
<p>We can see that the topics discovered using SGD correspond fairly well to these, bar a few anomalies.</p>
<h3 id="Comparing-Approaches">
<a class="anchor" href="#Comparing-Approaches" aria-hidden="true"><span class="octicon octicon-link"></span></a>Comparing Approaches<a class="anchor-link" href="#Comparing-Approaches"> </a>
</h3>
<p>If we compare our two approaches to calculating NMF.</p>
<p><strong>Scikit-Learn's NMF</strong></p>
<ul>
<li>Fast</li>
<li>No parameter tuning</li>
<li>Relies on decades of academic research, took experts a long time to implement</li>
<li>Can't be customised</li>
<li>Method can only be applied to calculating NMF</li>
</ul>
<p><strong>Using PyTorch and SGD</strong></p>
<ul>
<li>Took an hour to implement, didn't have to be NMF experts</li>
<li>Parameters were fiddly</li>
<li>Not as fast </li>
<li>Easily customised</li>
<li>Method can be applied to a vast range of problems</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">
<a class="anchor" href="#Conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion<a class="anchor-link" href="#Conclusion"> </a>
</h2>
<p>In this article we introduced Non-negative Matrix Factorization (NMF) and saw how it could be applied to the task of topic modelling in NLP. We also compared two approaches to calculating NMF using Scikit-Learn's library function as well as Stocastic Gradient Descent (SGD) and highlighted various pros and cons of each approach.</p>

</div>
</div>
</div>
</div>



  </div>
  
  <a class="u-url" href="/mathematics/linear-algebra/natural-language-processing/2021/12/28/topic-modelling-nmf.html" hidden></a>
</article><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="pranath/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>