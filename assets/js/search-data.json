{
  
    
        "post0": {
            "title": "The fastai Mid-level API",
            "content": "Introduction . In this article we will introduce and explore the fastai mid-level API, in particular it&#39;s data preparation features. The mid-level api offers more control and customisation than the high-level api. We will apply the mid-level api to the example of predicting Siamese Images. . The fastai library (as of 2021) is a layered API that has 4 levels of abstraction. . Application layer | High level API | Mid level API | Low level API | . . Mid-level API key concepts . Transforms . In a previous article on text classification we saw how tokenisation and numericalisation were used to prepare the text data for the model. . Both of these classes also have a decode() method, that allows us to reverse the process i.e. to convert tokens back into text, though this may not be exactly the same as the default tokeniser currently is not entirely reversable. . decode is also used by show_batch() and show_results(), as well as by other inference methods. . When we create an object of the tokenizer or numeraclize class, a setup method is called (which trains a tokenizer if needed and creates a vocab for the numericalizer) each is then applied to the text stream in turn. These transformation type tasks are common, so fastai has created a base level class to encapsulate them called the Transform class. Both Tokenize and Numericalize are Transforms. . In general, a Transform is an object that behaves like a function and has an optional setup method that will initialize some inner state (like the vocab inside num) and an optional decode that will reverse the function (this reversal may not be perfect, as we saw with tok). . Another aspect about transforms is that they are always used with tuples, as this reflects the common nature of our data in terms of input &amp; output variables. Also when we apply a transform to this tuple e.g. Resize we may want to apply it in a different way to the input and output variables (if at all). . Creating your own transform . So to create your own transform you can do this by writing a function, and then passing it to the Transform class. The Transform class will only apply the given function to one of a matching type, so for example because we have specified the type as int here the transform is not applied when the input is a floating point number. . def f(x:int): return x+1 tfm = Transform(f) tfm(2),tfm(2.0) . (3, 2.0) . Also note here no setup() or decode() methods have been created here. . This approach of passing a function as an argument to another function is called a decorator which is specified by being preceeded by an &#39;@&#39; symbol and putting it before a function definition. So we can do the same as above using this approach. . @Transform def f(x:int): return x+1 f(2),f(2.0) . (3, 2.0) . If we want to specify a setup or decode method we will instead need to subclass Transform and implement the methods that way. . class NormalizeMean(Transform): def setups(self, items): self.mean = sum(items)/len(items) def encodes(self, x): return x-self.mean def decodes(self, x): return x+self.mean . When used, this class will first run the setup method, then apply the encodes method. The decode method will do the reverse when run. . tfm = NormalizeMean() tfm.setup([1,2,3,4,5]) start = 2 y = tfm(start) z = tfm.decode(y) tfm.mean,y,z . (3.0, -1.0, 2.0) . Note the methods implmented and called are different i.e. setups vs setup. The reason for this is for example here setup also does some other things before then calling setup for you. . Pipelines . To join several transforms together we can use the Pipeline class, which is essentially a list of transforms. . tok = Tokenizer.from_folder(path) tok.setup(txts) toks = txts.map(tok) num = Numericalize() num.setup(toks) nums = toks.map(num) tfms = Pipeline([tok, num]) t = tfms(txts[0]); t[:20] . TensorText([ 2, 19, 932, 81, 27, 20, 32, 34, 7, 260, 119, 1256, 143, 62, 64, 11, 8, 415, 1289, 14]) . You can also decode the pipeline, but there is no setup. . tfms.decode(t)[:100] . &#39;xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the f&#39; . TfmdLists . The class we can use to connect our raw data (e.g. files) to a pipeline is the TfmdLists class. This can also run the appropriate setup methods for us. We can do this in a short, one line way for example. . tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize]) . When initialised, TfmdLists will run the setup method of each transform in order, passing the items transformed by the previous transform. We can see the result of the pipeline on any item by indexing into the objected created. . t = tls[0]; t[:20] . TensorText([ 2, 19, 1033, 73, 28, 20, 30, 35, 7, 265, 120, 1061, 176, 56, 70, 10, 8, 457, 1440, 14]) . TfmdLists also can decode. . tls.decode(t)[:100] . &#39;xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the f&#39; . And show. . tls.show(t) . xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the film stood out even when viewing it so many years after it was made . xxmaj the story by the little known c xxmaj virgil xxmaj georghiu is remarkable , almost resembling a xxmaj tolstoy - like story of a man buffeted by a cosmic scheme that he can not comprehend . xxmaj compare this film with better - known contemporary works such as xxmaj xxunk &#39;s &#34; schindler &#39;s xxmaj list &#34; and you begin to realize the trauma of the xxmaj world xxmaj war xxup ii should be seen against the larger canvas of racism beyond the simplistic xxmaj nazi notion of xxmaj aryan vs xxmaj jews . xxmaj this film touches on the xxmaj hungarians dislike for the xxmaj romanians , the xxmaj romanians dislike of the xxmaj russians and so on .. even touching on the xxmaj jews &#39; questionable relationships with their xxmaj christian xxmaj romanian friends , while under stress . xxmaj as i have not read the book , it is difficult to see how much has been changed by the director and screenplay writers . xxmaj for instance , it is interesting to study the xxmaj romanian peasant &#39;s view of emigrating to xxup usa with the view of making money only to return to xxmaj romania and invest his earnings there . xxmaj in my opinion , the character of xxmaj johann xxmaj moritz was probably one of the finest roles played by xxmaj anthony xxmaj quinn ranking alongside his work in &#34; la xxunk the xxmaj greek &#34; and &#34; xxunk &#34; . xxmaj the finest and most memorable sequence in the film is the final one with xxmaj anthony xxmaj quinn and xxmaj virna xxmaj lisi trying to smile . xxmaj the father carrying a daughter born out his wife &#39;s rape by xxmaj russians is a story in itself but the director is able to show the reconciliation by a simple gesture -- the act of carrying the child without slipping into melodramatic footage . xxmaj today after the death of xxmaj princess xxmaj diana we often remark about the insensitive paparazzi . xxmaj the final sequence is an indictment of the paparazzi and the insensitive media ( director xxmaj verneuil also makes a similar comment during the court scene as the cameramen get ready to pounce on xxmaj moritz ) . xxmaj the interaction between xxmaj church and xxmaj state was so beautifully summed up in the orthodox priest &#39;s laconic statement &#34; i pray to xxmaj god that xxmaj he guides those who have power to use them well . &#34; xxmaj some of the brief shots , such as those of a secretary of a minister doodling while listening to a petition -- said so much in so little footage . xxmaj the direction was so impressive that the editing takes a back seat . xxmaj finally what struck me most was the exquisite rich texture of colors provided by the cameraman xxmaj andreas xxmaj winding -- from the brilliant credit sequences to the end . i recalled that he was the cameraman of another favorite xxmaj french film of mine called &#34; ramparts of xxmaj clay &#34; directed by jean - louis xxmaj xxunk . i have not seen such use of colors in a long while save for the xxmaj david xxmaj lean epics . xxmaj there were flaws : i wish xxmaj virna xxmaj lisi &#39;s character was more fleshed out . i could never quite understand the xxmaj serge xxmaj xxunk character -- the only intellectual in the entire film . xxmaj the railroad station scene at the end seems to be lifted out of xxmaj sergio xxmaj leone westerns . xxmaj finally , the film was essentially built around a love story , that unfortunately takes a back seat . xxmaj to sum up this film impressed me in more departments than one . xxmaj the story is relevant today as it was when it was made . . TfmdLists is plural because it can accomodate both training and validation data using a splits parameter, you just need to pass the indicies for each set. . cut = int(len(files)*0.8) splits = [list(range(cut)), list(range(cut,len(files)))] tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize], splits=splits) . You can then access the train and validation parts using the train and valid attributes. . tls.valid[0][:20] . TensorText([ 2, 22, 15452, 12, 9, 8, 16833, 22, 16, 13, 483, 2773, 12, 2472, 596, 46, 13, 955, 24, 4841]) . You can also convert a TfmdLists object directly into a Dataloaders object using the dataloaders() method. . More generally, you will most likely have 2 or more parallel pipelines of transforms: one for processing raw data into inputs and one to process raw data into outputs/targets. . So in this example, to get the target (a label) we can get it from the parent folder. There is a function parent_label() that can do this for us. . lbls = files.map(parent_label) lbls . (#50000) [&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;...] . We then need a transform that can take these targets, and extract the unique class names to build a vocab during the setup() method, and transform these string class names into integers. The Categorize class can do this for us. . cat = Categorize() cat.setup(lbls) cat.vocab, cat(lbls[0]) . ([&#39;neg&#39;, &#39;pos&#39;], TensorCategory(1)) . So putting these together, from our raw files data we can create a TfmdLists object that will take our files reference, and chain these two transforms together so we get our processed target variable. . tls_y = TfmdLists(files, [parent_label, Categorize()]) tls_y[0] . TensorCategory(1) . But this means we have separate TfmdLists objects for our input and output variables. To bind these into one object we need the Datasets class. . Datasets and Dataloaders . The Datasets object allows us to create two or more piplines bound together and output a tuple result. It will do the setup() for us, and if we index into this Datasets object it will return a tuple with the results of each pipeline. . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms]) x,y = dsets[0] x[:20],y . (TensorText([ 2, 19, 1033, 73, 28, 20, 30, 35, 7, 265, 120, 1061, 176, 56, 70, 10, 8, 457, 1440, 14]), TensorCategory(1)) . As before if we pass a splits parameter, this will further split these into separate train and validation sets. . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms], splits=splits) x,y = dsets.valid[0] x[:20],y . (TensorText([ 2, 22, 15452, 12, 9, 8, 16833, 22, 16, 13, 483, 2773, 12, 2472, 596, 46, 13, 955, 24, 4841]), TensorCategory(0)) . We can also reverse the process to get back to our raw data using decode. . t = dsets.valid[0] dsets.decode(t) . (&#39;xxbos &#34; igor and the xxmaj lunatics &#34; is a totally inept and amateurish attempt at a crazy - hippie - cult - killing - spree horror movie . xxmaj apparently even nearly twenty years later , xxmaj charles xxmaj manson was still inspiring overenthusiastic but incompetent trash - filmmakers . xxmaj this is a typical xxmaj troma production , meaning in other words , there &#39;s a lot of boring and totally irrelevant padding footage to accompany the nonsensical plot . xxmaj there &#39;s a bit of random gore and gratuitous nudity on display x96 which is n &#39;t bad x96 but it &#39;s all so very pointless and ugly that it becomes frustrating to look at . &#34; igor and the xxmaj lunatics &#34; is so desperate that it &#39;s even using a lot of the footage twice , like the circle saw killing for example . xxmaj the incoherent plot tries to tell the story of a hippie cult run by the drug - addicted and xxmaj charlie xxmaj manson wannabe xxmaj paul . xxmaj one of xxmaj paul &#39;s lower ranked disciples , named xxmaj igor , becomes a little bit too obsessed with the xxmaj bible stories and drug orgies and gradually causes the entire cult to descent further into criminal insanity . xxmaj just to illustrate through a little example exactly how crazy xxmaj igor is : he tears the heart straight out of the chest of a really sexy black hitch - hiker girl ! xxmaj there &#39;s an annoying synthesizer soundtrack and some truly embarrassingly lame pseudo - artistic camera tricks , like slow - motion footage and lurid dream sequences . xxmaj maybe there &#39;s one sequence that more or less qualifies as worthwhile for trash fanatics and that &#39; is when a poor girl is cut in half with a machete . xxmaj for no particular reason , the camera holds the shot of the blade in the bloodied stomach for fifteen whole seconds .&#39;, &#39;neg&#39;) . Finally before we can use this data to train a model, we need to convert this Datasets object into a Dataloaders object. In this text example, we also need to pass along a special argument to take care of the padding problem with text data, just before we batch the elements which can do using the before_batch argument. . dls = dsets.dataloaders(bs=64, before_batch=pad_input) . dataloaders directly calls DataLoader on each subset of our Datasets. fastai&#39;s DataLoader expands the PyTorch class of the same name and is responsible for collating the items from our datasets into batches. It has a lot of points of customization, but the most important ones that you should know are: . after_item:: Applied on each item after grabbing it inside the dataset. This is the equivalent of item_tfms in DataBlock. | before_batch:: Applied on the list of items before they are collated. This is the ideal place to pad items to the same size. | after_batch:: Applied on the batch as a whole after its construction. This is the equivalent of batch_tfms in DataBlock. | . So putting all these steps together taking our raw data to ending up with a Dataloaders object ready to train a model. . tfms = [[Tokenizer.from_folder(path), Numericalize], [parent_label, Categorize]] files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;]) splits = GrandparentSplitter(valid_name=&#39;test&#39;)(files) dsets = Datasets(files, tfms, splits=splits) dls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input) . Note also the use of GrandparentSplitter and dl_type. This last argument is to tell dataloaders to use the SortedDL class of DataLoader, and not the usual one. SortedDL constructs batches by putting samples of roughly the same lengths into batches. . So the above is equalivilent to what we did with the high-level datablock api, just using the mid-level api which exposes more control, customisation and choices. The mid-level api version of all this was of course this. . path = untar_data(URLs.IMDB) dls = DataBlock( blocks=(TextBlock.from_folder(path),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path) . Applying the Mid-level API: Siamese Pair . So we will apply using the mid-level api to a Siamese pair use case. A Siamese model takes 2 images and has to decide if they are of the same category or not. We will use fastai&#39;s pets dataset for this exercise. . Lets get the data. . from fastai.vision.all import * path = untar_data(URLs.PETS) files = get_image_files(path/&quot;images&quot;) . If we didn&#39;t need to show our input data, we could just create one stream to process the input images. Since we would also like to be able to look at the input images as well, we need to do something different, creating a custom type. When you call the show() method on a TfmdLists or Datasets object, it will decode items till you end up with items of the same type of object that the show method is called upon. . We will create a SiameseImage class that is subclassed from fastuple and will contain 3 things: 2 images, and a boolean that indicates of they are the same class. We will also implement a custom show method, that joins the 2 images with a black line divider. . The most important part of this class are the last 3 lines. . class SiameseImage(fastuple): def show(self, ctx=None, **kwargs): img1,img2,same_breed = self if not isinstance(img1, Tensor): if img2.size != img1.size: img2 = img2.resize(img1.size) t1,t2 = tensor(img1),tensor(img2) t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1) else: t1,t2 = img1,img2 line = t1.new_zeros(t1.shape[0], t1.shape[1], 10) return show_image(torch.cat([t1,line,t2], dim=2), title=same_breed, ctx=ctx) . img = PILImage.create(files[0]) s = SiameseImage(img, img, True) s.show(); . img1 = PILImage.create(files[1]) s1 = SiameseImage(img, img1, False) s1.show(); . The key thing about transforms is that they dispatch over tuples or their subclasses. Thats why we subclassed from fastuple, so we can apply any transform that works on images to our SiameseImage object and it will be applied to each image in the tuple. . For example. . s2 = Resize(224)(s1) s2.show(); . Here the Resize transform is applied to each of the images, but not the boolean target variable. . Lets now build a better SiameseTransform class for training our model. . Lets first define a function that will extract the target classes of our images. . def label_func(fname): return re.match(r&#39;^(.*)_ d+.jpg$&#39;, fname.name).groups()[0] . So this is how we cill create our dataset. We will pick a series of images, and for each image we pick we will with a probability of 0.5 pick an image of the same or different class, and assign a true or false label accordingly. This will be done in the _draw() method. . There is also a difference between the training and validation sets, which is exactly why the transforms need to be initialised with the splits: with the training set we will make that random pick each time we read an image, whereas with the validation set we will make the random pick only once at initialisation - which is a kind of data augmentation that allows us to get more varied samples during training - but ensures a consistant validation set throughout. . class SiameseTransform(Transform): def __init__(self, files, label_func, splits): self.labels = files.map(label_func).unique() self.lbl2files = {l: L(f for f in files if label_func(f) == l) for l in self.labels} self.label_func = label_func self.valid = {f: self._draw(f) for f in files[splits[1]]} def encodes(self, f): f2,t = self.valid.get(f, self._draw(f)) img1,img2 = PILImage.create(f),PILImage.create(f2) return SiameseImage(img1, img2, t) def _draw(self, f): same = random.random() &lt; 0.5 cls = self.label_func(f) if not same: cls = random.choice(L(l for l in self.labels if l != cls)) return random.choice(self.lbl2files[cls]),same . splits = RandomSplitter()(files) tfm = SiameseTransform(files, label_func, splits) tfm(files[0]).show(); . So to recap: in the mid-level API we have 2 classes that can help us apply transforms to our data: TfmdLists and Datasets. One applies a single pipeline of transforms, while the other can apply several pipelines in parallel to build tuples. Given our new transform here already creates tuples, so we can just use TfmdLists in this case. . tls = TfmdLists(files, tfm, splits=splits) show_at(tls.valid, 0); . We are almost there, to create a Dataloader from this we just call the dataloaders method on this object. But we need to be careful, as our new transform class does not take item_tfms and batch_tfms like a DataBlock. . However the fastai DataLoader has several hooks that are named after events; here what we apply on the items after they are grabbed is called after_item, and what we apply on the batch once it&#39;s built is called after_batch. . dls = tls.dataloaders(after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]) . Note also we have to explictly pass more transforms than we would previously, this is because the DataBlock class/API usually adds these automatically, and since we have create a custom transform we need to explictly request these. . ToTensor is the one that converts images to tensors (again, it&#39;s applied on every part of the tuple). | IntToFloatTensor converts the tensor of images containing integers from 0 to 255 to a tensor of floats, and divides by 255 to make the values between 0 and 1. | . We now have the right DataLoaders object ready to train a model to predict on Siamese images. . Conclusion . We have seen how we can use fastai&#39;s mid-level api to do more custom work as needed, with more control than we would have with the high-level data block api. .",
            "url": "https://www.livingdatalab.com/fastai/2021/05/30/fastai-midlevel-api.html",
            "relUrl": "/fastai/2021/05/30/fastai-midlevel-api.html",
            "date": " • May 30, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Creating a custom text classifier for movie reviews",
            "content": "Introduction . In this article we are going to train a deep learning text classifier using the fastai library. We will do this for the IMDB movie reviews dataset. In particular, we will look at fastai&#39;s ULMFit approach which involves fine tuning a language model more with specialised text before using this language model as a basis for a classification model. . Text Pre-processing . So how might we proceed with building a language model, that we can then use for clasisifcation? Consider with one of the simplest neural networks, a collaberative filtering model. This uses embedding matrices to encode different items (such as films) and users, combine these using dot products to calculate a value, which we test against known ratings - and use gradient descent to learn the correct embedding matrices to best predict these ratings. . Optionally, we can create instead a deep learning model from this by concatinating the embedding matrices instead of the dot product, then putting the result through an activtion function, and more layers etc. . So we could use a similar approach, where we put a sequence of words through a neural network via encoding them in an embedding martix for words. However a significant difference from the collaberative filtering approach here is the idea of a sequence. . We can proceed with these 5 steps: . Tokenisation: convert words to recognised units | Numericalisation: convert tokens to numbers | Create data loader: Create a data loader to train the language model which creates a target variable offset by one word from the input variable from the text data | Train language model: We need to train a model that can take an amount of text data of variable length, and be able to predict the next word for any word in the sequence. | Train classifier model: Using what the language model has learned about the text as a basis, we can build on top of this to create and train a language model. | This is an approach pioneered by fastai called the Universal Langauage Model Fine-tuining (ULMFit) approach. . . Tokenisation . Lets get the data and tokenise it using the fastai library tools. . path = untar_data(URLs.IMDB) files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) # Show example text data txt = files[0].open().read(); txt[:75] . &#39;I caught up with this movie on TV after 30 years or more. Several aspects o&#39; . Fastai has an english word tokeniser, lets see how it works. . # Test word tokeniser function spacy = WordTokenizer() toks = first(spacy([txt])) print(coll_repr(toks, 30)) . (#626) [&#39;I&#39;,&#39;caught&#39;,&#39;up&#39;,&#39;with&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;on&#39;,&#39;TV&#39;,&#39;after&#39;,&#39;30&#39;,&#39;years&#39;,&#39;or&#39;,&#39;more&#39;,&#39;.&#39;,&#39;Several&#39;,&#39;aspects&#39;,&#39;of&#39;,&#39;the&#39;,&#39;film&#39;,&#39;stood&#39;,&#39;out&#39;,&#39;even&#39;,&#39;when&#39;,&#39;viewing&#39;,&#39;it&#39;,&#39;so&#39;,&#39;many&#39;,&#39;years&#39;,&#39;after&#39;,&#39;it&#39;...] . # Test word tokeniser class tkn = Tokenizer(spacy) print(coll_repr(tkn(txt), 31)) . (#699) [&#39;xxbos&#39;,&#39;i&#39;,&#39;caught&#39;,&#39;up&#39;,&#39;with&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;on&#39;,&#39;xxup&#39;,&#39;tv&#39;,&#39;after&#39;,&#39;30&#39;,&#39;years&#39;,&#39;or&#39;,&#39;more&#39;,&#39;.&#39;,&#39;xxmaj&#39;,&#39;several&#39;,&#39;aspects&#39;,&#39;of&#39;,&#39;the&#39;,&#39;film&#39;,&#39;stood&#39;,&#39;out&#39;,&#39;even&#39;,&#39;when&#39;,&#39;viewing&#39;,&#39;it&#39;,&#39;so&#39;,&#39;many&#39;,&#39;years&#39;...] . The class goes beyond just converting the text to tokens for words, for example it creates tokens like &#39;xxbos&#39; which is a special token to indicate the beginning of a new text sequence i.e. &#39;beggining of stream&#39; standard NLP concept. . The class applies a series fo rules and transformations to the text, here is a list of them. . defaults.text_proc_rules . [&lt;function fastai.text.core.fix_html&gt;, &lt;function fastai.text.core.replace_rep&gt;, &lt;function fastai.text.core.replace_wrep&gt;, &lt;function fastai.text.core.spec_add_spaces&gt;, &lt;function fastai.text.core.rm_useless_spaces&gt;, &lt;function fastai.text.core.replace_all_caps&gt;, &lt;function fastai.text.core.replace_maj&gt;, &lt;function fastai.text.core.lowercase&gt;] . Numericalisation . # Get first 2000 reviews to test txts = L(o.open().read() for o in files[:2000]) # Tokenise toks = tkn(txt) # Select subset of tokenised reviews toks200 = txts[:200].map(tkn) num = Numericalize() # Numericalise tokens - create a vocab num.setup(toks200) # Show first 20 tokens of vocab coll_repr(num.vocab,20) . &#34;(#2096) [&#39;xxunk&#39;,&#39;xxpad&#39;,&#39;xxbos&#39;,&#39;xxeos&#39;,&#39;xxfld&#39;,&#39;xxrep&#39;,&#39;xxwrep&#39;,&#39;xxup&#39;,&#39;xxmaj&#39;,&#39;the&#39;,&#39;.&#39;,&#39;,&#39;,&#39;and&#39;,&#39;a&#39;,&#39;of&#39;,&#39;to&#39;,&#39;is&#39;,&#39;in&#39;,&#39;it&#39;,&#39;i&#39;...]&#34; . # Now we can convert tokens to numbers for example nums = num(toks)[:20]; nums . TensorText([ 2, 19, 726, 79, 29, 21, 32, 31, 7, 314, 112, 1195, 138, 63, 71, 10, 8, 393, 1524, 14]) . Create data loader . So we need to join all the text together, and then divide it into specific sized batches of multiple lines of text of fixed length, which maintain the correct order of the text within each batch. At every epoch the order of the reviews is shuffled, but we then join these all together and construct mini-batches in order, which our model will process and learn from. This is all done automatically by the fastai library tools. . # Get some example numericalised tokens nums200 = toks200.map(num) # Pass to dataloader dl = LMDataLoader(nums200) # Get first batch of data and check sizes x,y = first(dl) x.shape,y.shape . (torch.Size([64, 72]), torch.Size([64, 72])) . # Examine example input variable should be start of a text &#39; &#39;.join(num.vocab[o] for o in x[0][:20]) . &#39;xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of&#39; . # Examine example target variable which is the same plus added next word - this is what we want to predict &#39; &#39;.join(num.vocab[o] for o in y[0][:20]) . &#39;i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the&#39; . Training a text classifier . Fine tune language model . We can further simplify the text preparation for training our language model by combining the tokenisation, numericalisation and dataloader creation into one step by creating a TextBlock and then a dataloader. . # Create text dataloader for language model training dls_lm = DataBlock( blocks=TextBlock.from_folder(path, is_lm=True), get_items=get_imdb, splitter=RandomSplitter(0.1) ).dataloaders(path, path=path, bs=128, seq_len=80) . # Create a language model learner, by default will use x-entropy loss learn = language_model_learner( dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() # Train model learn.fit_one_cycle(1, 2e-2) # Save model encoder learn.save_encoder(&#39;finetuned&#39;) . Fine tune classifier model . To fine tune the classifier model we create the data loader in a slightly different way. . # Create text dataloader for classifier model training - using lm vocab dls_clas = DataBlock( blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path, path=path, bs=128, seq_len=72) . # Create classifier learner learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16() # Load encoder from language model learn = learn.load_encoder(&#39;finetuned&#39;) . When fine tuning the classifier, it is found to be best if we gradually unfreeze layers to train, and this is best done in manual steps. The first fit will just train the last layer. . # Train model - last layer only learn.fit_one_cycle(1, 2e-2) . # Unfreeze a few more layers and train some more with discriminative learning rates learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . # Unfreeze more layers and train more learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . # Unfreeze whole model and train more learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . On this IMDB dataset we can achieve a classification accuracy of around 95% using this approach. . Conclusion . In this article we have looked in more detail at how we can train a text classifier using the 3 step ULMFit fastai approach, and achieve a good level of accuracy. We also saw in more detail what the fastai library does under the hood to make this process much easier. .",
            "url": "https://www.livingdatalab.com/projects/2021/05/29/custom-text-classifier-movie-reviews.html",
            "relUrl": "/projects/2021/05/29/custom-text-classifier-movie-reviews.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Automatic Recognition of Woodlands and Water from Satellite Imagery using AI and Deep Learning",
            "content": ". Introduction . Satellite imagery is being used together with AI and deep learning in many areas to produce stunning insights and discoveries. In this project I look at applying this approach to recognising buildings, woodlands &amp; water areas from satellite images. . Dataset . The dataset used for this project comes from the research paper LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands and Water from Aerial Imagery which gathered satellite imagery of different areas of Poland. The satellite images have 3 spectral bands so are RGB jpg images. The researchers chose to use 4 classes for identifying objects in these images: . Building | Woodland | Water | Background (i.e. everything else) | . . This is an image segmentation dataset, so the classes are expressed as colourmap shading by pixels for parts of the image that correspond to each class. These image colourmap/masks for the classes are represented as a png image, one of each of the satellite images. . . Methodology . For this project I used the fastai deep learning library which is based on Pytorch/Python. The dataset lends itself to the approach of image segmentation classification as the classes in the dataset are expressed as shaded regions, as opposed to say multi-label image classification using text labels. For this approach, the UNET deep learning architecture has prooven extremely good for image segmentation problems - which is what I chose to use here. . Prepare and load data . ## Set path for image files path = Path(DATA_PATH) ## Set the text for the classes codes = np.array([&quot;building&quot;, &quot;woodland&quot;, &quot;water&quot;, &quot;Background&quot;]) . ## Load image files from the path fnames = get_image_files(path/&quot;images&quot;) ## Define a function to get the label png file def label_func(fn): return path/&quot;labels&quot;/f&quot;{fn.stem}_m{&#39;.png&#39;}&quot; . ## Create a data loader for this image segmentation dataset dls = SegmentationDataLoaders.from_label_func( path, bs=BATCH_SIZE, fnames = fnames, label_func = label_func, codes = codes ) ## Show a batch of images dls.show_batch() . So we can see a nice feature of the fastai library is able to combine the original satellite image overlayed with the colourmap for the class labels with some transparency so we can see the image and labels together. . dls.show_batch(figsize=(10,10)) . dls.show_batch(figsize=(10,10)) . Training the UNET model . ## Create a UNET model using the resnet18 architecture learn = unet_learner(dls, resnet18) ## Train the model learn.fine_tune(3) ## Show the results learn.show_results() . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . epoch train_loss valid_loss time . 0 | 0.425658 | 0.249241 | 26:51 | . epoch train_loss valid_loss time . 0 | 0.207543 | 0.165862 | 27:07 | . 1 | 0.173056 | 0.227951 | 27:02 | . 2 | 0.128388 | 0.140451 | 27:00 | . So fastai&#39;s fine_tune() method will first freeze all but the last layer and train for 1 epoch, and then train for the specified number of epochs (3 in our case). Because image segmentation datasets are particularly big, these can take quite a while to train even on a GPU. In this case 1+3 epochs has taken around 2 hours of training time. . We can see though in this time both the training and validation loss have come down quite nicely, even after 4 epochs. Looking at our results we can see our UNET model has done extremely well when tested on validation images not previosuly seen by the model in the Target/Prediction pair examples above. . Lets see some more tests and results. . learn.show_results() . learn.show_results(max_n=4) . The model does seem to have generally done a good job at predicting the correct classes in the image for a wide range of different satellite image types and conditions. . Conclusion . In this project we have looked at a satellite image segmentation dataset and have achieved good results from only a limited amount of training. .",
            "url": "https://www.livingdatalab.com/projects/2021/05/29/_05_15_satellite_recognition_buildings_woodland_water_ai.html",
            "relUrl": "/projects/2021/05/29/_05_15_satellite_recognition_buildings_woodland_water_ai.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Collaberative filtering from scratch",
            "content": "Introduction . In this article we will look to build a collaberitive filtering model from scratch, using pure Pytorch and some support from the Fastai deep learning library. We will also look at the theory and mathematics behind collaberative filtering. . Dataset . We will use the MovieLens dataset, and a special subset curated by fastai of the 100,000 movies. This consists of 2 separate tables for ratings and movies, which we will join together. . path = untar_data(URLs.ML_100k) # Load ratings table ratings = pd.read_csv(path/&#39;u.data&#39;, delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . movies = pd.read_csv(path/&#39;u.item&#39;, delimiter=&#39;|&#39;, encoding=&#39;latin-1&#39;, usecols=(0,1), names=(&#39;movie&#39;,&#39;title&#39;), header=None) movies.head() . movie title . 0 1 | Toy Story (1995) | . 1 2 | GoldenEye (1995) | . 2 3 | Four Rooms (1995) | . 3 4 | Get Shorty (1995) | . 4 5 | Copycat (1995) | . ratings = ratings.merge(movies) ratings.head() . user movie rating timestamp title . 0 196 | 242 | 3 | 881250949 | Kolya (1996) | . 1 63 | 242 | 3 | 875747190 | Kolya (1996) | . 2 226 | 242 | 5 | 883888671 | Kolya (1996) | . 3 154 | 242 | 3 | 879138235 | Kolya (1996) | . 4 306 | 242 | 5 | 876503793 | Kolya (1996) | . dls = CollabDataLoaders.from_df(ratings, item_name=&#39;title&#39;, bs=64) dls.show_batch() . user title rating . 0 542 | My Left Foot (1989) | 4 | . 1 422 | Event Horizon (1997) | 3 | . 2 311 | African Queen, The (1951) | 4 | . 3 595 | Face/Off (1997) | 4 | . 4 617 | Evil Dead II (1987) | 1 | . 5 158 | Jurassic Park (1993) | 5 | . 6 836 | Chasing Amy (1997) | 3 | . 7 474 | Emma (1996) | 3 | . 8 466 | Jackie Chan&#39;s First Strike (1996) | 3 | . 9 554 | Scream (1996) | 3 | . Theory . The key data here is the ratings i.e. the user-movie ratings, as we can see in the listing above. In collaberative filtering, an easier way to see this is as a user-item matrix, with movies as columns, users as rows, and cells as the ratings for each user-movie combination. . . We can see here some cells are not filled in which are ratings we do not know, these are the values we would like to predict so we can know for each user which movie they would like. . So how might we approach this? If we imagine there are some reasons that effect peoples preferences, lets call them factors such as genre, actors etc then that might give us a basis to figure out which users would like each movie. What if we could represent these factors as a set of numbers? then we could represent each user and movie as a unique set of these numbers (or vectors) representing how much of each of the factors that user or movie represented. . Then we could say, we want each of these user and movie factors vectors when multipled to equal a rating. This would give us a basis to learn these factors, as we have ratings we know, and we could use these to estimate the ratings we don&#39;t know. This approach of using movie vectors multipled by user vectors and summed up is known as the dot product and is the basis of matrix multiplication. . . So we can randomly initialise these user and movie vectors, and learn the correct values for these that predict the ratings we know, using gradient descent. . So to do the dot product we could look up the index of each user and movie, then multiply the vectors. But neural networks don&#39;t know how to look up using an index, they only multiply matrices together. However we can do a index looking up using matrix multiplication by using one-hot encoded vectors. . The matrix you index by multiplying by a one-hot encoded matrix, is called an embedding or embedding matrix. So our model will learn the values of these embedding matrices for the users and movies, using gradient descent. . It&#39;s actually very easy to create a collaberative filtering model using fastai&#39;s higher level methods - but we are going to explore doing this from scratch in this article. . learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) . learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.937713 | 0.953276 | 00:11 | . 1 | 0.838276 | 0.873933 | 00:11 | . 2 | 0.717332 | 0.832581 | 00:11 | . 3 | 0.592723 | 0.818247 | 00:11 | . 4 | 0.476174 | 0.818869 | 00:11 | . Collaberative filtering - Model 1 . We will now create our first collaberative filtering model from scratch. This will contain the embedding matrices for the users and movies, and will implement a method (in Pytorch this is normally the forward method) to do a dot product of these 2 matrices. . So the number of factors for each user and movie matrix will be determined when the model is initialised. . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return (users * movies).sum(dim=1) . So the input x to the model will be a tensor of whatever the batch size is multiplied by 2 - where the first column (x[:, 0]) contains the user IDs and the second column (x[:, 1]) contains the movie IDs. So the input essentially has 2 columns. . x,y = dls.one_batch() x.shape . torch.Size([64, 2]) . So we have defined our architecture and so can now create a learner to optimise the model. Because we are building the model from scratch we will use the Learner class to do this. We will use MSE as our loss function as this is a regression problem i.e. we are predicting a number, the rating. . n_users = len(dls.classes[&#39;user&#39;]) n_movies = len(dls.classes[&#39;title&#39;]) # Create model with 50 factors for users and movies each model = DotProduct(n_users, n_movies, 50) # Create Learner object learn = Learner(dls, model, loss_func=MSELossFlat()) . learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 1.336391 | 1.275613 | 00:09 | . 1 | 1.111210 | 1.126141 | 00:09 | . 2 | 0.988222 | 1.014545 | 00:09 | . 3 | 0.844100 | 0.912820 | 00:09 | . 4 | 0.813798 | 0.898948 | 00:09 | . Collaberative filtering - Model 2 . So how can we improve the model? we know the predictions - the ratings: should be between 0-5. Perhaps we can help our model by ensuring the predictions are forced between these valid values? We can use a sigmoid function to do this. . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return sigmoid_range((users * movies).sum(dim=1), *self.y_range) . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.985542 | 1.002896 | 00:10 | . 1 | 0.869398 | 0.914294 | 00:10 | . 2 | 0.673619 | 0.873486 | 00:10 | . 3 | 0.480611 | 0.878555 | 00:10 | . 4 | 0.381930 | 0.882388 | 00:10 | . Collaberative filtering - Model 3 . So while that didn&#39;t make a huge difference, there is more we can do to improve. At the moment by using our user and movie embedding matrices, this only gives us a sense of how a particular movie or user is described as specific values for these latent factors. What we don&#39;t have is a way to indicate something general about a particular movie or user such as this person is really fussy, or this movie is generally good or not good. . We can encode this general skew for each movie and user by including a bias value for each, which we can add after we have done the dot product. So lets add bias to our model. . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.user_bias = Embedding(n_users, 1) self.movie_factors = Embedding(n_movies, n_factors) self.movie_bias = Embedding(n_movies, 1) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) res = (users * movies).sum(dim=1, keepdim=True) res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) return sigmoid_range(res, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.941588 | 0.955934 | 00:10 | . 1 | 0.844541 | 0.865852 | 00:10 | . 2 | 0.603601 | 0.862635 | 00:10 | . 3 | 0.420309 | 0.883469 | 00:10 | . 4 | 0.293037 | 0.890913 | 00:10 | . So this started much better, but then got worse! Why is this? This looks like a case of overfitting. So we can&#39;t use data augmentation for this type of model, so we need some other way to stop the model fitting too much to the data i.e. some kind of regularization. One way to do this is with weight decay . Weight decay . So with weight decay, aka L2 regularization - adds an extra term to the loss function as the sum of all the weights squared. This will penalise our model for getting more complex than it needs to be i.e. overfitting, so this will encorage our model to have weights as small as possible the get the job done i.e. occams razor. . Why weights squared? The idea is the larger the model parameters are, the steeper the slope of the loss function. This can cause the model to focus too much on the data points in the training set. Adding weight decay will make training harder, but will force our model to be as simple as possible, less able to memorise the training data - and force it to generalise better. . Rather than calculate the sum of all weights squared, we take the derivative which is 2 x parameters and addd to our loss e.g. . parameters.grad += wd 2 parameters . Where wd is a factor we can control. . x = np.linspace(-2,2,100) a_s = [1,2,5,10,50] ys = [a * x**2 for a in a_s] _,ax = plt.subplots(figsize=(8,6)) for a,y in zip(a_s,ys): ax.plot(x,y, label=f&#39;a={a}&#39;) ax.set_ylim([0,5]) ax.legend(); . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.928223 | 0.957245 | 00:11 | . 1 | 0.886639 | 0.881928 | 00:10 | . 2 | 0.771433 | 0.832266 | 00:11 | . 3 | 0.597242 | 0.821840 | 00:11 | . 4 | 0.506455 | 0.822054 | 00:10 | . Manual embeddings . So we used a pre-made Embeddings class to make our embedding matrices, but did&#39;nt see how it works so lets make our own now. So we need a randomly initialised weight matrix for each. By default Pytorch tensors are not added as trainable parameters (think why, data are tensors also) so we need to create it in a particular way to make the embeddings trainable, using the nn.Parameter class. . # Create tensor as parameter function, with random initialisation def create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) # Create model with our manually created embeddings class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.movie_factors = create_params([n_movies, n_factors]) self.movie_bias = create_params([n_movies]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] movies = self.movie_factors[x[:,1]] res = (users*movies).sum(dim=1) res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] return sigmoid_range(res, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.923637 | 0.948116 | 00:12 | . 1 | 0.869177 | 0.879707 | 00:11 | . 2 | 0.731731 | 0.836616 | 00:12 | . 3 | 0.590497 | 0.825614 | 00:11 | . 4 | 0.484070 | 0.825161 | 00:11 | . Collaberative filtering - Model 4 . Our models developed so far are not deep learining models, as they dont have many layers. To turn this into a deep learning model we need to take the results of the embedding lookup and concatenate those activations together - this will then give us instead a matrix that we can then pass through linear layers with activation functions (non-linearities) as we would in a deep learning model. . As we are concatinating embeddings rather than taking their dot product, the embedding matrices can have different sizes. Fastai has a handy function for reccomending optimal embedding sizes from the data. . embs = get_emb_sz(dls) embs . [(944, 74), (1665, 102)] . class CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) . model = CollabNN(*embs) . learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) . epoch train_loss valid_loss time . 0 | 0.943013 | 0.951147 | 00:11 | . 1 | 0.913711 | 0.900089 | 00:11 | . 2 | 0.851407 | 0.886212 | 00:11 | . 3 | 0.816868 | 0.878591 | 00:11 | . 4 | 0.772557 | 0.881083 | 00:11 | . Fastai lets you create a deep learning version of the model like this with the higher level function calls by passing use_nn=True and lets you easily create more layers e.g. here with two hidden layers, of size 100 and 50, respectively. . learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 1.002377 | 0.995780 | 00:13 | . 1 | 0.879825 | 0.928848 | 00:13 | . 2 | 0.888932 | 0.899229 | 00:13 | . 3 | 0.821391 | 0.871980 | 00:13 | . 4 | 0.796728 | 0.869211 | 00:13 | . Conclusion . So we have built a collaberative filtering model from scratch, and saw how it can learn latent factors from the data itself. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/05/25/collaberative-filtering-from-scratch.html",
            "relUrl": "/deep-learning-theory/2021/05/25/collaberative-filtering-from-scratch.html",
            "date": " • May 25, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "State-of-the-art Deep Learning image model techniques in 2021",
            "content": "Introduction . In this article we are going to look at some of the most advanced techniques available in 2021 for training deep learning vision models. These go beyond the basics of mini-batch gradient descent, learning rates, pre-sizing, transfer learning, discriminative learning rates, and mixed-precision training. . Library and Dataset . I will be using the fastai deep learning library for code examples, as well as the fastai curated Imagenette dataset which is a specially curated subset of the well known ImageNet dataet of 1.3 million images from 1,000 categories. The Imagenette dataset consists of a much smaller set of images and just 10 categories. . We will define a baseline model here using the dataset to then compare the effect of each advanced technique. . . Normalisation . When training a model, its helpful to ensure the image data is normalised. This ensures that different images end up with data that is in the same range of values, which helps the model better focus more on the content on the images. So here by normalised we mean we want the image data values to have a mean of 0 and a standard deviation of 1. . The fastai library will automatically normalise images per batch, and this is suitable for models that we might train from scratch. When using transfer learning this default approach is not a good idea, because a pre-trained model has been trained on image data with a particular mean and standard deviation. So to use a pre-trained model with new images, we need to ensure these new images are normalised to the same mean and standard deviation that the original model data was trained with. . We can do this my specifying normalisation stats in fastai, which already knows the stats for many common datasets, including of course fastai’s own Imagenette dataset which makes it much easier. . We can also define a function get_dls() which will make it quicker to define different types of data loader i.e. with different batch or image sizes. . . After applying our normalisation, we can see the mean and standard deviation are approximatly 0 and 1 respectively on a test batch of images. . Lets now try this normalised data and train our model. . . While normalisation here hasn’t made a huge improvement over our baseline model, normalisation does make a bigger difference especially with bigger models and more data. . Progressive resizing . Progressive re-sizing is another technique pioneered by fastai. Essentially this involves training models on smaller versions of the images first, before continuing training on bigger images. This has 2 major benefits: . Model training time is much faster | Model accuracy ends up better than if we trained the model only on bigger images | . How can this be the case? lets remember that with convolutional deep learning models, early layers focus on recognising primitive features like lines and edges, and later layers more composite features such as eyes or fur. So if we change the image size during training, our earlier model will still have learnt many useful things applicable to bigger and higher resolution images. . In a way, this is a bit like training a model in one area then re-using that model on a similar area - which might sound familiar? As it should since this is very much what transfer learning is about as well, which works very well. So we should perhaps not be so surprised that this could work. . Another benefit of using lower resolution/smaller versions of the images first is that this is another kind of data augmentation - which should also help our models generalise better. . So lets use our get_dls() function that we defined earlier to define a data loader for our smaller lower resolution images and train the model for a few epochs. . . We will then define a new data loader for bigger images, and continue to train our model with these. . . So we can see we are already getting much better results than our baseline with just a few epochs, and much more quickly. It’s worth considering for the desired task, if transfer learning can in some cases harm performance. This might happen for example if the pre-trained model is trained on images already quite similar to the new ones you want to recognise - as in this case the model parameters are likely already quite close to what is needed, and progressive resizing could move the parameters further away from this and good results. If the use case for the pre-rained model is very different to what it was originally trained on i.e. very different sizes, shapes, styles etc - then progressive resizing here might actually help. . In either case, trying things experimentally would probably be the best way to determine which was the better approach. . Test time augmentation . Training time data augmentation is a common technique to help improve model training by providing different versions of the same images to improve the way the model generalises and with less data. Common techniques include random resize crop, squish, stretch, and image flip for example. . Test time augmentation (TTA) is an interesting approach of using augmentation when using the model for inference. Essentially at inference time for a given image, different augmentations of the same image will be predicted on by the model, then we can use either the average or maximum of these versions as a measure of model performance. This can give us a better idea of the models true performance, and often results in improvements in performance. . In the fastai library its quite easy to apply TTA. . . While this does not add any extra time to training, it does make inference slower. . Mixup . Mixup is a technique introduced in the paper mixup: Beyond Empirical Risk Minimization by Hongyi Zhang et al. It’s a powerful data augmentation technique that seeks to address the weaknesses of many previous methods such as crop-resize, squishing etc. One of the key drawbacks to previous approaches was needing some expert knowledge of when those techniques were applicable or nor as well as how to apply them. . For example, take the flip method that augments by flipping the image vertically or horizontally - should one apply that one way or the other? it will probably depend on the kind of images you have. Also flipping is limited i.e. you can just apply it one way or the other, there are no ‘degrees of flipping’ for example. Having ‘degrees of’ or gradation of augmentation can be very useful for giving the model a rich variety of images along the spectrum to allow it to better learn and generalise. . Mixup essentially takes two images and combines them, with a randomly selected weight of transparency for each image for the combined image. We will then take a weighted average (using the same random weights) applied to the labels of each image, to get the labels for the mixed image. . So the combined image will have labels that are in proportion to the amount of each original image. . . Here the third image is built from 0.3 of the first one and 0.7 of the second one. The one-hot encoded labels for the first and second images and final mixed image would be say: . Image 1: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] | Image 2: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0] | Mixed: [0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0] | . We can use this Mixup technique in the fastai library in the following way. . . This model is likely going to be harder and longer to train, for all the many examples ‘in between’ that this method will generate, but it should allow the model to generalise better. The beauty of this approach is that unlike many previous approaches this doesn’t require extra knowledge about the dataset to use - the ‘appropriateness’ of each image is present in the augmentation - so its the degrees of which we vary here really. This also opens this method to use in other areas beyond even vision models, to NLP for example. . Mixup also helps with another problem. A ‘perfect’ dataset with perfect labels say of only 1 and 0, pushes the model to train towards a sense of ‘perfection’ and absolute confidence, this is of course the ideal that the cross-entropy loss function does well to optimise for. By removing ‘perfection’ from our labels, we force our model to train to become less absolutely confident in its predictions, we train it to become more nuanced and subtle in its predictions that err towards partial than perfect probabilities for label prediction. . Label smoothing . Deep learning vision models train for perfection, this is especially due to the nature of the most common classification loss function cross-entropy loss. For example, because our labels are often perfect i.e. 1 or 0 despite how perfect the expression of that label is in the image, the model will keep pushing for the perfection of 1 or 0 i.e. even 0.999 will not be good enough. This can lead to overfitting, and is a consequence of this kind of training and loss function. In practice, images often do not conform to the perfection of the labels assigned them. . With label smoothing rather than use perfect labels of 1 and 0, we use a number a bit less than 1 and a number a bit more than zero. By doing this we encourage our model to become less confident, more robust (e.g. if there is mislabelled data). This model should generalise better. This technique was introduced in the paper Rethinking the Inception Architecture for Computer Vision by C Szegedy et al. . . We can use this technique in the fastai library in the following way. . . As with Mixup, you generally won’t see significant improvements with this technique until you train more epochs. . Conclusion . In this article we have covered 5 state-of-the-art techniques for training deep learning vision models using the fastai deep learning library, each of which can significantly help produce the best results currently possible for vision models in 2021. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/05/22/state-of-the-art-deep-learning-image-model-2021.html",
            "relUrl": "/deep-learning-theory/2021/05/22/state-of-the-art-deep-learning-image-model-2021.html",
            "date": " • May 22, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "An Eye in the Sky - How AI and Satellite Imagery can help us better understand our changing world",
            "content": ". Introduction . Many of the greatest challenges the world faces today are global in nature, climate change being one of the clearest examples. While we also have huge amounts of data of different kinds, trying to make sense of all this data to help us make better decisions can be a significant challenge in itself when attempted by humans alone. AI is a powerful technology that holds huge potential for helping us use this data more easily, to help us make better decisions for the problems we face. . In the water industry where I work, satellite image data and AI holds great potential for helping solve a number of problems, such as the detection of leaks, water resource management to ensure on ongoing water supply accounting for changes in population and climate change, water quality monitoring, and flood protection. . Beyond the water industry, satellite images and AI are working together to provide critical insights in many diverse areas such as disaster response and recovery, the discovery of hidden archaeological sites, city infrastructure monitoring, combating illegal fishing, and predicting crop yields. . But how does this technology work? and can you understand the basics of how it works without any technical knowledge? The answer is I believe yes, and I will try to illustrate this by describing a recent project I completed using this approach. . Using AI to automatically recognise Woodlands and Water areas . In a recent project, I used satellite imagery from Poland to train an AI to automatically recognise areas in the images such as woodlands and water. So AI is just about throwing some data at it and some magic happens? Actually not quite! This is a common myth about how AI actually works. . . The key requirement for using AI is not just using any data, but something called labelled data. Labelled data is data that has been tagged with one or more labels that describe things inside the data. So in this project, the labels used for these satellite images were woodlands and water: if an image contains one of these things, the image would have a label or tag for that. This is how the AI learns, it looks at each satellite image, see’s which labels it has, and tries to learn what in the image indicates each label. So it’s not really magic how an AI learns at all, an AI just learns from examples of labelled things - that’s it basically. . Here are some more examples of the satellite images, now with labels. The labels are colours filled in, so for example water areas are coloured in pink and woodland areas in red. . . How do these images get their coloured labels? well some poor human has to painstakingly spend hours carefully colouring them all in with the right colours. But its well worth it, since we can train the AI to use these labelled satellite images (just 33 images) to learn to recognise these things in them, and once it can do this, we can then use the AI to recognise these things in new satellite images, as many times as we like. This is the real power of AI systems, which can learn to do things only humans could previously do, and then do them far more efficiently and quickly than a human could ever do, millions of times, without needing even a coffee break! . So how well does the AI learn to recognise these things? after running the training process a while, these are some of the results I got when I tested the AI on images it had never seen. Here the ‘Target’ on the left are the labels for images the AI has never seen, and the ‘Prediction’ on the right are what the AI thinks the label colour areas should be in the image. . . So I’d say the AI has done a pretty good job. You can see in these examples it seems to have recognised the correct water areas (in pink) and woodland areas (in red) pretty well? The AI was only trained for a limited time, most likely if I had trained it for longer it would have done even better. I could now use this AI on any new satellite images, and know it would do quite well at recognising woodland and water areas fairly accurately. Because the labels here are actually coloured dots on the image, we could add up all the dots for water or woodland on an image and get a fairly accurate measure for how much water or woodland there was there. . Just imagine what we could do with even this fairly simple AI. For example, we could use it to estimate the woodland and water areas of different parts of a country quite accurately, anywhere in the world. If we took different satellite photos of the same area over time, we could estimate how the water or woodland areas were changing over time, and by how much, all automatically. The possibilities are endless. . More about the technical details of this project can be found in this article . Conclusion . In this article I’ve introduced how satellite images and AI are a powerful new technology being used to provide valuable insights to a range of different challenges and tasks we face in the world today. By describing my own project using AI to recognise woodland and water areas in satellite images, I hope I have given you a better understanding of how this technology actually works, and of its huge potential for humanity. .",
            "url": "https://www.livingdatalab.com/opinion/2021/05/14/ai-satellite-images.html",
            "relUrl": "/opinion/2021/05/14/ai-satellite-images.html",
            "date": " • May 14, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "What AI can tell us about the hidden preferences of human beings",
            "content": ". Introduction . AI systems are increasingly being used in almost every area of human activity, from the online world, in streaming media, social media &amp; online shopping - to the offline world, in policing, healthcare and other public services as well as many different physical industries such as water, energy, agriculture and manufacturing. These applications of AI are having a huge impact, often beyond what is commonly known to the public, both positive and negative. . Most work in the field is focussed on trying to use AI to solve a particular problem at hand, and if the problem is ‘solved’ then little more thought is often done. Much less work goes on into understanding the fundamental nature of the AI created to solve a particular problem. To some extent this is of course because the main motivation is to solve the problem, but another reason is often because AI systems i.e. artificial neural networks, are often incredibly complicated, are not directly created by humans - and so can actually be very difficult to understand the inner workings of. Questions such as How is it doing it? Why is it doing it? What has it learnt? are questions often not addressed - as long as the AI system seems to ‘get the job done’. . It’s certainly my belief that this much neglected area is worth far more work than it often gets, not only to allow us to understand the AI system at a deeper level, but that it might also give us new insights and understandings into the area the AI system is being applied to. . In this article I’m going to look at one particular area of AI application called Recommendation Systems. For a recent project, I created an AI system for recommending new books to readers. I then extended the project to study how this AI book recommendation system itself was working, and discovered what it had learnt about the hidden preferences of book readers. . What are Recommendation Systems? . Recommendation systems are very common particularly in online services. For example, Amazon suggestions for alternative products, on Netflix suggestions for films, on Spotify for music, or on Facebook for the content you see on your newsfeed. All of these services and more use recommendation systems to suggest new content to people, but what are these systems and how do these actually work? . A very simple approach might be to recommend the most popular items to people. Of course, popular items would probably appeal to many - but not to everyone. With modern AI based recommendation systems we can do much better than this, to make more personalised suggestions that are unique to each person. There are two main approaches to this: content-based filtering and collaborative filtering. . . With content-based filtering, we look at the content a person has previously looked at (eg songs or films you have previously watched) as a basis to recommend new content. In this case, the AI system does the work here to understand what content is similar, for example what films are similar. This might be based on more obvious things such as the film genre, or which actors are in the film. However it can also be based on less obvious things that the AI can learn for itself about what makes films similar or different, things that are not given to the AI at all, but that the AI system can learn itself. These hidden aspects are called latent factors. . With collaborative filtering, we look at other people who are similar to us, and suggest items that they have liked, that we have not yet seen. Here the AI system does the work to understand which people are similar to us, as a basis to make suggestions. As a simple example, on a music service, we could find another listener who has listened to some of the same songs we have, find a song they have listened to that we have not, then recommend that to us. However, this simple strategy may not always be effective, just because 2 people like the same song, that does not always mean they would both like another song that one of them liked, let alone that both people are ‘similar’? What makes people similar and different, might be based on things like the genre of music they liked, which artists etc. . But what truly makes people similar or different in their preferences can also be based on less obvious things, more nuanced and subtle reasons, things people often do not perhaps even understand themselves, biases, etc that are hidden and unconscious, and yet drive and influence people’s choices and behaviour. These hidden biases and influences are things not given to the AI at all, how could they be? and yet, they are things AI systems can still learn about for itself, which are again here called latent factors. . Creating a book recommendation system . For my book recommendation system, I used the collaborative filtering approach. The data I used to create this system is the Book Crossing dataset which is data on peoples book ratings of around 27,000 different books, from the Book Crossing community website, gathered in September 2004. There are around 28,000 users of this website who are from all over the world, and from a range of different ages. The key data is a table of individual ratings of a person (identified by a unique user id), a book and a the value of the rating (a number between 0-10). . . These user ratings are not exhaustive, i.e. every user has not rated every book. Note also there is no extra information about the books such as categories, or about each person such as their ages. But we don’t actually need this extra data, we can create an AI collaborative filter based system (commonly called a ‘model’) that learns just from the table of users, books and ratings, to build up an understanding of the latent factors that uniquely describes each book, and each person. Once the model has been ‘trained’ on this data, and learnt these latent factors - the model can then use these latent factors to make recommendations for each person, about what new books they might like. . When the AI model learns, it doesn’t directly see the real ratings - it just tries to make guesses about what the ratings should be. We then compare the guesses to the actual ratings we know, and give the model back some measure of accuracy, which the model then uses to improve its guesses in the next cycle. This training process repeats for many cycles. . Going down the rabbit hole: what is our book recommendation system actually doing? . So we now have a book recommendation system that can suggest new books to people. But we can if we choose, take things further. Simply from the data of book ratings and the learning process, the system has had to understand something more, about the implicit reasons certain people prefer certain books over others - and indeed perhaps about general qualities that drive these choices. These qualities might correspond to categories we might recognise as more obvious book genres, but they might also correspond to other qualities that are not commonly recognised as ‘categories’ yet might still be factors that drive people to prefer different books. . How might we try and understand these latent factors that drive people’s preferences for books? . We actually have 2 types of latent factors, normal factors and bias factors. Bias factors represent a general bias towards a book, either positive or negative. This will mean for that book, regardless if it would be generally a good suggestion for a person - if it has a negative bias it will be far less likely to be suggested. Similarly, if a book has a very positive bias, it might be more likely to be suggested to you, even if you would not normally read that kind of book i.e. you would not normally read that genre. We can think of bias then as some kind of measure of ‘general popularity’ of a book. . Negative bias books . So these are the bottom 10 books with the most negative bias in the AI model: . Wild Animus | The Law of Love | Blood and Gold (Rice Anne Vampire Chronicles) | Gorky Park | The Cat Who Went into the Closet | Waiting to Exhale | Night Moves (Tom Clancy’s Net Force No. 3) | Ruthless.Com (Tom Clancy’s Power Plays) | Ground Zero and Beyond | Say Cheese and Die! | . Let us look at the 2 books with the most negative bias, ‘Wild Animus’ and ‘The Law of love’. So what is Wild Animus about? The synopsis reads: . “Sam Altman is an intense young man with an animal energy whose unleashed and increasingly unhinged imagination takes him first to Seattle and then farther north, to the remote Alaskan wilderness. …” . This book does have many many online reviews, on the whole which can be summarized by the review What the hell is Wild animus?. The review concludes with a quote from a goodreads reviewer: . “I’ll tell you the ending. A column of lava erupts from beneath his feet while he is dressed in a goat costume and wolves are in mid-air tearing him apart.” . On the whole it seems, Wild Animus seems to provoke a very unfavourable response from most reviewers! The next most negative bias book is ‘The law of love’, it’s synopsis reads: . “After one night of passion, Azucena, an astroanalyst in twenty-third-century Mexico City, is separated from her Twin Soul, Rodrigo, and journeys across the galaxy and through past lives to find her lost love, encountering a deadly enemy along the way…” . As it happens this book has as many positive reviews as negative online, in fact few reviews seem neutral at all. So this is not universally seen as a bad book, by humans who post reviews online anyway. Nevertheless, our AI model regards this as a book that should not really be suggested to anyone. Is that because the book seems to be so divisive? and perhaps there are other books that are ‘safer bets’? Either way, the computer says no. . Positive bias books . Let’s now look at the top 10 books with the most positive bias in the AI model: . The Lovely Bones: A Novel | Harry Potter and the Goblet of Fire | The Da Vinci Code | Harry Potter and the Prisoner of Azkaban | The Secret Life of Bees | Harry Potter and the Sorcerer’s Stone | Harry Potter and the Chamber of Secrets | Where the Heart Is | To Kill a Mockingbird | The Red Tent | . So looking in more detail at the 2 books with the most positive bias we have ‘The lovely bones’ and ‘Harry Potter and the Goblet of Fire’. So the synopsis of “The lovely bones” is as follows: . “It is the story of a teenage girl who, after being raped and murdered, watches from her personal Heaven as her family and friends struggle to move on with their lives while she comes to terms with her own death. “ . This book has a large number of very favourable reviews, in fact it was hard to find a very negative review of this book at all. The New York Time’s review perhaps sums up well the general sentiment felt by most reviewers of this book: . “…Sebold deals with almost unthinkable subjects with humor and intelligence and a kind of mysterious grace. The Lovely Bones takes the stuff of neighborhood tragedy – the unexplained disappearance of a child, the shattered family alone with its grief – and turns it into literature…” . So we can perhaps appreciate some of the reasons perhaps why the AI model thinks this is a good book to recommend to anyone, regardless of what their normal reading preferences might be. The second most positively biased book is “Harry Potter and the Goblet of fire”. Being one of a series of some of the most popular books of all time - this is perhaps not surprising at all that the AI model thinks this would be a good book to recommend to most people, regardless of their normal reading preferences. . Looking at other latent factors . So for the remaining latent factors, we actually have 50 of them for each of our 27,000 books - so quite a few! However we can use a process called dimensionality reduction to actually reduce these down, to the 2 most important latent factors for all books. We can then plot each book on a graph, with the measure that book has for each of these 2 key latent factors. . . A bigger view of this image of latent factors 1 &amp; 2 can be seen here . Here we can see 50 books plotted. On the horizontal axis that is a measure of how much of latent factor 1 each book has. On the vertical axis, that is a measure of how much of latent factor 2 each book has. . Let’s look into latent factor 1, which is the strongest latent factor used by the AI model to make book recommendations. . Books with high values for latent factor 1 . We can see in the bottom right corner of the chart ‘The lovely bones’. This has one of the highest measures of factor 1, because it is one of the furthest to the right. We also know from our look at bias factors, that this is the book with the strongest positive bias latent factor as well i.e. a generally popular book. Let’s also note it falls into the categories of ‘Crime, Thriller, Mystery’. . Looking at another book with a high factor 1, in the top right we have ‘Good in Bed’. The synopsis of the book is: . “It tells the story of an overweight Jewish female journalist, her love and work life and her emotional abuse issues with her father.” . It generally also has good reviews, and would fall into the category of ‘Women’s fiction’. Let’s look at a third book with a high factor 1, “The life of Pi”. The synopsis of this book is: . “After the tragic sinking of a cargo ship, a solitary lifeboat remains bobbing on the wild, blue Pacific. The only survivors from the wreck are a sixteen year-old boy named Pi, a hyena, a zebra (with a broken leg), a female orang-utan and a 450-pound Royal Bengal tiger.” . Again this book generated very good reviews, was very popular, and might fall into the category of ‘Contemporary fiction’. What are some common things to note about all of these books with a high latent factor 1? . They are all very popular and have great critical acclaim | 2 of these books turned into films, and the third is currently being adapted for film. | All 3 have a theme of a huge personal tragedy, which the protagonist is successful in overcoming and rising above by themselves | . So lets bear this in mind, while we look at books with the lowest latent factor 1. . Books with low values for latent factor 1 . Books with low values for factor one are on the far left of the chart. For example we have ‘A painted house’ the synopsis of this being: . “A Painted House is a moving story of one boy’s journey from innocence to experience, drawn from the personal experience of legendary legal thriller author John Grisham” . Let’s also note this would fall into the categories of ‘Contemporary fiction, mystery’. Looking at another book with a low factor 1 ‘The street lawyer’, the synopsis being: . “…about an attorney who leaves his high-priced firm to work for the less fortunate.” . This also seems to be another book by John Grisham, that would fall into categories such as ‘Thriller, Mystery’. Looking at Grisham’s work, how might we characterise his work more generally? He is well known for writing legal thrillers, and themes such as ‘the triumph of the underdog’, however A painted house seems not to quite fit these themes, an exception - so why is it here? A theme that might link both is ‘the triumph of working together’ in the case of the legal thrillers it’s the lawyer, the legal system his collaborators, in ‘a painted house’ its the family that needs to pull together to triumph, as explained in this review: . “…The primary theme is the importance of family: only by pulling together does the family achieve even moderate success” . In fact when we look at the other books with the lowest factor 1, on the far left of the chart, they pretty much are all John Grisham legal thriller books such as: . The Pelican brief | The Brethren | The Summons | The Firm | . So what is latent factor 1? . Let’s now consider what factor 1 might actually be about. Given most of these books, regardless of having a low or high value of factor 1, have all been popular and successful - so popularity I would argue has nothing to do with what factor 1 is really about. . Based on what we have learnt about these books so far, I would speculate that latent factor 1 might represent a measure of ‘The triumph of the group vs the triumph of the individual’ as a theme-axis. So, low values of factor 1 would correspond to ‘The triumph of the group’ type themes, and high values of factor 1 would correspond to ‘The triumph of the individual’ type themes for books. . Remember the AI model is given no information about book categories, authors, genres, themes etc. All the AI has to learn from is the ratings between users and books - that’s all. Not only has our AI model discovered this particular axis theme by itself from very limited information, but it has done so because the AI model has judged that this theme-axis, whatever it is, is one of the most useful for the purposes of making good book recommendations to people. . Discussion . So how do we make sense of our findings? We can’t conclusively say that my suggested ‘triumph of the group vs triumph of the individual’ theme-axis is generally true, or the key hidden factor for understanding generally why people prefer certain books over others. Firstly, it’s based on an inevitably limited data set of books, people and ratings. Perhaps the people who made those ratings are not representative of the general population? Secondly, we only randomly chose 50 books to plot for our latent factors. What if we randomly picked a different set of 50 books, would we see the same kind of themes for latent factor 1, or something else? If the ‘triumph of the group vs triumph of the individual’ theme axis does appear to be a key factor over many more books and people - why is this the case? and what does it suggest about human beings more generally? However these are questions that could be further investigated, researched, and better answered - with more time, and the conclusions of which could potentially be very interesting indeed. . What we can say is that from very limited information, looking at a limited number of books, and looking at some of its latent factors such as biases and the main key factor - this AI model seems to have discovered many relationships we could recognise as humans such as ‘generally popular books’ and ‘generally awful books’. The interpretation of the key latent factor as ‘triumph of the group vs triumph of the individual’ as a theme-axis is of course very speculative at this stage, and yet very intriguing! Would you come to a different conclusion looking at the books at either end of the axis of latent factor 1 on the chart? What do you think latent factor 1 on the horizontal axis is all about? What do you think latent factor 2 on the vertical axis is about? I’d love to hear your feedback and thoughts on this, so do feel free to comment below. . Conclusion . In this article I’ve tried to highlight a few key themes: that AI is being used everywhere, that little work is often done to understand how and why these AI systems work, and that we have so much to gain by actually trying to look inside and understand these AI systems better. I’d also argue this is becoming more and more important, given the growing impact of these increasingly powerful systems on our world and human life. . Looking inside these AI systems, even though not straightforward - gives us the chance to know more about what they are really doing and why, and can give us intriguing hints about the domains in which they are used, which I have tried to illustrate with my book recommendation project. . When these AI systems are used in the domain of human choices, the potential is there hidden within these systems to perhaps discover new insights, and pose new questions about ourselves and our choices, that we may have never even considered or knew to ask. Perhaps by looking a little deeper into AI, we can see a little deeper into ourselves. .",
            "url": "https://www.livingdatalab.com/opinion/2021/04/04/ai-human-preferences.html",
            "relUrl": "/opinion/2021/04/04/ai-human-preferences.html",
            "date": " • Apr 4, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Livingdatalab",
          "content": "Livingdatalab is the personal blog of Pranath Fernando, a Data Scientist in the water industry by day, an AI artist by night. . I have a special interest in deep learning, as well as applying deep learning, data science and AI techniques to generate artwork. . You can contact me via twitter or linkedin. .",
          "url": "https://www.livingdatalab.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.livingdatalab.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}