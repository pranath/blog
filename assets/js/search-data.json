{
  
    
        "post0": {
            "title": "Patient Selection for Diabetes Drug Testing",
            "content": "Patient Selection for Diabetes Drug Testing . Introduction . EHR data is becoming a key source of real-world evidence (RWE) for the pharmaceutical industry and regulators to make decisions on clinical trials. . For this project, we have a groundbreaking diabetes drug that is ready for clinical trial testing. It is a very unique and sensitive drug that requires administering the drug over at least 5-7 days of time in the hospital with frequent monitoring/testing and patient medication adherence training with a mobile application. We have been provided a patient dataset from a client partner and are tasked with building a predictive model that can identify which type of patients the company should focus their efforts testing this drug on. Target patients are people that are likely to be in the hospital for this duration of time and will not incur significant additional costs for administering this drug to the patient and monitoring. . In order to achieve our goal we must build a regression model that can predict the estimated hospitalization time for a patient and use this to select/filter patients for this study. . Approach . Utilizing a synthetic dataset (denormalized at the line level augmentation) built off of the UCI Diabetes readmission dataset, we will build a regression model that predicts the expected days of hospitalization time and then convert this to a binary prediction of whether to include or exclude that patient from the clinical trial. . This project will demonstrate the importance of building the right data representation at the encounter level, with appropriate filtering and preprocessing/feature engineering of key medical code sets. We will also analyze and interpret the model for biases across key demographic groups. . Dataset . Due to healthcare PHI regulations (HIPAA, HITECH), there are limited number of publicly available datasets and some datasets require training and approval. So, for the purpose of this study, we are using a dataset from UC Irvine that has been modified. . Dataset Loading and Schema Review . dataset_path = &quot;./data/final_project_dataset.csv&quot; df = pd.read_csv(dataset_path) . # Show first few rows df.head() . encounter_id patient_nbr race gender age weight admission_type_id discharge_disposition_id admission_source_id time_in_hospital payer_code medical_specialty primary_diagnosis_code other_diagnosis_codes number_outpatient number_inpatient number_emergency num_lab_procedures number_diagnoses num_medications num_procedures ndc_code max_glu_serum A1Cresult change readmitted . 0 2278392 | 8222157 | Caucasian | Female | [0-10) | ? | 6 | 25 | 1 | 1 | ? | Pediatrics-Endocrinology | 250.83 | ?|? | 0 | 0 | 0 | 41 | 1 | 1 | 0 | NaN | None | None | No | NO | . 1 149190 | 55629189 | Caucasian | Female | [10-20) | ? | 1 | 1 | 7 | 3 | ? | ? | 276 | 250.01|255 | 0 | 0 | 0 | 59 | 9 | 18 | 0 | 68071-1701 | None | None | Ch | &gt;30 | . 2 64410 | 86047875 | AfricanAmerican | Female | [20-30) | ? | 1 | 1 | 7 | 2 | ? | ? | 648 | 250|V27 | 2 | 1 | 0 | 11 | 6 | 13 | 5 | 0378-1110 | None | None | No | NO | . 3 500364 | 82442376 | Caucasian | Male | [30-40) | ? | 1 | 1 | 7 | 2 | ? | ? | 8 | 250.43|403 | 0 | 0 | 0 | 44 | 7 | 16 | 1 | 68071-1701 | None | None | Ch | NO | . 4 16680 | 42519267 | Caucasian | Male | [40-50) | ? | 1 | 1 | 7 | 1 | ? | ? | 197 | 157|250 | 0 | 0 | 0 | 51 | 5 | 8 | 0 | 0049-4110 | None | None | Ch | NO | . Determine Level of Dataset (Line or Encounter) . Given there are only 101766 unique encounter_id’s yet there are 143424 rows that are not nulls, this looks like the dataset is at the line level. . We would also want to aggregate on the primary_diagnosis_code as there is also only one of these per encounter. By aggregating on these 3 columns, we can create a encounter level dataset. . Analyze Dataset . # Look at range of values &amp; key stats for numerical columns numerical_feature_list = [&#39;time_in_hospital&#39;, &#39;number_outpatient&#39;, &#39;number_inpatient&#39;, &#39;number_emergency&#39;, &#39;num_lab_procedures&#39;, &#39;number_diagnoses&#39;, &#39;num_medications&#39;, &#39;num_procedures&#39; ] df[numerical_feature_list].describe() . time_in_hospital number_outpatient number_inpatient number_emergency num_lab_procedures number_diagnoses num_medications num_procedures . count 143424.000000 | 143424.000000 | 143424.000000 | 143424.000000 | 143424.000000 | 143424.000000 | 143424.000000 | 143424.000000 | . mean 4.490190 | 0.362429 | 0.600855 | 0.195086 | 43.255745 | 7.424434 | 16.776035 | 1.349021 | . std 2.999667 | 1.249295 | 1.207934 | 0.920410 | 19.657319 | 1.924872 | 8.397130 | 1.719104 | . min 1.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000000 | . 25% 2.000000 | 0.000000 | 0.000000 | 0.000000 | 32.000000 | 6.000000 | 11.000000 | 0.000000 | . 50% 4.000000 | 0.000000 | 0.000000 | 0.000000 | 44.000000 | 8.000000 | 15.000000 | 1.000000 | . 75% 6.000000 | 0.000000 | 1.000000 | 0.000000 | 57.000000 | 9.000000 | 21.000000 | 2.000000 | . max 14.000000 | 42.000000 | 21.000000 | 76.000000 | 132.000000 | 16.000000 | 81.000000 | 6.000000 | . # Define utility functions def create_cardinality_feature(df): num_rows = len(df) random_code_list = np.arange(100, 1000, 1) return np.random.choice(random_code_list, num_rows) def count_unique_values(df, cat_col_list): cat_df = df[cat_col_list] cat_df[&#39;principal_diagnosis_code&#39;] = create_cardinality_feature(cat_df) #add feature with high cardinality val_df = pd.DataFrame({&#39;columns&#39;: cat_df.columns, &#39;cardinality&#39;: cat_df.nunique() } ) return val_df categorical_feature_list = [ &#39;race&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;weight&#39;, &#39;payer_code&#39;, &#39;medical_specialty&#39;, &#39;primary_diagnosis_code&#39;, &#39;other_diagnosis_codes&#39;,&#39;ndc_code&#39;, &#39;max_glu_serum&#39;, &#39;A1Cresult&#39;, &#39;change&#39;, &#39;readmitted&#39;] categorical_df = count_unique_values(df, categorical_feature_list) categorical_df . columns cardinality . race race | 6 | . gender gender | 3 | . age age | 10 | . weight weight | 10 | . payer_code payer_code | 18 | . medical_specialty medical_specialty | 73 | . primary_diagnosis_code primary_diagnosis_code | 717 | . other_diagnosis_codes other_diagnosis_codes | 19374 | . ndc_code ndc_code | 251 | . max_glu_serum max_glu_serum | 4 | . A1Cresult A1Cresult | 4 | . change change | 2 | . readmitted readmitted | 3 | . principal_diagnosis_code principal_diagnosis_code | 900 | . Analysis key findings . The ndc_code field has a high amount of missing values (23460) | num_lab_procedures and num_medications seem to have a roughly normal distribution | Fields that have a high cardinality are - medical_specialty, primary_diagnosis_code, other_diagnosis_codes, ndc_code, and principal_diagnosis_code. This is because there are many thousands of these codes that correspond to the many disease and diagnosis sub-classes that exist in the medical field. | The distribution for the age field is approximately normal, which we would expect. The distribution for the gender field is roughly uniform &amp; equal. In this case we discount the very small number of Unknown/valid cases. Again this is not surprising, as the distribution of genders in the general population is also roughly equal so this seems to be a representitive sample from the general population. | . Reduce Dimensionality of the NDC Code Feature . NDC codes are a common format to represent the wide variety of drugs that are prescribed for patient care in the United States. The challenge is that there are many codes that map to the same or similar drug. We are provided with the ndc drug lookup file https://github.com/udacity/nd320-c1-emr-data-starter/blob/master/project/data_schema_references/ndc_lookup_table.csv derived from the National Drug Codes List site(https://ndclist.com/). . We can use this file to come up with a way to reduce the dimensionality of this field and create a new field in the dataset called “generic_drug_name” in the output dataframe. . #NDC code lookup file ndc_code_path = &quot;./medication_lookup_tables/final_ndc_lookup_table&quot; ndc_code_df = pd.read_csv(ndc_code_path) . # Check first new rows ndc_code_df.head() . NDC_Code Proprietary Name Non-proprietary Name Dosage Form Route Name Company Name Product Type . 0 0087-6060 | Glucophage | Metformin Hydrochloride | Tablet, Film Coated | Oral | Bristol-myers Squibb Company | Human Prescription Drug | . 1 0087-6063 | Glucophage XR | Metformin Hydrochloride | Tablet, Extended Release | Oral | Bristol-myers Squibb Company | Human Prescription Drug | . 2 0087-6064 | Glucophage XR | Metformin Hydrochloride | Tablet, Extended Release | Oral | Bristol-myers Squibb Company | Human Prescription Drug | . 3 0087-6070 | Glucophage | Metformin Hydrochloride | Tablet, Film Coated | Oral | Bristol-myers Squibb Company | Human Prescription Drug | . 4 0087-6071 | Glucophage | Metformin Hydrochloride | Tablet, Film Coated | Oral | Bristol-myers Squibb Company | Human Prescription Drug | . # Check for duplicate NDC_Code&#39;s ndc_code_df[ndc_code_df.duplicated(subset=[&#39;NDC_Code&#39;])] . NDC_Code Proprietary Name Non-proprietary Name Dosage Form Route Name Company Name Product Type . 263 0781-5634 | Pioglitazone Hydrochloride And Glimepiride | Pioglitazone Hydrochloride And Glimepiride | Tablet | Oral | Sandoz Inc | Human Prescription Drug | . 264 0781-5635 | Pioglitazone Hydrochloride And Glimepiride | Pioglitazone Hydrochloride And Glimepiride | Tablet | Oral | Sandoz Inc | Human Prescription Drug | . # Remove duplicates ndc_code_df = ndc_code_df.drop(ndc_code_df.index[[263,264]]) ndc_code_df[ndc_code_df.duplicated(subset=[&#39;NDC_Code&#39;])] . NDC_Code Proprietary Name Non-proprietary Name Dosage Form Route Name Company Name Product Type . Select First Encounter for each Patient . In order to simplify the aggregation of data for the model, we will only select the first encounter for each patient in the dataset. This is to reduce the risk of data leakage of future patient encounters and to reduce complexity of the data transformation and modeling steps. We will assume that sorting in numerical order on the encounter_id provides the time horizon for determining which encounters come before and after another. . from student_utils import select_first_encounter first_encounter_df = select_first_encounter(reduce_dim_df) . # unique patients in transformed dataset unique_patients = first_encounter_df[&#39;patient_nbr&#39;].nunique() print(&quot;Number of unique patients:{}&quot;.format(unique_patients)) # unique encounters in transformed dataset unique_encounters = first_encounter_df[&#39;encounter_id&#39;].nunique() print(&quot;Number of unique encounters:{}&quot;.format(unique_encounters)) original_unique_patient_number = reduce_dim_df[&#39;patient_nbr&#39;].nunique() # number of unique patients should be equal to the number of unique encounters and patients in the final dataset assert original_unique_patient_number == unique_patients assert original_unique_patient_number == unique_encounters . Number of unique patients:71518 | Number of unique encounters:71518 | . Aggregate Dataset to Right Level for Modelling . To make it simpler, we are creating dummy columns for each unique generic drug name and adding those are input features to the model. . exclusion_list = [&#39;generic_drug_name&#39;] grouping_field_list = [c for c in first_encounter_df.columns if c not in exclusion_list] agg_drug_df, ndc_col_list = aggregate_dataset(first_encounter_df, grouping_field_list, &#39;generic_drug_name&#39;) . assert len(agg_drug_df) == agg_drug_df[&#39;patient_nbr&#39;].nunique() == agg_drug_df[&#39;encounter_id&#39;].nunique() . Prepare Fields and Cast Dataset . Feature Selection . # Look at counts for payer_code categories ax = sns.countplot(x=&quot;payer_code&quot;, data=agg_drug_df) . . # Look at counts for weight categories ax = sns.countplot(x=&quot;weight&quot;, data=agg_drug_df) . . From the category counts above, we can see that for payer_code while there are many unknown values i.e. ‘?’, there are still many values for other payer codes, these may prove useful predictors for our target variable. For weight, there are so few unknown ‘?’ codes, that this feature is likely to be not very helpful for predicting our target variable. . # Selected features required_demo_col_list = [&#39;race&#39;, &#39;gender&#39;, &#39;age&#39;] student_categorical_col_list = [ &quot;change&quot;, &quot;readmitted&quot;, &quot;payer_code&quot;, &quot;medical_specialty&quot;, &quot;primary_diagnosis_code&quot;, &quot;other_diagnosis_codes&quot;, &quot;max_glu_serum&quot;, &quot;A1Cresult&quot;, &quot;admission_type_id&quot;, &quot;discharge_disposition_id&quot;, &quot;admission_source_id&quot;] + required_demo_col_list + ndc_col_list student_numerical_col_list = [&quot;number_outpatient&quot;, &quot;number_inpatient&quot;, &quot;number_emergency&quot;, &quot;num_lab_procedures&quot;, &quot;number_diagnoses&quot;, &quot;num_medications&quot;, &quot;num_procedures&quot;] PREDICTOR_FIELD = &#39;time_in_hospital&#39; . def select_model_features(df, categorical_col_list, numerical_col_list, PREDICTOR_FIELD, grouping_key=&#39;patient_nbr&#39;): selected_col_list = [grouping_key] + [PREDICTOR_FIELD] + categorical_col_list + numerical_col_list return agg_drug_df[selected_col_list] . selected_features_df = select_model_features(agg_drug_df, student_categorical_col_list, student_numerical_col_list, PREDICTOR_FIELD) . Preprocess Dataset - Casting and Imputing . We will cast and impute the dataset before splitting so that we do not have to repeat these steps across the splits in the next step. For imputing, there can be deeper analysis into which features to impute and how to impute but for the sake of time, we are taking a general strategy of imputing zero for only numerical features. . processed_df = preprocess_df(selected_features_df, student_categorical_col_list, student_numerical_col_list, PREDICTOR_FIELD, categorical_impute_value=&#39;nan&#39;, numerical_impute_value=0) . Split Dataset into Train, Validation, and Test Partitions . In order to prepare the data for being trained and evaluated by a deep learning model, we will split the dataset into three partitions, with the validation partition used for optimizing the model hyperparameters during training. One of the key parts is that we need to be sure that the data does not accidently leak across partitions. . We will split the input dataset into three partitions(train, validation, test) with the following requirements: . Approximately 60%/20%/20% train/validation/test split | Randomly sample different patients into each data partition | We need to take care that a patient’s data is not in more than one partition, so that we can avoid possible data leakage. | We need to take care the total number of unique patients across the splits is equal to the total number of unique patients in the original dataset | Total number of rows in original dataset = sum of rows across all three dataset partitions | . from student_utils import patient_dataset_splitter d_train, d_val, d_test = patient_dataset_splitter(processed_df, &#39;patient_nbr&#39;) . Total number of unique patients in train = 32563 | Total number of unique patients in validation = 10854 | Total number of unique patients in test = 10854 | Training partition has a shape = (32563, 43) | Validation partition has a shape = (10854, 43) | Test partition has a shape = (10854, 43) | . Demographic Representation Analysis of Split . After the split, we should check to see the distribution of key features/groups and make sure that there is representative samples across the partitions. . Label Distribution Across Partitions . Are the histogram distribution shapes similar across partitions? . show_group_stats_viz(processed_df, PREDICTOR_FIELD) . . show_group_stats_viz(d_train, PREDICTOR_FIELD) . . show_group_stats_viz(d_test, PREDICTOR_FIELD) . . Demographic Group Analysis . We should check that our partitions/splits of the dataset are similar in terms of their demographic profiles. . # Full dataset before splitting patient_demo_features = [&#39;race&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;patient_nbr&#39;] patient_group_analysis_df = processed_df[patient_demo_features].groupby(&#39;patient_nbr&#39;).head(1).reset_index(drop=True) show_group_stats_viz(patient_group_analysis_df, &#39;gender&#39;) . . # Training partition show_group_stats_viz(d_train, &#39;gender&#39;) . . # Test partition show_group_stats_viz(d_test, &#39;gender&#39;) . . Convert Dataset Splits to TF Dataset . # Convert dataset from Pandas dataframes to TF dataset batch_size = 128 diabetes_train_ds = df_to_dataset(d_train, PREDICTOR_FIELD, batch_size=batch_size) diabetes_val_ds = df_to_dataset(d_val, PREDICTOR_FIELD, batch_size=batch_size) diabetes_test_ds = df_to_dataset(d_test, PREDICTOR_FIELD, batch_size=batch_size) . # We use this sample of the dataset to show transformations later diabetes_batch = next(iter(diabetes_train_ds))[0] def demo(feature_column, example_batch): feature_layer = layers.DenseFeatures(feature_column) print(feature_layer(example_batch)) . Create Features . Create Categorical Features with TF Feature Columns . Before we can create the TF categorical features, we must first create the vocab files with the unique values for a given field that are from the training dataset. . # Build Vocabulary for Categorical Features vocab_file_list = build_vocab_files(d_train, student_categorical_col_list) . Create Categorical Features with Tensorflow Feature Column API . from student_utils import create_tf_categorical_feature_cols tf_cat_col_list = create_tf_categorical_feature_cols(student_categorical_col_list) . test_cat_var1 = tf_cat_col_list[0] print(&quot;Example categorical field: n{}&quot;.format(test_cat_var1)) demo(test_cat_var1, diabetes_batch) . Create Numerical Features with TF Feature Columns . from student_utils import create_tf_numeric_feature . def calculate_stats_from_train_data(df, col): mean = df[col].describe()[&#39;mean&#39;] std = df[col].describe()[&#39;std&#39;] return mean, std def create_tf_numerical_feature_cols(numerical_col_list, train_df): tf_numeric_col_list = [] for c in numerical_col_list: mean, std = calculate_stats_from_train_data(train_df, c) tf_numeric_feature = create_tf_numeric_feature(c, mean, std) tf_numeric_col_list.append(tf_numeric_feature) return tf_numeric_col_list . tf_cont_col_list = create_tf_numerical_feature_cols(student_numerical_col_list, d_train) . test_cont_var1 = tf_cont_col_list[0] print(&quot;Example continuous field: n{} n&quot;.format(test_cont_var1)) demo(test_cont_var1, diabetes_batch) . Build Deep Learning Regression Model with Sequential API and TF Probability Layers . Use DenseFeatures to combine features for model . Now that we have prepared categorical and numerical features using Tensorflow’s Feature Column API, we can combine them into a dense vector representation for the model. Below we will create this new input layer, which we will call ‘claim_feature_layer’. . claim_feature_columns = tf_cat_col_list + tf_cont_col_list claim_feature_layer = tf.keras.layers.DenseFeatures(claim_feature_columns) . Build Sequential API Model from DenseFeatures and TF Probability Layers . def build_sequential_model(feature_layer): model = tf.keras.Sequential([ feature_layer, tf.keras.layers.Dense(150, activation=&#39;relu&#39;), tf.keras.layers.Dense(200, activation=&#39;relu&#39;),# New tf.keras.layers.Dense(75, activation=&#39;relu&#39;), tfp.layers.DenseVariational(1+1, posterior_mean_field, prior_trainable), tfp.layers.DistributionLambda( lambda t:tfp.distributions.Normal(loc=t[..., :1], scale=1e-3 + tf.math.softplus(0.01 * t[...,1:]) ) ), ]) return model def build_diabetes_model(train_ds, val_ds, feature_layer, epochs=5, loss_metric=&#39;mse&#39;): model = build_sequential_model(feature_layer) opt = tf.keras.optimizers.Adam(learning_rate=0.01) model.compile(optimizer=opt, loss=loss_metric, metrics=[loss_metric]) #model.compile(optimizer=&#39;rmsprop&#39;, loss=loss_metric, metrics=[loss_metric]) #early_stop = tf.keras.callbacks.EarlyStopping(monitor=loss_metric, patience=3) history = model.fit(train_ds, validation_data=val_ds, #callbacks=[early_stop], epochs=epochs) return model, history . diabetes_model, history = build_diabetes_model(diabetes_train_ds, diabetes_val_ds, claim_feature_layer, epochs=10) . Show Model Uncertainty Range with TF Probability . Now that we have trained a model with TF Probability layers, we can extract the mean and standard deviation for each prediction. . feature_list = student_categorical_col_list + student_numerical_col_list diabetes_x_tst = dict(d_test[feature_list]) diabetes_yhat = diabetes_model(diabetes_x_tst) preds = diabetes_model.predict(diabetes_test_ds) . from student_utils import get_mean_std_from_preds m, s = get_mean_std_from_preds(diabetes_yhat) . Show Prediction Output . prob_outputs = { &quot;pred&quot;: preds.flatten(), &quot;actual_value&quot;: d_test[&#39;time_in_hospital&#39;].values, &quot;pred_mean&quot;: m.numpy().flatten(), &quot;pred_std&quot;: s.numpy().flatten() } prob_output_df = pd.DataFrame(prob_outputs) . prob_output_df.head() . pred actual_value pred_mean pred_std . 0 3.587955 | 3.0 | 4.673843 | 0.693749 | . 1 5.007016 | 2.0 | 4.673843 | 0.693749 | . 2 4.809363 | 9.0 | 4.673843 | 0.693749 | . 3 5.003417 | 2.0 | 4.673843 | 0.693749 | . 4 5.346958 | 8.0 | 4.673843 | 0.693749 | . prob_output_df.describe() . pred actual_value pred_mean pred_std . count 10854.000000 | 10854.000000 | 10854.000000 | 10854.000000 | . mean 4.376980 | 4.429888 | 4.673843 | 0.693749 | . std 0.908507 | 3.002044 | 0.000000 | 0.000000 | . min 0.976290 | 1.000000 | 4.673843 | 0.693749 | . 25% 3.755292 | 2.000000 | 4.673843 | 0.693749 | . 50% 4.382993 | 4.000000 | 4.673843 | 0.693749 | . 75% 5.002859 | 6.000000 | 4.673843 | 0.693749 | . max 7.529900 | 14.000000 | 4.673843 | 0.693749 | . Convert Regression Output to Classification Output for Patient Selection . from student_utils import get_student_binary_prediction student_binary_prediction = get_student_binary_prediction(prob_output_df, &#39;pred&#39;) . student_binary_prediction.value_counts() . 0:8137 | 1:2717 | . Add Binary Prediction to Test Dataframe . Using the student_binary_prediction output that is a numpy array with binary labels, we can use this to add to a dataframe to better visualize and also to prepare the data for the Aequitas toolkit. The Aequitas toolkit requires that the predictions be mapped to a binary label for the predictions (called ‘score’ field) and the actual value (called ‘label_value’). . def add_pred_to_test(test_df, pred_np, demo_col_list): for c in demo_col_list: test_df[c] = test_df[c].astype(str) test_df[&#39;score&#39;] = pred_np test_df[&#39;label_value&#39;] = test_df[&#39;time_in_hospital&#39;].apply(lambda x: 1 if x &gt;=5 else 0) return test_df pred_test_df = add_pred_to_test(d_test, student_binary_prediction, [&#39;race&#39;, &#39;gender&#39;]) . pred_test_df[[&#39;patient_nbr&#39;, &#39;gender&#39;, &#39;race&#39;, &#39;time_in_hospital&#39;, &#39;score&#39;, &#39;label_value&#39;]].head() . patient_nbr gender race time_in_hospital score label_value . 0 122896787 | Male | Caucasian | 3.0 | 0 | 0 | . 1 102598929 | Male | Caucasian | 2.0 | 1 | 0 | . 2 80367957 | Male | Caucasian | 9.0 | 0 | 1 | . 3 6721533 | Male | Caucasian | 2.0 | 1 | 0 | . 4 104346288 | Female | Caucasian | 8.0 | 1 | 1 | . Model Evaluation Metrics . Now it is time to use the newly created binary labels in the ‘pred_test_df’ dataframe to evaluate the model with some common classification metrics. We will create a report summary of the performance of the model and give the ROC AUC, F1 score(weighted), class precision and recall scores. . # AUC, F1, precision and recall # Summary y_true = pred_test_df[&#39;label_value&#39;].values y_pred = pred_test_df[&#39;score&#39;].values . accuracy_score(y_true, y_pred) . 0.5627418463239359 | . roc_auc_score(y_true, y_pred) . 0.5032089104088319 | . Precision-recall tradeoff - The model has been optimised to identify those patients correct for the trial with the fewest mistakes, while also trying to ensure we identify as many of them as possible. . Areas of imporovement - we could look to engineer new features that might help us better predict our target patients. . Evaluating Potential Model Biases with Aequitas Toolkit . Prepare Data For Aequitas Bias Toolkit . Using the gender and race fields, we will prepare the data for the Aequitas Toolkit. . # Aequitas from aequitas.preprocessing import preprocess_input_df from aequitas.group import Group from aequitas.plotting import Plot from aequitas.bias import Bias from aequitas.fairness import Fairness ae_subset_df = pred_test_df[[&#39;race&#39;, &#39;gender&#39;, &#39;score&#39;, &#39;label_value&#39;]] ae_df, _ = preprocess_input_df(ae_subset_df) g = Group() xtab, _ = g.get_crosstabs(ae_df) absolute_metrics = g.list_absolute_metrics(xtab) clean_xtab = xtab.fillna(-1) aqp = Plot() b = Bias() . model_id, score_thresholds 1 {‘rank_abs’: [2717]} | . absolute_metrics = g.list_absolute_metrics(xtab) xtab[[col for col in xtab.columns if col not in absolute_metrics]] . model_id score_threshold k attribute_name attribute_value pp pn fp fn tn tp group_label_pos group_label_neg group_size total_entities . 0 1 | binary 0/1 | 2717 | race | ? | 86 | 240 | 56 | 85 | 155 | 30 | 115 | 211 | 326 | 10854 | . 1 1 | binary 0/1 | 2717 | race | AfricanAmerican | 491 | 1530 | 291 | 592 | 938 | 200 | 792 | 1229 | 2021 | 10854 | . 2 1 | binary 0/1 | 2717 | race | Asian | 15 | 60 | 10 | 16 | 44 | 5 | 21 | 54 | 75 | 10854 | . 3 1 | binary 0/1 | 2717 | race | Caucasian | 2030 | 6038 | 1249 | 2298 | 3740 | 781 | 3079 | 4989 | 8068 | 10854 | . 4 1 | binary 0/1 | 2717 | race | Hispanic | 52 | 141 | 35 | 48 | 93 | 17 | 65 | 128 | 193 | 10854 | . 5 1 | binary 0/1 | 2717 | race | Other | 43 | 128 | 26 | 40 | 88 | 17 | 57 | 114 | 171 | 10854 | . 6 1 | binary 0/1 | 2717 | gender | Female | 1413 | 4306 | 820 | 1675 | 2631 | 593 | 2268 | 3451 | 5719 | 10854 | . 7 1 | binary 0/1 | 2717 | gender | Male | 1304 | 3831 | 847 | 1404 | 2427 | 457 | 1861 | 3274 | 5135 | 10854 | . Reference Group Selection . # Test reference group with Caucasian Male bdf = b.get_disparity_predefined_groups(clean_xtab, original_df=ae_df, ref_groups_dict={&#39;race&#39;:&#39;Caucasian&#39;, &#39;gender&#39;:&#39;Male&#39; }, alpha=0.05, check_significance=False) f = Fairness() fdf = f.get_group_value_fairness(bdf) . Race and Gender Bias Analysis for Patient Selection . # Plot two metrics # Is there significant bias in your model for either race or gender? fpr_disparity1 = aqp.plot_disparity(bdf, group_metric=&#39;fpr_disparity&#39;, attribute_name=&#39;race&#39;) . . We notice that while with most races, there is no significant indication of bias, there is an indication that Asians are less likely to be itentified by the model, based on the 0.4 disparity in relation to the Caucasian reference group. . fpr_disparity2 = aqp.plot_disparity(bdf, group_metric=&#39;fpr_disparity&#39;, attribute_name=&#39;gender&#39;) . . With gender, there does not seem to be any significant indication of bias. . Fairness Analysis Example - Relative to a Reference Group . # Reference group fairness plot fpr_fairness = aqp.plot_fairness_group(fdf, group_metric=&#39;fpr&#39;, title=True) . . Here again we can see that there appears to be signficant disparity with the Asian race being under-represented with a magnitude of 0.19. .",
            "url": "https://www.livingdatalab.com/health/deep-learning/2022/02/06/patient-select-diabetes.html",
            "relUrl": "/health/deep-learning/2022/02/06/patient-select-diabetes.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Pneumonia Detection From Chest X-Rays",
            "content": "Introduction . In this project, I will analyze data from the NIH Chest X-ray Dataset and train a CNN to classify a given chest x-ray for the presence or absence of pneumonia. This project will culminate in a model that aims to predict the presence of pneumonia with human radiologist-level accuracy that can be prepared for submission to the FDA for 510(k) clearance as software as a medical device. As part of the submission preparation, I will formally describe my model, the data that it was trained on, and a validation plan that meets FDA criteria. . The project will use a dataset of 112,000 chest x-rays with disease labels acquired from 30,000 patients. . Load, view and clean dataset . ## Read full image filepaths into a dataframe for easier manipulation ## Load the NIH data to all_xray_df all_xray_df = pd.read_csv(&#39;/data/Data_Entry_2017.csv&#39;) all_image_paths = {os.path.basename(x): x for x in glob(os.path.join(&#39;/data&#39;,&#39;images*&#39;, &#39;*&#39;, &#39;*.png&#39;))} print(&#39;Scans found:&#39;, len(all_image_paths), &#39;, Total Headers&#39;, all_xray_df.shape[0]) all_xray_df[&#39;path&#39;] = all_xray_df[&#39;Image Index&#39;].map(all_image_paths.get) all_xray_df.sample(3) . Scans found: 112120 , Total Headers 112120 . Image Index Finding Labels Follow-up # Patient ID Patient Age Patient Gender View Position OriginalImage[Width Height] OriginalImagePixelSpacing[x y] Unnamed: 11 path . 54694 00013684_000.png | No Finding | 0 | 13684 | 64 | M | PA | 2992 | 2991 | 0.143 | 0.143 | NaN | /data/images_006/images/00013684_000.png | . 61239 00015102_000.png | No Finding | 0 | 15102 | 48 | F | PA | 2446 | 2991 | 0.143 | 0.143 | NaN | /data/images_007/images/00015102_000.png | . 102440 00027295_002.png | No Finding | 2 | 27295 | 47 | M | PA | 2992 | 2991 | 0.143 | 0.143 | NaN | /data/images_011/images/00027295_002.png | . # Drop any unreasonable ages! all_xray_df = all_xray_df[all_xray_df[&#39;Patient Age&#39;] &lt; 120] all_xray_df.describe() . Follow-up # Patient ID Patient Age OriginalImage[Width Height] OriginalImagePixelSpacing[x y] Unnamed: 11 . count 112104.000000 | 112104.000000 | 112104.000000 | 112104.000000 | 112104.000000 | 112104.000000 | 112104.000000 | 0.0 | . mean 8.574172 | 14345.720724 | 46.872574 | 2646.035253 | 2486.393153 | 0.155651 | 0.155651 | NaN | . std 15.406734 | 8403.980520 | 16.598152 | 341.243771 | 401.270806 | 0.016174 | 0.016174 | NaN | . min 0.000000 | 1.000000 | 1.000000 | 1143.000000 | 966.000000 | 0.115000 | 0.115000 | NaN | . 25% 0.000000 | 7308.000000 | 35.000000 | 2500.000000 | 2048.000000 | 0.143000 | 0.143000 | NaN | . 50% 3.000000 | 13993.000000 | 49.000000 | 2518.000000 | 2544.000000 | 0.143000 | 0.143000 | NaN | . 75% 10.000000 | 20673.000000 | 59.000000 | 2992.000000 | 2991.000000 | 0.168000 | 0.168000 | NaN | . max 183.000000 | 30805.000000 | 95.000000 | 3827.000000 | 4715.000000 | 0.198800 | 0.198800 | NaN | . ## Create some extra columns in the table with binary indicators of certain diseases ## rather than working directly with the &#39;Finding Labels&#39; column # Re-format multi-label column into separate columns for each label binary encoded all_labels = np.unique(list(chain(*all_xray_df[&#39;Finding Labels&#39;].map(lambda x: x.split(&#39;|&#39;)).tolist()))) all_labels = [x for x in all_labels if len(x)&gt;0] print(&#39;All Labels ({}): {}&#39;.format(len(all_labels), all_labels)) for c_label in all_labels: if len(c_label)&gt;1: # ignore empty labels all_xray_df[c_label] = all_xray_df[&#39;Finding Labels&#39;].map(lambda finding: 1 if c_label in finding else 0) all_xray_df.sample(3) . All Labels (15): [&#39;Atelectasis&#39;, &#39;Cardiomegaly&#39;, &#39;Consolidation&#39;, &#39;Edema&#39;, &#39;Effusion&#39;, &#39;Emphysema&#39;, &#39;Fibrosis&#39;, &#39;Hernia&#39;, &#39;Infiltration&#39;, &#39;Mass&#39;, &#39;No Finding&#39;, &#39;Nodule&#39;, &#39;Pleural_Thickening&#39;, &#39;Pneumonia&#39;, &#39;Pneumothorax&#39;] . Image Index Finding Labels Follow-up # Patient ID Patient Age Patient Gender View Position OriginalImage[Width Height] OriginalImagePixelSpacing[x ... Emphysema Fibrosis Hernia Infiltration Mass No Finding Nodule Pleural_Thickening Pneumonia Pneumothorax . 43672 00011246_000.png | No Finding | 0 | 11246 | 40 | M | PA | 2992 | 2991 | 0.143 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 60931 00015040_001.png | Atelectasis|Effusion | 1 | 15040 | 48 | F | AP | 2500 | 2048 | 0.168 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 25642 00006741_001.png | No Finding | 1 | 6741 | 54 | F | PA | 2992 | 2991 | 0.143 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 3 rows × 28 columns . ## Here we can create a new column called &#39;pneumonia_class&#39; that will allow us to look at ## images with or without pneumonia for binary classification all_xray_df[&#39;pneumonia_class&#39;] = np.where(all_xray_df[&#39;Pneumonia&#39;]==1, &#39;Pneumonia&#39;, &#39;No Pneumonia&#39;) all_xray_df.head() . Image Index Finding Labels Follow-up # Patient ID Patient Age Patient Gender View Position OriginalImage[Width Height] OriginalImagePixelSpacing[x ... Fibrosis Hernia Infiltration Mass No Finding Nodule Pleural_Thickening Pneumonia Pneumothorax pneumonia_class . 0 00000001_000.png | Cardiomegaly | 0 | 1 | 58 | M | PA | 2682 | 2749 | 0.143 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | No Pneumonia | . 1 00000001_001.png | Cardiomegaly|Emphysema | 1 | 1 | 58 | M | PA | 2894 | 2729 | 0.143 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | No Pneumonia | . 2 00000001_002.png | Cardiomegaly|Effusion | 2 | 1 | 58 | M | PA | 2500 | 2048 | 0.168 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | No Pneumonia | . 3 00000002_000.png | No Finding | 0 | 2 | 81 | M | PA | 2500 | 2048 | 0.171 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | No Pneumonia | . 4 00000003_000.png | Hernia | 0 | 3 | 81 | F | PA | 2582 | 2991 | 0.143 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | No Pneumonia | . 5 rows × 29 columns . Split data into training and testing sets . # Total Pneumonia cases all_xray_df[&#39;Pneumonia&#39;].sum() . 1430 . So in our dataset we have: . Pneumonia cases: 1,430 or 1.2% | Non-Pneumonia cases: 110,674 or 98.8% | . Given that we want: . Our training set to be balanced between Pneumonia and Non-Pneumonia cases i.e. equal | Our test set to reflect the real world proportions i.e. Pneumonia 1.2% and Non-Pneumonia 98.8% | To split our data between training and test sets in a 80% to 20% proportion | . This leads to the following training &amp; test sets: . Training set: 1,144 (50%) Pneumonia cases, 1,144 (50%) Non-Pneumonia cases - Total 2,288 | Test set: 286 (1.2%) Pneumonia cases, 23,547 (98.8%) Non-Pneumonia cases - Total 23,833 | . def create_splits(vargs): ## It&#39;s important to consider here how balanced or imbalanced we want each of those sets to be ## for the presence of pneumonia # Select rows with Pneumonia cases pneumonia_df = all_xray_df[all_xray_df[&#39;Pneumonia&#39;] == 1] # Select rows with No-Pneumonia cases no_pneumonia_df = all_xray_df[all_xray_df[&#39;Pneumonia&#39;] == 0] # Split Pneumonia cases 80% - 20% between train and validation train_data, val_data = skl.train_test_split(pneumonia_df, test_size = 0.2) # Split No-Pneumonia cases into two separate groups equal size train_no_pneumonia_data, val_no_pneumonia_data = skl.train_test_split(no_pneumonia_df, test_size = 0.5) # Sample from No-Pneumonia train set to be same size as Pneumonia train set train_no_pneumonia_data = train_no_pneumonia_data.sample(train_data.shape[0]) # Merge No-Pneumonia train set into train set train_data = pd.concat([train_data, train_no_pneumonia_data]) # Calculate proportion required of No-Pneumonia cases for test set at 98.8% no_pneumonia_test_count = int((val_data.shape[0] / 1.2) * 98.8) # Sample from No-Pneumonia test set to be 98.8% of test set val_no_pneumonia_data = val_no_pneumonia_data.sample(no_pneumonia_test_count) # Merge No-Pneumonia test set into test set val_data = pd.concat([val_data, val_no_pneumonia_data]) return train_data, val_data # Create train and validation splits train_df, valid_df = create_splits(all_xray_df) . # View Pneumonia vs No-Pneumonia counts for training train_df[&#39;Pneumonia&#39;].value_counts() . 1 1144 0 1144 Name: Pneumonia, dtype: int64 . # View Pneumonia vs No-Pneumonia counts for validation valid_df[&#39;Pneumonia&#39;].value_counts() . 0 23547 1 286 Name: Pneumonia, dtype: int64 . Model building &amp; training . Perform some image augmentation . # Define image size IMG_SIZE = (224, 224) . def my_image_augmentation(train=True): ## Use Keras ImageDataGenerator ## with some of the built-in augmentations ## Keep an eye out for types of augmentation that are or are not appropriate for medical imaging data ## Also keep in mind what sort of augmentation is or is not appropriate for testing vs validation data # Create image generator if train: # Training augmentations + normalisation idg = ImageDataGenerator(rescale=1. / 255.0, horizontal_flip = True, vertical_flip = False, height_shift_range= 0.1, width_shift_range=0.1, rotation_range=10, shear_range = 0.1, zoom_range=0.1) else: # Otherwise test set - no augmentation! just normalisation idg = ImageDataGenerator(rescale=1. / 255.0) return idg def make_train_gen(df): ## Create the actual generators using the output of my_image_augmentation for your training data # train_gen = my_train_idg.flow_from_dataframe(dataframe=train_df, # directory=None, # x_col = , # y_col = , # class_mode = &#39;binary&#39;, # target_size = , # batch_size = # ) # Create image generator idg = my_image_augmentation() # Apply image generator to generate more images train_gen = idg.flow_from_dataframe(dataframe=df, directory=None, x_col = &#39;path&#39;, y_col = &#39;pneumonia_class&#39;, class_mode = &#39;binary&#39;, target_size = IMG_SIZE, batch_size = 16) return train_gen def make_val_gen(df): # val_gen = my_val_idg.flow_from_dataframe(dataframe = val_data, # directory=None, # x_col = , # y_col = &#39;, # class_mode = &#39;binary&#39;, # target_size = , # batch_size = ) # Create image generator idg = my_image_augmentation(train=False) # Apply image generator to generate more images - large batch 10% of total validation to get enough Pneumonia val_gen = idg.flow_from_dataframe(dataframe=df, directory=None, x_col = &#39;path&#39;, y_col = &#39;pneumonia_class&#39;, class_mode = &#39;binary&#39;, target_size = IMG_SIZE, batch_size = 2000) return val_gen . # Create training image generator train_gen = make_train_gen(train_df) # Create validation image generator val_gen = make_val_gen(valid_df) . Found 2288 validated image filenames belonging to 2 classes. Found 23833 validated image filenames belonging to 2 classes. . Let us check the distribution of key demographic values within the training &amp; validation sets. . # Compare age distributions of training vs validation data fig, axes = plt.subplots(1, 2) train_df[&#39;Patient Age&#39;].hist(ax=axes[0],figsize=(20,5)) valid_df[&#39;Patient Age&#39;].hist(ax=axes[1],figsize=(20,5)) axes[0].set_title(&#39;Distribution of ages for training data&#39;) axes[0].set_xlabel(&quot;Age&quot;) axes[0].set_ylabel(&quot;Number of x-ray observations&quot;) axes[1].set_title(&#39;Distribution of ages for validation data&#39;) axes[1].set_xlabel(&quot;Age&quot;) axes[1].set_ylabel(&quot;Number of x-ray observations&quot;) . Text(0, 0.5, &#39;Number of x-ray observations&#39;) . # Compare gender between training vs validation data fig, axes = plt.subplots(1, 2) train_df[&#39;Patient Gender&#39;].value_counts().plot(ax=axes[0],kind=&#39;bar&#39;,figsize=(20,5)) valid_df[&#39;Patient Gender&#39;].value_counts().plot(ax=axes[1],kind=&#39;bar&#39;,figsize=(20,5)) axes[0].set_title(&#39;Gender count for training data&#39;) axes[0].set_xlabel(&quot;Gender&quot;) axes[0].set_ylabel(&quot;Number of x-ray observations&quot;) axes[1].set_title(&#39;Gender count for validation data&#39;) axes[1].set_xlabel(&quot;Gender&quot;) axes[1].set_ylabel(&quot;Number of x-ray observations&quot;) . Text(0, 0.5, &#39;Number of x-ray observations&#39;) . # Compare view position training vs validation data fig, axes = plt.subplots(1, 2) train_df[&#39;View Position&#39;].value_counts().plot(ax=axes[0],kind=&#39;bar&#39;,figsize=(20,5)) valid_df[&#39;View Position&#39;].value_counts().plot(ax=axes[1],kind=&#39;bar&#39;,figsize=(20,5)) axes[0].set_title(&#39;View position count training data&#39;) axes[0].set_xlabel(&quot;View position&quot;) axes[0].set_ylabel(&quot;Number of x-ray observations&quot;) axes[1].set_title(&#39;View position count for validation data&#39;) axes[1].set_xlabel(&quot;View position&quot;) axes[1].set_ylabel(&quot;Number of x-ray observations&quot;) . Text(0, 0.5, &#39;Number of x-ray observations&#39;) . # Compare Pneumonia vs No Pneumonia cases between training vs validation data fig, axes = plt.subplots(1, 2) train_df[&#39;Pneumonia&#39;].value_counts().plot(ax=axes[0],kind=&#39;bar&#39;,figsize=(20,5)) valid_df[&#39;Pneumonia&#39;].value_counts().plot(ax=axes[1],kind=&#39;bar&#39;,figsize=(20,5)) axes[0].set_title(&#39;Pneumonia vs No Pneumonia for training data&#39;) axes[0].set_xlabel(&quot;Gender&quot;) axes[0].set_ylabel(&quot;Number of x-ray observations&quot;) axes[1].set_title(&#39;Pneumonia vs No Pneumonia for validation data&#39;) axes[1].set_xlabel(&quot;Gender&quot;) axes[1].set_ylabel(&quot;Number of x-ray observations&quot;) . Text(0, 0.5, &#39;Number of x-ray observations&#39;) . So these proportions of key features are as we wished and expected. The distributions of ages and proportions of gender in the training and validation are roughly the same. For the Pneumonia vs No Pneumonia cases, in our training set we have equal amounts of each case to give the model the best chance for training, while in the validation data we have a much smaller proportion of Pneumonia cases that matches the real world disease prevelance that we observed earlier here and in the EDA study. . Lets now look over more the training and validation data. . ## May want to pull a single large batch of random validation data for testing after each epoch: valX, valY = val_gen.next() . # Get a batch of training data t_x, t_y = next(train_gen) # Print mean and std dev of training batch print(&#39;Train mean &amp; std dev&#39;, t_x.mean(), t_x.std()) . Train mean &amp; std dev 0.54569376 0.23733293 . ## May want to look at some examples of our augmented training data. ## This is helpful for understanding the extent to which data is being manipulated prior to training, ## and can be compared with how the raw data look prior to augmentation fig, m_axs = plt.subplots(4, 4, figsize = (16, 16)) for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()): c_ax.imshow(c_x[:,:,0], cmap = &#39;bone&#39;) if c_y == 1: c_ax.set_title(&#39;Pneumonia&#39;) else: c_ax.set_title(&#39;No Pneumonia&#39;) c_ax.axis(&#39;off&#39;) . So these image augmentations seem reasonable. . Build model . Using a pre-trained network downloaded from Keras for fine-tuning . def load_pretrained_model(): # Load pre-trained resnet50 model with imagenet trained weights model = ResNet50(include_top=True, weights=&#39;imagenet&#39;) return model . def build_my_model(): # Load the pre-trained model model = load_pretrained_model() model.layers.pop() predictions = Dense(1, activation=&#39;sigmoid&#39;)(model.layers[-1].output) my_model = Model(inputs=model.input, outputs=predictions) my_model.compile(optimizer = Adam(lr=0.0001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;binary_accuracy&#39;]) # Print model structure my_model.summary() return my_model # Build model my_model = build_my_model() . Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5 102973440/102967424 [==============================] - 1s 0us/step Model: &#34;model_1&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) (None, 224, 224, 3) 0 __________________________________________________________________________________________________ conv1_pad (ZeroPadding2D) (None, 230, 230, 3) 0 input_1[0][0] __________________________________________________________________________________________________ conv1_conv (Conv2D) (None, 112, 112, 64) 9472 conv1_pad[0][0] __________________________________________________________________________________________________ conv1_bn (BatchNormalization) (None, 112, 112, 64) 256 conv1_conv[0][0] __________________________________________________________________________________________________ conv1_relu (Activation) (None, 112, 112, 64) 0 conv1_bn[0][0] __________________________________________________________________________________________________ pool1_pad (ZeroPadding2D) (None, 114, 114, 64) 0 conv1_relu[0][0] __________________________________________________________________________________________________ pool1_pool (MaxPooling2D) (None, 56, 56, 64) 0 pool1_pad[0][0] __________________________________________________________________________________________________ conv2_block1_1_conv (Conv2D) (None, 56, 56, 64) 4160 pool1_pool[0][0] __________________________________________________________________________________________________ conv2_block1_1_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block1_1_conv[0][0] __________________________________________________________________________________________________ conv2_block1_1_relu (Activation (None, 56, 56, 64) 0 conv2_block1_1_bn[0][0] __________________________________________________________________________________________________ conv2_block1_2_conv (Conv2D) (None, 56, 56, 64) 36928 conv2_block1_1_relu[0][0] __________________________________________________________________________________________________ conv2_block1_2_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block1_2_conv[0][0] __________________________________________________________________________________________________ conv2_block1_2_relu (Activation (None, 56, 56, 64) 0 conv2_block1_2_bn[0][0] __________________________________________________________________________________________________ conv2_block1_0_conv (Conv2D) (None, 56, 56, 256) 16640 pool1_pool[0][0] __________________________________________________________________________________________________ conv2_block1_3_conv (Conv2D) (None, 56, 56, 256) 16640 conv2_block1_2_relu[0][0] __________________________________________________________________________________________________ conv2_block1_0_bn (BatchNormali (None, 56, 56, 256) 1024 conv2_block1_0_conv[0][0] __________________________________________________________________________________________________ conv2_block1_3_bn (BatchNormali (None, 56, 56, 256) 1024 conv2_block1_3_conv[0][0] __________________________________________________________________________________________________ conv2_block1_add (Add) (None, 56, 56, 256) 0 conv2_block1_0_bn[0][0] conv2_block1_3_bn[0][0] __________________________________________________________________________________________________ conv2_block1_out (Activation) (None, 56, 56, 256) 0 conv2_block1_add[0][0] __________________________________________________________________________________________________ conv2_block2_1_conv (Conv2D) (None, 56, 56, 64) 16448 conv2_block1_out[0][0] __________________________________________________________________________________________________ conv2_block2_1_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block2_1_conv[0][0] __________________________________________________________________________________________________ conv2_block2_1_relu (Activation (None, 56, 56, 64) 0 conv2_block2_1_bn[0][0] __________________________________________________________________________________________________ conv2_block2_2_conv (Conv2D) (None, 56, 56, 64) 36928 conv2_block2_1_relu[0][0] __________________________________________________________________________________________________ conv2_block2_2_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block2_2_conv[0][0] __________________________________________________________________________________________________ conv2_block2_2_relu (Activation (None, 56, 56, 64) 0 conv2_block2_2_bn[0][0] __________________________________________________________________________________________________ conv2_block2_3_conv (Conv2D) (None, 56, 56, 256) 16640 conv2_block2_2_relu[0][0] __________________________________________________________________________________________________ conv2_block2_3_bn (BatchNormali (None, 56, 56, 256) 1024 conv2_block2_3_conv[0][0] __________________________________________________________________________________________________ conv2_block2_add (Add) (None, 56, 56, 256) 0 conv2_block1_out[0][0] conv2_block2_3_bn[0][0] __________________________________________________________________________________________________ conv2_block2_out (Activation) (None, 56, 56, 256) 0 conv2_block2_add[0][0] __________________________________________________________________________________________________ conv2_block3_1_conv (Conv2D) (None, 56, 56, 64) 16448 conv2_block2_out[0][0] __________________________________________________________________________________________________ conv2_block3_1_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block3_1_conv[0][0] __________________________________________________________________________________________________ conv2_block3_1_relu (Activation (None, 56, 56, 64) 0 conv2_block3_1_bn[0][0] __________________________________________________________________________________________________ conv2_block3_2_conv (Conv2D) (None, 56, 56, 64) 36928 conv2_block3_1_relu[0][0] __________________________________________________________________________________________________ conv2_block3_2_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block3_2_conv[0][0] __________________________________________________________________________________________________ conv2_block3_2_relu (Activation (None, 56, 56, 64) 0 conv2_block3_2_bn[0][0] __________________________________________________________________________________________________ conv2_block3_3_conv (Conv2D) (None, 56, 56, 256) 16640 conv2_block3_2_relu[0][0] __________________________________________________________________________________________________ conv2_block3_3_bn (BatchNormali (None, 56, 56, 256) 1024 conv2_block3_3_conv[0][0] __________________________________________________________________________________________________ conv2_block3_add (Add) (None, 56, 56, 256) 0 conv2_block2_out[0][0] conv2_block3_3_bn[0][0] __________________________________________________________________________________________________ conv2_block3_out (Activation) (None, 56, 56, 256) 0 conv2_block3_add[0][0] __________________________________________________________________________________________________ conv3_block1_1_conv (Conv2D) (None, 28, 28, 128) 32896 conv2_block3_out[0][0] __________________________________________________________________________________________________ conv3_block1_1_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block1_1_conv[0][0] __________________________________________________________________________________________________ conv3_block1_1_relu (Activation (None, 28, 28, 128) 0 conv3_block1_1_bn[0][0] __________________________________________________________________________________________________ conv3_block1_2_conv (Conv2D) (None, 28, 28, 128) 147584 conv3_block1_1_relu[0][0] __________________________________________________________________________________________________ conv3_block1_2_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block1_2_conv[0][0] __________________________________________________________________________________________________ conv3_block1_2_relu (Activation (None, 28, 28, 128) 0 conv3_block1_2_bn[0][0] __________________________________________________________________________________________________ conv3_block1_0_conv (Conv2D) (None, 28, 28, 512) 131584 conv2_block3_out[0][0] __________________________________________________________________________________________________ conv3_block1_3_conv (Conv2D) (None, 28, 28, 512) 66048 conv3_block1_2_relu[0][0] __________________________________________________________________________________________________ conv3_block1_0_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block1_0_conv[0][0] __________________________________________________________________________________________________ conv3_block1_3_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block1_3_conv[0][0] __________________________________________________________________________________________________ conv3_block1_add (Add) (None, 28, 28, 512) 0 conv3_block1_0_bn[0][0] conv3_block1_3_bn[0][0] __________________________________________________________________________________________________ conv3_block1_out (Activation) (None, 28, 28, 512) 0 conv3_block1_add[0][0] __________________________________________________________________________________________________ conv3_block2_1_conv (Conv2D) (None, 28, 28, 128) 65664 conv3_block1_out[0][0] __________________________________________________________________________________________________ conv3_block2_1_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block2_1_conv[0][0] __________________________________________________________________________________________________ conv3_block2_1_relu (Activation (None, 28, 28, 128) 0 conv3_block2_1_bn[0][0] __________________________________________________________________________________________________ conv3_block2_2_conv (Conv2D) (None, 28, 28, 128) 147584 conv3_block2_1_relu[0][0] __________________________________________________________________________________________________ conv3_block2_2_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block2_2_conv[0][0] __________________________________________________________________________________________________ conv3_block2_2_relu (Activation (None, 28, 28, 128) 0 conv3_block2_2_bn[0][0] __________________________________________________________________________________________________ conv3_block2_3_conv (Conv2D) (None, 28, 28, 512) 66048 conv3_block2_2_relu[0][0] __________________________________________________________________________________________________ conv3_block2_3_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block2_3_conv[0][0] __________________________________________________________________________________________________ conv3_block2_add (Add) (None, 28, 28, 512) 0 conv3_block1_out[0][0] conv3_block2_3_bn[0][0] __________________________________________________________________________________________________ conv3_block2_out (Activation) (None, 28, 28, 512) 0 conv3_block2_add[0][0] __________________________________________________________________________________________________ conv3_block3_1_conv (Conv2D) (None, 28, 28, 128) 65664 conv3_block2_out[0][0] __________________________________________________________________________________________________ conv3_block3_1_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block3_1_conv[0][0] __________________________________________________________________________________________________ conv3_block3_1_relu (Activation (None, 28, 28, 128) 0 conv3_block3_1_bn[0][0] __________________________________________________________________________________________________ conv3_block3_2_conv (Conv2D) (None, 28, 28, 128) 147584 conv3_block3_1_relu[0][0] __________________________________________________________________________________________________ conv3_block3_2_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block3_2_conv[0][0] __________________________________________________________________________________________________ conv3_block3_2_relu (Activation (None, 28, 28, 128) 0 conv3_block3_2_bn[0][0] __________________________________________________________________________________________________ conv3_block3_3_conv (Conv2D) (None, 28, 28, 512) 66048 conv3_block3_2_relu[0][0] __________________________________________________________________________________________________ conv3_block3_3_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block3_3_conv[0][0] __________________________________________________________________________________________________ conv3_block3_add (Add) (None, 28, 28, 512) 0 conv3_block2_out[0][0] conv3_block3_3_bn[0][0] __________________________________________________________________________________________________ conv3_block3_out (Activation) (None, 28, 28, 512) 0 conv3_block3_add[0][0] __________________________________________________________________________________________________ conv3_block4_1_conv (Conv2D) (None, 28, 28, 128) 65664 conv3_block3_out[0][0] __________________________________________________________________________________________________ conv3_block4_1_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block4_1_conv[0][0] __________________________________________________________________________________________________ conv3_block4_1_relu (Activation (None, 28, 28, 128) 0 conv3_block4_1_bn[0][0] __________________________________________________________________________________________________ conv3_block4_2_conv (Conv2D) (None, 28, 28, 128) 147584 conv3_block4_1_relu[0][0] __________________________________________________________________________________________________ conv3_block4_2_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block4_2_conv[0][0] __________________________________________________________________________________________________ conv3_block4_2_relu (Activation (None, 28, 28, 128) 0 conv3_block4_2_bn[0][0] __________________________________________________________________________________________________ conv3_block4_3_conv (Conv2D) (None, 28, 28, 512) 66048 conv3_block4_2_relu[0][0] __________________________________________________________________________________________________ conv3_block4_3_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block4_3_conv[0][0] __________________________________________________________________________________________________ conv3_block4_add (Add) (None, 28, 28, 512) 0 conv3_block3_out[0][0] conv3_block4_3_bn[0][0] __________________________________________________________________________________________________ conv3_block4_out (Activation) (None, 28, 28, 512) 0 conv3_block4_add[0][0] __________________________________________________________________________________________________ conv4_block1_1_conv (Conv2D) (None, 14, 14, 256) 131328 conv3_block4_out[0][0] __________________________________________________________________________________________________ conv4_block1_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block1_1_conv[0][0] __________________________________________________________________________________________________ conv4_block1_1_relu (Activation (None, 14, 14, 256) 0 conv4_block1_1_bn[0][0] __________________________________________________________________________________________________ conv4_block1_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block1_1_relu[0][0] __________________________________________________________________________________________________ conv4_block1_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block1_2_conv[0][0] __________________________________________________________________________________________________ conv4_block1_2_relu (Activation (None, 14, 14, 256) 0 conv4_block1_2_bn[0][0] __________________________________________________________________________________________________ conv4_block1_0_conv (Conv2D) (None, 14, 14, 1024) 525312 conv3_block4_out[0][0] __________________________________________________________________________________________________ conv4_block1_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block1_2_relu[0][0] __________________________________________________________________________________________________ conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block1_0_conv[0][0] __________________________________________________________________________________________________ conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block1_3_conv[0][0] __________________________________________________________________________________________________ conv4_block1_add (Add) (None, 14, 14, 1024) 0 conv4_block1_0_bn[0][0] conv4_block1_3_bn[0][0] __________________________________________________________________________________________________ conv4_block1_out (Activation) (None, 14, 14, 1024) 0 conv4_block1_add[0][0] __________________________________________________________________________________________________ conv4_block2_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block1_out[0][0] __________________________________________________________________________________________________ conv4_block2_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block2_1_conv[0][0] __________________________________________________________________________________________________ conv4_block2_1_relu (Activation (None, 14, 14, 256) 0 conv4_block2_1_bn[0][0] __________________________________________________________________________________________________ conv4_block2_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block2_1_relu[0][0] __________________________________________________________________________________________________ conv4_block2_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block2_2_conv[0][0] __________________________________________________________________________________________________ conv4_block2_2_relu (Activation (None, 14, 14, 256) 0 conv4_block2_2_bn[0][0] __________________________________________________________________________________________________ conv4_block2_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block2_2_relu[0][0] __________________________________________________________________________________________________ conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block2_3_conv[0][0] __________________________________________________________________________________________________ conv4_block2_add (Add) (None, 14, 14, 1024) 0 conv4_block1_out[0][0] conv4_block2_3_bn[0][0] __________________________________________________________________________________________________ conv4_block2_out (Activation) (None, 14, 14, 1024) 0 conv4_block2_add[0][0] __________________________________________________________________________________________________ conv4_block3_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block2_out[0][0] __________________________________________________________________________________________________ conv4_block3_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block3_1_conv[0][0] __________________________________________________________________________________________________ conv4_block3_1_relu (Activation (None, 14, 14, 256) 0 conv4_block3_1_bn[0][0] __________________________________________________________________________________________________ conv4_block3_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block3_1_relu[0][0] __________________________________________________________________________________________________ conv4_block3_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block3_2_conv[0][0] __________________________________________________________________________________________________ conv4_block3_2_relu (Activation (None, 14, 14, 256) 0 conv4_block3_2_bn[0][0] __________________________________________________________________________________________________ conv4_block3_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block3_2_relu[0][0] __________________________________________________________________________________________________ conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block3_3_conv[0][0] __________________________________________________________________________________________________ conv4_block3_add (Add) (None, 14, 14, 1024) 0 conv4_block2_out[0][0] conv4_block3_3_bn[0][0] __________________________________________________________________________________________________ conv4_block3_out (Activation) (None, 14, 14, 1024) 0 conv4_block3_add[0][0] __________________________________________________________________________________________________ conv4_block4_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block3_out[0][0] __________________________________________________________________________________________________ conv4_block4_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block4_1_conv[0][0] __________________________________________________________________________________________________ conv4_block4_1_relu (Activation (None, 14, 14, 256) 0 conv4_block4_1_bn[0][0] __________________________________________________________________________________________________ conv4_block4_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block4_1_relu[0][0] __________________________________________________________________________________________________ conv4_block4_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block4_2_conv[0][0] __________________________________________________________________________________________________ conv4_block4_2_relu (Activation (None, 14, 14, 256) 0 conv4_block4_2_bn[0][0] __________________________________________________________________________________________________ conv4_block4_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block4_2_relu[0][0] __________________________________________________________________________________________________ conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block4_3_conv[0][0] __________________________________________________________________________________________________ conv4_block4_add (Add) (None, 14, 14, 1024) 0 conv4_block3_out[0][0] conv4_block4_3_bn[0][0] __________________________________________________________________________________________________ conv4_block4_out (Activation) (None, 14, 14, 1024) 0 conv4_block4_add[0][0] __________________________________________________________________________________________________ conv4_block5_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block4_out[0][0] __________________________________________________________________________________________________ conv4_block5_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block5_1_conv[0][0] __________________________________________________________________________________________________ conv4_block5_1_relu (Activation (None, 14, 14, 256) 0 conv4_block5_1_bn[0][0] __________________________________________________________________________________________________ conv4_block5_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block5_1_relu[0][0] __________________________________________________________________________________________________ conv4_block5_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block5_2_conv[0][0] __________________________________________________________________________________________________ conv4_block5_2_relu (Activation (None, 14, 14, 256) 0 conv4_block5_2_bn[0][0] __________________________________________________________________________________________________ conv4_block5_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block5_2_relu[0][0] __________________________________________________________________________________________________ conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block5_3_conv[0][0] __________________________________________________________________________________________________ conv4_block5_add (Add) (None, 14, 14, 1024) 0 conv4_block4_out[0][0] conv4_block5_3_bn[0][0] __________________________________________________________________________________________________ conv4_block5_out (Activation) (None, 14, 14, 1024) 0 conv4_block5_add[0][0] __________________________________________________________________________________________________ conv4_block6_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block5_out[0][0] __________________________________________________________________________________________________ conv4_block6_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block6_1_conv[0][0] __________________________________________________________________________________________________ conv4_block6_1_relu (Activation (None, 14, 14, 256) 0 conv4_block6_1_bn[0][0] __________________________________________________________________________________________________ conv4_block6_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block6_1_relu[0][0] __________________________________________________________________________________________________ conv4_block6_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block6_2_conv[0][0] __________________________________________________________________________________________________ conv4_block6_2_relu (Activation (None, 14, 14, 256) 0 conv4_block6_2_bn[0][0] __________________________________________________________________________________________________ conv4_block6_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block6_2_relu[0][0] __________________________________________________________________________________________________ conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block6_3_conv[0][0] __________________________________________________________________________________________________ conv4_block6_add (Add) (None, 14, 14, 1024) 0 conv4_block5_out[0][0] conv4_block6_3_bn[0][0] __________________________________________________________________________________________________ conv4_block6_out (Activation) (None, 14, 14, 1024) 0 conv4_block6_add[0][0] __________________________________________________________________________________________________ conv5_block1_1_conv (Conv2D) (None, 7, 7, 512) 524800 conv4_block6_out[0][0] __________________________________________________________________________________________________ conv5_block1_1_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block1_1_conv[0][0] __________________________________________________________________________________________________ conv5_block1_1_relu (Activation (None, 7, 7, 512) 0 conv5_block1_1_bn[0][0] __________________________________________________________________________________________________ conv5_block1_2_conv (Conv2D) (None, 7, 7, 512) 2359808 conv5_block1_1_relu[0][0] __________________________________________________________________________________________________ conv5_block1_2_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block1_2_conv[0][0] __________________________________________________________________________________________________ conv5_block1_2_relu (Activation (None, 7, 7, 512) 0 conv5_block1_2_bn[0][0] __________________________________________________________________________________________________ conv5_block1_0_conv (Conv2D) (None, 7, 7, 2048) 2099200 conv4_block6_out[0][0] __________________________________________________________________________________________________ conv5_block1_3_conv (Conv2D) (None, 7, 7, 2048) 1050624 conv5_block1_2_relu[0][0] __________________________________________________________________________________________________ conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048) 8192 conv5_block1_0_conv[0][0] __________________________________________________________________________________________________ conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048) 8192 conv5_block1_3_conv[0][0] __________________________________________________________________________________________________ conv5_block1_add (Add) (None, 7, 7, 2048) 0 conv5_block1_0_bn[0][0] conv5_block1_3_bn[0][0] __________________________________________________________________________________________________ conv5_block1_out (Activation) (None, 7, 7, 2048) 0 conv5_block1_add[0][0] __________________________________________________________________________________________________ conv5_block2_1_conv (Conv2D) (None, 7, 7, 512) 1049088 conv5_block1_out[0][0] __________________________________________________________________________________________________ conv5_block2_1_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block2_1_conv[0][0] __________________________________________________________________________________________________ conv5_block2_1_relu (Activation (None, 7, 7, 512) 0 conv5_block2_1_bn[0][0] __________________________________________________________________________________________________ conv5_block2_2_conv (Conv2D) (None, 7, 7, 512) 2359808 conv5_block2_1_relu[0][0] __________________________________________________________________________________________________ conv5_block2_2_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block2_2_conv[0][0] __________________________________________________________________________________________________ conv5_block2_2_relu (Activation (None, 7, 7, 512) 0 conv5_block2_2_bn[0][0] __________________________________________________________________________________________________ conv5_block2_3_conv (Conv2D) (None, 7, 7, 2048) 1050624 conv5_block2_2_relu[0][0] __________________________________________________________________________________________________ conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048) 8192 conv5_block2_3_conv[0][0] __________________________________________________________________________________________________ conv5_block2_add (Add) (None, 7, 7, 2048) 0 conv5_block1_out[0][0] conv5_block2_3_bn[0][0] __________________________________________________________________________________________________ conv5_block2_out (Activation) (None, 7, 7, 2048) 0 conv5_block2_add[0][0] __________________________________________________________________________________________________ conv5_block3_1_conv (Conv2D) (None, 7, 7, 512) 1049088 conv5_block2_out[0][0] __________________________________________________________________________________________________ conv5_block3_1_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block3_1_conv[0][0] __________________________________________________________________________________________________ conv5_block3_1_relu (Activation (None, 7, 7, 512) 0 conv5_block3_1_bn[0][0] __________________________________________________________________________________________________ conv5_block3_2_conv (Conv2D) (None, 7, 7, 512) 2359808 conv5_block3_1_relu[0][0] __________________________________________________________________________________________________ conv5_block3_2_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block3_2_conv[0][0] __________________________________________________________________________________________________ conv5_block3_2_relu (Activation (None, 7, 7, 512) 0 conv5_block3_2_bn[0][0] __________________________________________________________________________________________________ conv5_block3_3_conv (Conv2D) (None, 7, 7, 2048) 1050624 conv5_block3_2_relu[0][0] __________________________________________________________________________________________________ conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048) 8192 conv5_block3_3_conv[0][0] __________________________________________________________________________________________________ conv5_block3_add (Add) (None, 7, 7, 2048) 0 conv5_block2_out[0][0] conv5_block3_3_bn[0][0] __________________________________________________________________________________________________ conv5_block3_out (Activation) (None, 7, 7, 2048) 0 conv5_block3_add[0][0] __________________________________________________________________________________________________ avg_pool (GlobalAveragePooling2 (None, 2048) 0 conv5_block3_out[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 1) 2049 avg_pool[0][0] ================================================================================================== Total params: 23,589,761 Trainable params: 23,536,641 Non-trainable params: 53,120 __________________________________________________________________________________________________ . ## Add checkpoints to model to save the &#39;best&#39; version of your model by comparing it to previous epochs of training weight_path=&quot;{}_my_model.best.hdf5&quot;.format(&#39;xray_class&#39;) checkpoint = ModelCheckpoint(weight_path, monitor=&#39;val_loss&#39;, verbose=1, save_best_only=True, mode=&#39;min&#39;, save_weights_only = True) early = EarlyStopping(monitor=&#39;val_loss&#39;, mode=&#39;min&#39;, patience=10) callbacks_list = [checkpoint, early] . Train model . ## train model history = my_model.fit_generator(train_gen, validation_data = (valX, valY), epochs = 20, callbacks = callbacks_list) . Epoch 1/20 143/143 [==============================] - 177s 1s/step - loss: 0.7218 - binary_accuracy: 0.5800 - val_loss: 1.7579 - val_binary_accuracy: 0.0090 Epoch 00001: val_loss improved from inf to 1.75792, saving model to xray_class_my_model.best.hdf5 Epoch 2/20 143/143 [==============================] - 136s 953ms/step - loss: 0.6603 - binary_accuracy: 0.6329 - val_loss: 0.8520 - val_binary_accuracy: 0.0120 Epoch 00002: val_loss improved from 1.75792 to 0.85198, saving model to xray_class_my_model.best.hdf5 Epoch 3/20 143/143 [==============================] - 133s 927ms/step - loss: 0.6492 - binary_accuracy: 0.6482 - val_loss: 0.6309 - val_binary_accuracy: 0.8430 Epoch 00003: val_loss improved from 0.85198 to 0.63085, saving model to xray_class_my_model.best.hdf5 Epoch 4/20 143/143 [==============================] - 133s 931ms/step - loss: 0.6207 - binary_accuracy: 0.6726 - val_loss: 0.3729 - val_binary_accuracy: 0.9910 Epoch 00004: val_loss improved from 0.63085 to 0.37292, saving model to xray_class_my_model.best.hdf5 Epoch 5/20 143/143 [==============================] - 133s 932ms/step - loss: 0.5719 - binary_accuracy: 0.7050 - val_loss: 0.7439 - val_binary_accuracy: 0.4025 Epoch 00005: val_loss did not improve from 0.37292 Epoch 6/20 143/143 [==============================] - 133s 931ms/step - loss: 0.5624 - binary_accuracy: 0.7185 - val_loss: 0.8474 - val_binary_accuracy: 0.3575 Epoch 00006: val_loss did not improve from 0.37292 Epoch 7/20 143/143 [==============================] - 133s 929ms/step - loss: 0.5092 - binary_accuracy: 0.7535 - val_loss: 0.8491 - val_binary_accuracy: 0.5005 Epoch 00007: val_loss did not improve from 0.37292 Epoch 8/20 143/143 [==============================] - 133s 932ms/step - loss: 0.5162 - binary_accuracy: 0.7513 - val_loss: 1.6125 - val_binary_accuracy: 0.0600 Epoch 00008: val_loss did not improve from 0.37292 Epoch 9/20 143/143 [==============================] - 133s 933ms/step - loss: 0.4646 - binary_accuracy: 0.7898 - val_loss: 0.3865 - val_binary_accuracy: 0.8650 Epoch 00009: val_loss did not improve from 0.37292 Epoch 10/20 143/143 [==============================] - 133s 933ms/step - loss: 0.4275 - binary_accuracy: 0.8046 - val_loss: 1.2120 - val_binary_accuracy: 0.3795 Epoch 00010: val_loss did not improve from 0.37292 Epoch 11/20 143/143 [==============================] - 134s 934ms/step - loss: 0.4122 - binary_accuracy: 0.8094 - val_loss: 0.8254 - val_binary_accuracy: 0.5480 Epoch 00011: val_loss did not improve from 0.37292 Epoch 12/20 143/143 [==============================] - 133s 932ms/step - loss: 0.3847 - binary_accuracy: 0.8291 - val_loss: 0.2506 - val_binary_accuracy: 0.9140 Epoch 00012: val_loss improved from 0.37292 to 0.25058, saving model to xray_class_my_model.best.hdf5 Epoch 13/20 143/143 [==============================] - 134s 935ms/step - loss: 0.3418 - binary_accuracy: 0.8575 - val_loss: 0.7763 - val_binary_accuracy: 0.6620 Epoch 00013: val_loss did not improve from 0.25058 Epoch 14/20 143/143 [==============================] - 133s 932ms/step - loss: 0.3291 - binary_accuracy: 0.8558 - val_loss: 0.4665 - val_binary_accuracy: 0.8085 Epoch 00014: val_loss did not improve from 0.25058 Epoch 15/20 143/143 [==============================] - 133s 933ms/step - loss: 0.3020 - binary_accuracy: 0.8728 - val_loss: 0.1557 - val_binary_accuracy: 0.9455 Epoch 00015: val_loss improved from 0.25058 to 0.15575, saving model to xray_class_my_model.best.hdf5 Epoch 16/20 143/143 [==============================] - 133s 933ms/step - loss: 0.2802 - binary_accuracy: 0.8824 - val_loss: 1.2408 - val_binary_accuracy: 0.5035 Epoch 00016: val_loss did not improve from 0.15575 Epoch 17/20 143/143 [==============================] - 133s 931ms/step - loss: 0.2636 - binary_accuracy: 0.8872 - val_loss: 0.3008 - val_binary_accuracy: 0.9000 Epoch 00017: val_loss did not improve from 0.15575 Epoch 18/20 143/143 [==============================] - 133s 932ms/step - loss: 0.2531 - binary_accuracy: 0.9003 - val_loss: 0.7074 - val_binary_accuracy: 0.7445 Epoch 00018: val_loss did not improve from 0.15575 Epoch 19/20 143/143 [==============================] - 133s 932ms/step - loss: 0.2507 - binary_accuracy: 0.9078 - val_loss: 0.4600 - val_binary_accuracy: 0.8335 Epoch 00019: val_loss did not improve from 0.15575 Epoch 20/20 143/143 [==============================] - 133s 933ms/step - loss: 0.2182 - binary_accuracy: 0.9161 - val_loss: 0.9295 - val_binary_accuracy: 0.6160 Epoch 00020: val_loss did not improve from 0.15575 . After training for some time, look at the performance of your model by plotting some performance statistics: . Note, these figures will come in handy for your FDA documentation later in the project . ## After training, make some predictions to assess model&#39;s overall performance my_model.load_weights(weight_path) pred_Y = my_model.predict(valX, batch_size = 32, verbose = True) . 2000/2000 [==============================] - 24s 12ms/step . # Plotting the history of model training: def plot_history(history): N = len(history.history[&quot;loss&quot;]) plt.style.use(&quot;ggplot&quot;) plt.figure() plt.plot(np.arange(0, N), history.history[&quot;loss&quot;], label=&quot;train_loss&quot;) plt.plot(np.arange(0, N), history.history[&quot;val_loss&quot;], label=&quot;val_loss&quot;) plt.title(&quot;Training vs Validation Loss&quot;) plt.xlabel(&quot;Epoch #&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend(loc=&quot;lower left&quot;) plt.figure() plt.plot(np.arange(0, N), history.history[&quot;binary_accuracy&quot;], label=&quot;train_acc&quot;) plt.plot(np.arange(0, N), history.history[&quot;val_binary_accuracy&quot;], label=&quot;val_acc&quot;) plt.title(&quot;Training vs Validation Accuracy&quot;) plt.xlabel(&quot;Epoch #&quot;) plt.ylabel(&quot;Accuracy&quot;) plt.legend(loc=&quot;lower left&quot;) plot_history(history) . So after trying a few different model variations I have settled for this simpler model, given the limited time for this project. This simplier model made more progress training in a shorter time, due to having fewer trainable parameters. . Dispite this, the training is still relatively unstable even after 20 epochs, as can be seen from the highly volatile validation accuracy and loss we can see in the charts above. . FOR A REAL MEDICAL APPLICATION, I WOULD NOT SUGGEST THE USE OF THIS PARTICULAR MODEL DUE TO THE POOR RESULTS . However for this demonstration project, we will continue as if we were going to make the best of this model for an application. . Rather than let the model make fixed predictions on its own assumptions, we can get the best results from our model if we look at the raw probabilities - and then determine what the best threshold value might be to decide between the classes i.e. in our case to decide between Pneumonia and No Pneumonia cases. . With this in mind, let us first look at a histogram of the distribution of predictions for our validation data. . # Look at the distribution of the prediction probabilities plt.hist(pred_Y, bins=20) . (array([1217., 235., 97., 88., 80., 60., 43., 36., 18., 16., 24., 14., 18., 13., 10., 10., 6., 5., 4., 6.]), array([1.4908544e-06, 4.7190338e-02, 9.4379187e-02, 1.4156803e-01, 1.8875688e-01, 2.3594573e-01, 2.8313458e-01, 3.3032343e-01, 3.7751228e-01, 4.2470112e-01, 4.7188997e-01, 5.1907885e-01, 5.6626767e-01, 6.1345655e-01, 6.6064537e-01, 7.0783424e-01, 7.5502306e-01, 8.0221194e-01, 8.4940076e-01, 8.9658964e-01, 9.4377846e-01], dtype=float32), &lt;a list of 20 Patch objects&gt;) . So we can see from this right-skewed distribution that most of the predicted values are between 0.0 and 0.2. This is to be expected of course because: . The majority of the samples are for the prediction 0.0 i.e. &#39;No Pneumonia&#39; | From what we saw in our exploratory data analysis, the intensity profile of the Pneumonia examples can be very difficult to distinguish from other diseases i.e. from No Pneumonia cases | . We might therefore estimate our optimum threshold value might be somewhere between 0.0-0.2. . We will now also look at some further metrics to help determine the optimial threshold value. . The project suggests the use of the roc-auc metric. However this is not a very good metric to use when we have very imbalanced classes, such as our use-case. See This article and this paper for reasons why. . Instead I believe better metric for this would be the precison-recall curve. We will however plot both of these and compare as well as an f1-threshold plot. . # Get ROC curve FPR and TPR from true labels vs score values fpr, tpr, _ = roc_curve(valY, pred_Y) # Calculate ROC Area Under the Curve (AUC) from FPR and TPR data points roc_auc = auc(fpr, tpr) # Calculate precision and recall from true labels vs score values precision, recall, thresholds = precision_recall_curve(valY, pred_Y) # Calculate f1 vs threshold scores f1_scores = [] for i in thresholds: f1 = f1_score(valY.astype(int), binarize(pred_Y,i)) f1_scores.append(f1) # Plot charts fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,5)) lw = 2 ax1.plot(fpr, tpr, color=&#39;darkorange&#39;, lw=lw, label=&#39;ROC curve (area = %0.4f)&#39; % roc_auc) ax1.plot([0, 1], [0, 1], color=&#39;navy&#39;, lw=lw, linestyle=&#39;--&#39;) ax1.set_xlabel(&#39;False Positive Rate&#39;) ax1.set_ylabel(&#39;True Positive Rate&#39;) ax1.title.set_text(&#39;ROC Curve&#39;) ax1.legend(loc=&quot;upper left&quot;) ax1.grid(True) ax2.step(recall, precision, color=&#39;orange&#39;, where=&#39;post&#39;) ax2.set_xlabel(&#39;Recall&#39;) ax2.set_ylabel(&#39;Precision&#39;) ax2.title.set_text(&#39;Precision-Recall Curve&#39;) ax2.grid(True) ax3.plot(thresholds, f1_scores, label = &#39;F1 Score&#39;) ax3.set_xlabel(&#39;Threshold&#39;) ax3.set_ylabel(&#39;F1 Score&#39;) ax3.title.set_text(&#39;F1 Score vs Threshold&#39;) ax3.legend(loc=&quot;upper left&quot;) ax3.grid(True) plt.show() . So lets us first state what in any trade off we prefer between false negatives and false positives. I would argue we would prefer to minimise false negatives over false positives - why? we are better to avoid missing any actual Pneumonia cases, even if that means we flag up more people as having Pneumonia. A healthy person being incorrectly diagnosed could get a second test or diagnosis to confirm, this is more an inconvenience. But if we fail to flag a person that actually has Pneumonia, this is far more serious. So these will be our priorities and how we define our type 1 vs type 2 errors. . Looking at the ROC curve we can see the model seems to have some skill (area above diagonal) but I am skeptical for this interpretation given the unbalanced classes and note the articles I referred to area. So Instead I would look more to the Precison-Recall curve, given we have few examples of a positive event i.e. Pneumonia, and we are less interested in the many true negatives. Here we see the curve is very low, and not far off the &#39;no skill&#39; line of our imbalanced dataset which would be around the proportion of one class to another which in our validation sample of 2000 cases was 21/1971 which is around 0.01. . So we will now explore threshold values between 0.05 to 0.2 and for each of these, observe the confusion matrix, and the precison, recall and f1 scores. Given we want to prioritise minimising false negatives, we will want to find a threshold that gives a higher value for Recall for the postive class 1.0. . for threshold in [0.05, 0.1, 0.15, 0.2]: # test 3 score thresholds which are used to determine if a class is predicted to be 0 or 1 print(&quot;threshold:&quot;, threshold) print(&quot;-&quot;) y_pred = [0 if y &lt; threshold else 1 for y in pred_Y] # from sklearn.metrics import confusion_matrix cm = confusion_matrix(valY, y_pred) # Pandas &#39;crosstab&#39; displays a better formated confusion matrix than the one in sklearn cm = pd.crosstab(pd.Series(valY), pd.Series(y_pred), rownames=[&#39;Reality&#39;], colnames=[&#39;Predicted&#39;], margins=True) print(cm) print() print(&quot;Classification report:&quot;) print(classification_report(valY, y_pred)) print() ## Minimise false negatives so highest recall . threshold: 0.05 - Predicted 0 1 All Reality 0.0 1231 751 1982 1.0 8 10 18 All 1239 761 2000 Classification report: precision recall f1-score support 0.0 0.99 0.62 0.76 1982 1.0 0.01 0.56 0.03 18 accuracy 0.62 2000 macro avg 0.50 0.59 0.40 2000 weighted avg 0.98 0.62 0.76 2000 threshold: 0.1 - Predicted 0 1 All Reality 0.0 1456 526 1982 1.0 9 9 18 All 1465 535 2000 Classification report: precision recall f1-score support 0.0 0.99 0.73 0.84 1982 1.0 0.02 0.50 0.03 18 accuracy 0.73 2000 macro avg 0.51 0.62 0.44 2000 weighted avg 0.99 0.73 0.84 2000 threshold: 0.15 - Predicted 0 1 All Reality 0.0 1553 429 1982 1.0 11 7 18 All 1564 436 2000 Classification report: precision recall f1-score support 0.0 0.99 0.78 0.88 1982 1.0 0.02 0.39 0.03 18 accuracy 0.78 2000 macro avg 0.50 0.59 0.45 2000 weighted avg 0.98 0.78 0.87 2000 threshold: 0.2 - Predicted 0 1 All Reality 0.0 1646 336 1982 1.0 13 5 18 All 1659 341 2000 Classification report: precision recall f1-score support 0.0 0.99 0.83 0.90 1982 1.0 0.01 0.28 0.03 18 accuracy 0.83 2000 macro avg 0.50 0.55 0.47 2000 weighted avg 0.98 0.83 0.90 2000 . Generally we can see that regardless of threshold value, the model struggles to do a good job classifying positive Pneumonia cases - with roughly half getting mis-classified in all cases. . We can see from the above metrics that this is a difficult threshold value to balance. While the threshold value of 0.05 gives us the highest Recall value of 0.56 for the 1.0 Pneumonia cases - and the lowest false negatives, we can see this comes at a great cost of creating 751 false positives (as seen in the confusion matrix). While we want to priortise reducing false negatives, we still care about false positives. . If we look at the next threshold value of 0.1, while it has a slightly lower recall value of 0.50 and just one more false negative, this drastically reduces the false postives from 751 down to 526 false postives. So on balance, for this model I would suggest the best threshold value, would be 0.1. . At this threshhold of 0.1, we should expect a false positive rate of 526/(526+1456) = 0.27 = 27%. . At this threshhold of 0.1, we should expect a false negative rate of 9/(9+9) = 0.5 = 50%. . Conclusion . As I noted earlier, I would not suggest this model has a good enough performance to be used in a real life medical application setting - it has been difficult to train this model to a high level of performance in such a limited time for a study project. . However that said, I have looked at how for the model I have made, how we might get the best performance from it for classifying Pneuomonia cases by finding the optimal threshold value to decide between positive and negative cases. In my judgment I have suggested that for this model, a threshold value of 0.1 gives us the best balance of results for the classifier. . ## Save model architecture to a .json: model_json = my_model.to_json() with open(&quot;my_model.json&quot;, &quot;w&quot;) as json_file: json_file.write(model_json) .",
            "url": "https://www.livingdatalab.com/health/deep-learning/2022/02/06/detect-pneumonia-chest-xrays.html",
            "relUrl": "/health/deep-learning/2022/02/06/detect-pneumonia-chest-xrays.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Python Power Tools for Data Science - Pycaret",
            "content": "Python Power Tools for Data Science . In this series of articles Python Power Tools for Data Science I will be looking at a series of python tools that can make a significant improvement on common Data Science tasks. In particular, Python Power Tools are python tools that can significantly automate or simplify common tasks a Data Scientist would need to perform. . Automation and simplifcation of common tasks can bring many benefits such as: . Less time needed to complete tasks | Reduction of mistakes due to less complex code | Improved readability and understanding of code | Increased consistancy of approach to different problems | Easier reproducability, verification, and comparison of results | . Pycaret . Pycaret is a low code python library that aims to automate many tasks required for machine learning. Tasks that would usually take hundreds of lines of code can often be replaced with just a couple of lines. It was inspired by the Caret library in R. . In comparison with the other open-source machine learning libraries, PyCaret is an alternate low-code library that can be used to replace hundreds of lines of code with few words only. This makes experiments exponentially fast and efficient. PyCaret is essentially a Python wrapper around several machine learning libraries and frameworks such as scikit-learn, XGBoost, LightGBM, CatBoost, spaCy, Optuna, Hyperopt, Ray, and many more. (Pycaret Documentation) . Pycaret has different modules specialised for different machine learning use-cases these include:- Classification- Regression . Clustering | Anomaly Detection | Natural Language Processing | Assocation Rule Mining | Time Series | . See further articles about these other Pycaret modules and what they can offer. . In this article to demonstrate the caperbilities of Pycaret we will use the classification module which has over 18 algorithms and 14 plots to analyze the results, plus many other features. . Dataset - Palmer Penguins . We will use Pycaret on the Palmer Penguins Dataset which contains size and other measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica. We will use the Pycaret classification module to train a model to predict the penguin species category. Given there are 3 species of Penguin, this would be considered a Multiclass classification problem . . # Load penguins dataset and show first few rows penguins_df = load_penguins() penguins_df.head() . species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year . 0 Adelie | Torgersen | 39.1 | 18.7 | 181.0 | 3750.0 | male | 2007 | . 1 Adelie | Torgersen | 39.5 | 17.4 | 186.0 | 3800.0 | female | 2007 | . 2 Adelie | Torgersen | 40.3 | 18.0 | 195.0 | 3250.0 | female | 2007 | . 3 Adelie | Torgersen | NaN | NaN | NaN | NaN | NaN | 2007 | . 4 Adelie | Torgersen | 36.7 | 19.3 | 193.0 | 3450.0 | female | 2007 | . # Some more info on the data penguins_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 344 entries, 0 to 343 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 species 344 non-null object 1 island 344 non-null object 2 bill_length_mm 342 non-null float64 3 bill_depth_mm 342 non-null float64 4 flipper_length_mm 342 non-null float64 5 body_mass_g 342 non-null float64 6 sex 333 non-null object 7 year 344 non-null int64 dtypes: float64(4), int64(1), object(3) memory usage: 21.6+ KB . # Percentage of penguins of each species in dataset penguins_df[&#39;species&#39;].value_counts(normalize=True) . Adelie 0.441860 Gentoo 0.360465 Chinstrap 0.197674 Name: species, dtype: float64 . . We can see that the dataset has different proportions of each penguin species. . The data consists of a mixture of numeric and categorical data, which should help us test the caperbilities of Pycaret with regards to the machine learning workflow. . Data Preparation . We will split our data into a training and test subset of our data to validate our final trained classification model on, this needs to be done without the use of Pycaret. We will ensure that our training and testing subsets have the same proportion for each penguin species as the original dataset. . # Split data into train/test and stratified on target class X = penguins_df.iloc[:,1:] Y = penguins_df[&#39;species&#39;] X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.1) train_df = X_train train_df[&#39;species&#39;] = y_train test_df = X_test test_df[&#39;species&#39;] = y_test # Verify datasets have same proportion of each penguin species as the original print(train_df.shape) print(test_df.shape) print(train_df[&#39;species&#39;].value_counts(normalize=True)) print(test_df[&#39;species&#39;].value_counts(normalize=True)) . (309, 8) (35, 8) Adelie 0.443366 Gentoo 0.359223 Chinstrap 0.197411 Name: species, dtype: float64 Adelie 0.428571 Gentoo 0.371429 Chinstrap 0.200000 Name: species, dtype: float64 . Pycaret workflow . Setup . The Pycaret setup() is the first part of the workflow that always needs to be performed, and is a function that takes our data in the form of a pandas dataframe as well as the name of the target class to predict, and performs a number of tasks to get reading for the machine learning pipeline. . # Prepare data for further processing predict_penguin_species_experiment = setup(data = train_df, target = &#39;species&#39;, session_id=123) . Description Value . 0 session_id | 123 | . 1 Target | species | . 2 Target Type | Multiclass | . 3 Label Encoded | Adelie: 0, Chinstrap: 1, Gentoo: 2 | . 4 Original Data | (309, 8) | . 5 Missing Values | True | . 6 Numeric Features | 4 | . 7 Categorical Features | 3 | . 8 Ordinal Features | False | . 9 High Cardinality Features | False | . 10 High Cardinality Method | None | . 11 Transformed Train Set | (216, 13) | . 12 Transformed Test Set | (93, 13) | . 13 Shuffle Train-Test | True | . 14 Stratify Train-Test | False | . 15 Fold Generator | StratifiedKFold | . 16 Fold Number | 10 | . 17 CPU Jobs | -1 | . 18 Use GPU | False | . 19 Log Experiment | False | . 20 Experiment Name | clf-default-name | . 21 USI | ee22 | . 22 Imputation Type | simple | . 23 Iterative Imputation Iteration | None | . 24 Numeric Imputer | mean | . 25 Iterative Imputation Numeric Model | None | . 26 Categorical Imputer | constant | . 27 Iterative Imputation Categorical Model | None | . 28 Unknown Categoricals Handling | least_frequent | . 29 Normalize | False | . 30 Normalize Method | None | . 31 Transformation | False | . 32 Transformation Method | None | . 33 PCA | False | . 34 PCA Method | None | . 35 PCA Components | None | . 36 Ignore Low Variance | False | . 37 Combine Rare Levels | False | . 38 Rare Level Threshold | None | . 39 Numeric Binning | False | . 40 Remove Outliers | False | . 41 Outliers Threshold | None | . 42 Remove Multicollinearity | False | . 43 Multicollinearity Threshold | None | . 44 Remove Perfect Collinearity | True | . 45 Clustering | False | . 46 Clustering Iteration | None | . 47 Polynomial Features | False | . 48 Polynomial Degree | None | . 49 Trignometry Features | False | . 50 Polynomial Threshold | None | . 51 Group Features | False | . 52 Feature Selection | False | . 53 Feature Selection Method | classic | . 54 Features Selection Threshold | None | . 55 Feature Interaction | False | . 56 Feature Ratio | False | . 57 Interaction Threshold | None | . 58 Fix Imbalance | False | . 59 Fix Imbalance Method | SMOTE | . Calling the setup() function with one line of code does the following in the background: . Data types will be inferred for each column | A table of key information about the dataset and configuration settings is generated | Included in this table are the names of the target categories and the numbers they will be encoded as | Based on the types inferred and configuration chosen, the dataset will be transformed to be ready for the machine learning algorithms | Split the data into training and validation (test) sets | . Various configuration settings are available, but defaults are selected so none are required. . Some key configuration settings available include: . Missing numeric values are imputed (default: mean) iterative option uses lightgbm model to estimate values | Missing categorical values are imputed (default: constant dummy value, alteratives include mode and iterative) | Encode categorical values as ordinal e.g. ‘low’, ‘medium’, ‘high’ | High cardinality (default: false) options to compress to fewer levels or replace with frequency or k-means clustering derived class. | Define date fields explictly | Ignore fields for training models | Normalise numeric fields (default: false) options include zscore, minmax, maxabs, robust | Power transforms (default: false) will transform to make data more gaussian options include yeo-johnson, quantile | PCA: Principal components analysis (default: false) reduce the dimensionality of the data down to a specified number of components | Remove outliers from training data (using SVD) | Remove features with high correlations with each other | Create cluster category based on data | Automatic feature selection (using ensemble models to identify best features) | Fix target class imbalance using SMOTE synthentic data generation or resampling | Stratify train-test split of datasetby target variable | Various cross-validation strategies for splitting data for model training | . Comparing All Models . In Pycaret we can use a single line command compare_models() to train 14 different classification models on our data with default parameters to find the best model. Each model is trained using cross-fold validation accross multiple folds (default 10) and the average metric scores for multiple classification metrics are shown, including Accuracy, F1, etc. . The results are shown in a grid, ranked by highest scoring on Accuracy by default. . # Train all classification models on data with default parameters using cross-fold validation best_model = compare_models() . Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) . ridge Ridge Classifier | 0.9955 | 0.0000 | 0.9917 | 0.9959 | 0.9952 | 0.9927 | 0.9930 | 0.013 | . lda Linear Discriminant Analysis | 0.9955 | 1.0000 | 0.9917 | 0.9959 | 0.9952 | 0.9927 | 0.9930 | 0.016 | . lr Logistic Regression | 0.9907 | 1.0000 | 0.9875 | 0.9916 | 0.9905 | 0.9852 | 0.9858 | 0.423 | . rf Random Forest Classifier | 0.9814 | 0.9988 | 0.9755 | 0.9832 | 0.9811 | 0.9706 | 0.9716 | 0.464 | . et Extra Trees Classifier | 0.9814 | 0.9987 | 0.9717 | 0.9840 | 0.9810 | 0.9701 | 0.9715 | 0.460 | . lightgbm Light Gradient Boosting Machine | 0.9766 | 0.9996 | 0.9721 | 0.9797 | 0.9765 | 0.9630 | 0.9643 | 0.090 | . gbc Gradient Boosting Classifier | 0.9721 | 0.9974 | 0.9630 | 0.9761 | 0.9708 | 0.9556 | 0.9580 | 0.250 | . dt Decision Tree Classifier | 0.9580 | 0.9685 | 0.9565 | 0.9638 | 0.9584 | 0.9353 | 0.9375 | 0.015 | . ada Ada Boost Classifier | 0.9494 | 0.9772 | 0.9356 | 0.9574 | 0.9486 | 0.9202 | 0.9241 | 0.093 | . nb Naive Bayes | 0.8333 | 0.9958 | 0.8726 | 0.9139 | 0.8388 | 0.7522 | 0.7853 | 0.016 | . knn K Neighbors Classifier | 0.7636 | 0.8905 | 0.6803 | 0.7660 | 0.7498 | 0.6143 | 0.6264 | 0.116 | . dummy Dummy Classifier | 0.4355 | 0.5000 | 0.3333 | 0.1904 | 0.2647 | 0.0000 | 0.0000 | 0.016 | . svm SVM - Linear Kernel | 0.4310 | 0.0000 | 0.3810 | 0.3575 | 0.3068 | 0.0860 | 0.1481 | 0.062 | . qda Quadratic Discriminant Analysis | 0.1758 | 0.0000 | 0.3333 | 0.0312 | 0.0529 | 0.0000 | 0.0000 | 0.018 | . We can see that the Extra Trees Classifier is the best performing model, which we would normally choose. For this example we will select a model that performs less well so has some mistakes, which will be useful later - so we will choose to use the Randon Forrest (rf) classifier. . Selecting and Fine Tuning the Model . So we will create a Random Forrest Model. When we do this, it will train the model on the training data, using cross-fold validation (default 10 folds) and show the metrics for each fold iteration. This will train our model with default parameters, so should give us the same result as we observed in the compare models process. . # Create and train the random forrest model on our data rf = create_model(&#39;rf&#39;) . Accuracy AUC Recall Prec. F1 Kappa MCC . 0 0.9545 | 1.0000 | 0.9167 | 0.9591 | 0.9525 | 0.9269 | 0.9302 | . 1 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 2 0.9545 | 0.9880 | 0.9167 | 0.9591 | 0.9525 | 0.9269 | 0.9302 | . 3 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 4 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 5 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 6 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 7 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 8 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 9 0.9048 | 1.0000 | 0.9213 | 0.9143 | 0.9058 | 0.8521 | 0.8552 | . Mean 0.9814 | 0.9988 | 0.9755 | 0.9832 | 0.9811 | 0.9706 | 0.9716 | . SD 0.0312 | 0.0036 | 0.0375 | 0.0281 | 0.0313 | 0.0489 | 0.0476 | . We can also print some details about our trained model. . # Print model summary print(rf) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=False, random_state=123, verbose=0, warm_start=False) . We can now fine tune our model to optimise parameters to get our best model using tune_model. This process uses Random Grid Search to find the best combination of parameters that produces the highest score. This will output the results of the cross-fold validation from our best model. . # Fine tune our model using Random Grid Search on parameters tuned_rf = tune_model(rf) . Accuracy AUC Recall Prec. F1 Kappa MCC . 0 0.9545 | 1.0000 | 0.9167 | 0.9591 | 0.9525 | 0.9269 | 0.9302 | . 1 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 2 0.9545 | 0.9819 | 0.9167 | 0.9591 | 0.9525 | 0.9269 | 0.9302 | . 3 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 4 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 5 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 6 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 7 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 8 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 9 0.9524 | 0.9893 | 0.9630 | 0.9619 | 0.9536 | 0.9263 | 0.9297 | . Mean 0.9861 | 0.9971 | 0.9796 | 0.9880 | 0.9859 | 0.9780 | 0.9790 | . SD 0.0212 | 0.0060 | 0.0333 | 0.0183 | 0.0216 | 0.0336 | 0.0321 | . We can observe that the grid search has improved our model Accuracy. . Model Evaluation . Once we have our best model, it&#39;s normal practice to look at the details of how its performing, what classification errors it makes, and what it gets correct.e can do this through a series of plots. The plot_model() function in Pycaret allows us to easily display a range of these plots to help with this. . A confusion matrix is a very common plot to show the details of classification predicted vs actual results which we can plot with one line. . # Plot confusion matrix plot_model(tuned_rf, plot = &#39;confusion_matrix&#39;) . We can see that our fine-tuned model only makes one mistake, predicting a penguin of class 0 as a class 2 penguin. Referring to our table from the setup() function we can see that the penguin species target class has the following number encodings: . Adelie: 0 | Chinstrap: 1 | Gentoo: 2 | . So it has predicted a Adelie penguin as a Gentoo penguin! . We can also plot a decision boundry for the model to see how it divides the parameter space to be able to classify the penguins. . # Plot model descision boundary plot_model(tuned_rf, plot=&#39;boundary&#39;) . We can see that for class 2 (Gentoo) penguins, there is a well defined decision boundry. However the decision boundry between the Adelie and Chinstrap penguins is more messy, implying its harder to distinguish between these two types of penguins. We will make a note of this for later. . We can also total up the errors in a error bar plot in Pycaret. . # Plot class prediction error bar plot plot_model(tuned_rf, plot = &#39;error&#39;) . Here we can see our one case of an Adelie penguin (blue/0) predicted as a Gentoo penguin (red/2) again. . Another common plot when trying to understand how our model works is a feature importance plot. This plot will show us the most important features for the model to be able to predict the penguin species class. . Again we can create this plot with one line of Pycaret. . # Plot feature importance plot_model(tuned_rf, plot = &#39;feature&#39;) . So it seems like bill length and flipper length are two of the most important features to help predict penguin species. . The interpret_model() function is available to use a Game Theory approach on the model predictions on training data to explain the output of the model. This is mostly based upon the python SHAP package. However this can only be used with tree-based models, which is why we deliberately chose the Random Forrest classifier earlier to be able to demonstrate this feature. . # Plot shapley values for model interpretation interpret_model(tuned_rf) . Prepare Model for Use . Once we are happy with our final model, we can prepare it for us with a range of functions. We can create our final model for deployment using the finalise_model() function, which will train the model on the entire training dataset. . # Train final model on all training data final_rf = finalize_model(tuned_rf) print(final_rf) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=&#39;balanced_subsample&#39;, criterion=&#39;entropy&#39;, max_depth=4, max_features=&#39;log2&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0002, min_impurity_split=None, min_samples_leaf=5, min_samples_split=9, min_weight_fraction_leaf=0.0, n_estimators=130, n_jobs=-1, oob_score=False, random_state=123, verbose=0, warm_start=False) . We can now test our final model on the holdout dataset we kept at the start, to get further confirmation of its performance. We can use the predict_model() function using our final model and the holdout test dataset to generate a set if predictions. . This will also automatically apply any data transformations we configured in our setup() function to this new test dataset before the data is passed to the model. . # Use holdout test dataset to generate predictions for final model new_predictions = predict_model(final_rf, data=test_df) new_predictions.head() . island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year species Label Score . 263 Biscoe | 49.8 | 15.9 | 229.0 | 5950.0 | male | 2009 | Gentoo | Gentoo | 0.9857 | . 216 Biscoe | 45.8 | 14.2 | 219.0 | 4700.0 | female | 2008 | Gentoo | Gentoo | 0.9802 | . 68 Torgersen | 35.9 | 16.6 | 190.0 | 3050.0 | female | 2008 | Adelie | Adelie | 0.9280 | . 55 Biscoe | 41.4 | 18.6 | 191.0 | 3700.0 | male | 2008 | Adelie | Adelie | 0.9251 | . 206 Biscoe | 46.5 | 14.4 | 217.0 | 4900.0 | female | 2008 | Gentoo | Gentoo | 0.9851 | . Note the predicted penguin class is in the newly created Label column. The actual penguin species is still in the original species column. We can use Pycaret&#39;s utility check_metric() function to apply a metric to our predictions, in this case we will calculate the F1 classification metric. . # Evaluate final model on test dataset predictions check_metric(new_predictions[&#39;species&#39;], new_predictions[&#39;Label&#39;], metric = &#39;F1&#39;) . 1.0 . So we can see our final model has performed exteremely well on our holdout test data, getting a perfect score of 1.0. . We can now save our final model using the save_model() function. . # Save final model (and data transformation pipeline process) save_model(final_rf,&#39;Final Penguin Model&#39;) . Transformation Pipeline and Model Successfully Saved . (Pipeline(memory=None, steps=[(&#39;dtypes&#39;, DataTypes_Auto_infer(categorical_features=[], display_types=True, features_todrop=[], id_columns=[], ml_usecase=&#39;classification&#39;, numerical_features=[], target=&#39;species&#39;, time_features=[])), (&#39;imputer&#39;, Simple_Imputer(categorical_strategy=&#39;not_available&#39;, fill_value_categorical=None, fill_value_numerical=None, numeric_stra... RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=&#39;balanced_subsample&#39;, criterion=&#39;entropy&#39;, max_depth=4, max_features=&#39;log2&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0002, min_impurity_split=None, min_samples_leaf=5, min_samples_split=9, min_weight_fraction_leaf=0.0, n_estimators=130, n_jobs=-1, oob_score=False, random_state=123, verbose=0, warm_start=False)]], verbose=False), &#39;Final Penguin Model.pkl&#39;) . Our saved model is easily re-loaded for use using the load_model() function. Note this also loads any data transformation configured as well specifed in our original setup() function. . # Load final model (and data transformation pipeline process) saved_final_rf = load_model(&#39;Final Penguin Model&#39;) . Transformation Pipeline and Model Successfully Loaded . Review . Overall, Pycaret is an incredibly useful and powerful library for speeding up and automating the machine learning pipeline and process. Lets highlight some key pros and cons. . Pros . Less code: The library really lives up to its motto of being a &#39;low code&#39; library, often one of code will replace what would normally have been an entire manually coded process of many lines of code. Accross a whole project, as we have seen in this example project, hundreds of lines of code can be replace by just a few lines. Note how most of this article length is more due to describing what the code does, than the code itself! . Easy to use: Pycaret library functions are well named, intiutive and easy to use, and easy to customise and configure. . A more consistant approach: Another benefit of being a low code library where most processes have been automated is that this ensures a more consistant approach when using Pycaret accross different projects. This is important not only for scientific reproducability, but for reducing the possibility of errors that are more likely when more custom and manual code is required to be written for a process. . Good practice: Each step of the machine learning pipeline that Pycaret simplifies and automates for you, does so in such a way to bake in best practice in Data Science. For example, when testing models cross-fold validation is done by default on all models. When evaluating models, multiple and relevant metrics are used to evaluate performance. . Performs all key tasks and more: Pycaret automates every key task in machine learning process, from wrangling to preparing your data, for selecting a model, for optimising and evaluating a final model, then testing and saving a model ready for deployment and use. In addition, Pycaret offers easy access to extra functions while not always required, can be useful for particular projects - for example the ability to calculate Shapley values as we have seen for model interpretability. . Educational: Using this library helps all kinds of users, from amateurs to professional Data Scientists, keep up to date with the latest methods and techniques. For example, Pycaret maintains a list of the most widely used models which are included automatically when selecting a potential model. For model understanding and interpretation, a wide range of plots and analyses are available. I was not fully aware for example about Shapley values, and how they can help interpret models from a very different perspective. These are some of the many advantages of having an open source library like Pycaret that&#39;s intended to automate the Data Science process, everyone&#39;s collaberative effort to use and update the library helps keep highlighting and providing some of the latest and best techniques to all who use it. . Excellent data wrangling and transformation: As we saw with the setup() function there are many useful features available to perform many common tasks that would normally require many lines of code. For example, the inclusion of the SMOTE and resampling techniques often used to correct for imbalances in the target variable in a dataset. Sensible automatic imputation methods by default to deal with missing values, and normalisation methods to scale and prepare numeric data - are key common tasks that need to be performed, expertly automated by the Pycaret library. . Quick consideration of a wide range of models: Pycaret&#39;s compare_models(), create_model() and tune_model() functions allow you to quickly compare a wide range of the best models available (currently 18), then select and optimise the best model - in just 3 lines of code. . Creating a pipeline not just a model: The machine learning process is not just about producing a good model, you also need a process to transform the data into a format required for that model. This is often consider a separate bit of extra work, often referred to as an ETL process. (Extract, Transform &amp; Load). Pycaret blends these two essential things together for you, another benefit of the automation it provides, so when you save your model, you also save this data transformation process, all together. And when you load it ready for use, you load the data transformation and the model together - ready for immediate use - a huge saving of time and effort. . These are just some of the key pros of the Pycaret library, in my opinion there are many many more. To illustrate what a huge advance and benefit the Pycaret library is in the pros highlighted, compare this to a previous machine learning project of mine to classify breast cancer data, where I used the common and more manual process of many more lines of code for each part of the machine learning pipeline. . Cons . Not good for beginners: Despite being pitched for begginners, this library may not be ideal for beginners in my opinion. While the functions are easy for a beginner to use, and indeed as highlighted you can run the entire machine learning process very easily, I would say this can be a bit deceptive and misleading. Simply running the process with little understanding what is going on underneath, is not a substitute for understanding the basics. For example when, why &amp; how should we transform data? (e.g. normalisation of numeric values) which is the most appropriate metric to interpret results? (e.g. balanced vs imbalanced target variable). . No ability to customose plots: This is perhaps a minor issue, but it would be nice to be able to customise plots at least a little for example to adjust the size of plots. . Can&#39;t easily see what is going on under the hood: In a way, this is I feel both a Pro and a Con. If you know what is going on with these automated functions underneath, then to some extent it can be nice to not be overloaded with lots of detail about it. On the other hand, for both experienced Data Scientist&#39;s and begginners it can be helpful to actually understand more of what each automated function is doing. Many functions do give some insight as to what they are doing, but many things are hidden - and can only be discovered by reading the documentation, which I would suggest is a good idea for anyone using this library, experienced or not. But again I feel this is a relatively minor con, as its a difficult balance to achieve in the trade off between simplifying and automating the process vs making every part of the process transparent. . Conclusion . In this article we have looked at Pycaret as a potential Python Power Tool for Data Science. . While it does have some minor drawbacks in my view, overall I would say Pycaret is an incredibly useful and powerful tool that helps simplify the machine learning process. I will be using Pycaret from now on in my day to day Data Science work by default - I&#39;m hugely impressed by this library and its ongoing development. . In my honest opinion, I have no doubt in declaring the Pycaret is indeed a Python Power Tool for Data Science. .",
            "url": "https://www.livingdatalab.com/python-power-tools/pycaret/2022/02/05/python-power-tools-pycaret.html",
            "relUrl": "/python-power-tools/pycaret/2022/02/05/python-power-tools-pycaret.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Python Power Tools for Data Science - Pycaret Anomaly Detection",
            "content": "Python Power Tools for Data Science . In this series of articles Python Power Tools for Data Science I will be looking at a series of python tools that can make a significant improvement on common Data Science tasks. In particular, Python Power Tools are python tools that can significantly automate or simplify common tasks a Data Scientist would need to perform. . Automation and simplifcation of common tasks can bring many benefits such as: . Less time needed to complete tasks | Reduction of mistakes due to less complex code | Improved readability and understanding of code | Increased consistancy of approach to different problems | Easier reproducability, verification, and comparison of results | . Pycaret Anomaly Detection Module . Pycaret is a low code python library that aims to automate many tasks required for machine learning. Tasks that would usually take hundreds of lines of code can often be replaced with just a couple of lines. It was inspired by the Caret library in R. . In comparison with the other open-source machine learning libraries, PyCaret is an alternate low-code library that can be used to replace hundreds of lines of code with few words only. This makes experiments exponentially fast and efficient. PyCaret is essentially a Python wrapper around several machine learning libraries and frameworks such as scikit-learn, XGBoost, LightGBM, CatBoost, spaCy, Optuna, Hyperopt, Ray, and many more. (Pycaret Documentation) . Pycaret has different modules specialised for different machine learning use-cases these include:- Classification- Regression . Clustering | Anomaly Detection | Natural Language Processing | Assocation Rule Mining | Time Series | . See further articles about these other Pycaret modules and what they can offer. . In this article we will use the Anomaly Detection Module of Pycaret which is an unsupervised machine learning module that is used for identifying rare items, events, or observations. It has over 13 algorithms and plots to analyze the results, plus many other features. . Dataset - New York Taxi Passengers . The NYC Taxi &amp; Limousine Commission (TLC) has released public datasets that contain data for taxi trips in NYC, including timestamps, pickup &amp; drop-off locations, number of passengers, type of payment, and fare amount. . We will specifically use the data that contains the number of taxi passengers from July 2014 to January 2015 at half-hourly intervals, so this is a time series dataset. . # Download tax passenger data data = pd.read_csv(&#39;https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/nyc_taxi.csv&#39;) data[&#39;timestamp&#39;] = pd.to_datetime(data[&#39;timestamp&#39;]) # Show first few rows data.head() . timestamp value . 0 2014-07-01 00:00:00 | 10844 | . 1 2014-07-01 00:30:00 | 8127 | . 2 2014-07-01 01:00:00 | 6210 | . 3 2014-07-01 01:30:00 | 4656 | . 4 2014-07-01 02:00:00 | 3820 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; # Show last few rows data.tail() . timestamp value . 10315 2015-01-31 21:30:00 | 24670 | . 10316 2015-01-31 22:00:00 | 25721 | . 10317 2015-01-31 22:30:00 | 27309 | . 10318 2015-01-31 23:00:00 | 26591 | . 10319 2015-01-31 23:30:00 | 26288 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; # Plot dataset plt.figure(figsize=(20,10)) sns.lineplot(x = &quot;timestamp&quot;, y = &quot;value&quot;, data=data) plt.title(&#39;Number of NYC Taxi passengers by date July 2014 - January 2015&#39;) plt.show() . So we can&#39;t directly use timestamp data for anomaly detection models, we need to convert this data into other features such as day, year, hour etc before we can use it - so lets do this. . # Set timestamp to index data.set_index(&#39;timestamp&#39;, drop=True, inplace=True) # Resample timeseries to hourly data = data.resample(&#39;H&#39;).sum() # Create more features from date data[&#39;day&#39;] = [i.day for i in data.index] data[&#39;day_name&#39;] = [i.day_name() for i in data.index] data[&#39;day_of_year&#39;] = [i.dayofyear for i in data.index] data[&#39;week_of_year&#39;] = [i.weekofyear for i in data.index] data[&#39;hour&#39;] = [i.hour for i in data.index] data[&#39;is_weekday&#39;] = [i.isoweekday() for i in data.index] data.head() . value day day_name day_of_year week_of_year hour is_weekday . timestamp . 2014-07-01 00:00:00 18971 | 1 | Tuesday | 182 | 27 | 0 | 2 | . 2014-07-01 01:00:00 10866 | 1 | Tuesday | 182 | 27 | 1 | 2 | . 2014-07-01 02:00:00 6693 | 1 | Tuesday | 182 | 27 | 2 | 2 | . 2014-07-01 03:00:00 4433 | 1 | Tuesday | 182 | 27 | 3 | 2 | . 2014-07-01 04:00:00 4379 | 1 | Tuesday | 182 | 27 | 4 | 2 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Pycaret workflow . Setup . The Pycaret setup() is the first part of the workflow that always needs to be performed, and is a function that takes our data in the form of a pandas dataframe and performs a number of tasks to get reading for the machine learning pipeline. . # Setup from pycaret.anomaly import * s = setup(data, session_id = 123) . Description Value . 0 session_id | 123 | . 1 Original Data | (5160, 7) | . 2 Missing Values | False | . 3 Numeric Features | 5 | . 4 Categorical Features | 2 | . 5 Ordinal Features | False | . 6 High Cardinality Features | False | . 7 High Cardinality Method | None | . 8 Transformed Data | (5160, 19) | . 9 CPU Jobs | -1 | . 10 Use GPU | False | . 11 Log Experiment | False | . 12 Experiment Name | anomaly-default-name | . 13 USI | 5a80 | . 14 Imputation Type | simple | . 15 Iterative Imputation Iteration | None | . 16 Numeric Imputer | mean | . 17 Iterative Imputation Numeric Model | None | . 18 Categorical Imputer | mode | . 19 Iterative Imputation Categorical Model | None | . 20 Unknown Categoricals Handling | least_frequent | . 21 Normalize | False | . 22 Normalize Method | None | . 23 Transformation | False | . 24 Transformation Method | None | . 25 PCA | False | . 26 PCA Method | None | . 27 PCA Components | None | . 28 Ignore Low Variance | False | . 29 Combine Rare Levels | False | . 30 Rare Level Threshold | None | . 31 Numeric Binning | False | . 32 Remove Outliers | False | . 33 Outliers Threshold | None | . 34 Remove Multicollinearity | False | . 35 Multicollinearity Threshold | None | . 36 Remove Perfect Collinearity | False | . 37 Clustering | False | . 38 Clustering Iteration | None | . 39 Polynomial Features | False | . 40 Polynomial Degree | None | . 41 Trignometry Features | False | . 42 Polynomial Threshold | None | . 43 Group Features | False | . 44 Feature Selection | False | . 45 Feature Selection Method | classic | . 46 Features Selection Threshold | None | . 47 Feature Interaction | False | . 48 Feature Ratio | False | . 49 Interaction Threshold | None | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Calling the setup() function with one line of code does the following in the background: . Data types will be inferred for each column | A table of key information about the dataset and configuration settings is generated | Based on the types inferred and configuration chosen, the dataset will be transformed to be ready for the machine learning algorithms | . Various configuration settings are available, but defaults are selected so none are required. . Some key configuration settings available include: . Missing numeric values are imputed (default: mean) iterative option uses lightgbm model to estimate values | Missing categorical values are imputed (default: constant dummy value, alteratives include mode and iterative) | Encode categorical values as ordinal e.g. ‘low’, ‘medium’, ‘high’ | High cardinality (default: false) options to compress to fewer levels or replace with frequency or k-means clustering derived class. | Define date fields explictly | Normalise numeric fields (default: false) options include zscore, minmax, maxabs, robust | Power transforms (default: false) will transform to make data more gaussian options include yeo-johnson, quantile | PCA: Principal components analysis (default: false) reduce the dimensionality of the data down to a specified number of components | . Selecting and training a model . At time of writing this article, there are 12 different anomaly detection models available within Pycaret, which we can display with the models() function. . # Check list of available models models() . Name Reference . ID . abod Angle-base Outlier Detection | pyod.models.abod.ABOD | . cluster Clustering-Based Local Outlier | pyod.models.cblof.CBLOF | . cof Connectivity-Based Local Outlier | pyod.models.cof.COF | . iforest Isolation Forest | pyod.models.iforest.IForest | . histogram Histogram-based Outlier Detection | pyod.models.hbos.HBOS | . knn K-Nearest Neighbors Detector | pyod.models.knn.KNN | . lof Local Outlier Factor | pyod.models.lof.LOF | . svm One-class SVM detector | pyod.models.ocsvm.OCSVM | . pca Principal Component Analysis | pyod.models.pca.PCA | . mcd Minimum Covariance Determinant | pyod.models.mcd.MCD | . sod Subspace Outlier Detection | pyod.models.sod.SOD | . sos Stochastic Outlier Selection | pyod.models.sos.SOS | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We will choose to use the Isolation Forrest model. Isolation Forrest is similar to Random Forrest in that it&#39;s an algorithm based on multiple descison trees, however rather than aiming to model normal data points - Isolation Forrest explictly tries to identify anomalous data points. . There are many configuration hyperparameters for this model, which can be seen when we create and print the model details as we see below. . # Create model and print configuration hyper-parameters iforest = create_model(&#39;iforest&#39;) print(iforest) . IForest(behaviour=&#39;new&#39;, bootstrap=False, contamination=0.05, max_features=1.0, max_samples=&#39;auto&#39;, n_estimators=100, n_jobs=-1, random_state=123, verbose=0) . One of the key configuration options is contamination which is the proportion of outliers we are saying is in the data set. This is used when fitting the model to define the threshold on the scores of the samples. This is set by default to be 5% i.e. 0.05. . We will now train and assign the model to the dataset. . # Train and assign model to dataset iforest_results = assign_model(iforest) iforest_results.head() . value day day_name day_of_year week_of_year hour is_weekday Anomaly Anomaly_Score . timestamp . 2014-07-01 00:00:00 18971 | 1 | Tuesday | 182 | 27 | 0 | 2 | 0 | -0.015450 | . 2014-07-01 01:00:00 10866 | 1 | Tuesday | 182 | 27 | 1 | 2 | 0 | -0.006367 | . 2014-07-01 02:00:00 6693 | 1 | Tuesday | 182 | 27 | 2 | 2 | 0 | -0.010988 | . 2014-07-01 03:00:00 4433 | 1 | Tuesday | 182 | 27 | 3 | 2 | 0 | -0.017091 | . 2014-07-01 04:00:00 4379 | 1 | Tuesday | 182 | 27 | 4 | 2 | 0 | -0.017006 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; This adds 2 new columns to the dataset, an Anomaly column which gives a binary value if a datapoint is considered an anomaly or not, and a Anomaly_Score column which has a float value as a measure of how anomalous a datapoint is. . Model Evaluation . So lets now evaluate our model by examining the datapoints the model has labelled as anomalies. . # Show dates for first few anomalies iforest_results[iforest_results[&#39;Anomaly&#39;] == 1].head() . value day day_name day_of_year week_of_year hour is_weekday Anomaly Anomaly_Score . timestamp . 2014-07-13 50825 | 13 | Sunday | 194 | 28 | 0 | 7 | 1 | 0.002663 | . 2014-07-27 50407 | 27 | Sunday | 208 | 30 | 0 | 7 | 1 | 0.009264 | . 2014-08-03 48081 | 3 | Sunday | 215 | 31 | 0 | 7 | 1 | 0.003045 | . 2014-09-28 53589 | 28 | Sunday | 271 | 39 | 0 | 7 | 1 | 0.004440 | . 2014-10-05 48472 | 5 | Sunday | 278 | 40 | 0 | 7 | 1 | 0.000325 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; # Plot data with anomalies highlighted in red fig, ax = plt.subplots(figsize=(20,10)) # Create list of outlier_dates outliers = iforest_results[iforest_results[&#39;Anomaly&#39;] == 1] p1 = sns.scatterplot(data=outliers, x = outliers.index, y = &quot;value&quot;, ax=ax, color=&#39;r&#39;) p2 = sns.lineplot(x = iforest_results.index, y = &quot;value&quot;, data=iforest_results, color=&#39;b&#39;, ax=ax) plt.title(&#39;Number of NYC Taxi passengers by date July 2014 - January 2015: Anomalies highlighted&#39;) plt.show() . So we can see the model has labelled a few isolated points as anomalies between 2014-7 and the end of 2014. However near the end of 2014 and the start of 2015, we can see a huge number of anomalies, in particular for all of January 2015. . Let&#39;s focus in on the period from January 2015. . # Plot data with anomalies highlighted in red fig, ax = plt.subplots(figsize=(20,10)) # Focus on dates after Jan 2015 focus = iforest_results[iforest_results.index &gt; &#39;2015-01-01&#39;] # Create list of outlier_dates outliers = focus[focus[&#39;Anomaly&#39;] == 1] p1 = sns.scatterplot(data=outliers, x = outliers.index, y = &quot;value&quot;, ax=ax, color=&#39;r&#39;) p2 = sns.lineplot(x = focus.index, y = &quot;value&quot;, data=focus, color=&#39;b&#39;, ax=ax) plt.title(&#39;Number of NYC Taxi passengers by date January - Feburary 2015: Anomalies highlighted&#39;) plt.show() . So the model seems to be indicating that for all of Janurary 2015 we had a large number of highly unusual passenger number patterns. What might have been going on here? . Researching the date January 2015 in New York brings up many articles about the North American Blizzard of January 2015 : . The January 2015 North American blizzard was a powerful and severe blizzard that dumped up to 3 feet (910 mm) of snowfall in parts of New England. Originating from a disturbance just off the coast of the Northwestern United States on January 23, it initially produced a light swath of snow as it traveled southeastwards into the Midwest as an Alberta clipper on January 24–25. It gradually weakened as it moved eastwards towards the Atlantic Ocean, however, a new dominant low formed off the East Coast of the United States late on January 26, and rapidly deepened as it moved northeastwards towards southeastern New England, producing pronounced blizzard conditions. . Time lapsed satellite images from the period reveals the severe weather patterns that occured. . . Some photos from the New York area at the time of the Blizzard. . . So our model seems to have been able to detect very well this highly unusual pattern in taxi passenger behaviour caused by this Blizzard event. . Conclusion . In this article we have looked at the Pycaret Anomaly detection module as a potential Python Power Tool for Data Science. . With very little code, this module has helped us detect a well documented anomaly event even just using the default configuration. . Some key advantages of using this are: . Quick and easy to use with little code, default parameters can work well | The model library is kept up to date with the latest anomaly detection models, which can help make it easier to consider a range of different models quickly | Despite being simple and easy to use, the library has many configuration options, as well as extra funcationality such as data pre-processing, data visualisation tools, and the ability to load and save models together with the data pipleine easily | . Certrainly from this example, we can see that the Pycaret Anomaly detection module seems a great candidate as a Python Power Tool for Data Science. .",
            "url": "https://www.livingdatalab.com/python-power-tools/pycaret/2022/01/02/python-power-tools-pycaret-anomaly.html",
            "relUrl": "/python-power-tools/pycaret/2022/01/02/python-power-tools-pycaret-anomaly.html",
            "date": " • Jan 2, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Topic Modelling using Non-negative Matrix Factorization (NMF)",
            "content": "Introduction . Non-negative Matrix Factorization (NMF) is a method from Linear Algebra that is used in a wide range of applications in science and engineering, similar to Singular Value Decomopistion (SVD) which I covered in an earlier article. It can be used for tasks such as missing data imputation, audio signal processing and bioinformatics. . Topic modeling is an unsupervised machine learning technique used in Natural Language Processing (NLP) that’s capable of scanning a set of texts, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of documents. . In this article we will will use NMF to perform topic modelling. . This article is based in large part on the material from the fastai linear algebra course. . Dataset . We will use the 20 Newsgroups dataset which consists of 20,000 messages taken from 20 different newsgroups from the Usenet bulletin board service, which pre-dates the world-wide-web and websites. We will look at a subset of 4 of these newsgroup categories: . rec.motorcycles | talk.politics.mideast | sci.med | sci.crypt | . We will now get this data. . categories = [&#39;rec.motorcycles&#39;, &#39;talk.politics.mideast&#39;, &#39;sci.med&#39;, &#39;sci.crypt&#39;] remove = (&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;) newsgroups_train = fetch_20newsgroups(subset=&#39;train&#39;, categories=categories, remove=remove) newsgroups_test = fetch_20newsgroups(subset=&#39;test&#39;, categories=categories, remove=remove) . Let&#39;s check how many posts this gives us in total . newsgroups_train.filenames.shape, newsgroups_train.target.shape . ((2351,), (2351,)) . Let&#39;s print the first few lines of 3 of the posts to see what the text looks like . print(&quot; n&quot;.join(newsgroups_train.data[0].split(&quot; n&quot;)[:3])) . I am not an expert in the cryptography science, but some basic things seem evident to me, things which this Clinton Clipper do not address. . print(&quot; n&quot;.join(newsgroups_train.data[2].split(&quot; n&quot;)[:3])) . Does the Bates method work? I first heard about it in this newsgroup several years ago, and I have just got hold of a book, &#34;How to improve your sight - simple daily drills in relaxation&#34;, by Margaret D. Corbett, . print(&quot; n&quot;.join(newsgroups_train.data[5].split(&quot; n&quot;)[:3])) . Suggest McQuires #1 plastic polish. It will help somewhat but nothing will remove deep scratches without making it worse than it already is. . We can also get the newsgroup category for each from the &#39;target_names&#39; attribute . np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]] . array([&#39;sci.crypt&#39;, &#39;sci.med&#39;, &#39;sci.med&#39;], dtype=&#39;&lt;U21&#39;) . To use this text dataset for topic modelling we will need to convert this into a document-term matrix. This is a matrix where the rows will correspond to to each of the newsgroup posts (a &#39;document&#39; conceptually) and the columns will be for each of the words that exists in all posts (a &#39;term&#39; conceptually). The values of the matrix will be the count of the number of words that exists for a particular post for each post/word combination in the matrix. . . This method of converting text into a count of the words in the text matrix, without regard for anything else (such as order, context etc) is called a bag of words model. We can create this matrix using a CountVectoriser() function. . vectorizer = CountVectorizer(stop_words=&#39;english&#39;) vectors = vectorizer.fit_transform(newsgroups_train.data).todense() # (documents, vocab) vectors.shape . (2351, 32291) . We can see this matrix has the same number of rows as we have posts (2351) and we must have 32,291 unique words accross all posts which is the number of columns we have. . print(len(newsgroups_train.data), vectors.shape) . 2351 (2351, 32291) . If we print the matrix, its just an array of counts for each of the words in each post . vectors . matrix([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 2, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]]) . This matrix does not actually contain the names of the words, so it will be helpful for us to extract these as well to create a vocabulary of terms used in the matrix. We can extract these using get_feature_names() . vocab = np.array(vectorizer.get_feature_names()) vocab.shape . (32291,) . vocab[:32000] . array([&#39;00&#39;, &#39;000&#39;, &#39;0000&#39;, ..., &#39;yarn&#39;, &#39;yarvin&#39;, &#39;yashir&#39;], dtype=&#39;&lt;U79&#39;) . While we have the newsgroup categories here, we will not actually use them for our topic modelling exercise, where we want to create topics independantly based on the posts alone, but we would hope these will correspond to the newsgroup categories in some way, indeed this would be a good check that the topic modelling is working. . Now we have our Document-Term matrix and the vocabulary, we are now ready to use Singular Value Decompostion. . Non-negative Matrix Factorization (NMF) . NMF is a method of matrix decomposition, so for a given matrix A we can convert it into 2 other matrices: W and H. Also A most have non-negative values, and as such W and H will also have non-negative values. . . K is a value we choose in advance, in the case of our intention here K will repesent the number of topics we want to create for our topic model of the newsgroup posts. . So if we assume in the original matrix A for our exercise, N are the documents/posts and M are the words in our Document-Term matrix, each of these matricies represents the following: . W: Feature Matrix this has M rows for words and K columns for the topics, and indicates which words characterise which topics. | H: Coefficient Matrix this has K rows for topics, and N columns for documents/posts, and indicates which topics best describe which documents/posts. | . So one reason NMF can be more popular to use, is due to that fact that the factors it produces are always positive and so are more easily interpretable. Consider for example with SVD we could produce factors that indicated negative values for topics - what would that mean to say a text has &#39;negative indications for the topic of bikes&#39; ? . Another difference with SVD is that NMF is not an exact decompostion - which means if we multiply W and H matrices we won&#39;t get back our original matrix A exactly. . So we can peform NMF on our Document-Term matrix using the sklearn decomposition module. . # Define constants and functions m,n=vectors.shape d=10 # num topics num_top_words=8 def show_topics(a): top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]] topic_words = ([top_words(t) for t in a]) return [&#39; &#39;.join(t) for t in topic_words] . # Calculate NMF %time clf = decomposition.NMF(n_components=d, random_state=1) . CPU times: user 29 µs, sys: 0 ns, total: 29 µs Wall time: 34.3 µs . We can notice here this has run extremely fast taking just 19.6 microseconds. If we recall in an earlier article for the same dataset when we performed one of the fastest versions of SVD Randomised/Trucated SVD this took 20 seconds. . # Extract W and H matrices W1 = clf.fit_transform(vectors) H1 = clf.components_ # Show topics from H matrix print(&#39;Top 10 topics, described by top words in each topic&#39;) show_topics(H1) . Top 10 topics, described by top words in each topic . [&#39;db mov bh si cs byte al bl&#39;, &#39;people said didn know don went just like&#39;, &#39;privacy internet pub eff email information computer electronic&#39;, &#39;health 1993 use hiv medical 10 20 number&#39;, &#39;turkish jews turkey armenian jewish nazis ottoman war&#39;, &#39;anonymous anonymity posting internet anon service people users&#39;, &#39;key encryption des chip ripem use keys used&#39;, &#39;edu com cs david ca uk org john&#39;, &#39;dod rec denizens motorcycle motorcycles doom like terrible&#39;, &#39;version machines contact type edu pc comments ftp&#39;] . So if you recall our original news group categories were: . rec.motorcycles | talk.politics.mideast | sci.med | sci.crypt | . We can see that the topics discovered correspond fairly well to these, bar a few anomalies. . # Show dimensions of matrices print(W1.shape, H1.shape) . (2351, 10) (10, 32291) . The shapes of the matrices also make sense. Given our original matrix A was 2351 rows for posts and 32291 columns for words, and we requested 10 topics this NMF has returned: . Matrix W with 2351 rows for posts and 10 columns for topics | Matrix H with 10 rows for topics and 32291 columns for words | . NMF using Gradient Descent . So in the method just used, we performed NMF using a built in library function from Sklearn. One of the obvious benefits of using this is that it runs extremely fast. However, in order to create this function it took many years of research and expertise in this area. Using this function also means we are limited, if we want to do something slightly different, we can&#39;t really change it. . Alternatively, we can use a very different method to calculate the NMF matrices using Gradient Descent. . The basic process of Gradient Descent is as follows: . Randomly choose some weights to start | Loop: Use weights to calculate a prediction | Calculate the loss (loss is a measure of the difference between the prediction and what we want) | Calculate the derivative of the loss | Update the weights using this derivative to tell us how much to change them | . | Repeat step 2 lots of times. Eventually we end up with some decent weights | In our case, the weights would be the values of the matrices we want to calculate for NMF which are the values of W and H. . In Stocastic Gradient Decent (SGD) we evaluate our loss function on just a sample of our data (sometimes called a mini-batch). We would get different loss values on different samples of the data, so this is why it is stochastic. It turns out that this is still an effective way to optimize, and it&#39;s much more efficient. . SGD is also a key technique used in Deep Learning which I have covered in an earlier article. . Applying SGD to NMF . The Frobenius norm is a way to measure how different two matrices are. We can use this to calculate the loss by multipling W and H together to create a matrix, and then calculating the Frobenius norm between this matrix and our original matrix A to give us our loss value. . Goal: Decompose $A ;(m times n)$ into $$A approx WH$$ where $W ;(m times k)$ and $H ;(k times n)$, $W, ;H ;&gt;= ;0$, and we&#39;ve minimized the Frobenius norm of $A-WH$. . Approach: We will pick random positive $W$ &amp; $H$, and then use SGD to optimize. . We will also make use of the Pytorch library for these calculations for 2 key reasons: . It facilitates calculations on the GPU which enables matrix calculations to be run in parallel and therefore much faster | Pytorch has the autograd functionality which will automatically calculate the derivatives of functions for us and thereby give us the gradients that we need for the process in a convenient way | . # Define constants and functions required lam=1e6 lr = 0.05 # Create W and H matrices pW = Variable(tc.FloatTensor(m,d), requires_grad=True) pH = Variable(tc.FloatTensor(d,n), requires_grad=True) pW.data.normal_(std=0.01).abs_() pH.data.normal_(std=0.01).abs_() # Define report def report(): W,H = pW.data, pH.data print((A-pW.mm(pH)).norm(2).item(), W.min(), H.min(), (W&lt;0).sum(), (H&lt;0).sum()) # Define penalty - encourage positive and low loss values def penalty(P): return torch.pow((P&lt;0).type(tc.FloatTensor)*torch.clamp(P, max=0.), 2) # Define penalise - for both W and H matrices we want to improve def penalize(): return penalty(pW).mean() + penalty(pH).mean() # Define loss - Calculate the Frobenius norm between Matrix A and Matrices W x H def loss(): return (A-pW.mm(pH)).norm(2) + penalize()*lam # Define optimiser to update weights using gradients opt = torch.optim.Adam([pW,pH], lr=1e-3, betas=(0.9,0.9)) # Load our original matrix A onto the GPU t_vectors = torch.Tensor(v.astype(np.float32)).cuda() A = Variable(t_vectors).cuda() . Create and run the Stocastic Gradient Descent process . # For 1000 cycles for i in range(1000): # Clear the previous gradients opt.zero_grad() # Calculate the loss i.e. the Frobenius norm between Matrix A and Matrices W x H l = loss() # Calculate the gradients l.backward() # Update the values of Matrices W x H using the gradients opt.step() # Every 100 cycles print a report of progress if i % 100 == 99: report() lr *= 0.9 # learning rate annealling . 47.2258186340332 tensor(-0.0010, device=&#39;cuda:0&#39;) tensor(-0.0023, device=&#39;cuda:0&#39;) tensor(1013, device=&#39;cuda:0&#39;) tensor(42676, device=&#39;cuda:0&#39;) 46.8864631652832 tensor(-0.0008, device=&#39;cuda:0&#39;) tensor(-0.0027, device=&#39;cuda:0&#39;) tensor(1424, device=&#39;cuda:0&#39;) tensor(53463, device=&#39;cuda:0&#39;) 46.73139572143555 tensor(-0.0004, device=&#39;cuda:0&#39;) tensor(-0.0031, device=&#39;cuda:0&#39;) tensor(929, device=&#39;cuda:0&#39;) tensor(53453, device=&#39;cuda:0&#39;) 46.66544723510742 tensor(-0.0004, device=&#39;cuda:0&#39;) tensor(-0.0020, device=&#39;cuda:0&#39;) tensor(736, device=&#39;cuda:0&#39;) tensor(54012, device=&#39;cuda:0&#39;) 46.620338439941406 tensor(-0.0006, device=&#39;cuda:0&#39;) tensor(-0.0018, device=&#39;cuda:0&#39;) tensor(631, device=&#39;cuda:0&#39;) tensor(56201, device=&#39;cuda:0&#39;) 46.586158752441406 tensor(-0.0003, device=&#39;cuda:0&#39;) tensor(-0.0018, device=&#39;cuda:0&#39;) tensor(595, device=&#39;cuda:0&#39;) tensor(56632, device=&#39;cuda:0&#39;) 46.576072692871094 tensor(-0.0003, device=&#39;cuda:0&#39;) tensor(-0.0019, device=&#39;cuda:0&#39;) tensor(585, device=&#39;cuda:0&#39;) tensor(54036, device=&#39;cuda:0&#39;) 46.573974609375 tensor(-0.0003, device=&#39;cuda:0&#39;) tensor(-0.0018, device=&#39;cuda:0&#39;) tensor(578, device=&#39;cuda:0&#39;) tensor(53401, device=&#39;cuda:0&#39;) 46.573814392089844 tensor(-0.0003, device=&#39;cuda:0&#39;) tensor(-0.0017, device=&#39;cuda:0&#39;) tensor(667, device=&#39;cuda:0&#39;) tensor(52781, device=&#39;cuda:0&#39;) 46.573760986328125 tensor(-0.0003, device=&#39;cuda:0&#39;) tensor(-0.0019, device=&#39;cuda:0&#39;) tensor(662, device=&#39;cuda:0&#39;) tensor(52658, device=&#39;cuda:0&#39;) . # Show topics discovered h = pH.data.cpu().numpy() show_topics(h) . [&#39;msg don people know just food think like&#39;, &#39;clipper chip phone crypto phones government nsa secure&#39;, &#39;armenian armenians turkish genocide armenia turks turkey people&#39;, &#39;jews adam jewish land shostack das harvard arabs&#39;, &#39;com edu pgp mail faq rsa list ripem&#39;, &#39;israel israeli lebanese arab lebanon peace israelis arabs&#39;, &#39;key keys bit chip serial bits 80 number&#39;, &#39;encryption government technology law privacy enforcement administration use&#39;, &#39;geb dsl cadre chastity n3jxp pitt intellect shameful&#39;, &#39;bike bikes ride motorcycle riding dod dog good&#39;] . So if you recall our original news group categories were: . rec.motorcycles | talk.politics.mideast | sci.med | sci.crypt | . We can see that the topics discovered using SGD correspond fairly well to these, bar a few anomalies. . Comparing Approaches . If we compare our two approaches to calculating NMF. . Scikit-Learn&#39;s NMF . Fast | No parameter tuning | Relies on decades of academic research, took experts a long time to implement | Can&#39;t be customised | Method can only be applied to calculating NMF | . Using PyTorch and SGD . Took an hour to implement, didn&#39;t have to be NMF experts | Parameters were fiddly | Not as fast | Easily customised | Method can be applied to a vast range of problems | . Conclusion . In this article we introduced Non-negative Matrix Factorization (NMF) and saw how it could be applied to the task of topic modelling in NLP. We also compared two approaches to calculating NMF using Scikit-Learn&#39;s library function as well as Stocastic Gradient Descent (SGD) and highlighted various pros and cons of each approach. .",
            "url": "https://www.livingdatalab.com/mathematics/linear-algebra/natural-language-processing/2021/12/28/topic-modelling-nmf.html",
            "relUrl": "/mathematics/linear-algebra/natural-language-processing/2021/12/28/topic-modelling-nmf.html",
            "date": " • Dec 28, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Topic Modelling using Singular Value Decomposition (SVD)",
            "content": "Introduction . Singular Value Decomposition (SVD) is a method from Linear Algebra that is used in a wide range of applications in science and engineering. It can be used for tasks such as dimensionality reduction, image compression, and even understanding entanglement in quantum theory. . Topic modeling is an unsupervised machine learning technique used in Natural Language Processing (NLP) that’s capable of scanning a set of texts, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of documents. . In this article we will will use SVD to perform topic modelling. . This article is based in large part on the material from the fastai linear algebra course. . Dataset . We will use the 20 Newsgroups dataset which consists of 20,000 messages taken from 20 different newsgroups from the Usenet bulletin board service, which pre-dates the world-wide-web and websites. We will look at a subset of 4 of these newsgroup categories: . rec.motorcycles | talk.politics.mideast | sci.med | sci.crypt | . We will now get this data. . categories = [&#39;rec.motorcycles&#39;, &#39;talk.politics.mideast&#39;, &#39;sci.med&#39;, &#39;sci.crypt&#39;] remove = (&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;) newsgroups_train = fetch_20newsgroups(subset=&#39;train&#39;, categories=categories, remove=remove) newsgroups_test = fetch_20newsgroups(subset=&#39;test&#39;, categories=categories, remove=remove) . Let&#39;s check how many posts this gives us in total . newsgroups_train.filenames.shape, newsgroups_train.target.shape . ((2351,), (2351,)) . Let&#39;s print the first few lines of 3 of the posts to see what the text looks like . print(&quot; n&quot;.join(newsgroups_train.data[0].split(&quot; n&quot;)[:3])) . I am not an expert in the cryptography science, but some basic things seem evident to me, things which this Clinton Clipper do not address. . print(&quot; n&quot;.join(newsgroups_train.data[2].split(&quot; n&quot;)[:3])) . Does the Bates method work? I first heard about it in this newsgroup several years ago, and I have just got hold of a book, &#34;How to improve your sight - simple daily drills in relaxation&#34;, by Margaret D. Corbett, . print(&quot; n&quot;.join(newsgroups_train.data[5].split(&quot; n&quot;)[:3])) . Suggest McQuires #1 plastic polish. It will help somewhat but nothing will remove deep scratches without making it worse than it already is. . We can also get the newsgroup category for each from the &#39;target_names&#39; attribute . np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]] . array([&#39;sci.crypt&#39;, &#39;sci.med&#39;, &#39;sci.med&#39;], dtype=&#39;&lt;U21&#39;) . To use this text dataset for topic modelling we will need to convert this into a document-term matrix. This is a matrix where the rows will correspond to to each of the newsgroup posts (a &#39;document&#39; conceptually) and the columns will be for each of the words that exists in all posts (a &#39;term&#39; conceptually). The values of the matrix will be the count of the number of words that exists for a particular post for each post/word combination in the matrix. . . This method of converting text into a count of the words in the text matrix, without regard for anything else (such as order, context etc) is called a bag of words model. We can create this matrix using a CountVectoriser() function. . vectorizer = CountVectorizer(stop_words=&#39;english&#39;) vectors = vectorizer.fit_transform(newsgroups_train.data).todense() # (documents, vocab) vectors.shape . (2351, 32291) . We can see this matrix has the same number of rows as we have posts (2351) and we must have 32,291 unique words accross all posts which is the number of columns we have. . print(len(newsgroups_train.data), vectors.shape) . 2351 (2351, 32291) . If we print the matrix, its just an array of counts for each of the words in each post . vectors . matrix([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 2, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]]) . This matrix does not actually contain the names of the words, so it will be helpful for us to extract these as well to create a vocabulary of terms used in the matrix. We can extract these using get_feature_names() . vocab = np.array(vectorizer.get_feature_names()) vocab.shape . (32291,) . vocab[:32000] . array([&#39;00&#39;, &#39;000&#39;, &#39;0000&#39;, ..., &#39;yarn&#39;, &#39;yarvin&#39;, &#39;yashir&#39;], dtype=&#39;&lt;U79&#39;) . While we have the newsgroup categories here, we will not actually use them for our topic modelling exercise, where we want to create topics independantly based on the posts alone, but we would hope these will correspond to the newsgroup categories in some way, indeed this would be a good check that the topic modelling is working. . Now we have our Document-Term matrix and the vocabulary, we are now ready to use Singular Value Decompostion. . Singular Value Decomposition (SVD) . SVD is a method of matrix decomposition, so for a given matrix A we can convert it into 3 other matrices: U, $ sum_{}$, and $V^{T}$ . . R is a value we choose in advance, in the case of our intention here R will repesent the number of topics we want to create for our topic model of the newsgroup posts. . Each of these matricies represents the following . U: Left singular vectors this has the same number of rows as our original matrix A (m rows/posts) and a column for each of our chosen number of topics (r columns). This matrix has orthogonal (or orthonormal) columns i.e. vectors along the r topics column axis. | $ sum_{}$: Singular values has r rows by r columns, in our case this means topics by topics. This represents the ranked relative importance of each topic so the most important topic is topic 1 which is in row 1, column 1 - and the value at this index will be a measure of the importance, and so on for topic 2 etc. This is a matrix of diagonal singular values (all other values off the diagonal are zero). | $V^{T}$: Right singular vectors this has the same number of columns as our original matrix A (n columns) and a row for each of our chosen number of topics (r rows) | . If we were to choose a R value equal to N this would be an exact decompostion of the matrix A, which would mean if we were to multiply U, $ sum_{}$, and $V^{T}$ we would get back exactly the same matrix A. . However there are many reasons why in practice we may not want to do a full decompostion, including in the case of large matricies this can be extermely time consuming, and often we may not require all potential topics, just the most important. So in practice we are likely to choose a value for R that is far smaller than N. . Latent Semantic Analysis (LSA) or Latent Semantic Index (LSI) is a common name given to applying SVD to topic modelling in NLP in this way i.e. using a Document-Term matrix. . Another way to think about SVD more generally is that whatever is represented by a matrix A by columns M and N, is mapped into a &#39;latent space&#39; defined by the R dimension. Futhermore, this mapping is done in such a way that co-occuring values of N are projected into the same R dimensions with higher values, and conversley non-couccuring values on N are projected into different R dimensions. . In other words, the latent space R dimensions allow us to show which M are similar or different based on their values of N. . So we can peform full SVD on our Document-Term matrix using the scipy linalg module. . %time U, s, Vh = linalg.svd(vectors, full_matrices=False) . CPU times: user 1min 55s, sys: 5.34 s, total: 2min Wall time: 1min 2s . print(U.shape, s.shape, Vh.shape) . (2351, 2351) (2351,) (2351, 32291) . This has performed a full SVD, and took around 2 mins. . We can test that this is a decomposition by multipling these matrices and checking if they are close to equal to the original matrix using the allclose() function from numpy. . # Confirm that U, s, Vh is a decomposition of the var Vectors # Multiply matrices reconstructed_vectors = U @ np.diag(s) @ Vh # Calculate the Frobenius norm between the original matrix A and this reconstructed one - which is a measure of the distance/differences between these matrices np.linalg.norm(reconstructed_vectors - vectors) . 4.063801905115974e-12 . # Check if two matrices are approximately equal within a small difference np.allclose(reconstructed_vectors, vectors) . True . We can also check that U and Vh are orthonormal matrices. If we multiply these by their transpose this should be close to equal to the identity matrix for each of these (by definition).. . # Confirm that U, Vh are orthonormal np.allclose(U.T @ U, np.eye(U.shape[0])) np.allclose(Vh @ Vh.T, np.eye(Vh.shape[0])) . True . If we look at the singular values matrix, we can get an idea of the relative importance of each of the topics (topics on x axis) . plt.plot(s) plt.xlabel(&#39;Topic number&#39;) plt.ylabel(&#39;Importance&#39;) . Text(0, 0.5, &#39;Importance&#39;) . Let&#39;s have a look at the topics discovered by SVD, we will do this by looking at the top 8 words that score most highly for each topic. This will be orderded by most important topic first. . num_top_words=8 def show_topics(a): top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]] topic_words = ([top_words(t) for t in a]) return [&#39; &#39;.join(t) for t in topic_words] print(&#39;Top 10 topics, described by top words in each topic&#39;) show_topics(Vh[:10]) . Top 10 topics, described by top words in each topic . [&#39;melittin wimp disgruntled rebelling toxin sorta bikeless litte&#39;, &#39;db mov bh si bl di maxbyte cx&#39;, &#39;said didn people know don went apartment came&#39;, &#39;health 1993 hiv medical use 10 number 20&#39;, &#39;edu com anonymous health posting anon service cs&#39;, &#39;key privacy eff pub encryption use law health&#39;, &#39;internet email privacy anonymous anonymity health eff hiv&#39;, &#39;anonymous posting anonymity use anon key users postings&#39;, &#39;com edu encryption privacy government said chip technology&#39;, &#39;version machines contact type pc comments ftp keyboard&#39;] . So if you recall our original news group categories were: . rec.motorcycles | talk.politics.mideast | sci.med | sci.crypt | . We can see that the topics discovered correspond fairly well to these, bar a few anomalies. . Truncated SVD . So we saw from our attempt at full SVD was quite slow to calculate (approx 2 mins) we can imagine this is likely to get far worse with bigger matrices. We also know that perhaps we don&#39;t need to calculate a full set of topics, especially given for most practical applications we are most likely interested in using the strongest topics that distinguish posts, rather than topics that are not very useful. The approaches to calculate full SVD use particular algorithms to create the decomposition, and Halko et al highlighted some of the key disadvantages of this approach: . Matrices are &quot;stupendously big&quot; | Data are often missing or inaccurate. Why spend extra computational resources when imprecision of input limits precision of the output? | Data transfer now plays a major role in time of algorithms. Techniques the require fewer passes over the data may be substantially faster, even if they require more flops (flops = floating point operations). | Important to take advantage of GPUs. | . In the same paper, Halko et al argued for the advantages of using randomised approaches which include: . They are inherently stable | Performance guarantees do not depend on subtle spectral properties | Needed matrix-vector products can be done in parallel i.e. on a GPU | . So Truncated SVD using a randomised approach, allows us to calculate just the largest singular values and the corresponding matrices, which should be much quicker to calculate. . We can use sklearn&#39;s decomposition module to calculated randomised SVD, we will specify the top 10 topics only. . %time u, s, v = decomposition.randomized_svd(vectors, 10) . CPU times: user 18.7 s, sys: 2.09 s, total: 20.8 s Wall time: 15.2 s . Lets see the top 10 topics its discovered. . show_topics(v) . [&#39;db mov bh si cs byte al bl&#39;, &#39;people said know don didn anonymous privacy internet&#39;, &#39;privacy internet anonymous information pub email eff use&#39;, &#39;health 1993 hiv medical use 10 number 20&#39;, &#39;turkish jews turkey key privacy government armenian eff&#39;, &#39;turkish edu jews com turkey anonymous jewish nazis&#39;, &#39;key edu encryption des com ripem chip keys&#39;, &#39;com edu pub eff ftp electronic org computer&#39;, &#39;dod rec denizens motorcycle motorcycles doom ftp terrible&#39;, &#39;version machines contact type pc comments ftp keyboard&#39;] . So this is much faster taking a total of 20 seconds for randomised SVD compared to the full SVD of 2 minutes. . Facebook Research implemented a version of Randomised SVD based on the Halko paper. . Conclusion . In this article we introduced Singular Value Decomposition (SVD) and saw how it could be applied to the task of topic modelling in NLP. We also saw how this could be optimised for speed when only concerned with the most important topics, using truncated SVD implemented using a randomised approach. .",
            "url": "https://www.livingdatalab.com/mathematics/linear-algebra/natural-language-processing/2021/12/27/topic-modelling-svd.html",
            "relUrl": "/mathematics/linear-algebra/natural-language-processing/2021/12/27/topic-modelling-svd.html",
            "date": " • Dec 27, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Python Power Tools for Data Science - Pandas Profilling",
            "content": "Python Power Tools for Data Science . In this series of articles Python Power Tools for Data Science I will be looking at a series of python tools that can make a significant improvement on common Data Science tasks. In particular, Python Power Tools are python tools that can significantly automate or simplify common tasks a Data Scientist would need to perform. . Automation and simplifcation of common tasks can bring many benefits such as: . Less time needed to complete tasks | Reduction of mistakes due to less complex code | Improved readability and understanding of code | Increased consistancy of approach to different problems | Easier reproducability, verification, and comparison of results | . Pandas Profilling . Pandas Profilling is a python library that takes a Pandas dataframe and can perform Exploratory Data Analysis (EDA) automatically on it with just one line of code. In this article we will apply this library to a test dataset and then evaluate how good it is to helping with the task of EDA. . Dataset - Palmer Penguins . We will use Pandas Profilling on the Palmer Penguins Dataset which contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica. . . # Load penguins dataset and show first few rows penguins_df = load_penguins() penguins_df.head() . species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year . 0 Adelie | Torgersen | 39.1 | 18.7 | 181.0 | 3750.0 | male | 2007 | . 1 Adelie | Torgersen | 39.5 | 17.4 | 186.0 | 3800.0 | female | 2007 | . 2 Adelie | Torgersen | 40.3 | 18.0 | 195.0 | 3250.0 | female | 2007 | . 3 Adelie | Torgersen | NaN | NaN | NaN | NaN | NaN | 2007 | . 4 Adelie | Torgersen | 36.7 | 19.3 | 193.0 | 3450.0 | female | 2007 | . . The data consists of a mixture of numeric and categorical data, which should help us test the caperbilities of Pandas Profilling with regards to EDA. . Pandas Profilling renders the EDA as an HTML report, which has been included in this article. Note due to article formatting, the width is restricted which can make the report a bit cramped. See a wider version of this Pandas Profilling report on the Palmer Penguins here. . report = ProfileReport(penguins_df, title=&quot;Pandas Profiling Report - Palmer Penguins&quot;, explorative=True, html={&quot;style&quot;: {&quot;full_width&quot;: True}}) report . . Pandas Profilling Report Overview . For each column the following statistics - if relevant for the column type - are presented in the interactive HTML report: . Type inference: detect the types of columns in a dataframe. | Essentials: type, unique values, missing values | Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range | Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness | Most frequent values | Histograms | Correlations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices | Missing values matrix, count, heatmap and dendrogram of missing values | Duplicate rows Lists the most occurring duplicate rows | Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data | . The report itself is structured into 6 sections, which can be navigated from a top menu: . Overview: Summary stats, Alerts for notable missing values and high correlations, and report run details | Variables: For each variable - distinct values, missing values, bar chart (categorical), histogram/mean/min/max/zeros/negative (numeric) | Interactions: Scatterplots between all numeric variables | Correlations: Heatmap correlation plots between numeric &amp; categorical variables | Missing values: Various plots to highlight missing values | Sample: The first and last 10 rows from the dataset with all columns | . The report is also highly configurable allowing a large range of customisations. . Here are a few more examples of pandas profilling applied to other datasets . Review . Pandas Profilling certainly generates a lot of useful information, and very efficiently from simply one line of code. However, it does have some downsides as well for performing EDA on a dataset. Lets highlight some key pros and cons. . Pros . Simple to use: The report can be generated with one line of code, and uses a pandas dataframe which is the most common format for python data analysis. . Excellent summary stats: The summary stats of the overall dataset, such as variable types, missing values etc are exactly the kind of key information needed to get an overall view of the dataset for EDA. . Excellent univariate analysis: Univariate analysis is a key first step in EDA, and this report does this very well for a range of different variable types. . Some multi-variate analysis: The scatterplots for numeric variables and correlations for all variables provide some useful insights. . Nice format: The report layout is neat, modern, clean and spacious, making it easy to read. . Highly configurable: The report is highly customisable, which means it can be focussed on whats most important from the options available. . Good output formats: There are some great output formats for the report, such as inside a jupyter notebook, or exported as an html file. The report is highly interactive. . Cons . Difficult to use with large datasets: Because of the calculations done by default accross all variables, for large datasets the profilling report can be slow to run. Two possible approaches to deal with this are: . Running the report on a smaller randomly sampled subset of the data | Customising the report to reduce the number of sections &amp; calculations done in the report. | . Inadequate for multi-variate analysis: While for univariate analysis the report does an excellent job, for multi-variate analysis this report and the few plots and calculations provided e.g. scatterplots and correlation matrices, is unlikely to be sufficient for an EDA on most datasets to provide the depth of understanding required. . Lets consider looking at flipper length. Pandas Profilling provides a simple histogram: . . This gives us a sense of a wide distribution, with multiple peaks, but not much else. . In multi-variate analysis, its common to colour plots of numeric variables by a categorical variable to gain more insight. . In the original study, histograms for each penguin type are plotted separately with a unique colour, then overlaid with transparency: . . This customised plot gives us more insight, for example we can see different penguin types have disntinct ranges of values for flipper length. We can see for example that the Adelie and Chinstrap penguins generally have lower distribution of values for flipper lengths than the Gentoo penguins. . Let&#39;s also consider for example one the scatterplots generated in the report for body mass v flipper length: . . We can see there is a linear relationship between the two variables, but we can&#39;t see much else. . In the original study, a custom scatterplot of these variables was created, which also coloured the points by the categorical variable for the penguin type: . . This customised plot again gives us far more insight than we get from pandas profilling. We can see for example not only is there a linear relationship, but we can also see different penguin types have disntinct ranges of values for these variables. We can see for example that the Adelie and Chinstrap penguins generally have both lower body mass and flipper lengths than the Gentoo penguins. . It is perhaps not surprising the report falls short for multi-variate analysis, the number of potential combinations of variables and plot types grows exponentially, and what is important ot focus on often depends on domain and context knowledge specific to a dataset. For example, it would not be possible for Pandas Profilling to know that Penguin Type would be a good categorical variable to colour the scatter plots and histograms by. . Conclusion . In this article we have looked at Pandas Profilling as a potential Python Power Tool for Data Science. . While it does have some drawbacks, for example the limitations of use on large datasets, and that it is unlikely to be sufficient for a complete multi-variate analysis on a dataset - it still automatically performs many of the common tasks a Data Scientist would need to do for EDA on a dataset - and so I would argue Pandas Profilling is an exteremly useful Python Power Tool for Data Science. .",
            "url": "https://www.livingdatalab.com/python-power-tools/2021/11/20/python-power-tools-pandas-profilling.html",
            "relUrl": "/python-power-tools/2021/11/20/python-power-tools-pandas-profilling.html",
            "date": " • Nov 20, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Network Analysis Fundamentals - An Analysis of Zacharys Karate Club",
            "content": "Introduction . In this study we will introduce network analysis, and apply it to understanding the structure and functioning of a karate club. . What is Network Analysis? . So we will define a network as a group of objects and a set of relationships between them. The mathematical term for this is a Graph. This could represent a range of different things, such as a group of people, electrical circuits, the flight pattens of aeroplanes, a set of bridges or roads in a city, or biological networks. . Network Analysis helps us better understand the structure, relationships and functioning of a network. . This Study - Zachary&#39;s karate club . Zachary&#39;s karate club is a well known benchmark dataset in Network analysis. . The dataset is a network of friendships between the 34 members of a karate club at a US university, as described by Wayne Zachary in 1977. This was first used in the paper W. W. Zachary, An information flow model for conflict and fission in small groups, Journal of Anthropological Research 33, 452-473 (1977) . Network Fundamentials . Nodes and Edges . Before looking at our data lets first define some basic terms used to describe networks. Nodes (also called vertices) are the objects of the network, so in a network of people each node would represent a person. Edges (also called links) are the connections between nodes, so in a network of people each edge would represent a relationship or connection between two people. . . Our dataset is represented as a list of nodes and a list of edges. We will use the NetworkX python library for dealing with networks. . Lets load our Karate dataset and print some basic stats about it. . G = nx.karate_club_graph() # Print summary print(nx.info(G)) . Graph named &#34;Zachary&#39;s Karate Club&#34; with 34 nodes and 78 edges . The 34 nodes represent the members of the karate club, and the edges describes which people know each other i.e. the relationships that exist between different people. . So we have some very basic information here about our Graph already, i.e. the number of nodes and edges. . Attributes . Currently our Network is a set of people and the relationships that exist between them. But we can also add extra infromation about each person i.e. add extra information to each Node, these are called Attributes. . Lets see what attributes the nodes of our Karate network have. . # Print node attributes for all nodes for nodex in G.nodes(data=True): for b in (nodex[1]): print(b, &quot; &quot;) . club club club club club club club club club club club club club club club club club club club club club club club club club club club club club club club club club club . So we see to have just one attribute for all our nodes called &#39;club&#39;. Lets see what the values are for these for all our nodes. . for n in G.nodes(): print(n, G.nodes[n][&#39;club&#39;]) . 0 Mr. Hi 1 Mr. Hi 2 Mr. Hi 3 Mr. Hi 4 Mr. Hi 5 Mr. Hi 6 Mr. Hi 7 Mr. Hi 8 Mr. Hi 9 Officer 10 Mr. Hi 11 Mr. Hi 12 Mr. Hi 13 Mr. Hi 14 Officer 15 Officer 16 Mr. Hi 17 Mr. Hi 18 Officer 19 Mr. Hi 20 Officer 21 Mr. Hi 22 Officer 23 Officer 24 Officer 25 Officer 26 Officer 27 Officer 28 Officer 29 Officer 30 Officer 31 Officer 32 Officer 33 Officer . So we can see for club nodes either have a value of &#39;Officer&#39; or &#39;Mr. Hi&#39;. We will return to what these values mean later. . We can plot a very basic visualisation of the network using the Matplotlib python library. . plt.figure(figsize=(25,10)) ax = plt.gca() ax.set_title(&#39;Zacharys Karate Club - Network Circular Plot&#39;) nx.draw_circular(G,with_labels = True) . We can get a general sense that some nodes seem more connected to each other, for eample some of the nodes on the right have many more connections than most others. . Lets see if we can get more precise measurements of the properties of this network using metrics. . Network Metrics . Metrics allow us to start going beyond just nodes and edges and starting to really understand overall features that start to describe the unique charactersitics of this particular network. . As well as the number of nodes and edges, we also know we have one attribute for our nodes (club). We also assume in this case that these relationships are symmetrical i.e. if person A knows person B, then person B knows person A. This is not always the case, for example in a network of airline flights, just because there is a flight from city A to B, that does not always imply there is a reciprical flight from city B to A. Symmetrical relationship type graphs are known as undirected graphs, and non-symmetrical relationships are known as directed graphs. . These kind of properties such as the number of nodes and edges, available attributes, if the network is directed or not - determine the kind of things you can do with the network, including the types of analyses possible. For example, a network with too few nodes might be difficult to draw conclusions from, or an undirected network requires the appropriate usage of certain measures but not others. . For example, in our Karate dataset you can determine what communities people find themselves in, but you can’t determine the directional routes through which information might flow along the network (you’d need a directed network for that). By using the symmetric, undirected relationships in this case, you’ll be able to find sub-communities and the people who are important to those communities, a process that would be more difficult (though still possible) with a directed network. . NetworkX allows you to perform most analyses you might conceive, but you must understand the affordances of your dataset and realize some NetworkX algorithms are more appropriate than others. . Shape . While we got a sneak peek at the network by plotting that earlier, more complex networks can be difficult to understand by just plotting them out. Shape is a characteristic of a network we can get numerical measures for to help us understand it better in terms of overall structure for example do nodes cluster together, or are they equally spread out? Are there complex structures, or is every node arranged along a straight line? . We can plot again a basic plot of the network, but this time not in a circular layout, and lets increase the size of the nodes so we can identify each node number more clearly. . plt.figure(figsize=(25,15)) ax = plt.gca() ax.set_title(&#39;Zacharys Karate Club - Network Plot&#39;) nx.draw(G,with_labels = True, node_size=3000) . We can see that all nodes are part of one big network. Knowing how many groups or components of a network can help us focus calculations on whats most useful. . We can also observe again some nodes seem more connected than others, e.g. node 0 and node 33. Lets highlight these and plot with a circular style. . plt.figure(figsize=(25,15)) ax = plt.gca() ax.set_title(&#39;Zacharys Karate Club - Network Plot - Highlighted most connected nodes&#39;) # To plot using networkx we first need to get the positions we want for each node. circ_pos = nx.circular_layout(G) # Use the networkx draw function to easily visualise the graph nx.draw(G,circ_pos, with_labels = True, node_size=3000) #let&#39;s highlight two of the most connected nodes 0 and 33 nx.draw_networkx_nodes(G, circ_pos, nodelist=[0], node_color=&#39;g&#39;, alpha=1) nx.draw_networkx_nodes(G, circ_pos, nodelist=[33], node_color=&#39;r&#39;, alpha=1) . &lt;matplotlib.collections.PathCollection at 0x7fbcef8c6f90&gt; . We can now see what seem to be two of the most connected nodes highlighted in red and green. . However as mentioned earlier, a purely visual understanding of a network may not be accurate for large and complex networks, so numerical measures can be more useful and accurate. Quantitative metrics let you differentiate networks, learn about their topologies, and turn a jumble of nodes and edges into something you can learn from. . A good beggining metric is density which is a ratio of the actual edges in a network to all possible edges in a network. Density gives you a quick measure of how closely knit the network is. . density = nx.density(G) print(&quot;Network density:&quot;, density) . Network density: 0.13903743315508021 . The density value is 0.139, so this implies a not very dense network (on a scale from 0-1). . A shortest path measurement is a bit more complex. It calculates the shortest possible series of nodes and edges that stand between any two nodes, something hard to see in large network visualizations. This measure is essentially finding friends-of-friends—if my mother knows someone that I don’t, then mom is the shortest path between me and that person. The Six Degrees of Kevin Bacon game, is basically a game of finding shortest paths (with a path length of six or less) from Kevin Bacon to any other actor. . There are many network metrics derived from shortest path lengths. One such measure is diameter, which is the longest of all shortest paths. After calculating all shortest paths between every possible pair of nodes in the network, diameter is the length of the path between the two nodes that are furthest apart. The measure is designed to give you a sense of the network’s overall size, the distance from one end of the network to another. . Diameter uses a simple command: nx.diameter(G). However, running this command on a graph that is not full connected will give an error. . You can check this by first finding out if your Graph “is connected” (i.e. all one component) and, if not connected, finding the largest component and calculating diameter on that component alone. . print(nx.is_connected(G)) # Calculate diameter diameter = nx.diameter(G) print(&quot;Network diameter:&quot;, diameter) . True Network diameter: 5 . The network diameter is 5: there is a path length of 5 between the two farthest-apart nodes in the network. Unlike density which is scaled from 0 to 1, it is difficult to know from this number alone whether 5 is a large or small diameter. For some global metrics, it can be best to compare it to networks of similar size and shape. . The final structural calculation we will make on this network concerns the concept of triadic closure. Triadic closure supposes that if two people know the same person, they are likely to know each other. If Fox knows both Fell and Whitehead, then Fell and Whitehead may very well know each other, completing a triangle in the visualization of three edges connecting Fox, Fell, and Whitehead. The number of these enclosed triangles in the network can be used to find clusters and communities of individuals that all know each other fairly well. . One way of measuring triadic closure is called clustering coefficient because of this clustering tendency, but the structural network measure you will learn is known as transitivity. Transitivity is the ratio of all triangles over all possible triangles. A possible triangle exists when one person (Fox) knows two people (Fell and Whitehead). . So transitivity, like density, expresses how interconnected a graph is in terms of a ratio of actual over possible connections. Remember, measurements like transitivity and density concern likelihoods rather than certainties. All the outputs of the Python script must be interpreted, like any other object of research. Transitivity allows you a way of thinking about all the relationships in your graph that may exist but currently do not. . triadic_closure = nx.transitivity(G) print(&quot;Triadic closure:&quot;, triadic_closure) . Triadic closure: 0.2556818181818182 . Also like density, transitivity is scaled from 0 to 1, and you can see that the network’s transitivity is about 0.255, somewhat higher than its 0.139 density. Because the graph is not very dense, there are fewer possible triangles to begin with, which may result in slightly higher transitivity. That is, nodes that already have lots of connections are likely to be part of these enclosed triangles. To back this up, you’ll want to know more about nodes with many connections. . Centrality . Now we have some measures of the overall network i.e. measures of the shape of the network, a good next step can be to identify important nodes in the network. In network analysis, measures of the importance of nodes are referred to as centrality measures. Because there are many ways of approaching the question “Which nodes are the most important?” there are many different ways of calculating centrality. . Degree is the simplest and the most common way of finding important nodes. A node’s degree is the sum of its edges. If a node has three lines extending from it to other nodes, its degree is three. Five edges, its degree is five. It’s really that simple. Since each of those edges will always have a node on the other end, you might think of degree as the number of people to which a given person is directly connected. The nodes with the highest degree in a social network are the people who know the most people. These nodes are often referred to as hubs, and calculating degree is the quickest way of identifying hubs. . degree_dict = dict(G.degree(G.nodes())) nx.set_node_attributes(G, degree_dict, &#39;degree&#39;) . sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True) print(&quot;Top 20 nodes by degree:&quot;) for d in sorted_degree[:20]: print(d) . Top 20 nodes by degree: (33, 17) (0, 16) (32, 12) (2, 10) (1, 9) (3, 6) (31, 6) (8, 5) (13, 5) (23, 5) (5, 4) (6, 4) (7, 4) (27, 4) (29, 4) (30, 4) (4, 3) (10, 3) (19, 3) (24, 3) . Degree can tell you about the biggest hubs, but it can’t tell you that much about the rest of the nodes. And in many cases, those hubs it’s telling you about. We can see here for example this confirms our earlier intuition that nodes 33 and 0 are two of the most connected people, two of the biggest hubs. . Thankfully there are other centrality measures that can tell you about more than just hubs. Eigenvector centrality is a kind of extension of degree—it looks at a combination of a node’s edges and the edges of that node’s neighbors. Eigenvector centrality cares if you are a hub, but it also cares how many hubs you are connected to. It’s calculated as a value from 0 to 1: the closer to one, the greater the centrality. Eigenvector centrality is useful for understanding which nodes can get information to many other nodes quickly. If you know a lot of well-connected people, you could spread a message very efficiently. If you’ve used Google, then you’re already somewhat familiar with Eigenvector centrality. Their PageRank algorithm uses an extension of this formula to decide which webpages get to the top of its search results. . Betweenness centrality is a bit different from the other two measures in that it doesn’t care about the number of edges any one node or set of nodes has. Betweenness centrality looks at all the shortest paths that pass through a particular node (see above). To do this, it must first calculate every possible shortest path in your network, so keep in mind that betweenness centrality will take longer to calculate than other centrality measures (but it won’t be an issue in a dataset of this size). Betweenness centrality, which is also expressed on a scale of 0 to 1, is fairly good at finding nodes that connect two otherwise disparate parts of a network. If you’re the only thing connecting two clusters, every communication between those clusters has to pass through you. In contrast to a hub, this sort of node is often referred to as a broker. Betweenness centrality is not the only way of finding brokerage (and other methods are more systematic), but it’s a quick way of giving you a sense of which nodes are important not because they have lots of connections themselves but because they stand between groups, giving the network connectivity and cohesion. . These two centrality measures are even simpler to run than degree—they don’t need to be fed a list of nodes, just the graph G. You can run them with these functions: . betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality eigenvector_dict = nx.eigenvector_centrality(G) # Run eigenvector centrality # Assign each to an attribute in your network nx.set_node_attributes(G, betweenness_dict, &#39;betweenness&#39;) nx.set_node_attributes(G, eigenvector_dict, &#39;eigenvector&#39;) sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True) print(&quot;Top 20 nodes by betweenness centrality:&quot;) for b in sorted_betweenness[:20]: print(b) . Top 20 nodes by betweenness centrality: (0, 0.43763528138528146) (33, 0.30407497594997596) (32, 0.145247113997114) (2, 0.14365680615680618) (31, 0.13827561327561325) (8, 0.05592682780182781) (1, 0.053936688311688304) (13, 0.04586339586339586) (19, 0.03247504810004811) (5, 0.02998737373737374) (6, 0.029987373737373736) (27, 0.02233345358345358) (23, 0.017613636363636363) (30, 0.014411976911976909) (3, 0.011909271284271283) (25, 0.0038404882154882154) (29, 0.0029220779220779218) (24, 0.0022095959595959595) (28, 0.0017947330447330447) (9, 0.0008477633477633478) . Interestingly, nodes 33 and 0 again come up top for betweeness centrality as well. Lets rank everyone and show betweeness and degree together. . top_betweenness = sorted_betweenness[:20] #Then find and print their degree for tb in top_betweenness: # Loop through top_betweenness degree = degree_dict[tb[0]] # Use degree_dict to access a node&#39;s degree, see footnote 2 print(&quot;Person:&quot;, tb[0], &quot;| Betweenness Centrality:&quot;, tb[1], &quot;| Degree:&quot;, degree) . Person: 0 | Betweenness Centrality: 0.43763528138528146 | Degree: 16 Person: 33 | Betweenness Centrality: 0.30407497594997596 | Degree: 17 Person: 32 | Betweenness Centrality: 0.145247113997114 | Degree: 12 Person: 2 | Betweenness Centrality: 0.14365680615680618 | Degree: 10 Person: 31 | Betweenness Centrality: 0.13827561327561325 | Degree: 6 Person: 8 | Betweenness Centrality: 0.05592682780182781 | Degree: 5 Person: 1 | Betweenness Centrality: 0.053936688311688304 | Degree: 9 Person: 13 | Betweenness Centrality: 0.04586339586339586 | Degree: 5 Person: 19 | Betweenness Centrality: 0.03247504810004811 | Degree: 3 Person: 5 | Betweenness Centrality: 0.02998737373737374 | Degree: 4 Person: 6 | Betweenness Centrality: 0.029987373737373736 | Degree: 4 Person: 27 | Betweenness Centrality: 0.02233345358345358 | Degree: 4 Person: 23 | Betweenness Centrality: 0.017613636363636363 | Degree: 5 Person: 30 | Betweenness Centrality: 0.014411976911976909 | Degree: 4 Person: 3 | Betweenness Centrality: 0.011909271284271283 | Degree: 6 Person: 25 | Betweenness Centrality: 0.0038404882154882154 | Degree: 3 Person: 29 | Betweenness Centrality: 0.0029220779220779218 | Degree: 4 Person: 24 | Betweenness Centrality: 0.0022095959595959595 | Degree: 3 Person: 28 | Betweenness Centrality: 0.0017947330447330447 | Degree: 3 Person: 9 | Betweenness Centrality: 0.0008477633477633478 | Degree: 2 . This seems to confirm the importance of nodes 0 and 33, as both have the highest betweeness centrality and degree. . Community Detection . Another common thing to ask about a network dataset is what the subgroups or communities are within the larger social structure. Is your network one big, happy family where everyone knows everyone else? Or is it a collection of smaller subgroups that are only connected by one or two intermediaries? The field of community detection in networks is designed to answer these questions. There are many ways of calculating communities, cliques, and clusters in your network, but the most popular method currently is modularity. Modularity is a measure of relative density in your network: a community (called a module or modularity class) has high density relative to other nodes within its module but low density with those outside. Modularity gives you an overall score of how fractious your network is, and that score can be used to partition the network and return the individual communities. . Very dense networks are often more difficult to split into sensible partitions. Luckily, as you discovered earlier, this network is not all that dense. There aren’t nearly as many actual connections as possible connections. Its worthwhile partitioning this sparse network with modularity and seeing if the result make analytical sense. . communities = community.greedy_modularity_communities(G) modularity_dict = {} # Create a blank dictionary for i,c in enumerate(communities): # Loop through the list of communities, keeping track of the number for the community for name in c: # Loop through each person in a community modularity_dict[name] = i # Create an entry in the dictionary for the person, where the value is which group they belong to. # Now you can add modularity information like we did the other metrics nx.set_node_attributes(G, modularity_dict, &#39;modularity&#39;) . The method greedy_modularity_communities() tries to determine the number of communities appropriate for the graph, and groups all nodes into subsets based on these communities. Unlike the centrality functions, the above code will not create a dictionary. Instead it creates a list of special “frozenset” objects (similar to lists). There’s one set for each group, and the sets contain the node number of the people in each group. In order to add this information to your network in the now-familiar way, you must first create a dictionary that labels each person with a number value for the group to which they belong. . As always, you can combine these measures with others. For example, here’s how you find the highest eigenvector centrality nodes in modularity class 0 (the first one): . class0 = [n for n in G.nodes() if G.nodes[n][&#39;modularity&#39;] == 0] # Then create a dictionary of the eigenvector centralities of those nodes class0_eigenvector = {n:G.nodes[n][&#39;eigenvector&#39;] for n in class0} # Then sort that dictionary and print the first 5 results class0_sorted_by_eigenvector = sorted(class0_eigenvector.items(), key=itemgetter(1), reverse=True) print(&quot;Modularity Class 0 Sorted by Eigenvector Centrality:&quot;) for node in class0_sorted_by_eigenvector[:5]: print(&quot;Person:&quot;, node[0], &quot;| Eigenvector Centrality:&quot;, node[1]) . Modularity Class 0 Sorted by Eigenvector Centrality: Person: 33 | Eigenvector Centrality: 0.373371213013235 Person: 32 | Eigenvector Centrality: 0.3086510477336959 Person: 8 | Eigenvector Centrality: 0.2274050914716605 Person: 31 | Eigenvector Centrality: 0.19103626979791702 Person: 30 | Eigenvector Centrality: 0.17476027834493085 . Using eigenvector centrality as a ranking can give you a sense of the important people within this modularity class, so for example in this class we can see person 33 again has the highest eigenvector centrality and so this person is likely an important person within this group. . In smaller networks like this one, a common task is to find and list all of the modularity classes and their members. You can do this by looping through the communities list: . for i,c in enumerate(communities): # Loop through the list of communities print(&#39;Class &#39;+str(i)+&#39;:&#39;, list(c)) # Print out the classes and their members . Class 0: [8, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33] Class 1: [1, 2, 3, 7, 9, 12, 13, 17, 21] Class 2: [0, 16, 19, 4, 5, 6, 10, 11] . So we seem to have 3 groups, lets see who the most important people within each of these groups are. . for modularity_class in range(3): # First get a list of just the nodes in that class classN = [n for n in G.nodes() if G.nodes[n][&#39;modularity&#39;] == modularity_class] # Then create a dictionary of the eigenvector centralities of those nodes class_eigenvector = {n:G.nodes[n][&#39;eigenvector&#39;] for n in classN} # Then sort that dictionary and print the first 5 results class_sorted_by_eigenvector = sorted(class_eigenvector.items(), key=itemgetter(1), reverse=True) print(&#39; &#39;) print(&quot;Modularity Class &quot; + str(modularity_class) + &quot; Sorted by Eigenvector Centrality:&quot;) for node in class_sorted_by_eigenvector[:5]: print(&quot;Person:&quot;, node[0], &quot;| Eigenvector Centrality:&quot;, node[1]) . Modularity Class 0 Sorted by Eigenvector Centrality: Person: 33 | Eigenvector Centrality: 0.373371213013235 Person: 32 | Eigenvector Centrality: 0.3086510477336959 Person: 8 | Eigenvector Centrality: 0.2274050914716605 Person: 31 | Eigenvector Centrality: 0.19103626979791702 Person: 30 | Eigenvector Centrality: 0.17476027834493085 Modularity Class 1 Sorted by Eigenvector Centrality: Person: 2 | Eigenvector Centrality: 0.31718938996844476 Person: 1 | Eigenvector Centrality: 0.2659538704545025 Person: 13 | Eigenvector Centrality: 0.22646969838808148 Person: 3 | Eigenvector Centrality: 0.2111740783205706 Person: 7 | Eigenvector Centrality: 0.17095511498035434 Modularity Class 2 Sorted by Eigenvector Centrality: Person: 0 | Eigenvector Centrality: 0.3554834941851943 Person: 19 | Eigenvector Centrality: 0.14791134007618667 Person: 5 | Eigenvector Centrality: 0.07948057788594247 Person: 6 | Eigenvector Centrality: 0.07948057788594247 Person: 4 | Eigenvector Centrality: 0.07596645881657382 . So we seem to have 3 communities, with persons 33, 2 and 0 being the most important members of their communities. . Summary of initial findings . Having processed and reviewed an array of network metrics in Python, we now have evidence from which arguments can be made and conclusions drawn about this network of people in the Karate club. . We know, for example, that the network has relatively low density, suggesting loose associations and/or incomplete original data. We know that the community is organized around several disproportionately large hubs, in particular persons 0 and 33. . Finally we learned that the network is made of 3 distinct communities. . Each of these findings is an invitation to more research rather than an endpoint or proof. Network analysis is a set of tools for asking targeted questions about the structure of relationships within a dataset, and NetworkX provides a relatively simple interface to many of the common techniques and metrics. Networks are a useful way of extending your research into a group by providing information about community structure. . Validation against ground truth . For this network beyond the data, we actually have other information to give us insight into the nature of relations at this karate club from Zachary&#39;s research paper. During the study a conflict arose between the administrator &quot;John A&quot; and instructor &quot;Mr. Hi&quot; (pseudonyms), which led to the split of the club into two. Half of the members formed a new club around Mr. Hi; members from the other part found a new instructor or gave up karate. . In our dataset person 0 is Mr Hi, and person 33 is John A. Also the network node attribute &#39;club&#39; highlighted earlier, corresponds to the final faction each member of the club ended up becoming a member of e.g. Mr Hi is &#39;Mr Hi&#39;, and John A is &#39;Officer&#39;. . So does our network and analysis support this ground truth? Certainly our analysis has correctly identified Mr Hi and John A as key players in this group, indeed central hubs. Lets see how the idenfified 3 communities relate to each faction. . plt.figure(figsize=(25,15)) ax = plt.gca() ax.set_title(&#39;Zacharys Karate Club - Network Plot - Predicted Communities (colour) vs Actual Factions (text)&#39;) # Define communities community_0 = sorted(communities[0]) community_1 = sorted(communities[1]) community_2 = sorted(communities[2]) #Let&#39;s display the labels of which club each member ended up joining club_labels = nx.get_node_attributes(G,&#39;club&#39;) # draw each set of nodes in a seperate colour nx.draw_networkx_nodes(G,circ_pos, nodelist=community_0, node_color=&#39;g&#39;, alpha=0.5) nx.draw_networkx_nodes(G,circ_pos, nodelist=community_1, node_color=&#39;r&#39;, alpha=0.5) nx.draw_networkx_nodes(G,circ_pos, nodelist=community_2, node_color=&#39;b&#39;, alpha=0.5) # now we can add edges to the drawing nx.draw_networkx_edges(G,circ_pos, style=&#39;dashed&#39;,width = 0.2) # finally we can add labels to each node corresponding to the final club each member joined nx.draw_networkx_labels(G,circ_pos,club_labels,font_size=18) plt.show() . So here, the colour represents the predicted community from our network analysis, and the text label represents the ground truth actual faction each person joined that we know. . Firstly we can see a strong relationship between the green community and the Officer (John A) faction, in fact its almost a perfect match bar once exception at the top where one green node ends up in Mr Hi faction. Both blue and red communities seem to match perfectly with Mr Hi&#39;s faction. Lets merge the blue comminity into the red one together to see this more clearly. . plt.figure(figsize=(25,15)) ax = plt.gca() ax.set_title(&#39;Zacharys Karate Club - Network Plot - Predicted + Merged Communities (colour) vs Actual Factions (text)&#39;) combined_community = community_1 + community_2 # draw each set of nodes in a seperate colour nx.draw_networkx_nodes(G,circ_pos, nodelist=community_0, node_color=&#39;g&#39;, alpha=0.5) nx.draw_networkx_nodes(G,circ_pos, nodelist=combined_community, node_color=&#39;r&#39;, alpha=0.5) # now we can add edges to the drawing nx.draw_networkx_edges(G,circ_pos, style=&#39;dashed&#39;,width = 0.2) # finally we can add labels to each node corresponding to the final club each member joined nx.draw_networkx_labels(G,circ_pos,club_labels,font_size=18) plt.show() . So firstly we might conclude that Mr Hi&#39;s faction might consist of 2 sub-communites. Secondly, that our analysis predicts the actual factions very well making only one mistake, so with an accuracy of around 94%, based on the data of assocations within the club alone. . Conclusion . This study demonstrates the potential power of network analysis to understand real life networks and how they function. The idea that we can develop a mathmatical framework that can predict an individuals choices based off of their relationships with others is immensely powerful. We live in an interconnected world and the study of networks allows us to explore those connections. . Each of these findings is an invitation to more research rather than an endpoint or proof. Network analysis is a set of tools for asking targeted questions about the structure of relationships within a dataset, and NetworkX provides a relatively simple interface to many of the common techniques and metrics. Networks are a useful way of extending your research into a group by providing information about community structure. .",
            "url": "https://www.livingdatalab.com/network-analysis/2021/10/31/network-analysis-karate.html",
            "relUrl": "/network-analysis/2021/10/31/network-analysis-karate.html",
            "date": " • Oct 31, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Understanding CNN's with a CAM - A Class Activation Map",
            "content": "Introduction . In this article we will look at how Class Acivation Maps (CAM&#39;s) can be used to understand and interpret the decisions that Convolutional Neural Networks (CNN&#39;s) make. . CAM and Pytorch hooks . A Class Activation Map (CAM) and help us understand why Convolutional Neural Networks (CNN&#39;s) make the descisions they do. CAM&#39;s do this by looking at the outputs of the last convolutional layer just before the average pooling layer - combined with the predictions, to give a heatmap visualisation of why the model made that descision. . At each point in our final convolutional layer, we have as many channels as in the last linear layer. We can compute a dot product of those activations with the final weights to get for each location in our feature map, the score of the feature that was used to make that decision. In other words, we can identify the relationships between the parts of the network that are most active in generating the correct choice. . We can access activations inside the network using Pytorch hooks. Wheras fastai callbacks allow you to inject code into the training loop, Pytorch hooks allow you to inject code into the forward and backward calculations themselves.. . Lets see an example looking at a dataset of cats and dogs. . path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=21, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1) . Downloading: &#34;https://download.pytorch.org/models/resnet34-b627a593.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth . . /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . epoch train_loss valid_loss error_rate time . 0 | 0.138940 | 0.025390 | 0.008796 | 00:48 | . epoch train_loss valid_loss error_rate time . 0 | 0.047596 | 0.024207 | 0.007442 | 00:52 | . We can get a cat image. For CAM we want to store the activations of the last convolutional layer, lets create a hook function in a class with a state. . img = PILImage.create(image_cat()) x, = first(dls.test_dl([img])) class Hook(): def hook_func(self, m, i, o): self.stored = o.detach().clone() . We can then instantiate a hook and attach it to any layer, in this case the last layer of the CNN body. . hook_output = Hook() hook = learn.model[0].register_forward_hook(hook_output.hook_func) . Then we can grab a batch of images and feed it through our model. . with torch.no_grad(): output = learn.model.eval()(x) . Then we can extract our stored activations . act = hook_output.stored[0] . And check our predictions. . F.softmax(output, dim=-1) . tensor([[1.1078e-08, 1.0000e+00]], device=&#39;cuda:0&#39;) . So 0 means dog, but just to check. . dls.vocab . [False, True] . So the model seems quite confident the image is a cat. . To perform our dot product of the weight matrix with the activations we can use einsum. . x.shape . torch.Size([1, 3, 224, 224]) . cam_map = torch.einsum(&#39;ck,kij-&gt;cij&#39;, learn.model[1][-1].weight, act) cam_map.shape . torch.Size([2, 7, 7]) . So for each image in the batch, we get a 7x7 channel map that tells us which activations were higher or lower, which will allow us to see what parts of the image most influenced the models choice. . x_dec = TensorImage(dls.train.decode((x,))[0][0]) _,ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map[1].detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . The parts in bright yellow correspond to higher activations and purple lower activations. So we can see the paws are the main area that made the model decide it was a cat. Its good to remove a hook once used as it can leak memory. . hook.remove() . We can manage hooks better by using a class, to handle all these things automatically. . class Hook(): def __init__(self, m): self.hook = m.register_forward_hook(self.hook_func) def hook_func(self, m, i, o): self.stored = o.detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() with Hook(learn.model[0]) as hook: with torch.no_grad(): output = learn.model.eval()(x.cuda()) act = hook.stored . This Hook class is provided by fastai. This approach only works for the last layer. . Gradient CAM . The previous approach only works for the last layer, but what if we want to look at activations for earlier layers? Gradient CAM lets us do this. Normally the gradients for weights are not stored after the backward pass, but we can store them, and then pick them up with a hook. . class HookBwd(): def __init__(self, m): self.hook = m.register_backward_hook(self.hook_func) def hook_func(self, m, gi, go): self.stored = go[0].detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() . Let&#39;s try this approach on the last layer, as we did before. However we can use this approach to calculate the gradients for any layer, with respect to the output. . cls = 1 with HookBwd(learn.model[0]) as hookg: with Hook(learn.model[0]) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0,cls].backward() grad = hookg.stored . /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(&#34;Using a non-full backward hook when the forward contains multiple autograd Nodes &#34; . The weights for the Grad-CAM approach are given by the average of our gradients accross the feature/channel map. . w = grad[0].mean(dim=[1,2], keepdim=True) cam_map = (w * act[0]).sum(0) _,ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . Let&#39;s now try this on a different layer, the second to last ResNet group layer. . with HookBwd(learn.model[0][-2]) as hookg: with Hook(learn.model[0][-2]) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0,cls].backward() grad = hookg.stored w = grad[0].mean(dim=[1,2], keepdim=True) cam_map = (w * act[0]).sum(0) _,ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(&#34;Using a non-full backward hook when the forward contains multiple autograd Nodes &#34; . Conclusion . In this article we saw how we can use Class Activation Map&#39;s to understand and interpret the choices a CNN makes. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/06/19/understanding-cnn-with-cam-class-activation-maps.html",
            "relUrl": "/deep-learning-theory/2021/06/19/understanding-cnn-with-cam-class-activation-maps.html",
            "date": " • Jun 19, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Building a Neural Network from the Foundations",
            "content": "Introduction . In this article we will cover building a basic neural network from the most basic elements (arrays and Pytorch modules). We will also cover some of the key theory required for this. . This article and it&#39;s content is based on the fastai deep learning course, chapter 17. . Building a Neural Network from basic elements . Creating a neuron . A neuron takes a series of inputs, each of which is multipled by a weight, summing up all those inputs, and adding a bias - this input is then put thorugh an activation function. We could represent these as: . output = sum([x*w for x,w in zip(inputs,weights)]) + bias . def relu(x): return x if x &gt;= 0 else 0 . A deep learning model stacks many of these neurons in layers. So for the output of an entire layer, using matrices we would have: . y = x @ w.t() + b . Matrix multiplication . So we can define a function to manually do a matrix product using loops. . import torch from torch import tensor def matmul(a,b): ar,ac = a.shape # n_rows * n_cols br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): for k in range(ac): c[i,j] += a[i,k] * b[k,j] return c . However this is hugely slower than we can do using Pytorch matrix multiplciation. . Elementwise calculations . We can do element wise operations on tensors - as long as they are the same shape, for example. . a = tensor([10., 6, -4]) b = tensor([2., 8, 7]) a + b . tensor([12., 14., 3.]) . Broadcasting . Broadcasting allows 2 arrays of different sizes to be compatible for arthimetic operations, by repeating the smaller array so it matches the size of the larger one. . For example we can use unsqeeze in Pytorch to add extra dimensions explictly. . c = tensor([10.,20,30]) c.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape . (torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1])) . We can now replace our matrix multiplication with 3 loops with a broadcasting equivilent much shorter. . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): # c[i,j] = (a[i,:] * b[:,j]).sum() # previous c[i] = (a[i ].unsqueeze(-1) * b).sum(dim=0) return c . Forward and Backward passes of a Neural Network . Defining and initialising a layer . So we can define a basic linear layer in the following way. . def lin(x, w, b): return x @ w + b . Let&#39;s create some dummy data, and some simple layers. . x = torch.randn(200, 100) y = torch.randn(200) w1 = torch.randn(100,50) b1 = torch.zeros(50) w2 = torch.randn(50,1) b2 = torch.zeros(1) l1 = lin(x, w1, b1) l1.shape . torch.Size([200, 50]) . But we have a problem to do with how the parameters are initialised consider . l1.mean(), l1.std() . (tensor(-0.2733), tensor(10.1770)) . The std dev is 10, consider how if this is one layer which multiples by 10 how many layers could generate huge numbers that would be unmanagable and be a network hard to train. So we want our std dev to be close to one, and there is an equation for scaling our weights to this is so. . $1/ sqrt{n_{in}}$ . where $n_{in}$ represents the number of inputs. This is known as Xavier initialization (or Glorot initialization). . For example if we have 100 inputs, we should scale our weights by 0.1. . x = torch.randn(200, 100) for i in range(50): x = x @ (torch.randn(100,100) * 0.1) print(x[0:5,0:5]) print(x.std()) . tensor([[-0.6374, -0.3009, 0.4669, -0.7221, 0.1983], [-1.0054, 0.0244, 0.3540, -1.0580, 0.2675], [ 0.0789, 0.6670, 0.2132, 0.2511, -1.3466], [ 0.7786, -0.2874, -1.2391, 0.4132, 1.9071], [ 2.1194, 0.0046, -1.7749, 1.5797, 1.4981]]) tensor(1.1794) . Re-working our model with this in mind . x = torch.randn(200, 100) y = torch.randn(200) from math import sqrt w1 = torch.randn(100,50) / sqrt(100) b1 = torch.zeros(50) w2 = torch.randn(50,1) / sqrt(50) b2 = torch.zeros(1) l1 = lin(x, w1, b1) l1.mean(),l1.std() . (tensor(-0.0135), tensor(1.0176)) . Now we need to define an activation function. . def relu(x): return x.clamp_min(0.) l2 = relu(l1) l2.mean(),l2.std() . (tensor(0.3988), tensor(0.5892)) . So now the mean is no longer zero and our std dev is less like 1. So the Glorot method is not intended to be used with Relu and was invented before. . A newer initialisation by Kaiming He et al workes better with Relu. It&#39;s formula is: . $ sqrt{2 / n_{in}}$ . where $n_{in}$ is the number of inputs of our model. . Applying this. . x = torch.randn(200, 100) y = torch.randn(200) w1 = torch.randn(100,50) * sqrt(2 / 100) b1 = torch.zeros(50) w2 = torch.randn(50,1) * sqrt(2 / 50) b2 = torch.zeros(1) l1 = lin(x, w1, b1) l2 = relu(l1) l2.mean(), l2.std() . (tensor(0.5710), tensor(0.8222)) . Now we can define a whole model. . def model(x): l1 = lin(x, w1, b1) l2 = relu(l1) l3 = lin(l2, w2, b2) return l3 out = model(x) out.shape . torch.Size([200, 1]) . So we don&#39;t want this unit dimension. We can define a loss function and also get rid of this unit dimension. . def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() loss = mse(out, y) . Gradients and the Backwards Pass . So PyTorch computes the gradients for us with loss.backward but behind the scenes is a bit of calculus. Given the whole network is a huge function, with each part a sub-function, lets start with the final part the loss function. . We can calculate the loss with the loss function. If we take the derivative of the loss function with respect to the final weights, we can calculate the loss with respect to these weights. We can then use the chain rule to propagate these values backward, and calculate the loss with respect to every parameter in the model. . Lets define a function to calculate the gradients of the loss function with respect to the final weights. . def mse_grad(inp, targ): # grad of loss with respect to output of previous layer inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0] . Let&#39;s now define functions to calculate the gradients for the activation functions and also the linear layers. . def relu_grad(inp, out): # grad of relu with respect to input activations inp.g = (inp&gt;0).float() * out.g def lin_grad(inp, out, w, b): # grad of matmul with respect to input inp.g = out.g @ w.t() w.g = inp.t() @ out.g b.g = out.g.sum(0) . Model refactoring . Let&#39;s now put together everything: the model, the forward and backward pass methods. . class Relu(): def __call__(self, inp): self.inp = inp self.out = inp.clamp_min(0.) return self.out def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g class Lin(): def __init__(self, w, b): self.w,self.b = w,b def __call__(self, inp): self.inp = inp self.out = inp@self.w + self.b return self.out def backward(self): self.inp.g = self.out.g @ self.w.t() self.w.g = self.inp.t() @ self.out.g self.b.g = self.out.g.sum(0) class Mse(): def __call__(self, inp, targ): self.inp = inp self.targ = targ self.out = (inp.squeeze() - targ).pow(2).mean() return self.out def backward(self): x = (self.inp.squeeze()-self.targ).unsqueeze(-1) self.inp.g = 2.*x/self.targ.shape[0] class Model(): def __init__(self, w1, b1, w2, b2): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() # Create model model = Model(w1, b1, w2, b2) # Forward pass loss = model(x, y) # Backward pass model.backward() loss . tensor(2.7466) . Converting the model to Pytorch . We could build this more simply using Pytorch methods, and in fact fastai methods built on these. . class Model(Module): def __init__(self, n_in, nh, n_out): self.layers = nn.Sequential( nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)) self.loss = mse def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ) . Conclusion . In this article we have build a neural network from the most basic elements. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/06/17/neural-network-from-foundations.html",
            "relUrl": "/deep-learning-theory/2021/06/17/neural-network-from-foundations.html",
            "date": " • Jun 17, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Optimisation Methods for Deep Learning",
            "content": "Introduction . In this article we will look at methods to improve gradient decent optimisation for training neural networks beyond SGD. These include momentum, RMSProp and Adam. We will also look at the fastai library system of callbacks which make changes to the training loop easier. . This article is based on content from the fastai deep learning course, chapter 16. . Basic SGD . We will first define a baseline using basic SGD to compare how further enhancements improve results. We will use the fastai curated imagenette dataset here. . dls = get_data(URLs.IMAGENETTE_160, 160, 128) . A new version of this dataset is available, downloading... . File downloaded is broken. Remove /root/.fastai/archive/imagenette2-160.tgz and try again. . We will also create an untrained ResNet-34 architecture for our model which we will train from scratch. . def get_learner(**kwargs): return cnn_learner(dls, resnet34, pretrained=False, metrics=accuracy, **kwargs).to_fp16() learn = get_learner() learn.fit_one_cycle(3, 0.003) . epoch train_loss valid_loss accuracy time . 0 | 2.611032 | 1.885956 | 0.362293 | 00:26 | . 1 | 1.987230 | 1.666735 | 0.449172 | 00:26 | . 2 | 1.615224 | 1.509878 | 0.567134 | 00:26 | . That was with all the default settings used by fastai. Lets explicitly use just basic SGD. . learn = get_learner(opt_func=SGD) learn.lr_find() . SuggestedLRs(lr_min=0.005754399299621582, lr_steep=6.309573450380412e-07) . So we will need to use a higher learning rate than we normally use. We will also need to explictly turn momentum off, as we are here trying to illustrate just using basic SGD. . learn.fit_one_cycle(3, 0.03, moms=(0,0,0)) . epoch train_loss valid_loss accuracy time . 0 | 2.869628 | 2.315048 | 0.284586 | 00:25 | . 1 | 2.269993 | 1.699830 | 0.414522 | 00:25 | . 2 | 1.978710 | 1.616934 | 0.444841 | 00:25 | . Defining a generic optimiser . The fastai library provides a flexible approach to optimisers that makes it easier to add custom changes using optimiser callbacks. A key part of this is the Optimiser class which includes these two methods. . def zero_grad(self): for p,*_ in self.all_params(): p.grad.detach_() p.grad.zero_() def step(self): for p,pg,state,hyper in self.all_params(): for cb in self.cbs: state = _update(state, cb(p, **{**state, **hyper})) self.state[p] = state . zero_grad is handy for clearing all the gradients. Note the step method loops through other potential callbacks which is how different aspects of optimisation old and new are done. Even basic SGD is one of these callbacks. . def sgd_cb(p, lr, **kwargs): p.data.add_(-lr, p.grad.data) . We can add this as a callback like this. . opt_func = partial(Optimizer, cbs=[sgd_cb]) . Let&#39;s now train with this. . learn = get_learner(opt_func=opt_func) learn.fit(3, 0.03) . epoch train_loss valid_loss accuracy time . 0 | 2.663601 | 1.871811 | 0.344968 | 00:25 | . 1 | 2.256670 | 1.914813 | 0.354650 | 00:25 | . 2 | 1.995262 | 1.813828 | 0.442548 | 00:25 | . Momentum . So the idea of Momentum is we want to go faster in the direction we are going with gradient decent to get there sooner. We could for example use a moving average. . weight.avg = beta weight.avg + (1-beta) weight.grad . new_weight = weight - lr * weight.avg . beta helps control how much momentum to use, so if its zero there is no momentum and we have just basic SGD. But if closer to 1 then the main direction is the average of the previous steps. . High beta can help us get over small &#39;bumps&#39; in the loss landscape and keep going faster in the general direction of progress followed so far, but if too high can cause us to overshoot completly. . Beta too high means we really miss important changes in direction. . fit_one_cycle starts with a high beta of 0.95, going down to 0.85 then back up to 0.95. . Let&#39;s add momentum, by keeping track of the moving average gradient, which we can do with another callback. . def average_grad(p, mom, grad_avg=None, **kwargs): if grad_avg is None: grad_avg = torch.zeros_like(p.grad.data) return {&#39;grad_avg&#39;: grad_avg*mom + p.grad.data} def momentum_step(p, lr, grad_avg, **kwargs): p.data.add_(-lr, grad_avg) opt_func = partial(Optimizer, cbs=[average_grad,momentum_step], mom=0.9) . Note Learner will automatically schedule the momentum and learning rate mom and lr, so fit_one_cycle will even work with our custom Optimiser. . learn = get_learner(opt_func=opt_func) learn.fit_one_cycle(3, 0.03) . epoch train_loss valid_loss accuracy time . 0 | 2.744289 | 2.736736 | 0.278471 | 00:25 | . 1 | 2.402794 | 1.715736 | 0.425732 | 00:25 | . 2 | 2.038843 | 1.557327 | 0.485096 | 00:25 | . learn.recorder.plot_sched() . RMSProp . RMSProp uses an adaptive learning rate, each parameter gets its own learning rate controlled by a global learning rate. The individual learning rate can be determined by looking at the gradients, for example if the gradients are close to zero for a while it might need a higher learning rate, and vice versa if the gradients are too high or unstable. . We can use a moving average to get the general direction, specifically a moving average of the gradients squared. . w.square_avg = alpha w.square_avg + (1-alpha) (w.grad ** 2) . new_w = w - lr * w.grad / math.sqrt(w.square_avg + eps) . The eps (epsilon) is added for numerical stability (usually set at 1e-8), and the default value for alpha is usually 0.99. . def average_sqr_grad(p, sqr_mom, sqr_avg=None, **kwargs): if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data) return {&#39;sqr_avg&#39;: sqr_mom*sqr_avg + (1-sqr_mom)*p.grad.data**2} def rms_prop_step(p, lr, sqr_avg, eps, grad_avg=None, **kwargs): denom = sqr_avg.sqrt().add_(eps) p.data.addcdiv_(-lr, p.grad, denom) opt_func = partial(Optimizer, cbs=[average_sqr_grad,rms_prop_step], sqr_mom=0.99, eps=1e-7) learn = get_learner(opt_func=opt_func) learn.fit_one_cycle(3, 0.003) . epoch train_loss valid_loss accuracy time . 0 | 2.810043 | nan | 0.108535 | 00:26 | . 1 | 2.242717 | 1.917789 | 0.354140 | 00:26 | . 2 | 1.790359 | 1.510692 | 0.496815 | 00:26 | . Adam . Adam combines SGD, momentum and RMSProp together. One difference is Adam uses an unbiased moving average. . w.avg = beta w.avg + (1-beta) w.grad . unbias_avg = w.avg / (1 - (beta**(i+1))) . With all the steps combined we have: . w.avg = beta1 w.avg + (1-beta1) w.grad . unbias_avg = w.avg / (1 - (beta1**(i+1))) . w.sqr_avg = beta2 w.sqr_avg + (1-beta2) (w.grad ** 2) . new_w = w - lr * unbias_avg / sqrt(w.sqr_avg + eps) . Adam is the default optimiser in fastai. . Decoupled Weight Decay . When using Adam, we need to use a different kind of weight decay. Recall basic weight decay. . new_weight = weight - lrweight.grad - lrwd*weight . And alternative formulation is: . weight.grad += wd*weight . With SGD these are the same, but not for Adam. So we need to use decoupled weight decay when using Adam. . Fastai Callbacks . Fastai callbacks allow you to add custom behaviour to the training loop at any point. . . This has enabled easier adding of many new custom changes such as the below examples. . . Creating a Callback . Let&#39;s try defining a model reset callback. . class ModelResetter(Callback): def begin_train(self): self.model.reset() def begin_validate(self): self.model.reset() . Here is another example RNN regulariser callback. . class RNNRegularizer(Callback): def __init__(self, alpha=0., beta=0.): self.alpha,self.beta = alpha,beta def after_pred(self): self.raw_out,self.out = self.pred[1],self.pred[2] self.learn.pred = self.pred[0] def after_loss(self): if not self.training: return if self.alpha != 0.: self.learn.loss += self.alpha * self.out[-1].float().pow(2).mean() if self.beta != 0.: h = self.raw_out[-1] if len(h)&gt;1: self.learn.loss += self.beta * (h[:,1:] - h[:,:-1] ).float().pow(2).mean() . Inside the callback you can access global variables and objects such as self.model. . Callback Ordering and Exceptions . Callbacks can also interrupt any part of the training loop by using a system of exceptions, for example to skip a batch or stop training completely. . This callback will stop training any time the loss becomes infinate. . class TerminateOnNaNCallback(Callback): run_before=Recorder def after_batch(self): if torch.isinf(self.loss) or torch.isnan(self.loss): raise CancelFitException . Sometimes callbacks need to be called in a particular order. You can use run_before or run_after in the callback to set the ordering needed. . Conclusion . In this article we looked at standard SGD enhacements for optimisation, as well as looking at the fastai&#39;s library callbacks that help make changes easier. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/06/13/optimisation-methods-for-deep-learning.html",
            "relUrl": "/deep-learning-theory/2021/06/13/optimisation-methods-for-deep-learning.html",
            "date": " • Jun 13, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Resnets - The Key to Training Deeper Neural Networks",
            "content": "Introduction . In this article we will build a ResNet type convolutional image networks from scratch using PyTorch. We will see why these type of networks are key to enabling the building much deeper networks that can be easily trained and perform well. . This article and it&#39;s content is based on the fast ai deep learning course, chapter 14. . Improving Convolutional Networks - Average pooling . In an earlier article about convolutional networks in the models we used we ended up with a single vector of activations for each image by using enough stride-2 convolutions to down-sample each layer of activations so that we would end up with a grid size of 1. . If we tried this approach with other, bigger images we would face 2 issues: . We would need many more layers | The model would not be able to work on images of a different size to which it was trained on | . By using this type of architecture, we are in essence hard coding the architecture and making it difficult to reuse. We could for example flatten the final layer regardless of the grid size it was beyond 1x1, which was indeed an earlier approach followed, but this would still not work on images of a different size, and takes a lot of memory. . The problem was better solved by using fully convolutional networks which take the average of activations accross a final grid e.g. over the x and y axis. . def avg_pool(x): return x.mean((2,3)) . This will always convert a grid of activations into a single activation per image. . A full convolutional network then has a number of convolutional layers some of stride 2, at the end of which is an adaptive average pooling layer - to a layer to flatten and remove the unit axis, and a final linear layer. . We can define a fully convoltional network in the following way. . def block(ni, nf): return ConvLayer(ni, nf, stride=2) def get_model(): return nn.Sequential( block(3, 16), block(16, 32), block(32, 64), block(64, 128), block(128, 256), nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(256, dls.c)) . Because of the nature fo average pooling, this may not be suitable for some vision tasks, as you&#39;re loosing certain types of information. For example if you were trying to recognise digits of 6 and 9, the orientation and relational aspect of groups of pixels matters - so fully convoltuonal may not be good here. However for other images like animals, the orientation does&#39;nt really matter - a cat is a cat even if its upside down! So the fully convolutional networks which loose this relational information would be fine here. . When we come out of the convolutional layers, we have activations of dimensions bs x ch x h x w (batch size, a certain number of channels, height, and width). We want to end up with a tensor of bs x ch, so we can take the average over the last two dimensions and flatten the trailing 1×1 dimension like we did in our previous model. . There are other types of pooling we could use for example max pooling. For instance, max pooling layers of size 2, which were very popular in older CNNs, reduce the size of our image by half on each dimension by taking the maximum of each 2×2 window (with a stride of 2). . We are going to use a new dataset Imagenette which is a smaller version of the famous ImageNet dataset, this smaller one being with just 10 classes of image. . Lets get the data and train our new model. . def get_data(url, presize, resize): path = untar_data(url) return DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;val&#39;), get_y=parent_label, item_tfms=Resize(presize), batch_tfms=[*aug_transforms(min_scale=0.5, size=resize), Normalize.from_stats(*imagenet_stats)], ).dataloaders(path, bs=128) dls = get_data(URLs.IMAGENETTE_160, 160, 128) dls.show_batch(max_n=4) . File downloaded is broken. Remove /root/.fastai/archive/imagenette2-160.tgz and try again. . def get_learner(m): return Learner(dls, m, loss_func=nn.CrossEntropyLoss(), metrics=accuracy ).to_fp16() learn = get_learner(get_model()) learn.lr_find() . SuggestedLRs(lr_min=0.002290867641568184, lr_steep=0.007585775572806597) . # 3e-3 often a good learning rate for CNN&#39;s learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.882259 | 1.813273 | 0.404076 | 00:30 | . 1 | 1.522370 | 1.521868 | 0.504459 | 00:30 | . 2 | 1.276501 | 1.225626 | 0.606624 | 00:30 | . 3 | 1.135786 | 1.183137 | 0.623185 | 00:30 | . 4 | 1.042103 | 1.048710 | 0.665733 | 00:30 | . This is quite a good result, considering this is not a pre-trained model trying and to predict 10 image categories from scratch. But to improve this, we will need to do more than just add more layers. . Modern CNN&#39;s - ResNet . Skip connections . The authors of the original ResNet paper noticed when training deeper models, even when using BatchNorm, that a network with more layers often did worse than a network with less layers. . . It seems that a bigger network has a lot of trouble discovering the parameters of even the smaller better network when left by itself to just train this bigger network. . While this had been noticed before, what the authors of the paper did that was new was to realise it should be possible to create a deeper network that should do at least as well as a more shallow network, by essentially turning off the extra layers i.e. using an identity mapping. . An identity mapping is where you are passing through the signal from earlier layers directly, skipping over the current layer. Remember from Batch norm layers we have the transformative factors of gamma and beta - if we set gamma to zero for the extra layers - this would essentially turn off the actions of the extra layers - and allow the signal from the earlier layers to come through unaltered. This is called the skip connection. . . This can allow the model to only change the later layers gradually. The original ResNet paper actually defined the skip connection as jumping over 2 layers, as seen in the diagram above. . Another way to think about ResNet&#39;s and these skip connections is to consider the function here i.e. . Y = X + block(X) . So we are not asking this block layer to directly predict the output Y, we are asking the block to learn to predict the difference between X and Y to minimise the error i.e. block(X) wants to help X get closer to Y. So a ResNet is good at learning about slight differences between doing nothing and adding a little something to the previous signal to make it better. This is how ResNet&#39;s got their name, as they are predicting &#39;residuals&#39; i.e. a residual is the prediction - target. . Also what is key here is the idea of making learning more gradual and easier. Even though the Universal Approximation Theorem states that a sufficiently large network can learn any function, in practice there is a difference between how different architectures can make it easy and difficult to learn. . Let&#39;s define a ResNet block with a skip connection, here norm_type=NormType.BatchZero causes fastai to init the gamma weights of the last batchnorm layer to zero). . class ResBlock(Module): def __init__(self, ni, nf): self.convs = nn.Sequential( ConvLayer(ni,nf), ConvLayer(nf,nf, norm_type=NormType.BatchZero)) def forward(self, x): return x + self.convs(x) . There are 2 problems with this though, it can&#39;t handle strides of more than 1, and it needs ni=nf. If we recall, convolutional operations change the dimensions of the output based on the output channels, as do strides of more than 1. This would prevent us from adding X to conv(X) as they would be of different dimensions. . To remedy this, we need a way to change the dimensions of x to match conv(x). So we can halve the grid size using and average pooling layer with stride 2, and we can change the number of channels using a convolution. We need to make the convolution as simple as possible, and that would be one with a kernal size of 1. . So we can now define a better ResBlock that uses these tricks to handle the changing shape of the skip connection. . def _conv_block(ni,nf,stride): return nn.Sequential( ConvLayer(ni, nf, stride=stride), ConvLayer(nf, nf, act_cls=None, norm_type=NormType.BatchZero)) . class ResBlock(Module): def __init__(self, ni, nf, stride=1): self.convs = _conv_block(ni,nf,stride) self.idconv = noop if ni==nf else ConvLayer(ni, nf, 1, act_cls=None) self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True) def forward(self, x): return F.relu(self.convs(x) + self.idconv(self.pool(x))) . We are using the noop function here which just returns the input unchanged, so idconv does nothing if ni==nf, and pool does nothing if stride=1 - which is what we want in our skip connection. . Also we moved the Relu after both layers, treating as the whole ResNet block like one layer. . Lets try this model. . def block(ni,nf): return ResBlock(ni, nf, stride=2) learn = get_learner(get_model()) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.947870 | 1.877467 | 0.335796 | 00:32 | . 1 | 1.671832 | 1.602260 | 0.456561 | 00:32 | . 2 | 1.379121 | 1.492799 | 0.533503 | 00:32 | . 3 | 1.170203 | 1.069924 | 0.662166 | 00:33 | . 4 | 1.032529 | 1.050656 | 0.672357 | 00:33 | . While this is not spectacularly better, the point is this allows us to now train deeper models more easily. For example we can make a model with twice as many layers in the following way. . def block(ni, nf): return nn.Sequential(ResBlock(ni, nf, stride=2), ResBlock(nf, nf)) learn = get_learner(get_model()) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.945738 | 1.871942 | 0.353631 | 00:36 | . 1 | 1.632775 | 1.519365 | 0.492484 | 00:36 | . 2 | 1.331637 | 1.168114 | 0.622930 | 00:36 | . 3 | 1.081849 | 1.036962 | 0.665733 | 00:35 | . 4 | 0.944774 | 0.946332 | 0.695287 | 00:36 | . This deeper model is now doing better with the same number of epochs. . For the ResNet breakthrough and many others a key note might be that many of these breakthroughs have come through experimental observations of odd things, and then trying to figure out why these occour. So deep learning is a very experimental field where many breakthroughs come through experiments. . Further work exploring ResNet&#39;s showed how the skip connections actually helped smooth the loss landscape making training easier, more gradual, and easier to avoid getting stuck in a local minima. . . State of the art ResNet&#39;s . Current ResNet&#39;s used have a few further tweaks that improve their performance. This include the earlier layers being just convolutional layers followed by a max pooling layer, without a full ResNet block and skip connections. These earlier layers are called the stem of the network. . def _resnet_stem(*sizes): return [ ConvLayer(sizes[i], sizes[i+1], 3, stride = 2 if i==0 else 1) for i in range(len(sizes)-1) ] + [nn.MaxPool2d(kernel_size=3, stride=2, padding=1)] . _resnet_stem(3,32,32,64) . Why this approach? with deep convolutional networks, most of the computation occours in the earlier layers of the network. Therefore it helps to keep the earlier layers as simple as possible.. ResNet blocks take far more computation than a plain convolutional block. . Lets now try this approach with improving out ResNet architecture with these improvements in mind. . class ResNet(nn.Sequential): def __init__(self, n_out, layers, expansion=1): stem = _resnet_stem(3,32,32,64) self.block_szs = [64, 64, 128, 256, 512] for i in range(1,5): self.block_szs[i] *= expansion blocks = [self._make_layer(*o) for o in enumerate(layers)] super().__init__(*stem, *blocks, nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(self.block_szs[-1], n_out)) def _make_layer(self, idx, n_layers): stride = 1 if idx==0 else 2 ch_in,ch_out = self.block_szs[idx:idx+2] return nn.Sequential(*[ ResBlock(ch_in if i==0 else ch_out, ch_out, stride if i==0 else 1) for i in range(n_layers) ]) . The various versions of the models (ResNet-18, -34, -50, etc.) just change the number of blocks in each of those groups. This is the definition of a ResNet-18: . rn = ResNet(dls.c, [2,2,2,2]) . Let&#39;s try training this new Resnet-18 architecture. . learn = get_learner(rn) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.625527 | 2.041075 | 0.396688 | 00:55 | . 1 | 1.329917 | 1.507927 | 0.541147 | 00:54 | . 2 | 1.065707 | 1.900392 | 0.499618 | 00:54 | . 3 | 0.870085 | 0.987169 | 0.682293 | 00:54 | . 4 | 0.765841 | 0.779386 | 0.753631 | 00:54 | . Bottleneck Layers . We can use another method when making even deeper models to try and reduce the amount of memory used to make it faster, this might be fore ResNet&#39;s of depth 50 or more. . Rather than stacking 2 convolutions with a kernal size of 3, we can use 3 different convolutions two 1x1 at the start and end, and one 3x3. This is called a bottleneck layer. . . How does this help? 1x1 convolutions are much faster, so this type of block runs much faster than the ones with only 3x3 kernals. This then allows us to use more channels, 4 times more in fact (we end up with 256 channels out instead of just 64) which reduce then restore the number of channels (ie the name bottleneck). . So we end up using more channels in the same amout of time with this type of block architecture. Lets try improving our model with a bottleneck block and use it to build a bigger model ResNet-50. . def _conv_block(ni,nf,stride): return nn.Sequential( ConvLayer(ni, nf//4, 1), ConvLayer(nf//4, nf//4, stride=stride), ConvLayer(nf//4, nf, 1, act_cls=None, norm_type=NormType.BatchZero)) . To get better results from this bigger model we will need to train it longer and we can use bigger images as well. . dls = get_data(URLs.IMAGENETTE_320, presize=320, resize=224) rn = ResNet(dls.c, [3,4,6,3], 4) . File downloaded is broken. Remove /root/.fastai/archive/imagenette2-320.tgz and try again. . Bear in mind even though we are using bigger images, we don&#39;t need to really change our network due to this because its fully convolutional it works just fine (remember the use of pooling layers). This also allows us to use the fastai technique of progressive resizing. . Conclusion . In this article we have built a ResNet convolutional deep learning image model from scratch, using many iterations and variations - including some of the most recent state of the art techniques. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/06/12/resnets-the-key-to-training-deep-neural-networks.html",
            "relUrl": "/deep-learning-theory/2021/06/12/resnets-the-key-to-training-deep-neural-networks.html",
            "date": " • Jun 12, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Fastai Application Architectures",
            "content": "Introduction . The fastai deep learning library (as of 2021) is a layered API that has 4 levels of abstraction. . Application layer | High level API | Mid level API | Low level API | . . In this article we will look at how to build custom applications in the fastai library, by looking at how current fastai image model applications are actually built. . Fastai Image Model Applications . cnn_learner . When using this application, the first parameter we need to give it is an architecture which will be used as the body of the network. Usually this will be a ResNet architecture we pre-trained weights that is automaticially downloaded for you. . Next the final layer of the pre-trained model is cut, in fact all layers after the final pooling layer is also cut as well. Within each model we have a dictionary of information that allows us to identify these different points within the layers called model_meta here for example for ResNet50. . model_meta[resnet50] . {&#39;cut&#39;: -2, &#39;split&#39;: &lt;function fastai.vision.learner._resnet_split&gt;, &#39;stats&#39;: ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])} . Key parts of the network are: . Head - The part of the network specialised for a particular task i.e. with a CNN the part after the adaptive average pooling layer | Body - Everything else not the Head including the Stem | Stem - The first layers of the network | . We we take all the layers before the cut point of -2, we get the body of the model that fastai will keep to use for transfer learning. Then we can add a new head. . create_head(20,2) . With this function we can choose how many extra layers should be added at the end as well as how much dropout and pooling. Fastai by default adds 2 linear layers rather than just one, as fastai have found this helps transfer learning work more quickly and easily than just one extra layer. . unet_learner . This architecture is most often used for image segmentation tasks. . We start of building this in the same way as the cnn_learner, chopping off the old head. For image segmentation, we are going to have to add a very different type of head to end up with a model that actually generates an image for segmentation. . One way we could do this is to add layers that can increase the grid size in a CNN, for example duplicating each of the pixels to make an image twice as big - this is known as nearest neighbour interpolation. Another approach uses strides, in this case a stride of half, which is known as transposed convolution. However neither of these approaches works well in practice. . They key problem here is there is simply not enough information in these downsampled activations alone to be able to recreate something like the oroginal image quality needed for segmentation - its a big ask! And perhaps not realistic. . The solution to this problem here is our friend again skip connections however using them not accross one layer - but reaching these connections far accross to the opposite side of the architecture. . . Here on the left half of the model is a CNN, and the transposed convolutional layers on the right, with the extra skip connections in gray. This helps the Unet do a much better job at generate the type of images we want for segmentation. One challenge with Unet&#39;s is the exact architecture does in this case depend on the image size, however fastai has a DynamicUnet object that automatically generates the correct architecture based on the data and image sizes given. . A Siamese Network . Let&#39;s now try to create a custom model. In an earlier article we looked at creating a Siamese network model. Let&#39;s recap the details of that model. . Let&#39;s now build a custom model for the Siamese task. We will use a pre-trained model, pass 2 images through it, concatinate the results, then send them to a custom head that will return 2 predictions. . In terms of overall architecture and models lets define it like this. . class SiameseModel(Module): def __init__(self, encoder, head): self.encoder,self.head = encoder,head def forward(self, x1, x2): ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim=1) return self.head(ftrs) . We can create a body/encoder by taking a pre-trained model and cutting it, we just need to specify where we want to cut. The cut position for a ResNet is -2. . encoder = create_body(resnet34, cut=-2) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . . We can then create a head. If we look at the encoder/body it will tell us the last layer has 512 features, so this head will take 2*512 - as we will have 2 images. . head = create_head(512*2, 2, ps=0.5) . We can now build our model from our constructed head and body. . model = SiameseModel(encoder, head) . Before we can use a Learner to train the model we need to define 2 more things. Firstly, a loss function. We might use here cross-entropy, but as our targets are boolean we need to convert them to integers or Pytorch will throw and error. . Secondly, we need to define a custom splitter that will tell the fastai library how to split the model into parameter groups, which will help train only the head of the model when we do transfer learning. Here we want 2 parameter groups one for the encoder/body and one for the head. So lets define a splitter as well. . def loss_func(out, targ): return nn.CrossEntropyLoss()(out, targ.long()) def siamese_splitter(model): return [params(model.encoder), params(model.head)] . We can now define a learner using our data, model, loss function, splitter and a metric. As we are defining a learner manually here, we also have to call freeze manually as well, to ensure only the last paramete group i.e. the head is trained. . learn = Learner(dls, model, loss_func=loss_func, splitter=siamese_splitter, metrics=accuracy) learn.freeze() . Let&#39;s now train our model. . learn.fit_one_cycle(4, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.523447 | 0.334643 | 0.861299 | 03:03 | . 1 | 0.373501 | 0.231564 | 0.913396 | 03:02 | . 2 | 0.299143 | 0.209658 | 0.920162 | 03:02 | . 3 | 0.251663 | 0.188553 | 0.928281 | 03:03 | . This has trained only our head. Lets now unfreeze the whole model to make it all trainable, and use discriminative learning rates. This will give a lower learning rate for the body and a higher one for the head. . learn.unfreeze() learn.fit_one_cycle(4, slice(1e-6,1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.235140 | 0.188717 | 0.924222 | 04:15 | . 1 | 0.233328 | 0.179823 | 0.932341 | 04:12 | . 2 | 0.210744 | 0.172465 | 0.928958 | 04:12 | . 3 | 0.224448 | 0.176144 | 0.930311 | 04:14 | . Points to consider with architectures . There are a few points to consider when training models in practice. if you are running out of memory or time - then training a smaller model could be a good approach. If you are not training long enough to actually overfit, then you are probably not taking advantage of the capacity of your model. . So one should first try to get to the point where your model is overfitting. . . Often many people when faced with a model that overfits, start with the wrong thing first i.e. to use a smaller model, or more regularization. Using a smaller model should be one of the last steps one tries, as this reduces the capaity of your model to actually learn what is needed. . A better approach is to actually try to use more data, such as adding more labels to the data, or using data augmentation for example. Mixup can be useful for this. Only once you are using much more data and are still overfitting, one could consider more generalisable architectures - for example adding batch norm could help here. . After this if its still not working, one could use regularisation, such as adding dropout to the last layers, but also throughout the model. Only after these have failed one should consider using a smaller model. . Conclusion . In this article we have looked at how to build custom fastai application architectures, using image model examples. .",
            "url": "https://www.livingdatalab.com/fastai/2021/06/12/fastai-application-architectures.html",
            "relUrl": "/fastai/2021/06/12/fastai-application-architectures.html",
            "date": " • Jun 12, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Building a Convolutional Image Model from scratch",
            "content": "Introduction . In this article we are going to look at building a convolutional neural network from scratch, using Pytorch. We are also going to look at other techniques than help train models better, such as one-cycle training and batch normalisation. . This article is based on the content from the fastai deep learning course chapter 13. . Convolutions in PyTorch . Pytorch defines a convolutional layer using the method F.conv2d. This uses two rank 4 tensors. . Input tensor (minibatch, in_channels, iH, iW) | Weight tensor (out_channels, in_channels, kH, kW) | . Where iH, iW, kH, kW are the image and kernal widths and heights respectively. Pytorch expects rank 4 tensors as it will process an entire mini-batch of images in one go, as well as applying multiple kernals in one go - which is more efficient to do on a GPU. . Lets try this out by creating a tensor of multiple kernals. . top_edge = tensor([[-1,-1,-1], [ 0, 0, 0], [ 1, 1, 1]]).float() left_edge = tensor([[-1,1,0], [-1,1,0], [-1,1,0]]).float() diag1_edge = tensor([[ 0,-1, 1], [-1, 1, 0], [ 1, 0, 0]]).float() diag2_edge = tensor([[ 1,-1, 0], [ 0, 1,-1], [ 0, 0, 1]]).float() edge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge]) edge_kernels.shape . torch.Size([4, 3, 3]) . We can also create a data loader, and extract one minibatch to test. . mnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(), get_y=parent_label) dls = mnist.dataloaders(path) xb,yb = first(dls.valid) xb,yb = to_cpu(xb),to_cpu(yb) xb.shape . torch.Size([64, 1, 28, 28]) . So we are not quite there, because currently our composite kernal is a rank 3 tensor, and it needs to be rank 4. So in this case we need to define an axis for the number of input channels which will be one (because our greyscale images have one channel). So we can insert an extra axis of 1 in the right place using the unsqueeze method. . edge_kernels.shape,edge_kernels.unsqueeze(1).shape . (torch.Size([4, 3, 3]), torch.Size([4, 1, 3, 3])) . We can now pass the kernals to the convolutional layer along with the data and process the image. . edge_kernels = edge_kernels.unsqueeze(1) batch_features = F.conv2d(xb, edge_kernels) batch_features.shape . torch.Size([64, 4, 26, 26]) . This gives us a tensor of a batch of 64 images, with 4 kernals and 26x26 image (we lost one pixel from each side by convolutions of 28x28). Lets look at one of the images on one channel to see the applied convolution. . show_image(batch_features[0,0]); . So with pure convolutions we lost parts of the image, which become a bit smaller. We can get around this by using padding. We can also use stride to end up with a smaller sampled image. . Model 1 - A basic Convolutional Neural Network to predict 2 digits . We are going to build a simple model to predict 2 digits a 3 or 7, as a multi-class classification problem so we will expect probabilities for each image for the likelihood of it being either 3 or 7. . So we can use gradient descent to actually learn the best values for each of the kernals. . nn.Conv2d is a better method to use when creating a network as it automatically creates a weight matrix. Lets try a very simple model. . broken_cnn = sequential( nn.Conv2d(1,30, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(30,1, kernel_size=3, padding=1) ) broken_cnn(xb).shape . torch.Size([64, 1, 28, 28]) . Note we didn&#39;t need to specify an input size as we do with normal linear layers, as a convolution is applied to every pixel whatever the size of the image. The weights of a convolutional layer are to do with the number of input and output channels and the kernal size. . Putting our test batch through this, we can see it produces an output of 28x28 activations which is not ideal for classification. We could use a series of layers with strides, to reduce down the output activations. . # Stride 2 convolutional layer which will downsample our image def conv(ni, nf, ks=3, act=True): res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2) if act: res = nn.Sequential(res, nn.ReLU()) return res simple_cnn = sequential( conv(1 ,4), #14x14 conv(4 ,8), #7x7 conv(8 ,16), #4x4 conv(16,32), #2x2 conv(32,2, act=False), #1x1 Flatten(), ) simple_cnn(xb).shape . torch.Size([64, 2]) . Create a learner object with this. . learn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy) learn.summary() . Sequential (Input shape: 64) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 4 x 14 x 14 Conv2d 40 True ReLU ____________________________________________________________________________ 64 x 8 x 7 x 7 Conv2d 296 True ReLU ____________________________________________________________________________ 64 x 16 x 4 x 4 Conv2d 1168 True ReLU ____________________________________________________________________________ 64 x 32 x 2 x 2 Conv2d 4640 True ReLU ____________________________________________________________________________ 64 x 2 x 1 x 1 Conv2d 578 True ____________________________________________________________________________ [] Flatten ____________________________________________________________________________ Total params: 6,722 Total trainable params: 6,722 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7ff1128d58c0&gt; Loss function: &lt;function cross_entropy at 0x7ff13e6b55f0&gt; Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Note that the final conv layer output is 64x2x1x1 and flatten removes these unit axes, which is basically the squeeze function but as a network layer. . Let&#39;s try training this model. . learn.fit_one_cycle(2, 0.01) . epoch train_loss valid_loss accuracy time . 0 | 0.080185 | 0.035895 | 0.988714 | 00:13 | . 1 | 0.024726 | 0.029886 | 0.990186 | 00:13 | . Convolutional arithmetic . So we can see in this example the input size is 64x1x28x28, and these axes are batch, channel, height, width. This is often represented in Pytorch as NCHW (where N is batch size). . When we use a stride-2 convolution, we often increase the number of features because we&#39;re decreasing the number of activations in the activation map by a factor of 4; we don&#39;t want to decrease the capacity of a layer by too much at a time. . We also have one bias weight for each channel. So in this example, our stide 2 convolutions halve the grid size - but we also double the number of filters at each layer. This means we get the same amount of computation done. . A receptive field is the area of an image involved in the calculation of a layer. As we go deeper through the layers, a larger area of the original image layers progressively contribute to smaller areas of later layers that have smaller grid sizes. . Model 2 - A Convolutional Neural Network to predict 10 digits . As our previous model did well on predicting 2 digits, we will now try to build a model that predicts all 10 digits, using the full MNIST dataset. . path = untar_data(URLs.MNIST) . The images are in 2 folders training and testing, so we can use the GrandparentSplitter but need to tell it explictly as by default it expects train and valid. . We will define a function to make it easy to define different dataloaders with different batch sizes. . def get_dls(bs=64): return DataBlock( blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(&#39;training&#39;,&#39;testing&#39;), get_y=parent_label, batch_tfms=Normalize() ).dataloaders(path, bs=bs) dls = get_dls() dls.show_batch(max_n=9, figsize=(4,4)) . So we will try and improve our previous model with one with more activations, and we will probably need more filters to handle more numbers, so we could double them for each layer. . But there is a potential problem, if we add more filters we are producing an image of a similar size to our input, which does not force the network to learn useful features. If we use a larger kernal in the first layer, such as 5x5 instead of 3x3, this will force the network to find more useful features from this more limited information. . from fastai.callback.hook import * def simple_cnn(): return sequential( conv(1 ,8, ks=5), #14x14 conv(8 ,16), #7x7 conv(16,32), #4x4 conv(32,64), #2x2 conv(64,10, act=False), #1x1 Flatten(), ) def fit(epochs=1): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit(epochs, 0.06) return learn learn = fit() . /usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 0.694318 | 0.672285 | 0.793600 | 01:06 | . So we trained for one epoch, but that did&#39;nt do well. We can use callbacks to investigate why right after training. The ActivationStats callback use some useful plots, for example we can plot the mean and std dev of the activations of a layer you give the index for, along with the percentage of activations which are zero. . learn.activation_stats.plot_layer_stats(0) . So ideally we want our model to have a smooth mean and std dev during training. Activations near zero are not helpful, as it gives gradient descent little to work with. If we have little to zero activations in earlier layers, this gets even worse in later layers. . learn.activation_stats.plot_layer_stats(-2) . We could try to make training more stable by increasing the batch size with better info for gradients, but less often updated due to larger batch sizes. . Lets try larger batch size. . dls = get_dls(512) learn = fit() . /usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 2.327641 | 2.302224 | 0.113500 | 00:56 | . learn.activation_stats.plot_layer_stats(-2) . This has&#39;nt helped much with the activations, lets see what else we can do. . One cycle training . We have been training our model at the same learning rate, but it may be more helpful to vary the learning rate at different points - for example to have it higher when we are far in the loss landscape from the minimum, and have it lower when we are in the minimum area. In one cycle training, we start at a lower learning rate, building up gradually to a maximum, then gradually reducing the learning rate again. . def fit(epochs=1, lr=0.06): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit_one_cycle(epochs, lr) return learn learn = fit() . /usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 0.197716 | 0.083136 | 0.975800 | 00:55 | . learn.recorder.plot_sched() . Once cycle training also involves varying the momentum with the opposite pattern to the learning rate. . Looking at our layer stats again, we can see there is some improvement but still a large percentage of zero weights. . We can use the colour_dim module to show how the activations of a layer changes through the training accross time. . learn.activation_stats.color_dim(-2) . Here for example we can see on the far left mostly white is with most of the activations at zero, then as time passes from left to right, we can see an expontential build up of activations, which then collapses into zero activations (white bands). Eventually the bands go and you get more consistant activations for most of the model. . Ideally if we can avoid this crashing of activations this can result in better training. . Batch Normalisation . Batch norm is a method we can use to stablise training to try and avoid extreme rises and crashes in activations. Essentially batch norm normalises the activations of each layer using the mean and std dev of the activations. Batch norm also uses extra 2 learnable parameters per layer gamma and beta which are addative and multiplicative factors that can then scale the activations of a layer. . Batchnorm layer output = (Normalised Activations * gamma) + beta . So each layer has its own normalisation and scaling with batchnorm layers. The normalisation is different during training vs validation: in training we use the mean and std dev of a batch to normalise, in validation we use the running mean and std dev calculated during training. . Lets add batchnorm to our layer definition. . def conv(ni, nf, ks=3, act=True): layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)] if act: layers.append(nn.ReLU()) layers.append(nn.BatchNorm2d(nf)) return nn.Sequential(*layers) learn = fit() . /usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 0.129701 | 0.057382 | 0.985700 | 00:58 | . learn.activation_stats.color_dim(-4) . This has given us more gradual training without the crashes in activations. Now we are using batchnorm in our layers it should make it easier to learn at a higher learning rate. . learn = fit(5, lr=0.1) . /usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 0.180499 | 0.142405 | 0.957800 | 00:58 | . 1 | 0.078111 | 0.064472 | 0.979600 | 00:58 | . 2 | 0.051010 | 0.052857 | 0.983600 | 00:58 | . 3 | 0.031543 | 0.030566 | 0.990000 | 00:58 | . 4 | 0.015607 | 0.024703 | 0.991900 | 00:58 | . Conclusion . In this article we look at how we can build a convolutional neural network using Pytorch, as well as useful extra techniques to help with model training such as one-cycle training and batch normalisation. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/06/11/convolutional-image-model-from-scratch.html",
            "relUrl": "/deep-learning-theory/2021/06/11/convolutional-image-model-from-scratch.html",
            "date": " • Jun 11, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Building an LSTM Language Model from scratch",
            "content": "Introduction . In this article we will look at how we build an LSTM language model that is able to predict the next word in a sequence of words. As part of this, we will also explore several regularization methods. We will build a range of models using basic python &amp; Pytorch to illustrate the fundamentals of this type of model, while also using aspects of the fastai library. We will end up exploring all the different aspects that make up the AWD-LSTM model architecture. . This work is based on material from the fastai deep learning book, chapter 12. . Dataset . We will use the fastai curated Human Numbers dataset for this exercise. This is a dataset of the first 10,000 numbers written as words in english. . path = untar_data(URLs.HUMAN_NUMBERS) Path.BASE_PATH = path path.ls() . (#2) [Path(&#39;valid.txt&#39;),Path(&#39;train.txt&#39;)] . lines = L() with open(path/&#39;train.txt&#39;) as f: lines += L(*f.readlines()) with open(path/&#39;valid.txt&#39;) as f: lines += L(*f.readlines()) lines . (#9998) [&#39;one n&#39;,&#39;two n&#39;,&#39;three n&#39;,&#39;four n&#39;,&#39;five n&#39;,&#39;six n&#39;,&#39;seven n&#39;,&#39;eight n&#39;,&#39;nine n&#39;,&#39;ten n&#39;...] . text = &#39; . &#39;.join([l.strip() for l in lines]) text[:100] . &#39;one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo&#39; . tokens = text.split(&#39; &#39;) tokens[:10] . [&#39;one&#39;, &#39;.&#39;, &#39;two&#39;, &#39;.&#39;, &#39;three&#39;, &#39;.&#39;, &#39;four&#39;, &#39;.&#39;, &#39;five&#39;, &#39;.&#39;] . vocab = L(*tokens).unique() vocab . (#30) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;five&#39;,&#39;six&#39;,&#39;seven&#39;,&#39;eight&#39;,&#39;nine&#39;...] . word2idx = {w:i for i,w in enumerate(vocab)} nums = L(word2idx[i] for i in tokens) nums . (#63095) [0,1,2,1,3,1,4,1,5,1...] . Language Model 1 - Linear Neural Network . Lets first try a simple linear model that will aim to predict each word based on the previous 3 words. To do this we can create our input variable as every sequence of 3 words, and our output/target variable as the next word after each sequence of 3. . So in python as tokens and pytorch tensors as numeric values seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3)) seqswe could construct these variables in the following way. . L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3)) . (#21031) [([&#39;one&#39;, &#39;.&#39;, &#39;two&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;three&#39;, &#39;.&#39;], &#39;four&#39;),([&#39;four&#39;, &#39;.&#39;, &#39;five&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;six&#39;, &#39;.&#39;], &#39;seven&#39;),([&#39;seven&#39;, &#39;.&#39;, &#39;eight&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;nine&#39;, &#39;.&#39;], &#39;ten&#39;),([&#39;ten&#39;, &#39;.&#39;, &#39;eleven&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;twelve&#39;, &#39;.&#39;], &#39;thirteen&#39;),([&#39;thirteen&#39;, &#39;.&#39;, &#39;fourteen&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;fifteen&#39;, &#39;.&#39;], &#39;sixteen&#39;)...] . seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3)) seqs . (#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1, 14]), 1),(tensor([ 1, 15, 1]), 16)...] . We can group these into batches using the DataLoader class. . bs = 64 cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False) . So we will create a linear neural network with 3 layers, and a couple of specific features. . The first feature is to do with using embeddings. The first layer will take the first word embeddings, the second layer the second word embeddings plus the first layer activations, and the third layer the third word embeddings plus the second layer activations. The key observation here is that each word/layer is interpreted in the context of the previous word/layer. . The second feature is that each of these 3 layers will actually be the same layer, that it will have just one weight matrix. Each layer would run into different words even as separate, so really this layer should be able to be repeatedly used to do the same job for each of the 3 words. In other words, while activation values will change as words move through the network, the layer weights will not change from layer to layer. . This way, a layer doesn&#39;t just learn to handle one position i.e. second word position, its forced to generalise and learn to handle all 3 word positions. . class LMModel1(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) def forward(self, x): h = F.relu(self.h_h(self.i_h(x[:,0]))) h = h + self.i_h(x[:,1]) h = F.relu(self.h_h(h)) h = h + self.i_h(x[:,2]) h = F.relu(self.h_h(h)) return self.h_o(h) . So we have 3 key layers: . An embedding layer | A linear layer to create activations (for next word) | A final layer to predict the target 4th word | . Lets try training a model built with this architecture. . learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.824297 | 1.970941 | 0.467554 | 00:01 | . 1 | 1.386973 | 1.823242 | 0.467554 | 00:01 | . 2 | 1.417556 | 1.654497 | 0.494414 | 00:01 | . 3 | 1.376440 | 1.650849 | 0.494414 | 00:01 | . So how might we establish a baseline to judge these results? What if we defined a naive predictor that simply predicted the most common word. Lets find the most common word, and then calculate an accuracy when predicting always the most common word. . n,counts = 0,torch.zeros(len(vocab)) for x,y in dls.valid: n += y.shape[0] for i in range_of(vocab): counts[i] += (y==i).long().sum() idx = torch.argmax(counts) idx, vocab[idx.item()], counts[idx].item()/n . (tensor(29), &#39;thousand&#39;, 0.15165200855716662) . Language Model 2 - Recurrent Neural Network . So in the forward() method rather than repeating the lines for each layer, we could convert this into a for loop which would not only make our code simplier, but allow us to extend to data that was more than 3 words long and of different lengths. . class LMModel2(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) def forward(self, x): h = 0 for i in range(3): h = h + self.i_h(x[:,i]) h = F.relu(self.h_h(h)) return self.h_o(h) . learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.816274 | 1.964143 | 0.460185 | 00:01 | . 1 | 1.423805 | 1.739964 | 0.473259 | 00:01 | . 2 | 1.430327 | 1.685172 | 0.485382 | 00:01 | . 3 | 1.388390 | 1.657033 | 0.470406 | 00:01 | . Note that each time we go through the loop, the resulting activations are passed along to the next loop using the h variable, which is called the hidden state. A recurrent neural network is simply a network that is defined using a loop like this. . Language Model 3 - A better RNN . So notice in the latest model we initialise the hidden state to zero with each run through i.e. each batch, this means our batch size greatly effects the amount of information carried over. Also is there a way we can have more &#39;signal&#39;? rather than just the 4th word, we could try to predict the others for example. . To not loose our hidden state so frequently and carry over more useful information, we could initialise it outside the forward method. However this now makes our model as deep as the sequence of tokens i.e. 10,000 tokens leads to a 10,000 layer network, which will mean to calculate all the gradients back to the first word/layer could be very time consuming. . So rather than calculate all gradients, we can just keep the last 3 layers. To delete all the gradient history in Pytorch we use the detach() method. . This version of the model now carries over activations between calls to forward(), we could call this kind of model stateful. . class LMModel3(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): for i in range(3): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) out = self.h_o(self.h) self.h = self.h.detach() return out def reset(self): self.h = 0 . To use this model we need to ensure our data is in the correct order, for example here we are going to divide it into 64 equally sized parts, with each text of size 3. . m = len(seqs)//bs m,bs,len(seqs) . (328, 64, 21031) . def group_chunks(ds, bs): m = len(ds) // bs new_ds = L() for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs)) return new_ds cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets( group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) batch = dls.one_batch() batch[0].size() . torch.Size([64, 3]) . learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.708583 | 1.873094 | 0.401202 | 00:01 | . 1 | 1.264271 | 1.781330 | 0.433173 | 00:01 | . 2 | 1.087642 | 1.535732 | 0.521875 | 00:01 | . 3 | 1.007973 | 1.578549 | 0.542308 | 00:01 | . 4 | 0.945740 | 1.660635 | 0.569231 | 00:01 | . 5 | 0.902835 | 1.605541 | 0.551923 | 00:01 | . 6 | 0.878297 | 1.527385 | 0.579087 | 00:01 | . 7 | 0.814197 | 1.451913 | 0.606250 | 00:01 | . 8 | 0.783523 | 1.509463 | 0.604087 | 00:01 | . 9 | 0.763500 | 1.511033 | 0.608413 | 00:01 | . Language Model 4 - Creating more signal . So with the current model we still predict just one word for every 3 words which limits the amount of signal - what if we predicted the next word after every word? . To do this we need to restructure our data, so that the target variable has the 3 next words after the 3 first words, we can make this a variable sl for sequence length in this case to 16. . sl = 16 seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) batch = dls.one_batch() batch[0].size() . torch.Size([64, 16]) . [L(vocab[o] for o in s) for s in seqs[0]] . [(#16) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;...], (#16) [&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;,&#39;six&#39;...]] . Now we can refactor our model to predict the next word after each word rather than after each 3 word sequence. . class LMModel4(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): outs = [] for i in range(sl): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) outs.append(self.h_o(self.h)) self.h = self.h.detach() return torch.stack(outs, dim=1) def reset(self): self.h = 0 # Need to reshape output before passing to loss function def loss_func(inp, targ): return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1)) . learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.226453 | 3.039626 | 0.200033 | 00:00 | . 1 | 2.295425 | 1.925965 | 0.439697 | 00:00 | . 2 | 1.743091 | 1.818798 | 0.423258 | 00:00 | . 3 | 1.471100 | 1.779967 | 0.467285 | 00:00 | . 4 | 1.267640 | 1.823129 | 0.504883 | 00:00 | . 5 | 1.100705 | 1.991244 | 0.500814 | 00:00 | . 6 | 0.960767 | 2.086404 | 0.545085 | 00:00 | . 7 | 0.857365 | 2.240561 | 0.556803 | 00:00 | . 8 | 0.776844 | 2.004017 | 0.568766 | 00:00 | . 9 | 0.711604 | 1.991193 | 0.588949 | 00:00 | . 10 | 0.659614 | 2.064157 | 0.585775 | 00:00 | . 11 | 0.619464 | 2.033359 | 0.606283 | 00:00 | . 12 | 0.587681 | 2.100323 | 0.614176 | 00:00 | . 13 | 0.565472 | 2.145048 | 0.603760 | 00:00 | . 14 | 0.553879 | 2.149167 | 0.605550 | 00:00 | . Because the task is now harder (predicting after each word) we need to train for longer, but we still do well. Since this is effectively a very deep NN, the results can vary each time because the gradients and vary hugely. . Language Model 5 - Multi-layer RNN . While we already in a sense have a multi-layer NN, our repeated part is just once layer still. A deeper RNN gives us more computational power to do better at each loop. . We can use the RNN class to effectively replace the previous class, and allows us to build a new model with multiple stacked RNN&#39;s rather than just the previous one we had. . class LMModel5(Module): def __init__(self, vocab_sz, n_hidden, n_layers): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = torch.zeros(n_layers, bs, n_hidden) def forward(self, x): res,h = self.rnn(self.i_h(x), self.h) self.h = h.detach() return self.h_o(res) def reset(self): self.h.zero_() . learn = Learner(dls, LMModel5(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.008033 | 2.559917 | 0.449707 | 00:00 | . 1 | 2.113339 | 1.726179 | 0.471273 | 00:00 | . 2 | 1.688941 | 1.823044 | 0.389648 | 00:00 | . 3 | 1.466082 | 1.699160 | 0.462646 | 00:00 | . 4 | 1.319908 | 1.701673 | 0.516764 | 00:00 | . 5 | 1.177464 | 1.837683 | 0.543050 | 00:00 | . 6 | 1.041084 | 2.043768 | 0.554688 | 00:00 | . 7 | 0.923601 | 2.067982 | 0.549886 | 00:00 | . 8 | 0.819859 | 2.061354 | 0.562988 | 00:00 | . 9 | 0.735049 | 2.076721 | 0.568685 | 00:00 | . 10 | 0.664878 | 2.080706 | 0.570231 | 00:00 | . 11 | 0.614425 | 2.117641 | 0.586263 | 00:00 | . 12 | 0.577034 | 2.142265 | 0.588053 | 00:00 | . 13 | 0.554870 | 2.124338 | 0.591227 | 00:00 | . 14 | 0.543019 | 2.121613 | 0.590658 | 00:00 | . So this model actually did worse than our previous - why? Because we have a deeper model now (just by one extra layer) we probably have exploding and vanishing activations. . Generally having a deeper layered model gives us more compute to get better results, however this also makes it more difficult to train because the compunded activations can explode or vanish - think matrix multiplications! . Researchers have developed 2 approaches to try and rectify this: long short-term memory layers (LSTM&#39;s) and gated reccurent units (GRU&#39;s). . Language Model 6 - LSTM&#39;s . LSTM&#39;s were invented by Jürgen Schmidhuber and Sepp Hochreiter in 1997, and they have 2 hidden states. . In our previous RNN we have one hidden state &#39;h&#39; that does 2 things: . Holds signal to help predict the next word | Holds signal of all previous words | . These are potentially very different things to remember together in one value, and in practice RRN&#39;s are not very good at retaining the second long term information. LSTM&#39;s have a second hidden state called a cell state specifically to focus on this second requirement as a kind of long short-term memory. . Lets look at the architecture of a LSTM. . . So the inputs come in from the left which are: . Xt: input | ht-1: previous hidden state | ct-1: previous cell state | . The 4 orange boxes are layers with either sigmoid or tanh activation functions. The green circles are element-wise operations. The outputs on the right are: . ht: new hidden state | ct: new cell state | . Which will be used at the next input. The 4 orange layers are called gates. Note also how little the cell state at the top is changed, this is what allows it to better persist over time. . The 4 Gates of an LSTM . Forget gate | Input gate | Cell gate | Output gate | The first gate the forget gate, is a linear layer followed by a sigmoid, gives the LSTM the ability to forget things about its long term state held in the cell state. For example, when the input is a xxbos token, we might expect the LTSM will learn to trigger this to reset its cell state. . The second and third gates work together to update/add to the cell state. The input gate decides which parts of the cell state to update, and the cell gate decides what those updated values should be. . The output gate decides what information from the cell state is used to generate the output. . We can define this as the following class. . class LSTMCell(Module): def __init__(self, ni, nh): self.forget_gate = nn.Linear(ni + nh, nh) self.input_gate = nn.Linear(ni + nh, nh) self.cell_gate = nn.Linear(ni + nh, nh) self.output_gate = nn.Linear(ni + nh, nh) def forward(self, input, state): h,c = state h = torch.cat([h, input], dim=1) forget = torch.sigmoid(self.forget_gate(h)) c = c * forget inp = torch.sigmoid(self.input_gate(h)) cell = torch.tanh(self.cell_gate(h)) c = c + inp * cell out = torch.sigmoid(self.output_gate(h)) h = out * torch.tanh(c) return h, (h,c) . We can refactor the code to make this more efficient, in particular creating just one big matrix multiplication rather than 4 smaller ones. . class LSTMCell(Module): def __init__(self, ni, nh): self.ih = nn.Linear(ni,4*nh) self.hh = nn.Linear(nh,4*nh) def forward(self, input, state): h,c = state # One big multiplication for all the gates is better than 4 smaller ones gates = (self.ih(input) + self.hh(h)).chunk(4, 1) ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3]) cellgate = gates[3].tanh() c = (forgetgate*c) + (ingate*cellgate) h = outgate * c.tanh() return h, (h,c) . The Pytorch chunk method helps us split our tensor into 4 parts. . t = torch.arange(0,10); t . tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . t.chunk(2) . (tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9])) . Here we will define a 2 layer LSTM which is the same network as model 5. We can actually train this at a higher learning rate for less time and do better, as this network should be more stable and easier to train. . class LMModel6(Module): def __init__(self, vocab_sz, n_hidden, n_layers): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)] def forward(self, x): res,h = self.rnn(self.i_h(x), self.h) self.h = [h_.detach() for h_ in h] return self.h_o(res) def reset(self): for h in self.h: h.zero_() . learn = Learner(dls, LMModel6(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 3.007779 | 2.770814 | 0.284017 | 00:01 | . 1 | 2.204949 | 1.782870 | 0.425944 | 00:01 | . 2 | 1.606196 | 1.831585 | 0.462402 | 00:01 | . 3 | 1.296969 | 1.999463 | 0.479411 | 00:01 | . 4 | 1.080299 | 1.889699 | 0.553141 | 00:01 | . 5 | 0.828938 | 1.813550 | 0.593262 | 00:01 | . 6 | 0.623377 | 1.710710 | 0.662516 | 00:01 | . 7 | 0.479048 | 1.723749 | 0.687663 | 00:01 | . 8 | 0.350940 | 1.458227 | 0.718913 | 00:01 | . 9 | 0.260764 | 1.484386 | 0.732096 | 00:01 | . 10 | 0.201649 | 1.384711 | 0.752523 | 00:01 | . 11 | 0.158970 | 1.384149 | 0.753011 | 00:01 | . 12 | 0.132954 | 1.377875 | 0.750244 | 00:01 | . 13 | 0.117867 | 1.367185 | 0.756104 | 00:01 | . 14 | 0.109761 | 1.366078 | 0.756104 | 00:01 | . Language Model 7 - Weight-Tied Regularized LSTM&#39;s . While this new LSTM model did much better, we can see it&#39;s overfitting to the training data i.e. notice how while the training loss is going down, the validation loss does not really improve so the model is not generalising well. Dropout can be a regularization method that we can use here to try to prevent overfitting. And architecture that uses dropout as well as an LSTM is called an AWD-LSTM. . Activation regularization (AR) and temporal activation regularization (TAR) are two regularization methods very similar to weight decay. . To regularize the final activations these need to be stored, then we add the means of the squares of them to the loss (times a factor alpha for control). . loss += alpha * activations.pow(2).mean() . TAR is connected to the sequential nature of text i.e. that that outputs of LSTM&#39;s should make sense when in order. TAR encourages this by penalising large differences between consequtive activations so to encourage them to be as small as possible. . loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean() . AR is usually applied to dropped out activations (to not penalise activations zeroed) while TAR is applied to non-dropped out activations for the opposite reasons. The RNNRegularizer callback will apply both of these. . With Weight tying we make use of a symmeterical aspect of embeddings in this model. At the start of the model the embedding layer converts words to embedding numbers, at the end of the model we map the final layer to words. We might expect these could be very similar mappings if not the same, so we can explictly encourage this by actually making the weights the same for this first and final layers/embeddings. . self.h_o.weight = self.i_h.weight . So we can combine dropout with AR &amp; TAR and weight tying to train our LSTM. . class LMModel7(Module): def __init__(self, vocab_sz, n_hidden, n_layers, p): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.drop = nn.Dropout(p) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h_o.weight = self.i_h.weight self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)] def forward(self, x): raw,h = self.rnn(self.i_h(x), self.h) out = self.drop(raw) self.h = [h_.detach() for h_ in h] return self.h_o(out),raw,out def reset(self): for h in self.h: h.zero_() # Create regularized learner using RNNRegularizer learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)]) # This is the equivilent as the TextLearner automatically adds these callbacks learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . # Train the model and add extra regularization with weight decay learn.fit_one_cycle(15, 1e-2, wd=0.1) . epoch train_loss valid_loss accuracy time . 0 | 2.513700 | 1.898873 | 0.498942 | 00:01 | . 1 | 1.559825 | 1.421029 | 0.651937 | 00:01 | . 2 | 0.810041 | 1.324630 | 0.703695 | 00:01 | . 3 | 0.406249 | 0.870849 | 0.801514 | 00:01 | . 4 | 0.211201 | 1.012451 | 0.776774 | 00:01 | . 5 | 0.117430 | 0.748297 | 0.827474 | 00:01 | . 6 | 0.072397 | 0.652809 | 0.843587 | 00:01 | . 7 | 0.050372 | 0.740491 | 0.826172 | 00:01 | . 8 | 0.037560 | 0.796995 | 0.831462 | 00:01 | . 9 | 0.028582 | 0.669326 | 0.850830 | 00:01 | . 10 | 0.022323 | 0.614551 | 0.855632 | 00:01 | . 11 | 0.018281 | 0.670560 | 0.858317 | 00:01 | . 12 | 0.014915 | 0.645430 | 0.856771 | 00:01 | . 13 | 0.012732 | 0.656426 | 0.855387 | 00:01 | . 14 | 0.011765 | 0.683027 | 0.853271 | 00:01 | . Conclusion . In this article we have examined how we build an LSTM language model, in particular the AWD-LSTM architecture, which also makes use of several regularization techniques. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/natural-language-processing/2021/05/31/lstm-language-model-from-scratch.html",
            "relUrl": "/deep-learning-theory/natural-language-processing/2021/05/31/lstm-language-model-from-scratch.html",
            "date": " • May 31, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "The fastai Mid-level API",
            "content": "Introduction . In this article we will introduce and explore the fastai mid-level API, in particular it&#39;s data preparation features. The mid-level api offers more control and customisation than the high-level api. We will apply the mid-level api to the example of predicting Siamese Images. . The fastai library (as of 2021) is a layered API that has 4 levels of abstraction. . Application layer | High level API | Mid level API | Low level API | . . Mid-level API key concepts . Transforms . In a previous article on text classification we saw how tokenisation and numericalisation were used to prepare the text data for the model. . Both of these classes also have a decode() method, that allows us to reverse the process i.e. to convert tokens back into text, though this may not be exactly the same as the default tokeniser currently is not entirely reversable. . decode is also used by show_batch() and show_results(), as well as by other inference methods. . When we create an object of the tokenizer or numeraclize class, a setup method is called (which trains a tokenizer if needed and creates a vocab for the numericalizer) each is then applied to the text stream in turn. These transformation type tasks are common, so fastai has created a base level class to encapsulate them called the Transform class. Both Tokenize and Numericalize are Transforms. . In general, a Transform is an object that behaves like a function and has an optional setup method that will initialize some inner state (like the vocab inside num) and an optional decode that will reverse the function (this reversal may not be perfect, as we saw with tok). . Another aspect about transforms is that they are always used with tuples, as this reflects the common nature of our data in terms of input &amp; output variables. Also when we apply a transform to this tuple e.g. Resize we may want to apply it in a different way to the input and output variables (if at all). . Creating your own transform . So to create your own transform you can do this by writing a function, and then passing it to the Transform class. The Transform class will only apply the given function to one of a matching type, so for example because we have specified the type as int here the transform is not applied when the input is a floating point number. . def f(x:int): return x+1 tfm = Transform(f) tfm(2),tfm(2.0) . (3, 2.0) . Also note here no setup() or decode() methods have been created here. . This approach of passing a function as an argument to another function is called a decorator which is specified by being preceeded by an &#39;@&#39; symbol and putting it before a function definition. So we can do the same as above using this approach. . @Transform def f(x:int): return x+1 f(2),f(2.0) . (3, 2.0) . If we want to specify a setup or decode method we will instead need to subclass Transform and implement the methods that way. . class NormalizeMean(Transform): def setups(self, items): self.mean = sum(items)/len(items) def encodes(self, x): return x-self.mean def decodes(self, x): return x+self.mean . When used, this class will first run the setup method, then apply the encodes method. The decode method will do the reverse when run. . tfm = NormalizeMean() tfm.setup([1,2,3,4,5]) start = 2 y = tfm(start) z = tfm.decode(y) tfm.mean,y,z . (3.0, -1.0, 2.0) . Note the methods implmented and called are different i.e. setups vs setup. The reason for this is for example here setup also does some other things before then calling setup for you. . Pipelines . To join several transforms together we can use the Pipeline class, which is essentially a list of transforms. . tok = Tokenizer.from_folder(path) tok.setup(txts) toks = txts.map(tok) num = Numericalize() num.setup(toks) nums = toks.map(num) tfms = Pipeline([tok, num]) t = tfms(txts[0]); t[:20] . TensorText([ 2, 19, 932, 81, 27, 20, 32, 34, 7, 260, 119, 1256, 143, 62, 64, 11, 8, 415, 1289, 14]) . You can also decode the pipeline, but there is no setup. . tfms.decode(t)[:100] . &#39;xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the f&#39; . TfmdLists . The class we can use to connect our raw data (e.g. files) to a pipeline is the TfmdLists class. This can also run the appropriate setup methods for us. We can do this in a short, one line way for example. . tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize]) . When initialised, TfmdLists will run the setup method of each transform in order, passing the items transformed by the previous transform. We can see the result of the pipeline on any item by indexing into the objected created. . t = tls[0]; t[:20] . TensorText([ 2, 19, 1033, 73, 28, 20, 30, 35, 7, 265, 120, 1061, 176, 56, 70, 10, 8, 457, 1440, 14]) . TfmdLists also can decode. . tls.decode(t)[:100] . &#39;xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the f&#39; . And show. . tls.show(t) . xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the film stood out even when viewing it so many years after it was made . xxmaj the story by the little known c xxmaj virgil xxmaj georghiu is remarkable , almost resembling a xxmaj tolstoy - like story of a man buffeted by a cosmic scheme that he can not comprehend . xxmaj compare this film with better - known contemporary works such as xxmaj xxunk &#39;s &#34; schindler &#39;s xxmaj list &#34; and you begin to realize the trauma of the xxmaj world xxmaj war xxup ii should be seen against the larger canvas of racism beyond the simplistic xxmaj nazi notion of xxmaj aryan vs xxmaj jews . xxmaj this film touches on the xxmaj hungarians dislike for the xxmaj romanians , the xxmaj romanians dislike of the xxmaj russians and so on .. even touching on the xxmaj jews &#39; questionable relationships with their xxmaj christian xxmaj romanian friends , while under stress . xxmaj as i have not read the book , it is difficult to see how much has been changed by the director and screenplay writers . xxmaj for instance , it is interesting to study the xxmaj romanian peasant &#39;s view of emigrating to xxup usa with the view of making money only to return to xxmaj romania and invest his earnings there . xxmaj in my opinion , the character of xxmaj johann xxmaj moritz was probably one of the finest roles played by xxmaj anthony xxmaj quinn ranking alongside his work in &#34; la xxunk the xxmaj greek &#34; and &#34; xxunk &#34; . xxmaj the finest and most memorable sequence in the film is the final one with xxmaj anthony xxmaj quinn and xxmaj virna xxmaj lisi trying to smile . xxmaj the father carrying a daughter born out his wife &#39;s rape by xxmaj russians is a story in itself but the director is able to show the reconciliation by a simple gesture -- the act of carrying the child without slipping into melodramatic footage . xxmaj today after the death of xxmaj princess xxmaj diana we often remark about the insensitive paparazzi . xxmaj the final sequence is an indictment of the paparazzi and the insensitive media ( director xxmaj verneuil also makes a similar comment during the court scene as the cameramen get ready to pounce on xxmaj moritz ) . xxmaj the interaction between xxmaj church and xxmaj state was so beautifully summed up in the orthodox priest &#39;s laconic statement &#34; i pray to xxmaj god that xxmaj he guides those who have power to use them well . &#34; xxmaj some of the brief shots , such as those of a secretary of a minister doodling while listening to a petition -- said so much in so little footage . xxmaj the direction was so impressive that the editing takes a back seat . xxmaj finally what struck me most was the exquisite rich texture of colors provided by the cameraman xxmaj andreas xxmaj winding -- from the brilliant credit sequences to the end . i recalled that he was the cameraman of another favorite xxmaj french film of mine called &#34; ramparts of xxmaj clay &#34; directed by jean - louis xxmaj xxunk . i have not seen such use of colors in a long while save for the xxmaj david xxmaj lean epics . xxmaj there were flaws : i wish xxmaj virna xxmaj lisi &#39;s character was more fleshed out . i could never quite understand the xxmaj serge xxmaj xxunk character -- the only intellectual in the entire film . xxmaj the railroad station scene at the end seems to be lifted out of xxmaj sergio xxmaj leone westerns . xxmaj finally , the film was essentially built around a love story , that unfortunately takes a back seat . xxmaj to sum up this film impressed me in more departments than one . xxmaj the story is relevant today as it was when it was made . . TfmdLists is plural because it can accomodate both training and validation data using a splits parameter, you just need to pass the indicies for each set. . cut = int(len(files)*0.8) splits = [list(range(cut)), list(range(cut,len(files)))] tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize], splits=splits) . You can then access the train and validation parts using the train and valid attributes. . tls.valid[0][:20] . TensorText([ 2, 22, 15452, 12, 9, 8, 16833, 22, 16, 13, 483, 2773, 12, 2472, 596, 46, 13, 955, 24, 4841]) . You can also convert a TfmdLists object directly into a Dataloaders object using the dataloaders() method. . More generally, you will most likely have 2 or more parallel pipelines of transforms: one for processing raw data into inputs and one to process raw data into outputs/targets. . So in this example, to get the target (a label) we can get it from the parent folder. There is a function parent_label() that can do this for us. . lbls = files.map(parent_label) lbls . (#50000) [&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;...] . We then need a transform that can take these targets, and extract the unique class names to build a vocab during the setup() method, and transform these string class names into integers. The Categorize class can do this for us. . cat = Categorize() cat.setup(lbls) cat.vocab, cat(lbls[0]) . ([&#39;neg&#39;, &#39;pos&#39;], TensorCategory(1)) . So putting these together, from our raw files data we can create a TfmdLists object that will take our files reference, and chain these two transforms together so we get our processed target variable. . tls_y = TfmdLists(files, [parent_label, Categorize()]) tls_y[0] . TensorCategory(1) . But this means we have separate TfmdLists objects for our input and output variables. To bind these into one object we need the Datasets class. . Datasets and Dataloaders . The Datasets object allows us to create two or more piplines bound together and output a tuple result. It will do the setup() for us, and if we index into this Datasets object it will return a tuple with the results of each pipeline. . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms]) x,y = dsets[0] x[:20],y . (TensorText([ 2, 19, 1033, 73, 28, 20, 30, 35, 7, 265, 120, 1061, 176, 56, 70, 10, 8, 457, 1440, 14]), TensorCategory(1)) . As before if we pass a splits parameter, this will further split these into separate train and validation sets. . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms], splits=splits) x,y = dsets.valid[0] x[:20],y . (TensorText([ 2, 22, 15452, 12, 9, 8, 16833, 22, 16, 13, 483, 2773, 12, 2472, 596, 46, 13, 955, 24, 4841]), TensorCategory(0)) . We can also reverse the process to get back to our raw data using decode. . t = dsets.valid[0] dsets.decode(t) . (&#39;xxbos &#34; igor and the xxmaj lunatics &#34; is a totally inept and amateurish attempt at a crazy - hippie - cult - killing - spree horror movie . xxmaj apparently even nearly twenty years later , xxmaj charles xxmaj manson was still inspiring overenthusiastic but incompetent trash - filmmakers . xxmaj this is a typical xxmaj troma production , meaning in other words , there &#39;s a lot of boring and totally irrelevant padding footage to accompany the nonsensical plot . xxmaj there &#39;s a bit of random gore and gratuitous nudity on display x96 which is n &#39;t bad x96 but it &#39;s all so very pointless and ugly that it becomes frustrating to look at . &#34; igor and the xxmaj lunatics &#34; is so desperate that it &#39;s even using a lot of the footage twice , like the circle saw killing for example . xxmaj the incoherent plot tries to tell the story of a hippie cult run by the drug - addicted and xxmaj charlie xxmaj manson wannabe xxmaj paul . xxmaj one of xxmaj paul &#39;s lower ranked disciples , named xxmaj igor , becomes a little bit too obsessed with the xxmaj bible stories and drug orgies and gradually causes the entire cult to descent further into criminal insanity . xxmaj just to illustrate through a little example exactly how crazy xxmaj igor is : he tears the heart straight out of the chest of a really sexy black hitch - hiker girl ! xxmaj there &#39;s an annoying synthesizer soundtrack and some truly embarrassingly lame pseudo - artistic camera tricks , like slow - motion footage and lurid dream sequences . xxmaj maybe there &#39;s one sequence that more or less qualifies as worthwhile for trash fanatics and that &#39; is when a poor girl is cut in half with a machete . xxmaj for no particular reason , the camera holds the shot of the blade in the bloodied stomach for fifteen whole seconds .&#39;, &#39;neg&#39;) . Finally before we can use this data to train a model, we need to convert this Datasets object into a Dataloaders object. In this text example, we also need to pass along a special argument to take care of the padding problem with text data, just before we batch the elements which can do using the before_batch argument. . dls = dsets.dataloaders(bs=64, before_batch=pad_input) . dataloaders directly calls DataLoader on each subset of our Datasets. fastai&#39;s DataLoader expands the PyTorch class of the same name and is responsible for collating the items from our datasets into batches. It has a lot of points of customization, but the most important ones that you should know are: . after_item:: Applied on each item after grabbing it inside the dataset. This is the equivalent of item_tfms in DataBlock. | before_batch:: Applied on the list of items before they are collated. This is the ideal place to pad items to the same size. | after_batch:: Applied on the batch as a whole after its construction. This is the equivalent of batch_tfms in DataBlock. | . So putting all these steps together taking our raw data to ending up with a Dataloaders object ready to train a model. . tfms = [[Tokenizer.from_folder(path), Numericalize], [parent_label, Categorize]] files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;]) splits = GrandparentSplitter(valid_name=&#39;test&#39;)(files) dsets = Datasets(files, tfms, splits=splits) dls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input) . Note also the use of GrandparentSplitter and dl_type. This last argument is to tell dataloaders to use the SortedDL class of DataLoader, and not the usual one. SortedDL constructs batches by putting samples of roughly the same lengths into batches. . So the above is equalivilent to what we did with the high-level datablock api, just using the mid-level api which exposes more control, customisation and choices. The mid-level api version of all this was of course this. . path = untar_data(URLs.IMDB) dls = DataBlock( blocks=(TextBlock.from_folder(path),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path) . Applying the Mid-level API: Siamese Pair . So we will apply using the mid-level api to a Siamese pair use case. A Siamese model takes 2 images and has to decide if they are of the same category or not. We will use fastai&#39;s pets dataset for this exercise. . Lets get the data. . from fastai.vision.all import * path = untar_data(URLs.PETS) files = get_image_files(path/&quot;images&quot;) . If we didn&#39;t need to show our input data, we could just create one stream to process the input images. Since we would also like to be able to look at the input images as well, we need to do something different, creating a custom type. When you call the show() method on a TfmdLists or Datasets object, it will decode items till you end up with items of the same type of object that the show method is called upon. . We will create a SiameseImage class that is subclassed from fastuple and will contain 3 things: 2 images, and a boolean that indicates of they are the same class. We will also implement a custom show method, that joins the 2 images with a black line divider. . The most important part of this class are the last 3 lines. . class SiameseImage(fastuple): def show(self, ctx=None, **kwargs): img1,img2,same_breed = self if not isinstance(img1, Tensor): if img2.size != img1.size: img2 = img2.resize(img1.size) t1,t2 = tensor(img1),tensor(img2) t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1) else: t1,t2 = img1,img2 line = t1.new_zeros(t1.shape[0], t1.shape[1], 10) return show_image(torch.cat([t1,line,t2], dim=2), title=same_breed, ctx=ctx) . img = PILImage.create(files[0]) s = SiameseImage(img, img, True) s.show(); . img1 = PILImage.create(files[1]) s1 = SiameseImage(img, img1, False) s1.show(); . The key thing about transforms is that they dispatch over tuples or their subclasses. Thats why we subclassed from fastuple, so we can apply any transform that works on images to our SiameseImage object and it will be applied to each image in the tuple. . For example. . s2 = Resize(224)(s1) s2.show(); . Here the Resize transform is applied to each of the images, but not the boolean target variable. . Lets now build a better SiameseTransform class for training our model. . Lets first define a function that will extract the target classes of our images. . def label_func(fname): return re.match(r&#39;^(.*)_ d+.jpg$&#39;, fname.name).groups()[0] . So this is how we cill create our dataset. We will pick a series of images, and for each image we pick we will with a probability of 0.5 pick an image of the same or different class, and assign a true or false label accordingly. This will be done in the _draw() method. . There is also a difference between the training and validation sets, which is exactly why the transforms need to be initialised with the splits: with the training set we will make that random pick each time we read an image, whereas with the validation set we will make the random pick only once at initialisation - which is a kind of data augmentation that allows us to get more varied samples during training - but ensures a consistant validation set throughout. . class SiameseTransform(Transform): def __init__(self, files, label_func, splits): self.labels = files.map(label_func).unique() self.lbl2files = {l: L(f for f in files if label_func(f) == l) for l in self.labels} self.label_func = label_func self.valid = {f: self._draw(f) for f in files[splits[1]]} def encodes(self, f): f2,t = self.valid.get(f, self._draw(f)) img1,img2 = PILImage.create(f),PILImage.create(f2) return SiameseImage(img1, img2, t) def _draw(self, f): same = random.random() &lt; 0.5 cls = self.label_func(f) if not same: cls = random.choice(L(l for l in self.labels if l != cls)) return random.choice(self.lbl2files[cls]),same . splits = RandomSplitter()(files) tfm = SiameseTransform(files, label_func, splits) tfm(files[0]).show(); . So to recap: in the mid-level API we have 2 classes that can help us apply transforms to our data: TfmdLists and Datasets. One applies a single pipeline of transforms, while the other can apply several pipelines in parallel to build tuples. Given our new transform here already creates tuples, so we can just use TfmdLists in this case. . tls = TfmdLists(files, tfm, splits=splits) show_at(tls.valid, 0); . We are almost there, to create a Dataloader from this we just call the dataloaders method on this object. But we need to be careful, as our new transform class does not take item_tfms and batch_tfms like a DataBlock. . However the fastai DataLoader has several hooks that are named after events; here what we apply on the items after they are grabbed is called after_item, and what we apply on the batch once it&#39;s built is called after_batch. . dls = tls.dataloaders(after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]) . Note also we have to explictly pass more transforms than we would previously, this is because the DataBlock class/API usually adds these automatically, and since we have create a custom transform we need to explictly request these. . ToTensor is the one that converts images to tensors (again, it&#39;s applied on every part of the tuple). | IntToFloatTensor converts the tensor of images containing integers from 0 to 255 to a tensor of floats, and divides by 255 to make the values between 0 and 1. | . We now have the right DataLoaders object ready to train a model to predict on Siamese images. . Conclusion . We have seen how we can use fastai&#39;s mid-level api to do more custom work as needed, with more control than we would have with the high-level data block api. .",
            "url": "https://www.livingdatalab.com/fastai/2021/05/30/fastai-midlevel-api.html",
            "relUrl": "/fastai/2021/05/30/fastai-midlevel-api.html",
            "date": " • May 30, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Creating a custom text classifier for movie reviews",
            "content": "Introduction . In this article we are going to train a deep learning text classifier using the fastai library. We will do this for the IMDB movie reviews dataset. In particular, we will look at fastai&#39;s ULMFit approach which involves fine tuning a language model more with specialised text before using this language model as a basis for a classification model. . Text Pre-processing . So how might we proceed with building a language model, that we can then use for clasisifcation? Consider with one of the simplest neural networks, a collaberative filtering model. This uses embedding matrices to encode different items (such as films) and users, combine these using dot products to calculate a value, which we test against known ratings - and use gradient descent to learn the correct embedding matrices to best predict these ratings. . Optionally, we can create instead a deep learning model from this by concatinating the embedding matrices instead of the dot product, then putting the result through an activtion function, and more layers etc. . So we could use a similar approach, where we put a sequence of words through a neural network via encoding them in an embedding martix for words. However a significant difference from the collaberative filtering approach here is the idea of a sequence. . We can proceed with these 5 steps: . Tokenisation: convert words to recognised units | Numericalisation: convert tokens to numbers | Create data loader: Create a data loader to train the language model which creates a target variable offset by one word from the input variable from the text data | Train language model: We need to train a model that can take an amount of text data of variable length, and be able to predict the next word for any word in the sequence. | Train classifier model: Using what the language model has learned about the text as a basis, we can build on top of this to create and train a language model. | This is an approach pioneered by fastai called the Universal Langauage Model Fine-tuining (ULMFit) approach. . . Tokenisation . Lets get the data and tokenise it using the fastai library tools. . path = untar_data(URLs.IMDB) files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) # Show example text data txt = files[0].open().read(); txt[:75] . &#39;I caught up with this movie on TV after 30 years or more. Several aspects o&#39; . Fastai has an english word tokeniser, lets see how it works. . # Test word tokeniser function spacy = WordTokenizer() toks = first(spacy([txt])) print(coll_repr(toks, 30)) . (#626) [&#39;I&#39;,&#39;caught&#39;,&#39;up&#39;,&#39;with&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;on&#39;,&#39;TV&#39;,&#39;after&#39;,&#39;30&#39;,&#39;years&#39;,&#39;or&#39;,&#39;more&#39;,&#39;.&#39;,&#39;Several&#39;,&#39;aspects&#39;,&#39;of&#39;,&#39;the&#39;,&#39;film&#39;,&#39;stood&#39;,&#39;out&#39;,&#39;even&#39;,&#39;when&#39;,&#39;viewing&#39;,&#39;it&#39;,&#39;so&#39;,&#39;many&#39;,&#39;years&#39;,&#39;after&#39;,&#39;it&#39;...] . # Test word tokeniser class tkn = Tokenizer(spacy) print(coll_repr(tkn(txt), 31)) . (#699) [&#39;xxbos&#39;,&#39;i&#39;,&#39;caught&#39;,&#39;up&#39;,&#39;with&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;on&#39;,&#39;xxup&#39;,&#39;tv&#39;,&#39;after&#39;,&#39;30&#39;,&#39;years&#39;,&#39;or&#39;,&#39;more&#39;,&#39;.&#39;,&#39;xxmaj&#39;,&#39;several&#39;,&#39;aspects&#39;,&#39;of&#39;,&#39;the&#39;,&#39;film&#39;,&#39;stood&#39;,&#39;out&#39;,&#39;even&#39;,&#39;when&#39;,&#39;viewing&#39;,&#39;it&#39;,&#39;so&#39;,&#39;many&#39;,&#39;years&#39;...] . The class goes beyond just converting the text to tokens for words, for example it creates tokens like &#39;xxbos&#39; which is a special token to indicate the beginning of a new text sequence i.e. &#39;beggining of stream&#39; standard NLP concept. . The class applies a series fo rules and transformations to the text, here is a list of them. . defaults.text_proc_rules . [&lt;function fastai.text.core.fix_html&gt;, &lt;function fastai.text.core.replace_rep&gt;, &lt;function fastai.text.core.replace_wrep&gt;, &lt;function fastai.text.core.spec_add_spaces&gt;, &lt;function fastai.text.core.rm_useless_spaces&gt;, &lt;function fastai.text.core.replace_all_caps&gt;, &lt;function fastai.text.core.replace_maj&gt;, &lt;function fastai.text.core.lowercase&gt;] . Numericalisation . # Get first 2000 reviews to test txts = L(o.open().read() for o in files[:2000]) # Tokenise toks = tkn(txt) # Select subset of tokenised reviews toks200 = txts[:200].map(tkn) num = Numericalize() # Numericalise tokens - create a vocab num.setup(toks200) # Show first 20 tokens of vocab coll_repr(num.vocab,20) . &#34;(#2096) [&#39;xxunk&#39;,&#39;xxpad&#39;,&#39;xxbos&#39;,&#39;xxeos&#39;,&#39;xxfld&#39;,&#39;xxrep&#39;,&#39;xxwrep&#39;,&#39;xxup&#39;,&#39;xxmaj&#39;,&#39;the&#39;,&#39;.&#39;,&#39;,&#39;,&#39;and&#39;,&#39;a&#39;,&#39;of&#39;,&#39;to&#39;,&#39;is&#39;,&#39;in&#39;,&#39;it&#39;,&#39;i&#39;...]&#34; . # Now we can convert tokens to numbers for example nums = num(toks)[:20]; nums . TensorText([ 2, 19, 726, 79, 29, 21, 32, 31, 7, 314, 112, 1195, 138, 63, 71, 10, 8, 393, 1524, 14]) . Create data loader . So we need to join all the text together, and then divide it into specific sized batches of multiple lines of text of fixed length, which maintain the correct order of the text within each batch. At every epoch the order of the reviews is shuffled, but we then join these all together and construct mini-batches in order, which our model will process and learn from. This is all done automatically by the fastai library tools. . # Get some example numericalised tokens nums200 = toks200.map(num) # Pass to dataloader dl = LMDataLoader(nums200) # Get first batch of data and check sizes x,y = first(dl) x.shape,y.shape . (torch.Size([64, 72]), torch.Size([64, 72])) . # Examine example input variable should be start of a text &#39; &#39;.join(num.vocab[o] for o in x[0][:20]) . &#39;xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of&#39; . # Examine example target variable which is the same plus added next word - this is what we want to predict &#39; &#39;.join(num.vocab[o] for o in y[0][:20]) . &#39;i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the&#39; . Training a text classifier . Fine tune language model . We can further simplify the text preparation for training our language model by combining the tokenisation, numericalisation and dataloader creation into one step by creating a TextBlock and then a dataloader. . # Create text dataloader for language model training dls_lm = DataBlock( blocks=TextBlock.from_folder(path, is_lm=True), get_items=get_imdb, splitter=RandomSplitter(0.1) ).dataloaders(path, path=path, bs=128, seq_len=80) . # Create a language model learner, by default will use x-entropy loss learn = language_model_learner( dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() # Train model learn.fit_one_cycle(1, 2e-2) # Save model encoder learn.save_encoder(&#39;finetuned&#39;) . Fine tune classifier model . To fine tune the classifier model we create the data loader in a slightly different way. . # Create text dataloader for classifier model training - using lm vocab dls_clas = DataBlock( blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path, path=path, bs=128, seq_len=72) . # Create classifier learner learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16() # Load encoder from language model learn = learn.load_encoder(&#39;finetuned&#39;) . When fine tuning the classifier, it is found to be best if we gradually unfreeze layers to train, and this is best done in manual steps. The first fit will just train the last layer. . # Train model - last layer only learn.fit_one_cycle(1, 2e-2) . # Unfreeze a few more layers and train some more with discriminative learning rates learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . # Unfreeze more layers and train more learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . # Unfreeze whole model and train more learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . On this IMDB dataset we can achieve a classification accuracy of around 95% using this approach. . Conclusion . In this article we have looked in more detail at how we can train a text classifier using the 3 step ULMFit fastai approach, and achieve a good level of accuracy. We also saw in more detail what the fastai library does under the hood to make this process much easier. .",
            "url": "https://www.livingdatalab.com/projects/natural-language-processing/2021/05/29/custom-text-classifier-movie-reviews.html",
            "relUrl": "/projects/natural-language-processing/2021/05/29/custom-text-classifier-movie-reviews.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Collaberative filtering from scratch",
            "content": "Introduction . In this article we will look to build a collaberitive filtering model from scratch, using pure Pytorch and some support from the Fastai deep learning library. We will also look at the theory and mathematics behind collaberative filtering. . Dataset . We will use the MovieLens dataset, and a special subset curated by fastai of the 100,000 movies. This consists of 2 separate tables for ratings and movies, which we will join together. . path = untar_data(URLs.ML_100k) # Load ratings table ratings = pd.read_csv(path/&#39;u.data&#39;, delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . movies = pd.read_csv(path/&#39;u.item&#39;, delimiter=&#39;|&#39;, encoding=&#39;latin-1&#39;, usecols=(0,1), names=(&#39;movie&#39;,&#39;title&#39;), header=None) movies.head() . movie title . 0 1 | Toy Story (1995) | . 1 2 | GoldenEye (1995) | . 2 3 | Four Rooms (1995) | . 3 4 | Get Shorty (1995) | . 4 5 | Copycat (1995) | . ratings = ratings.merge(movies) ratings.head() . user movie rating timestamp title . 0 196 | 242 | 3 | 881250949 | Kolya (1996) | . 1 63 | 242 | 3 | 875747190 | Kolya (1996) | . 2 226 | 242 | 5 | 883888671 | Kolya (1996) | . 3 154 | 242 | 3 | 879138235 | Kolya (1996) | . 4 306 | 242 | 5 | 876503793 | Kolya (1996) | . dls = CollabDataLoaders.from_df(ratings, item_name=&#39;title&#39;, bs=64) dls.show_batch() . user title rating . 0 542 | My Left Foot (1989) | 4 | . 1 422 | Event Horizon (1997) | 3 | . 2 311 | African Queen, The (1951) | 4 | . 3 595 | Face/Off (1997) | 4 | . 4 617 | Evil Dead II (1987) | 1 | . 5 158 | Jurassic Park (1993) | 5 | . 6 836 | Chasing Amy (1997) | 3 | . 7 474 | Emma (1996) | 3 | . 8 466 | Jackie Chan&#39;s First Strike (1996) | 3 | . 9 554 | Scream (1996) | 3 | . Theory . The key data here is the ratings i.e. the user-movie ratings, as we can see in the listing above. In collaberative filtering, an easier way to see this is as a user-item matrix, with movies as columns, users as rows, and cells as the ratings for each user-movie combination. . . We can see here some cells are not filled in which are ratings we do not know, these are the values we would like to predict so we can know for each user which movie they would like. . So how might we approach this? If we imagine there are some reasons that effect peoples preferences, lets call them factors such as genre, actors etc then that might give us a basis to figure out which users would like each movie. What if we could represent these factors as a set of numbers? then we could represent each user and movie as a unique set of these numbers (or vectors) representing how much of each of the factors that user or movie represented. . Then we could say, we want each of these user and movie factors vectors when multipled to equal a rating. This would give us a basis to learn these factors, as we have ratings we know, and we could use these to estimate the ratings we don&#39;t know. This approach of using movie vectors multipled by user vectors and summed up is known as the dot product and is the basis of matrix multiplication. . . So we can randomly initialise these user and movie vectors, and learn the correct values for these that predict the ratings we know, using gradient descent. . So to do the dot product we could look up the index of each user and movie, then multiply the vectors. But neural networks don&#39;t know how to look up using an index, they only multiply matrices together. However we can do a index looking up using matrix multiplication by using one-hot encoded vectors. . The matrix you index by multiplying by a one-hot encoded matrix, is called an embedding or embedding matrix. So our model will learn the values of these embedding matrices for the users and movies, using gradient descent. . It&#39;s actually very easy to create a collaberative filtering model using fastai&#39;s higher level methods - but we are going to explore doing this from scratch in this article. . learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) . learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.937713 | 0.953276 | 00:11 | . 1 | 0.838276 | 0.873933 | 00:11 | . 2 | 0.717332 | 0.832581 | 00:11 | . 3 | 0.592723 | 0.818247 | 00:11 | . 4 | 0.476174 | 0.818869 | 00:11 | . Collaberative filtering - Model 1 . We will now create our first collaberative filtering model from scratch. This will contain the embedding matrices for the users and movies, and will implement a method (in Pytorch this is normally the forward method) to do a dot product of these 2 matrices. . So the number of factors for each user and movie matrix will be determined when the model is initialised. . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return (users * movies).sum(dim=1) . So the input x to the model will be a tensor of whatever the batch size is multiplied by 2 - where the first column (x[:, 0]) contains the user IDs and the second column (x[:, 1]) contains the movie IDs. So the input essentially has 2 columns. . x,y = dls.one_batch() x.shape . torch.Size([64, 2]) . So we have defined our architecture and so can now create a learner to optimise the model. Because we are building the model from scratch we will use the Learner class to do this. We will use MSE as our loss function as this is a regression problem i.e. we are predicting a number, the rating. . n_users = len(dls.classes[&#39;user&#39;]) n_movies = len(dls.classes[&#39;title&#39;]) # Create model with 50 factors for users and movies each model = DotProduct(n_users, n_movies, 50) # Create Learner object learn = Learner(dls, model, loss_func=MSELossFlat()) . learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 1.336391 | 1.275613 | 00:09 | . 1 | 1.111210 | 1.126141 | 00:09 | . 2 | 0.988222 | 1.014545 | 00:09 | . 3 | 0.844100 | 0.912820 | 00:09 | . 4 | 0.813798 | 0.898948 | 00:09 | . Collaberative filtering - Model 2 . So how can we improve the model? we know the predictions - the ratings: should be between 0-5. Perhaps we can help our model by ensuring the predictions are forced between these valid values? We can use a sigmoid function to do this. . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return sigmoid_range((users * movies).sum(dim=1), *self.y_range) . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.985542 | 1.002896 | 00:10 | . 1 | 0.869398 | 0.914294 | 00:10 | . 2 | 0.673619 | 0.873486 | 00:10 | . 3 | 0.480611 | 0.878555 | 00:10 | . 4 | 0.381930 | 0.882388 | 00:10 | . Collaberative filtering - Model 3 . So while that didn&#39;t make a huge difference, there is more we can do to improve. At the moment by using our user and movie embedding matrices, this only gives us a sense of how a particular movie or user is described as specific values for these latent factors. What we don&#39;t have is a way to indicate something general about a particular movie or user such as this person is really fussy, or this movie is generally good or not good. . We can encode this general skew for each movie and user by including a bias value for each, which we can add after we have done the dot product. So lets add bias to our model. . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.user_bias = Embedding(n_users, 1) self.movie_factors = Embedding(n_movies, n_factors) self.movie_bias = Embedding(n_movies, 1) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) res = (users * movies).sum(dim=1, keepdim=True) res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) return sigmoid_range(res, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.941588 | 0.955934 | 00:10 | . 1 | 0.844541 | 0.865852 | 00:10 | . 2 | 0.603601 | 0.862635 | 00:10 | . 3 | 0.420309 | 0.883469 | 00:10 | . 4 | 0.293037 | 0.890913 | 00:10 | . So this started much better, but then got worse! Why is this? This looks like a case of overfitting. So we can&#39;t use data augmentation for this type of model, so we need some other way to stop the model fitting too much to the data i.e. some kind of regularization. One way to do this is with weight decay . Weight decay . So with weight decay, aka L2 regularization - adds an extra term to the loss function as the sum of all the weights squared. This will penalise our model for getting more complex than it needs to be i.e. overfitting, so this will encorage our model to have weights as small as possible the get the job done i.e. occams razor. . Why weights squared? The idea is the larger the model parameters are, the steeper the slope of the loss function. This can cause the model to focus too much on the data points in the training set. Adding weight decay will make training harder, but will force our model to be as simple as possible, less able to memorise the training data - and force it to generalise better. . Rather than calculate the sum of all weights squared, we take the derivative which is 2 x parameters and addd to our loss e.g. . parameters.grad += wd 2 parameters . Where wd is a factor we can control. . x = np.linspace(-2,2,100) a_s = [1,2,5,10,50] ys = [a * x**2 for a in a_s] _,ax = plt.subplots(figsize=(8,6)) for a,y in zip(a_s,ys): ax.plot(x,y, label=f&#39;a={a}&#39;) ax.set_ylim([0,5]) ax.legend(); . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.928223 | 0.957245 | 00:11 | . 1 | 0.886639 | 0.881928 | 00:10 | . 2 | 0.771433 | 0.832266 | 00:11 | . 3 | 0.597242 | 0.821840 | 00:11 | . 4 | 0.506455 | 0.822054 | 00:10 | . Manual embeddings . So we used a pre-made Embeddings class to make our embedding matrices, but did&#39;nt see how it works so lets make our own now. So we need a randomly initialised weight matrix for each. By default Pytorch tensors are not added as trainable parameters (think why, data are tensors also) so we need to create it in a particular way to make the embeddings trainable, using the nn.Parameter class. . # Create tensor as parameter function, with random initialisation def create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) # Create model with our manually created embeddings class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.movie_factors = create_params([n_movies, n_factors]) self.movie_bias = create_params([n_movies]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] movies = self.movie_factors[x[:,1]] res = (users*movies).sum(dim=1) res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] return sigmoid_range(res, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.923637 | 0.948116 | 00:12 | . 1 | 0.869177 | 0.879707 | 00:11 | . 2 | 0.731731 | 0.836616 | 00:12 | . 3 | 0.590497 | 0.825614 | 00:11 | . 4 | 0.484070 | 0.825161 | 00:11 | . Collaberative filtering - Model 4 . Our models developed so far are not deep learining models, as they dont have many layers. To turn this into a deep learning model we need to take the results of the embedding lookup and concatenate those activations together - this will then give us instead a matrix that we can then pass through linear layers with activation functions (non-linearities) as we would in a deep learning model. . As we are concatinating embeddings rather than taking their dot product, the embedding matrices can have different sizes. Fastai has a handy function for reccomending optimal embedding sizes from the data. . embs = get_emb_sz(dls) embs . [(944, 74), (1665, 102)] . class CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) . model = CollabNN(*embs) . learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) . epoch train_loss valid_loss time . 0 | 0.943013 | 0.951147 | 00:11 | . 1 | 0.913711 | 0.900089 | 00:11 | . 2 | 0.851407 | 0.886212 | 00:11 | . 3 | 0.816868 | 0.878591 | 00:11 | . 4 | 0.772557 | 0.881083 | 00:11 | . Fastai lets you create a deep learning version of the model like this with the higher level function calls by passing use_nn=True and lets you easily create more layers e.g. here with two hidden layers, of size 100 and 50, respectively. . learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 1.002377 | 0.995780 | 00:13 | . 1 | 0.879825 | 0.928848 | 00:13 | . 2 | 0.888932 | 0.899229 | 00:13 | . 3 | 0.821391 | 0.871980 | 00:13 | . 4 | 0.796728 | 0.869211 | 00:13 | . Conclusion . So we have built a collaberative filtering model from scratch, and saw how it can learn latent factors from the data itself. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/05/25/collaberative-filtering-from-scratch.html",
            "relUrl": "/deep-learning-theory/2021/05/25/collaberative-filtering-from-scratch.html",
            "date": " • May 25, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "State-of-the-art Deep Learning image model techniques in 2021",
            "content": "Introduction . In this article we are going to look at some of the most advanced techniques available in 2021 for training deep learning vision models. These go beyond the basics of mini-batch gradient descent, learning rates, pre-sizing, transfer learning, discriminative learning rates, and mixed-precision training. . Library and Dataset . I will be using the fastai deep learning library for code examples, as well as the fastai curated Imagenette dataset which is a specially curated subset of the well known ImageNet dataet of 1.3 million images from 1,000 categories. The Imagenette dataset consists of a much smaller set of images and just 10 categories. . We will define a baseline model here using the dataset to then compare the effect of each advanced technique. . . Normalisation . When training a model, its helpful to ensure the image data is normalised. This ensures that different images end up with data that is in the same range of values, which helps the model better focus more on the content on the images. So here by normalised we mean we want the image data values to have a mean of 0 and a standard deviation of 1. . The fastai library will automatically normalise images per batch, and this is suitable for models that we might train from scratch. When using transfer learning this default approach is not a good idea, because a pre-trained model has been trained on image data with a particular mean and standard deviation. So to use a pre-trained model with new images, we need to ensure these new images are normalised to the same mean and standard deviation that the original model data was trained with. . We can do this my specifying normalisation stats in fastai, which already knows the stats for many common datasets, including of course fastai’s own Imagenette dataset which makes it much easier. . We can also define a function get_dls() which will make it quicker to define different types of data loader i.e. with different batch or image sizes. . . After applying our normalisation, we can see the mean and standard deviation are approximatly 0 and 1 respectively on a test batch of images. . Lets now try this normalised data and train our model. . . While normalisation here hasn’t made a huge improvement over our baseline model, normalisation does make a bigger difference especially with bigger models and more data. . Progressive resizing . Progressive re-sizing is another technique pioneered by fastai. Essentially this involves training models on smaller versions of the images first, before continuing training on bigger images. This has 2 major benefits: . Model training time is much faster | Model accuracy ends up better than if we trained the model only on bigger images | . How can this be the case? lets remember that with convolutional deep learning models, early layers focus on recognising primitive features like lines and edges, and later layers more composite features such as eyes or fur. So if we change the image size during training, our earlier model will still have learnt many useful things applicable to bigger and higher resolution images. . In a way, this is a bit like training a model in one area then re-using that model on a similar area - which might sound familiar? As it should since this is very much what transfer learning is about as well, which works very well. So we should perhaps not be so surprised that this could work. . Another benefit of using lower resolution/smaller versions of the images first is that this is another kind of data augmentation - which should also help our models generalise better. . So lets use our get_dls() function that we defined earlier to define a data loader for our smaller lower resolution images and train the model for a few epochs. . . We will then define a new data loader for bigger images, and continue to train our model with these. . . So we can see we are already getting much better results than our baseline with just a few epochs, and much more quickly. It’s worth considering for the desired task, if transfer learning can in some cases harm performance. This might happen for example if the pre-trained model is trained on images already quite similar to the new ones you want to recognise - as in this case the model parameters are likely already quite close to what is needed, and progressive resizing could move the parameters further away from this and good results. If the use case for the pre-rained model is very different to what it was originally trained on i.e. very different sizes, shapes, styles etc - then progressive resizing here might actually help. . In either case, trying things experimentally would probably be the best way to determine which was the better approach. . Test time augmentation . Training time data augmentation is a common technique to help improve model training by providing different versions of the same images to improve the way the model generalises and with less data. Common techniques include random resize crop, squish, stretch, and image flip for example. . Test time augmentation (TTA) is an interesting approach of using augmentation when using the model for inference. Essentially at inference time for a given image, different augmentations of the same image will be predicted on by the model, then we can use either the average or maximum of these versions as a measure of model performance. This can give us a better idea of the models true performance, and often results in improvements in performance. . In the fastai library its quite easy to apply TTA. . . While this does not add any extra time to training, it does make inference slower. . Mixup . Mixup is a technique introduced in the paper mixup: Beyond Empirical Risk Minimization by Hongyi Zhang et al. It’s a powerful data augmentation technique that seeks to address the weaknesses of many previous methods such as crop-resize, squishing etc. One of the key drawbacks to previous approaches was needing some expert knowledge of when those techniques were applicable or nor as well as how to apply them. . For example, take the flip method that augments by flipping the image vertically or horizontally - should one apply that one way or the other? it will probably depend on the kind of images you have. Also flipping is limited i.e. you can just apply it one way or the other, there are no ‘degrees of flipping’ for example. Having ‘degrees of’ or gradation of augmentation can be very useful for giving the model a rich variety of images along the spectrum to allow it to better learn and generalise. . Mixup essentially takes two images and combines them, with a randomly selected weight of transparency for each image for the combined image. We will then take a weighted average (using the same random weights) applied to the labels of each image, to get the labels for the mixed image. . So the combined image will have labels that are in proportion to the amount of each original image. . . Here the third image is built from 0.3 of the first one and 0.7 of the second one. The one-hot encoded labels for the first and second images and final mixed image would be say: . Image 1: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] | Image 2: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0] | Mixed: [0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0] | . We can use this Mixup technique in the fastai library in the following way. . . This model is likely going to be harder and longer to train, for all the many examples ‘in between’ that this method will generate, but it should allow the model to generalise better. The beauty of this approach is that unlike many previous approaches this doesn’t require extra knowledge about the dataset to use - the ‘appropriateness’ of each image is present in the augmentation - so its the degrees of which we vary here really. This also opens this method to use in other areas beyond even vision models, to NLP for example. . Mixup also helps with another problem. A ‘perfect’ dataset with perfect labels say of only 1 and 0, pushes the model to train towards a sense of ‘perfection’ and absolute confidence, this is of course the ideal that the cross-entropy loss function does well to optimise for. By removing ‘perfection’ from our labels, we force our model to train to become less absolutely confident in its predictions, we train it to become more nuanced and subtle in its predictions that err towards partial than perfect probabilities for label prediction. . Label smoothing . Deep learning vision models train for perfection, this is especially due to the nature of the most common classification loss function cross-entropy loss. For example, because our labels are often perfect i.e. 1 or 0 despite how perfect the expression of that label is in the image, the model will keep pushing for the perfection of 1 or 0 i.e. even 0.999 will not be good enough. This can lead to overfitting, and is a consequence of this kind of training and loss function. In practice, images often do not conform to the perfection of the labels assigned them. . With label smoothing rather than use perfect labels of 1 and 0, we use a number a bit less than 1 and a number a bit more than zero. By doing this we encourage our model to become less confident, more robust (e.g. if there is mislabelled data). This model should generalise better. This technique was introduced in the paper Rethinking the Inception Architecture for Computer Vision by C Szegedy et al. . . We can use this technique in the fastai library in the following way. . . As with Mixup, you generally won’t see significant improvements with this technique until you train more epochs. . Conclusion . In this article we have covered 5 state-of-the-art techniques for training deep learning vision models using the fastai deep learning library, each of which can significantly help produce the best results currently possible for vision models in 2021. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/05/22/state-of-the-art-deep-learning-image-model-2021.html",
            "relUrl": "/deep-learning-theory/2021/05/22/state-of-the-art-deep-learning-image-model-2021.html",
            "date": " • May 22, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Automatic Recognition of Woodlands and Water from Satellite Imagery using AI and Deep Learning",
            "content": ". Introduction . Satellite imagery is being used together with AI and deep learning in many areas to produce stunning insights and discoveries. In this project I look at applying this approach to recognising buildings, woodlands &amp; water areas from satellite images. . Dataset . The dataset used for this project comes from the research paper LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands and Water from Aerial Imagery which gathered satellite imagery of different areas of Poland. The satellite images have 3 spectral bands so are RGB jpg images. The researchers chose to use 4 classes for identifying objects in these images: . Building | Woodland | Water | Background (i.e. everything else) | . . This is an image segmentation dataset, so the classes are expressed as colourmap shading by pixels for parts of the image that correspond to each class. These image colourmap/masks for the classes are represented as a png image, one of each of the satellite images. . . Methodology . For this project I used the fastai deep learning library which is based on Pytorch/Python. The dataset lends itself to the approach of image segmentation classification as the classes in the dataset are expressed as shaded regions, as opposed to say multi-label image classification using text labels. For this approach, the UNET deep learning architecture has prooven extremely good for image segmentation problems - which is what I chose to use here. . Prepare and load data . ## Set path for image files path = Path(DATA_PATH) ## Set the text for the classes codes = np.array([&quot;building&quot;, &quot;woodland&quot;, &quot;water&quot;, &quot;Background&quot;]) . ## Load image files from the path fnames = get_image_files(path/&quot;images&quot;) ## Define a function to get the label png file def label_func(fn): return path/&quot;labels&quot;/f&quot;{fn.stem}_m{&#39;.png&#39;}&quot; . ## Create a data loader for this image segmentation dataset dls = SegmentationDataLoaders.from_label_func( path, bs=BATCH_SIZE, fnames = fnames, label_func = label_func, codes = codes ) ## Show a batch of images dls.show_batch() . So we can see a nice feature of the fastai library is able to combine the original satellite image overlayed with the colourmap for the class labels with some transparency so we can see the image and labels together. . dls.show_batch(figsize=(10,10)) . dls.show_batch(figsize=(10,10)) . Training the UNET model . ## Create a UNET model using the resnet18 architecture learn = unet_learner(dls, resnet18) ## Train the model learn.fine_tune(3) ## Show the results learn.show_results() . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . epoch train_loss valid_loss time . 0 | 0.425658 | 0.249241 | 26:51 | . epoch train_loss valid_loss time . 0 | 0.207543 | 0.165862 | 27:07 | . 1 | 0.173056 | 0.227951 | 27:02 | . 2 | 0.128388 | 0.140451 | 27:00 | . So fastai&#39;s fine_tune() method will first freeze all but the last layer and train for 1 epoch, and then train for the specified number of epochs (3 in our case). Because image segmentation datasets are particularly big, these can take quite a while to train even on a GPU. In this case 1+3 epochs has taken around 2 hours of training time. . We can see though in this time both the training and validation loss have come down quite nicely, even after 4 epochs. Looking at our results we can see our UNET model has done extremely well when tested on validation images not previosuly seen by the model in the Target/Prediction pair examples above. . Lets see some more tests and results. . learn.show_results() . learn.show_results(max_n=4) . The model does seem to have generally done a good job at predicting the correct classes in the image for a wide range of different satellite image types and conditions. . Conclusion . In this project we have looked at a satellite image segmentation dataset and have achieved good results from only a limited amount of training. .",
            "url": "https://www.livingdatalab.com/projects/2021/05/15/satellite-recognition-buildings-woodland-water-ai.html",
            "relUrl": "/projects/2021/05/15/satellite-recognition-buildings-woodland-water-ai.html",
            "date": " • May 15, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "An Eye in the Sky - How AI and Satellite Imagery can help us better understand our changing world",
            "content": ". Introduction . Many of the greatest challenges the world faces today are global in nature, climate change being one of the clearest examples. While we also have huge amounts of data of different kinds, trying to make sense of all this data to help us make better decisions can be a significant challenge in itself when attempted by humans alone. AI is a powerful technology that holds huge potential for helping us use this data more easily, to help us make better decisions for the problems we face. . In the water industry where I work, satellite image data and AI holds great potential for helping solve a number of problems, such as the detection of leaks, water resource management to ensure on ongoing water supply accounting for changes in population and climate change, water quality monitoring, and flood protection. . Beyond the water industry, satellite images and AI are working together to provide critical insights in many diverse areas such as disaster response and recovery, the discovery of hidden archaeological sites, city infrastructure monitoring, combating illegal fishing, and predicting crop yields. . But how does this technology work? and can you understand the basics of how it works without any technical knowledge? The answer is I believe yes, and I will try to illustrate this by describing a recent project I completed using this approach. . Using AI to automatically recognise Woodlands and Water areas . In a recent project, I used satellite imagery from Poland to train an AI to automatically recognise areas in the images such as woodlands and water. So AI is just about throwing some data at it and some magic happens? Actually not quite! This is a common myth about how AI actually works. . . The key requirement for using AI is not just using any data, but something called labelled data. Labelled data is data that has been tagged with one or more labels that describe things inside the data. So in this project, the labels used for these satellite images were woodlands and water: if an image contains one of these things, the image would have a label or tag for that. This is how the AI learns, it looks at each satellite image, see’s which labels it has, and tries to learn what in the image indicates each label. So it’s not really magic how an AI learns at all, an AI just learns from examples of labelled things - that’s it basically. . Here are some more examples of the satellite images, now with labels. The labels are colours filled in, so for example water areas are coloured in pink and woodland areas in red. . . How do these images get their coloured labels? well some poor human has to painstakingly spend hours carefully colouring them all in with the right colours. But its well worth it, since we can train the AI to use these labelled satellite images (just 33 images) to learn to recognise these things in them, and once it can do this, we can then use the AI to recognise these things in new satellite images, as many times as we like. This is the real power of AI systems, which can learn to do things only humans could previously do, and then do them far more efficiently and quickly than a human could ever do, millions of times, without needing even a coffee break! . So how well does the AI learn to recognise these things? after running the training process a while, these are some of the results I got when I tested the AI on images it had never seen. Here the ‘Target’ on the left are the labels for images the AI has never seen, and the ‘Prediction’ on the right are what the AI thinks the label colour areas should be in the image. . . So I’d say the AI has done a pretty good job. You can see in these examples it seems to have recognised the correct water areas (in pink) and woodland areas (in red) pretty well? The AI was only trained for a limited time, most likely if I had trained it for longer it would have done even better. I could now use this AI on any new satellite images, and know it would do quite well at recognising woodland and water areas fairly accurately. Because the labels here are actually coloured dots on the image, we could add up all the dots for water or woodland on an image and get a fairly accurate measure for how much water or woodland there was there. . Just imagine what we could do with even this fairly simple AI. For example, we could use it to estimate the woodland and water areas of different parts of a country quite accurately, anywhere in the world. If we took different satellite photos of the same area over time, we could estimate how the water or woodland areas were changing over time, and by how much, all automatically. The possibilities are endless. . More about the technical details of this project can be found in this article . Conclusion . In this article I’ve introduced how satellite images and AI are a powerful new technology being used to provide valuable insights to a range of different challenges and tasks we face in the world today. By describing my own project using AI to recognise woodland and water areas in satellite images, I hope I have given you a better understanding of how this technology actually works, and of its huge potential for humanity. .",
            "url": "https://www.livingdatalab.com/opinion/2021/05/14/ai-satellite-images.html",
            "relUrl": "/opinion/2021/05/14/ai-satellite-images.html",
            "date": " • May 14, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "What AI can tell us about the hidden preferences of human beings",
            "content": ". Introduction . AI systems are increasingly being used in almost every area of human activity, from the online world, in streaming media, social media &amp; online shopping - to the offline world, in policing, healthcare and other public services as well as many different physical industries such as water, energy, agriculture and manufacturing. These applications of AI are having a huge impact, often beyond what is commonly known to the public, both positive and negative. . Most work in the field is focussed on trying to use AI to solve a particular problem at hand, and if the problem is ‘solved’ then little more thought is often done. Much less work goes on into understanding the fundamental nature of the AI created to solve a particular problem. To some extent this is of course because the main motivation is to solve the problem, but another reason is often because AI systems i.e. artificial neural networks, are often incredibly complicated, are not directly created by humans - and so can actually be very difficult to understand the inner workings of. Questions such as How is it doing it? Why is it doing it? What has it learnt? are questions often not addressed - as long as the AI system seems to ‘get the job done’. . It’s certainly my belief that this much neglected area is worth far more work than it often gets, not only to allow us to understand the AI system at a deeper level, but that it might also give us new insights and understandings into the area the AI system is being applied to. . In this article I’m going to look at one particular area of AI application called Recommendation Systems. For a recent project, I created an AI system for recommending new books to readers. I then extended the project to study how this AI book recommendation system itself was working, and discovered what it had learnt about the hidden preferences of book readers. . What are Recommendation Systems? . Recommendation systems are very common particularly in online services. For example, Amazon suggestions for alternative products, on Netflix suggestions for films, on Spotify for music, or on Facebook for the content you see on your newsfeed. All of these services and more use recommendation systems to suggest new content to people, but what are these systems and how do these actually work? . A very simple approach might be to recommend the most popular items to people. Of course, popular items would probably appeal to many - but not to everyone. With modern AI based recommendation systems we can do much better than this, to make more personalised suggestions that are unique to each person. There are two main approaches to this: content-based filtering and collaborative filtering. . . With content-based filtering, we look at the content a person has previously looked at (eg songs or films you have previously watched) as a basis to recommend new content. In this case, the AI system does the work here to understand what content is similar, for example what films are similar. This might be based on more obvious things such as the film genre, or which actors are in the film. However it can also be based on less obvious things that the AI can learn for itself about what makes films similar or different, things that are not given to the AI at all, but that the AI system can learn itself. These hidden aspects are called latent factors. . With collaborative filtering, we look at other people who are similar to us, and suggest items that they have liked, that we have not yet seen. Here the AI system does the work to understand which people are similar to us, as a basis to make suggestions. As a simple example, on a music service, we could find another listener who has listened to some of the same songs we have, find a song they have listened to that we have not, then recommend that to us. However, this simple strategy may not always be effective, just because 2 people like the same song, that does not always mean they would both like another song that one of them liked, let alone that both people are ‘similar’? What makes people similar and different, might be based on things like the genre of music they liked, which artists etc. . But what truly makes people similar or different in their preferences can also be based on less obvious things, more nuanced and subtle reasons, things people often do not perhaps even understand themselves, biases, etc that are hidden and unconscious, and yet drive and influence people’s choices and behaviour. These hidden biases and influences are things not given to the AI at all, how could they be? and yet, they are things AI systems can still learn about for itself, which are again here called latent factors. . Creating a book recommendation system . For my book recommendation system, I used the collaborative filtering approach. The data I used to create this system is the Book Crossing dataset which is data on peoples book ratings of around 27,000 different books, from the Book Crossing community website, gathered in September 2004. There are around 28,000 users of this website who are from all over the world, and from a range of different ages. The key data is a table of individual ratings of a person (identified by a unique user id), a book and a the value of the rating (a number between 0-10). . . These user ratings are not exhaustive, i.e. every user has not rated every book. Note also there is no extra information about the books such as categories, or about each person such as their ages. But we don’t actually need this extra data, we can create an AI collaborative filter based system (commonly called a ‘model’) that learns just from the table of users, books and ratings, to build up an understanding of the latent factors that uniquely describes each book, and each person. Once the model has been ‘trained’ on this data, and learnt these latent factors - the model can then use these latent factors to make recommendations for each person, about what new books they might like. . When the AI model learns, it doesn’t directly see the real ratings - it just tries to make guesses about what the ratings should be. We then compare the guesses to the actual ratings we know, and give the model back some measure of accuracy, which the model then uses to improve its guesses in the next cycle. This training process repeats for many cycles. . Going down the rabbit hole: what is our book recommendation system actually doing? . So we now have a book recommendation system that can suggest new books to people. But we can if we choose, take things further. Simply from the data of book ratings and the learning process, the system has had to understand something more, about the implicit reasons certain people prefer certain books over others - and indeed perhaps about general qualities that drive these choices. These qualities might correspond to categories we might recognise as more obvious book genres, but they might also correspond to other qualities that are not commonly recognised as ‘categories’ yet might still be factors that drive people to prefer different books. . How might we try and understand these latent factors that drive people’s preferences for books? . We actually have 2 types of latent factors, normal factors and bias factors. Bias factors represent a general bias towards a book, either positive or negative. This will mean for that book, regardless if it would be generally a good suggestion for a person - if it has a negative bias it will be far less likely to be suggested. Similarly, if a book has a very positive bias, it might be more likely to be suggested to you, even if you would not normally read that kind of book i.e. you would not normally read that genre. We can think of bias then as some kind of measure of ‘general popularity’ of a book. . Negative bias books . So these are the bottom 10 books with the most negative bias in the AI model: . Wild Animus | The Law of Love | Blood and Gold (Rice Anne Vampire Chronicles) | Gorky Park | The Cat Who Went into the Closet | Waiting to Exhale | Night Moves (Tom Clancy’s Net Force No. 3) | Ruthless.Com (Tom Clancy’s Power Plays) | Ground Zero and Beyond | Say Cheese and Die! | . Let us look at the 2 books with the most negative bias, ‘Wild Animus’ and ‘The Law of love’. So what is Wild Animus about? The synopsis reads: . “Sam Altman is an intense young man with an animal energy whose unleashed and increasingly unhinged imagination takes him first to Seattle and then farther north, to the remote Alaskan wilderness. …” . This book does have many many online reviews, on the whole which can be summarized by the review What the hell is Wild animus?. The review concludes with a quote from a goodreads reviewer: . “I’ll tell you the ending. A column of lava erupts from beneath his feet while he is dressed in a goat costume and wolves are in mid-air tearing him apart.” . On the whole it seems, Wild Animus seems to provoke a very unfavourable response from most reviewers! The next most negative bias book is ‘The law of love’, it’s synopsis reads: . “After one night of passion, Azucena, an astroanalyst in twenty-third-century Mexico City, is separated from her Twin Soul, Rodrigo, and journeys across the galaxy and through past lives to find her lost love, encountering a deadly enemy along the way…” . As it happens this book has as many positive reviews as negative online, in fact few reviews seem neutral at all. So this is not universally seen as a bad book, by humans who post reviews online anyway. Nevertheless, our AI model regards this as a book that should not really be suggested to anyone. Is that because the book seems to be so divisive? and perhaps there are other books that are ‘safer bets’? Either way, the computer says no. . Positive bias books . Let’s now look at the top 10 books with the most positive bias in the AI model: . The Lovely Bones: A Novel | Harry Potter and the Goblet of Fire | The Da Vinci Code | Harry Potter and the Prisoner of Azkaban | The Secret Life of Bees | Harry Potter and the Sorcerer’s Stone | Harry Potter and the Chamber of Secrets | Where the Heart Is | To Kill a Mockingbird | The Red Tent | . So looking in more detail at the 2 books with the most positive bias we have ‘The lovely bones’ and ‘Harry Potter and the Goblet of Fire’. So the synopsis of “The lovely bones” is as follows: . “It is the story of a teenage girl who, after being raped and murdered, watches from her personal Heaven as her family and friends struggle to move on with their lives while she comes to terms with her own death. “ . This book has a large number of very favourable reviews, in fact it was hard to find a very negative review of this book at all. The New York Time’s review perhaps sums up well the general sentiment felt by most reviewers of this book: . “…Sebold deals with almost unthinkable subjects with humor and intelligence and a kind of mysterious grace. The Lovely Bones takes the stuff of neighborhood tragedy – the unexplained disappearance of a child, the shattered family alone with its grief – and turns it into literature…” . So we can perhaps appreciate some of the reasons perhaps why the AI model thinks this is a good book to recommend to anyone, regardless of what their normal reading preferences might be. The second most positively biased book is “Harry Potter and the Goblet of fire”. Being one of a series of some of the most popular books of all time - this is perhaps not surprising at all that the AI model thinks this would be a good book to recommend to most people, regardless of their normal reading preferences. . Looking at other latent factors . So for the remaining latent factors, we actually have 50 of them for each of our 27,000 books - so quite a few! However we can use a process called dimensionality reduction to actually reduce these down, to the 2 most important latent factors for all books. We can then plot each book on a graph, with the measure that book has for each of these 2 key latent factors. . . A bigger view of this image of latent factors 1 &amp; 2 can be seen here . Here we can see 50 books plotted. On the horizontal axis that is a measure of how much of latent factor 1 each book has. On the vertical axis, that is a measure of how much of latent factor 2 each book has. . Let’s look into latent factor 1, which is the strongest latent factor used by the AI model to make book recommendations. . Books with high values for latent factor 1 . We can see in the bottom right corner of the chart ‘The lovely bones’. This has one of the highest measures of factor 1, because it is one of the furthest to the right. We also know from our look at bias factors, that this is the book with the strongest positive bias latent factor as well i.e. a generally popular book. Let’s also note it falls into the categories of ‘Crime, Thriller, Mystery’. . Looking at another book with a high factor 1, in the top right we have ‘Good in Bed’. The synopsis of the book is: . “It tells the story of an overweight Jewish female journalist, her love and work life and her emotional abuse issues with her father.” . It generally also has good reviews, and would fall into the category of ‘Women’s fiction’. Let’s look at a third book with a high factor 1, “The life of Pi”. The synopsis of this book is: . “After the tragic sinking of a cargo ship, a solitary lifeboat remains bobbing on the wild, blue Pacific. The only survivors from the wreck are a sixteen year-old boy named Pi, a hyena, a zebra (with a broken leg), a female orang-utan and a 450-pound Royal Bengal tiger.” . Again this book generated very good reviews, was very popular, and might fall into the category of ‘Contemporary fiction’. What are some common things to note about all of these books with a high latent factor 1? . They are all very popular and have great critical acclaim | 2 of these books turned into films, and the third is currently being adapted for film. | All 3 have a theme of a huge personal tragedy, which the protagonist is successful in overcoming and rising above by themselves | . So lets bear this in mind, while we look at books with the lowest latent factor 1. . Books with low values for latent factor 1 . Books with low values for factor one are on the far left of the chart. For example we have ‘A painted house’ the synopsis of this being: . “A Painted House is a moving story of one boy’s journey from innocence to experience, drawn from the personal experience of legendary legal thriller author John Grisham” . Let’s also note this would fall into the categories of ‘Contemporary fiction, mystery’. Looking at another book with a low factor 1 ‘The street lawyer’, the synopsis being: . “…about an attorney who leaves his high-priced firm to work for the less fortunate.” . This also seems to be another book by John Grisham, that would fall into categories such as ‘Thriller, Mystery’. Looking at Grisham’s work, how might we characterise his work more generally? He is well known for writing legal thrillers, and themes such as ‘the triumph of the underdog’, however A painted house seems not to quite fit these themes, an exception - so why is it here? A theme that might link both is ‘the triumph of working together’ in the case of the legal thrillers it’s the lawyer, the legal system his collaborators, in ‘a painted house’ its the family that needs to pull together to triumph, as explained in this review: . “…The primary theme is the importance of family: only by pulling together does the family achieve even moderate success” . In fact when we look at the other books with the lowest factor 1, on the far left of the chart, they pretty much are all John Grisham legal thriller books such as: . The Pelican brief | The Brethren | The Summons | The Firm | . So what is latent factor 1? . Let’s now consider what factor 1 might actually be about. Given most of these books, regardless of having a low or high value of factor 1, have all been popular and successful - so popularity I would argue has nothing to do with what factor 1 is really about. . Based on what we have learnt about these books so far, I would speculate that latent factor 1 might represent a measure of ‘The triumph of the group vs the triumph of the individual’ as a theme-axis. So, low values of factor 1 would correspond to ‘The triumph of the group’ type themes, and high values of factor 1 would correspond to ‘The triumph of the individual’ type themes for books. . Remember the AI model is given no information about book categories, authors, genres, themes etc. All the AI has to learn from is the ratings between users and books - that’s all. Not only has our AI model discovered this particular axis theme by itself from very limited information, but it has done so because the AI model has judged that this theme-axis, whatever it is, is one of the most useful for the purposes of making good book recommendations to people. . Discussion . So how do we make sense of our findings? We can’t conclusively say that my suggested ‘triumph of the group vs triumph of the individual’ theme-axis is generally true, or the key hidden factor for understanding generally why people prefer certain books over others. Firstly, it’s based on an inevitably limited data set of books, people and ratings. Perhaps the people who made those ratings are not representative of the general population? Secondly, we only randomly chose 50 books to plot for our latent factors. What if we randomly picked a different set of 50 books, would we see the same kind of themes for latent factor 1, or something else? If the ‘triumph of the group vs triumph of the individual’ theme axis does appear to be a key factor over many more books and people - why is this the case? and what does it suggest about human beings more generally? However these are questions that could be further investigated, researched, and better answered - with more time, and the conclusions of which could potentially be very interesting indeed. . What we can say is that from very limited information, looking at a limited number of books, and looking at some of its latent factors such as biases and the main key factor - this AI model seems to have discovered many relationships we could recognise as humans such as ‘generally popular books’ and ‘generally awful books’. The interpretation of the key latent factor as ‘triumph of the group vs triumph of the individual’ as a theme-axis is of course very speculative at this stage, and yet very intriguing! Would you come to a different conclusion looking at the books at either end of the axis of latent factor 1 on the chart? What do you think latent factor 1 on the horizontal axis is all about? What do you think latent factor 2 on the vertical axis is about? I’d love to hear your feedback and thoughts on this, so do feel free to comment below. . Conclusion . In this article I’ve tried to highlight a few key themes: that AI is being used everywhere, that little work is often done to understand how and why these AI systems work, and that we have so much to gain by actually trying to look inside and understand these AI systems better. I’d also argue this is becoming more and more important, given the growing impact of these increasingly powerful systems on our world and human life. . Looking inside these AI systems, even though not straightforward - gives us the chance to know more about what they are really doing and why, and can give us intriguing hints about the domains in which they are used, which I have tried to illustrate with my book recommendation project. . When these AI systems are used in the domain of human choices, the potential is there hidden within these systems to perhaps discover new insights, and pose new questions about ourselves and our choices, that we may have never even considered or knew to ask. Perhaps by looking a little deeper into AI, we can see a little deeper into ourselves. .",
            "url": "https://www.livingdatalab.com/opinion/2021/04/04/ai-human-preferences.html",
            "relUrl": "/opinion/2021/04/04/ai-human-preferences.html",
            "date": " • Apr 4, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Livingdatalab",
          "content": "Livingdatalab is the personal blog of Pranath Fernando, a Data Scientist in the water industry. . You can contact me via twitter or linkedin. .",
          "url": "https://www.livingdatalab.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.livingdatalab.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}