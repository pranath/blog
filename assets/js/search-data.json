{
  
    
        "post0": {
            "title": "Feature Transformation with Amazon SageMaker Processing Job and Feature Store",
            "content": "Introduction . In earlier articles we introduced AWS cloud services for data science, and showed how it can help with different stages of the data science &amp; machine learning workflow. . . In this article we will look at the Prepare &amp; Transform stage using AWS including: . Feature engineering | Feature store | . Using the raw Women&#39;s Clothing Reviews dataset - we will prepare it to train a BERT-based natural language processing (NLP) model. The model will be used to classify customer reviews into positive (1), neutral (0) and negative (-1) sentiment. . We will convert the original review text into machine-readable features used by BERT. To perform the required feature transformation we will configure an Amazon SageMaker processing job, which will be running a custom Python script. . The Bert language model . BERT stands for &#39;Bidirectional Encoder Representations from Transformers&#39;. So Bert language models are based on the transformer type models first created in 2017. . . In a previous article we used a Blazing Text Language Model to create a text classifier. Blazing Text language models are in turn based on Word2Vec type language models. But how do word2vec/Blazing text language models work? essentially these models convert individual words into a series of numbers or a vector. . . I used word2vec in one of my first data science/deep learning projects back in 2019 classifying disaster text messages. . This means with word2vec similar meaning words will have similar numbers and vector positions, this is what this language model learns. The downside of this approach though is it allows only for one sense of what a word might mean - but we know in practice the meaning of a word can be effected by the context. . For example, if we were trying to decide if these two phrases were positive or negative: . I love the dress | I love the dress, but not the price | . A word2vec model might end up giving quite positive sentiment to both of these phrases when summing up the meaning of these words individually, yet we can see that the second phrase might have more neutral if not negative sentiment, because here &#39;love&#39;, usually positive, has been modified by the context of the words its within. . This is one key thing that transformer models such as BERT or GPT can do, they can take into account the context of a word, and indeed process an entire phrase in one go to give a vector for that group of words, rather than for one word at a time. . . In particular transformers use attention to capture the relationship and meaning between words used together. You can find out more about the differences between word2vec and transformer models here. . Feature Engineering at Scale . Amazon SageMaker processing allows you to perform data related tasks such as, preprocessing, postprocessing, and model evaluation at scale. SageMaker processing provides this capability by using a distributed cluster. By specifying some parameters, you can control how many notes and the type of the notes that make up the distributed cluster. . . Sagemaker Feature Store is a fully managed service that provides purpose-built feature store. SageMaker Feature Store provides you with a centralized repository to securely save and serve features from. . Next, SageMaker Feature Store provides you with the capabilities to reuse the features, not just across a single machine learning project, but across multiple projects. A typical challenge that data scientist sees is training an inference skew that could result from discrepancies in the data used for training and the data used for inferencing. Sagemaker Feature Store helps reduce the skew by reusing the features across training and inference traces and by keeping the features consistent. . Finally, SageMaker Feature Store provides the capabilities to create it for the features both in real time and batch. The ability to creating for features in real time suppose use cases such as near real time ML predictions. Similarly, the ability to look up features in batch mode can be used to support use cases, such as model training. . . Import Libraries &amp; Initialise . import boto3 import sagemaker import botocore config = botocore.config.Config(user_agent_extra=&#39;dlai-pds/c2/w1&#39;) # low-level service client of the boto3 session sm = boto3.client(service_name=&#39;sagemaker&#39;, config=config) featurestore_runtime = boto3.client(service_name=&#39;sagemaker-featurestore-runtime&#39;, config=config) sess = sagemaker.Session(sagemaker_client=sm, sagemaker_featurestore_runtime_client=featurestore_runtime) bucket = sess.default_bucket() role = sagemaker.get_execution_role() region = sess.boto_region_name . Configure the SageMaker Feature Store . Configure dataset . The raw dataset is in the public S3 bucket. Let&#39;s start by specifying the S3 location of it: . raw_input_data_s3_uri = &#39;s3://dlai-practical-data-science/data/raw/&#39; print(raw_input_data_s3_uri) . s3://dlai-practical-data-science/data/raw/ . List the files in the S3 bucket (in this case it will be just one file): . !aws s3 ls $raw_input_data_s3_uri . 2021-04-30 02:21:06 8457214 womens_clothing_ecommerce_reviews.csv . Configure the SageMaker feature store . As the result of the transformation, in addition to generating files in S3 bucket, we will also save the transformed data in the Amazon SageMaker Feature Store to be used by others in our organization, for example. . To configure a Feature Store we need to setup a Feature Group. This is the main resource containing all of the metadata related to the data stored in the Feature Store. . A Feature Group should contain a list of Feature Definitions. A Feature Definition consists of a name and the data type. The Feature Group also contains an online store configuration and an offline store configuration controlling where the data is stored. Enabling the online store allows quick access to the latest value for a record via the GetRecord API. The offline store allows storage of the data in your S3 bucket. We will be using the offline store here. . Let&#39;s setup the Feature Group name and the Feature Store offline prefix in S3 bucket. . import time timestamp = int(time.time()) feature_group_name = &#39;reviews-feature-group-&#39; + str(timestamp) feature_store_offline_prefix = &#39;reviews-feature-store-&#39; + str(timestamp) print(&#39;Feature group name: {}&#39;.format(feature_group_name)) print(&#39;Feature store offline prefix in S3: {}&#39;.format(feature_store_offline_prefix)) . Feature group name: reviews-feature-group-1675799708 Feature store offline prefix in S3: reviews-feature-store-1675799708 . Taking two features from the original raw dataset (Review Text and Rating), we will transform it preparing to be used for the model training and then to be saved in the Feature Store. Here we will define the related features to be stored as a list of FeatureDefinition. . from sagemaker.feature_store.feature_definition import ( FeatureDefinition, FeatureTypeEnum, ) feature_definitions= [ # unique ID of the review FeatureDefinition(feature_name=&#39;review_id&#39;, feature_type=FeatureTypeEnum.STRING), # ingestion timestamp FeatureDefinition(feature_name=&#39;date&#39;, feature_type=FeatureTypeEnum.STRING), # sentiment: -1 (negative), 0 (neutral) or 1 (positive). It will be found the Rating values (1, 2, 3, 4, 5) FeatureDefinition(feature_name=&#39;sentiment&#39;, feature_type=FeatureTypeEnum.STRING), # label ID of the target class (sentiment) FeatureDefinition(feature_name=&#39;label_id&#39;, feature_type=FeatureTypeEnum.STRING), # reviews encoded with the BERT tokenizer FeatureDefinition(feature_name=&#39;input_ids&#39;, feature_type=FeatureTypeEnum.STRING), # original Review Text FeatureDefinition(feature_name=&#39;review_body&#39;, feature_type=FeatureTypeEnum.STRING), # train/validation/test label FeatureDefinition(feature_name=&#39;split_type&#39;, feature_type=FeatureTypeEnum.STRING) ] . Let&#39;s create the feature group using the feature definitions defined above. . from sagemaker.feature_store.feature_group import FeatureGroup feature_group = FeatureGroup( name=feature_group_name, feature_definitions=feature_definitions, sagemaker_session=sess ) print(feature_group) . FeatureGroup(name=&#39;reviews-feature-group-1675799708&#39;, sagemaker_session=&lt;sagemaker.session.Session object at 0x7f9cb912c350&gt;, feature_definitions=[FeatureDefinition(feature_name=&#39;review_id&#39;, feature_type=&lt;FeatureTypeEnum.STRING: &#39;String&#39;&gt;), FeatureDefinition(feature_name=&#39;date&#39;, feature_type=&lt;FeatureTypeEnum.STRING: &#39;String&#39;&gt;), FeatureDefinition(feature_name=&#39;sentiment&#39;, feature_type=&lt;FeatureTypeEnum.STRING: &#39;String&#39;&gt;), FeatureDefinition(feature_name=&#39;label_id&#39;, feature_type=&lt;FeatureTypeEnum.STRING: &#39;String&#39;&gt;), FeatureDefinition(feature_name=&#39;input_ids&#39;, feature_type=&lt;FeatureTypeEnum.STRING: &#39;String&#39;&gt;), FeatureDefinition(feature_name=&#39;review_body&#39;, feature_type=&lt;FeatureTypeEnum.STRING: &#39;String&#39;&gt;), FeatureDefinition(feature_name=&#39;split_type&#39;, feature_type=&lt;FeatureTypeEnum.STRING: &#39;String&#39;&gt;)]) . We will use the defined Feature Group later in this project, the actual creation of the Feature Group will take place in the processing job. Now let&#39;s move into the setup of the processing job to transform the dataset. . Transform the dataset . We will configure a SageMaker processing job to run a custom Python script to balance and transform the raw data into a format used by BERT model. . Let&#39;s set the transformation parameters including the instance type, instance count, and train/validation/test split percentages. We will use a relatively small instance type for this project. Please refer to this link for additional instance types that may work for your use case. . We can also choose whether you want to balance the dataset or not. In this case, we will balance the dataset to avoid class imbalance in the target variable, sentiment. . Another important parameter of the model is the max_seq_length, which specifies the maximum length of the classified reviews for the RoBERTa model. If the sentence is shorter than the maximum length parameter, it will be padded. In another case, when the sentence is longer, it will be truncated from the right side. . Since a smaller max_seq_length leads to faster training and lower resource utilization, you want to find the smallest power-of-2 that captures 100% of our reviews. For this dataset, the 100th percentile is 115. However, it&#39;s best to stick with powers-of-2 when using BERT. So let&#39;s choose 128 as this is the smallest power-of-2 greater than 115. We will see below how the shorter sentences will be padded to a maximum length. . mean 52.512374 std 31.387048 min 1.000000 10% 10.000000 20% 22.000000 30% 32.000000 40% 41.000000 50% 51.000000 60% 61.000000 70% 73.000000 80% 88.000000 90% 97.000000 100% 115.000000 max 115.000000 . . processing_instance_type=&#39;ml.c5.xlarge&#39; processing_instance_count=1 train_split_percentage=0.90 validation_split_percentage=0.05 test_split_percentage=0.05 balance_dataset=True max_seq_length=128 . To balance and transform our data, we will use a scikit-learn-based processing job. This is essentially a generic Python processing job with scikit-learn pre-installed. We can specify the version of scikit-learn we wish to use. Also we will pass the SageMaker execution role, processing instance type and instance count. . from sagemaker.sklearn.processing import SKLearnProcessor processor = SKLearnProcessor( framework_version=&#39;0.23-1&#39;, role=role, instance_type=processing_instance_type, instance_count=processing_instance_count, env={&#39;AWS_DEFAULT_REGION&#39;: region}, max_runtime_in_seconds=7200 ) . The processing job will be running the Python code from the file src/prepare_data.py. . import sys, importlib sys.path.append(&#39;src/&#39;) # import the `prepare_data.py` module import prepare_data # reload the module if it has been previously loaded if &#39;prepare_data&#39; in sys.modules: importlib.reload(prepare_data) input_ids = prepare_data.convert_to_bert_input_ids(&quot;this product is great!&quot;, max_seq_length) updated_correctly = False if len(input_ids) != max_seq_length: raise Exception(&#39;Please check that the function &#39;convert_to_bert_input_ids &#39; in the file src/prepare_data.py is complete.&#39;) else: print(&#39;##################&#39;) print(&#39;Updated correctly!&#39;) print(&#39;##################&#39;) updated_correctly = True . ################## Updated correctly! ################## . input_ids = prepare_data.convert_to_bert_input_ids(&quot;this product is great!&quot;, max_seq_length) print(input_ids) print(&#39;Length of the sequence: {}&#39;.format(len(input_ids))) . [0, 9226, 1152, 16, 372, 328, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] Length of the sequence: 128 . Now we launch the processing job with the custom script passing defined above parameters. . from sagemaker.processing import ProcessingInput, ProcessingOutput if (updated_correctly): processor.run(code=&#39;src/prepare_data.py&#39;, inputs=[ ProcessingInput(source=raw_input_data_s3_uri, destination=&#39;/opt/ml/processing/input/data/&#39;, s3_data_distribution_type=&#39;ShardedByS3Key&#39;) ], outputs=[ ProcessingOutput(output_name=&#39;sentiment-train&#39;, source=&#39;/opt/ml/processing/output/sentiment/train&#39;, s3_upload_mode=&#39;EndOfJob&#39;), ProcessingOutput(output_name=&#39;sentiment-validation&#39;, source=&#39;/opt/ml/processing/output/sentiment/validation&#39;, s3_upload_mode=&#39;EndOfJob&#39;), ProcessingOutput(output_name=&#39;sentiment-test&#39;, source=&#39;/opt/ml/processing/output/sentiment/test&#39;, s3_upload_mode=&#39;EndOfJob&#39;) ], arguments=[&#39;--train-split-percentage&#39;, str(train_split_percentage), &#39;--validation-split-percentage&#39;, str(validation_split_percentage), &#39;--test-split-percentage&#39;, str(test_split_percentage), &#39;--balance-dataset&#39;, str(balance_dataset), &#39;--max-seq-length&#39;, str(max_seq_length), &#39;--feature-store-offline-prefix&#39;, str(feature_store_offline_prefix), &#39;--feature-group-name&#39;, str(feature_group_name) ], logs=True, wait=False) . Job Name: sagemaker-scikit-learn-2023-02-07-19-57-59-405 Inputs: [{&#39;InputName&#39;: &#39;input-1&#39;, &#39;AppManaged&#39;: False, &#39;S3Input&#39;: {&#39;S3Uri&#39;: &#39;s3://dlai-practical-data-science/data/raw/&#39;, &#39;LocalPath&#39;: &#39;/opt/ml/processing/input/data/&#39;, &#39;S3DataType&#39;: &#39;S3Prefix&#39;, &#39;S3InputMode&#39;: &#39;File&#39;, &#39;S3DataDistributionType&#39;: &#39;ShardedByS3Key&#39;, &#39;S3CompressionType&#39;: &#39;None&#39;}}, {&#39;InputName&#39;: &#39;code&#39;, &#39;AppManaged&#39;: False, &#39;S3Input&#39;: {&#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-951182689916/sagemaker-scikit-learn-2023-02-07-19-57-59-405/input/code/prepare_data.py&#39;, &#39;LocalPath&#39;: &#39;/opt/ml/processing/input/code&#39;, &#39;S3DataType&#39;: &#39;S3Prefix&#39;, &#39;S3InputMode&#39;: &#39;File&#39;, &#39;S3DataDistributionType&#39;: &#39;FullyReplicated&#39;, &#39;S3CompressionType&#39;: &#39;None&#39;}}] Outputs: [{&#39;OutputName&#39;: &#39;sentiment-train&#39;, &#39;AppManaged&#39;: False, &#39;S3Output&#39;: {&#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-951182689916/sagemaker-scikit-learn-2023-02-07-19-57-59-405/output/sentiment-train&#39;, &#39;LocalPath&#39;: &#39;/opt/ml/processing/output/sentiment/train&#39;, &#39;S3UploadMode&#39;: &#39;EndOfJob&#39;}}, {&#39;OutputName&#39;: &#39;sentiment-validation&#39;, &#39;AppManaged&#39;: False, &#39;S3Output&#39;: {&#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-951182689916/sagemaker-scikit-learn-2023-02-07-19-57-59-405/output/sentiment-validation&#39;, &#39;LocalPath&#39;: &#39;/opt/ml/processing/output/sentiment/validation&#39;, &#39;S3UploadMode&#39;: &#39;EndOfJob&#39;}}, {&#39;OutputName&#39;: &#39;sentiment-test&#39;, &#39;AppManaged&#39;: False, &#39;S3Output&#39;: {&#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-951182689916/sagemaker-scikit-learn-2023-02-07-19-57-59-405/output/sentiment-test&#39;, &#39;LocalPath&#39;: &#39;/opt/ml/processing/output/sentiment/test&#39;, &#39;S3UploadMode&#39;: &#39;EndOfJob&#39;}}] . You can see the information about the processing jobs using the describe function. The result is in dictionary format. Let&#39;s pull the processing job name: . scikit_processing_job_name = processor.jobs[-1].describe()[&#39;ProcessingJobName&#39;] print(&#39;Processing job name: {}&#39;.format(scikit_processing_job_name)) . Processing job name: sagemaker-scikit-learn-2023-02-07-19-57-59-405 . Let&#39;s pull the processing job status from the processing job description. . print(processor.jobs[-1].describe().keys()) . dict_keys([&#39;ProcessingInputs&#39;, &#39;ProcessingOutputConfig&#39;, &#39;ProcessingJobName&#39;, &#39;ProcessingResources&#39;, &#39;StoppingCondition&#39;, &#39;AppSpecification&#39;, &#39;Environment&#39;, &#39;RoleArn&#39;, &#39;ProcessingJobArn&#39;, &#39;ProcessingJobStatus&#39;, &#39;LastModifiedTime&#39;, &#39;CreationTime&#39;, &#39;ResponseMetadata&#39;]) . scikit_processing_job_status = processor.jobs[-1].describe()[&#39;ProcessingJobStatus&#39;] print(&#39;Processing job status: {}&#39;.format(scikit_processing_job_status)) . Processing job status: InProgress . %%time running_processor = sagemaker.processing.ProcessingJob.from_processing_name( processing_job_name=scikit_processing_job_name, sagemaker_session=sess ) running_processor.wait(logs=False) . ....................................................................................................................................!CPU times: user 647 ms, sys: 44.3 ms, total: 691 ms Wall time: 11min 13s . Let&#39;s inspect the transformed and balanced data in the S3 bucket. . processing_job_description = running_processor.describe() output_config = processing_job_description[&#39;ProcessingOutputConfig&#39;] for output in output_config[&#39;Outputs&#39;]: if output[&#39;OutputName&#39;] == &#39;sentiment-train&#39;: processed_train_data_s3_uri = output[&#39;S3Output&#39;][&#39;S3Uri&#39;] if output[&#39;OutputName&#39;] == &#39;sentiment-validation&#39;: processed_validation_data_s3_uri = output[&#39;S3Output&#39;][&#39;S3Uri&#39;] if output[&#39;OutputName&#39;] == &#39;sentiment-test&#39;: processed_test_data_s3_uri = output[&#39;S3Output&#39;][&#39;S3Uri&#39;] print(processed_train_data_s3_uri) print(processed_validation_data_s3_uri) print(processed_test_data_s3_uri) . s3://sagemaker-us-east-1-951182689916/sagemaker-scikit-learn-2023-02-07-19-57-59-405/output/sentiment-train s3://sagemaker-us-east-1-951182689916/sagemaker-scikit-learn-2023-02-07-19-57-59-405/output/sentiment-validation s3://sagemaker-us-east-1-951182689916/sagemaker-scikit-learn-2023-02-07-19-57-59-405/output/sentiment-test . !aws s3 ls $processed_train_data_s3_uri/ . 2023-02-07 20:10:54 4896333 part-algo-1-womens_clothing_ecommerce_reviews.tsv . !aws s3 ls $processed_validation_data_s3_uri/ . 2023-02-07 20:10:54 269735 part-algo-1-womens_clothing_ecommerce_reviews.tsv . !aws s3 ls $processed_test_data_s3_uri/ . 2023-02-07 20:10:55 269933 part-algo-1-womens_clothing_ecommerce_reviews.tsv . Now we copy the data into the folder balanced. . !aws s3 cp $processed_train_data_s3_uri/part-algo-1-womens_clothing_ecommerce_reviews.tsv ./balanced/sentiment-train/ !aws s3 cp $processed_validation_data_s3_uri/part-algo-1-womens_clothing_ecommerce_reviews.tsv ./balanced/sentiment-validation/ !aws s3 cp $processed_test_data_s3_uri/part-algo-1-womens_clothing_ecommerce_reviews.tsv ./balanced/sentiment-test/ . download: s3://sagemaker-us-east-1-951182689916/sagemaker-scikit-learn-2023-02-07-19-57-59-405/output/sentiment-train/part-algo-1-womens_clothing_ecommerce_reviews.tsv to balanced/sentiment-train/part-algo-1-womens_clothing_ecommerce_reviews.tsv download: s3://sagemaker-us-east-1-951182689916/sagemaker-scikit-learn-2023-02-07-19-57-59-405/output/sentiment-validation/part-algo-1-womens_clothing_ecommerce_reviews.tsv to balanced/sentiment-validation/part-algo-1-womens_clothing_ecommerce_reviews.tsv download: s3://sagemaker-us-east-1-951182689916/sagemaker-scikit-learn-2023-02-07-19-57-59-405/output/sentiment-test/part-algo-1-womens_clothing_ecommerce_reviews.tsv to balanced/sentiment-test/part-algo-1-womens_clothing_ecommerce_reviews.tsv . Let&#39;s review the training, validation and test data outputs: . !head -n 5 ./balanced/sentiment-train/part-algo-1-womens_clothing_ecommerce_reviews.tsv . review_id sentiment label_id input_ids review_body date 15231 -1 0 [0, 100, 657, 13855, 27734, 111, 4682, 13, 42, 65, 4, 5, 10199, 16, 38596, 4, 24, 18, 227, 4136, 8, 5, 1468, 14, 51, 146, 9287, 66, 9, 4, 5, 5780, 16, 15652, 8, 5, 14893, 62, 5, 760, 32, 2422, 11962, 4, 5, 3318, 631, 14, 18, 95, 7209, 89, 116, 1437, 24, 18, 10, 3318, 631, 14, 95, 23835, 89, 4, 24, 630, 75, 1437, 356, 205, 7209, 1437, 8, 24, 630, 75, 356, 205, 3016, 4, 1437, 42, 13855, 6439, 56, 98, 203, 801, 4, 939, 437, 2299, 5779, 4, 1437, 13, 39328, 5135, 1437, 939, 524, 195, 108, 245, 113, 1437, 16157, 1437, 2631, 438, 8, 10, 650, 21, 1969, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] &#34;I love jumpsuits - except for this one. the fabric is blah. it&#39;s between plastic and the material that they make flags out of. the print is adorable and the buttons up the front are super cute. the tie thing that&#39;s just hanging there? it&#39;s a tie thing that just hangs there. it doesn&#39;t look good hanging and it doesn&#39;t look good tied. this jumpsuit had so much potential. i&#39;m definitely disappointed. for sizing reference i am 5&#39;5&#34;&#34; 135 34c and a small was perfect.&#34; 2023-02-07T20:04:40Z 8389 -1 0 [0, 100, 269, 770, 7, 101, 209, 1437, 53, 51, 95, 399, 75, 356, 235, 15, 127, 195, 108, 246, 2345, 102, 35156, 5120, 4, 939, 33, 380, 35841, 8, 460, 619, 66, 9, 317, 2498, 2084, 6149, 1033, 1437, 98, 2085, 939, 437, 95, 45, 5, 235, 1002, 13, 209, 1437, 53, 51, 1415, 98, 11962, 8, 939, 770, 7, 492, 106, 10, 860, 4, 5, 13977, 21, 350, 239, 13, 127, 25896, 1437, 8, 5, 2985, 18459, 58, 350, 380, 8, 851, 162, 10, 33062, 3786, 356, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] I really wanted to like these but they just didn&#39;t look right on my 5&#39;3 sorta bulky frame. i have big thighs and always feel out of place wearing leggings so maybe i&#39;m just not the right target for these but they looked so cute and i wanted to give them a try. the waist was too high for my liking and the leg openings were too big and gave me a stumpified look. 2023-02-07T20:04:40Z 17752 1 2 [0, 713, 16, 10, 1528, 5262, 299, 42514, 571, 26875, 1827, 8, 1237, 650, 4, 939, 2333, 3568, 10, 650, 50, 4761, 11, 6215, 13657, 1437, 53, 15679, 219, 939, 460, 1836, 62, 4, 939, 437, 10, 2491, 438, 1437, 8, 10, 739, 10698, 1969, 4, 5, 760, 16, 10, 828, 11708, 1437, 53, 45, 98, 203, 47, 240, 10, 740, 5602, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] This is a true tiny top........gorgeous and runs small. i usually wear a small or medium in retailer tops but timy i always size up. i&#39;m a 36c and a large fits perfect. the front is a bit sheer but not so much you need a cami. 2023-02-07T20:04:40Z 65 1 2 [0, 100, 3584, 42, 299, 11, 41, 9876, 1001, 1400, 94, 186, 4, 5, 1318, 16, 4613, 8, 5, 2272, 1173, 2440, 3195, 16, 182, 2216, 4, 5, 3089, 11556, 34, 10, 2721, 4140, 219, 740, 7042, 1020, 459, 14, 16, 7391, 23, 5, 10762, 1437, 53, 64, 28, 2928, 30, 11803, 4, 939, 362, 29, 372, 77, 10610, 80, 430, 1319, 4, 939, 5328, 24, 19, 5, 2205, 4104, 66, 1437, 8, 24, 3723, 15390, 149, 5, 3089, 11556, 23, 5, 2576, 4, 24, 67, 1326, 372, 77, 5, 11021, 354, 4104, 16, 10610, 11, 4, 127, 129, 2813, 16, 14, 24, 74, 283, 11, 10, 4716, 1459, 1836, 25, 24, 18, 10, 2842, 380, 23, 5, 10762, 8, 5397, 3572, 2, 1, 1, 1, 1, 1] I purchased this top in an antro store last week. the quality is wonderful and the greenish blue color is very unique. the blouse has a beautiful stretchy camsiole that is attached at the shoulders but can be removed by snaps. i tooks great when worn two different ways. i wore it with the campole out and it peeks through the blouse at the bottom. it also looks great when the camisole is worn in. my only wish is that it would come in a petite size as it&#39;s a touch big at the shoulders and neckli 2023-02-07T20:04:40Z . !head -n 5 ./balanced/sentiment-validation/part-algo-1-womens_clothing_ecommerce_reviews.tsv . review_id sentiment label_id input_ids review_body date 5506 1 2 [0, 19065, 3588, 11, 1110, 9, 1468, 1437, 1318, 1437, 5780, 734, 18891, 59, 5, 2408, 19, 5, 14187, 156, 24, 45, 173, 13, 162, 4, 939, 2740, 65, 1836, 159, 25, 5131, 30, 97, 34910, 1437, 53, 14, 399, 75, 173, 131, 89, 21, 350, 203, 10199, 13, 5, 5933, 8, 5, 14187, 156, 24, 356, 19351, 4, 939, 2740, 10, 4761, 8, 939, 113, 119, 195, 108, 245, 113, 15, 5, 5350, 11454, 526, 4, 14223, 157, 4, 939, 348, 56, 98, 203, 6620, 19, 97, 6215, 3365, 98, 939, 437, 45, 350, 5779, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] &#34;Great dress in terms of material quality print...something about the weight with the lining made it not work for me. i ordered one size down as recommended by other reviewers but that didn&#39;t work; there was too much fabric for the length and the lining made it look heavier. i ordered a medium and i&#34;&#34;m 5&#39;5&#34;&#34; on the curvy side. oh well. i&#39;ve had so much luck with other retailer orders so i&#39;m not too disappointed.&#34; 2023-02-07T20:04:40Z 8480 0 1 [0, 713, 2170, 473, 45, 109, 42, 16576, 2427, 4, 24, 16, 12058, 4, 959, 1437, 5, 13977, 21, 98, 650, 14, 5, 16721, 1344, 11532, 88, 127, 13977, 442, 162, 206, 9, 10, 25818, 11809, 2187, 4, 9574, 1437, 24, 21, 5, 1154, 1836, 98, 939, 64, 75, 1836, 62, 4, 939, 437, 204, 108, 1225, 113, 98, 5, 5933, 21, 1969, 111, 24, 376, 7, 235, 1065, 127, 15145, 4, 939, 657, 5, 16576, 98, 203, 14, 939, 437, 2811, 11356, 366, 27345, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] &#34;This picture does not do this skirt justice. it is gorgeous. however the waist was so small that the sequins dug into my waist making me think of a medieval torture device. unfortunately it was the largest size so i can&#39;t size up. i&#39;m 4&#39;11&#34;&#34; so the length was perfect - it came to right above my knees. i love the skirt so much that i&#39;m considering liposuction.&#34; 2023-02-07T20:04:40Z 66 0 1 [0, 100, 829, 42, 6399, 11, 127, 6097, 3023, 29, 8, 24, 10698, 6683, 4, 939, 116, 119, 45, 5373, 11, 657, 19, 24, 53, 939, 67, 218, 116, 90, 28101, 4, 5, 6399, 16, 15, 5, 7174, 526, 4, 109, 939, 240, 7, 3568, 10, 740, 5602, 12213, 24, 1437, 117, 4, 127, 2212, 16, 6538, 4, 24, 473, 8736, 162, 9, 10, 1468, 14, 115, 2179, 103, 6538, 71, 103, 3568, 8, 21, 5065, 4, 19, 14, 145, 26, 939, 116, 890, 10397, 42, 6399, 11, 2569, 514, 8, 6713, 3841, 8, 5952, 14, 40, 2097, 6538, 31, 2623, 4, 5, 5933, 16, 2051, 8, 939, 109, 101, 5, 3369, 2629, 11, 760, 116, 405, 3639, 10, 410, 14548, 2, 1, 1, 1, 1, 1, 1] I received this shirt in my typical xs and it fits perfectly. i?m not crazy in love with it but i also don?t dislike. the shirt is on the thin side. do i need to wear a cami underneath it no. my concern is holes. it does remind me of a material that could develop some holes after some wear and washes. with that being said i?ll wash this shirt in cold water and hang dry and hopefully that will prevent holes from developing. the length is fine and i do like the slits in front?it adds a little dim 2023-02-07T20:04:40Z 10411 -1 0 [0, 100, 33, 57, 546, 23, 42, 23204, 804, 187, 24, 78, 376, 66, 8, 939, 1747, 2740, 24, 77, 24, 21, 843, 207, 160, 4, 939, 2740, 10, 1836, 475, 4716, 1459, 1437, 16748, 77, 24, 2035, 8, 939, 1381, 24, 15, 1437, 24, 21, 182, 2233, 219, 1437, 13116, 101, 1437, 8, 222, 45, 3041, 101, 24, 1415, 15, 5, 1421, 804, 98, 939, 1051, 24, 124, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] I have been looking at this sweater online since it first came out and i finally ordered it when it was 40% off. i ordered a size m petite sadly when it arrived and i tried it on it was very boxy stiff like and did not flow like it looked on the model online so i sent it back. 2023-02-07T20:04:40Z . !head -n 5 ./balanced/sentiment-test/part-algo-1-womens_clothing_ecommerce_reviews.tsv . review_id sentiment label_id input_ids review_body date 4815 0 1 [0, 100, 300, 5, 1275, 1437, 61, 21, 765, 30145, 5202, 4, 5, 6399, 1495, 21, 98, 11962, 1437, 53, 5, 2564, 16, 182, 2233, 219, 4, 939, 300, 10, 650, 8, 24, 21, 169, 350, 1810, 4, 444, 6012, 8, 10941, 11, 5, 13977, 87, 5, 2170, 924, 4, 939, 524, 5074, 7, 671, 1437, 53, 24, 817, 162, 356, 101, 10, 3925, 4, 36, 43882, 14, 16, 43, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] I got the red which was short sleeved. the shirt itself was so cute but the fit is very boxy. i got a small and it was way too wide. far wider and shorter in the waist than the picture shows. i am sad to return but it makes me look like a square. (shape that is) 2023-02-07T20:04:40Z 1933 1 2 [0, 1708, 5, 124, 9, 24, 1437, 30, 5, 13977, 1437, 15713, 5559, 95, 10, 5262, 828, 4, 114, 939, 120, 2671, 1437, 24, 40, 28, 350, 251, 4, 53, 939, 657, 5, 16576, 1437, 24, 16, 34203, 8, 11962, 4, 45, 24, 17414, 13, 162, 190, 114, 5, 1270, 161, 24787, 4, 939, 2740, 5, 16273, 642, 8, 5, 5933, 16, 1256, 203, 25, 7092, 1437, 95, 874, 5, 4117, 11, 760, 4, 5, 13977, 16, 41783, 1437, 2671, 24, 74, 1136, 55, 15, 127, 28097, 36, 2457, 1755, 5, 350, 251, 1129, 656, 4, 36, 15314, 23246, 1437, 973, 12, 2518, 11, 13977, 1437, 765, 5856, 41137, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] But the back of it by the waist bunches just a tiny bit. if i get bigger it will be too long. but i love the skirt it is flattering and cute. not itchy for me even if the title says wool. i ordered the 00p and the length is pretty much as pictured just below the knee in front. the waist is snug bigger it would fall more on my hips (hence the too long comment earlier. (115 lbs 26-27 in waist short legs...) 2023-02-07T20:04:40Z 14029 -1 0 [0, 100, 269, 657, 5, 6184, 8, 5, 356, 15, 5, 1421, 1437, 8, 939, 802, 939, 74, 657, 24, 4, 939, 2740, 804, 1437, 98, 939, 222, 45, 860, 15, 11, 1400, 4, 77, 939, 1381, 24, 15, 1437, 24, 34, 169, 350, 203, 10199, 198, 5, 13977, 8, 16576, 4, 24, 16, 7992, 10199, 25, 157, 1437, 8, 34, 10, 14187, 1437, 8, 5, 13977, 34, 1823, 10199, 13, 5, 1521, 1437, 8, 24, 34, 12189, 1437, 98, 24, 70, 3639, 62, 7, 28, 169, 350, 35156, 4, 24, 16, 45, 34203, 23, 70, 8, 156, 162, 356, 158, 2697, 19351, 4, 939, 524, 3357, 42, 3588, 4, 5074, 1437, 142, 24, 1326, 98, 9869, 15, 5, 1421, 4, 939, 524, 2, 1, 1, 1, 1] I really love the pattern and the look on the model and i thought i would love it. i ordered online so i did not try on in store. when i tried it on it has way too much fabric around the waist and skirt. it is thick fabric as well and has a lining and the waist has extra fabric for the design and it has pockets so it all adds up to be way too bulky. it is not flattering at all and made me look 10 pounds heavier. i am returning this dress. sad because it looks so lovely on the model. i am 2023-02-07T20:04:40Z 10468 0 1 [0, 713, 6966, 18605, 16, 182, 157, 156, 8, 190, 39083, 906, 11, 621, 4, 939, 437, 195, 108, 398, 113, 8, 59, 17445, 2697, 4, 939, 2333, 3568, 10, 1836, 231, 4, 939, 3568, 10, 2631, 417, 11689, 4, 939, 303, 5, 3235, 7, 422, 10, 828, 650, 4, 939, 1835, 24, 142, 1437, 1135, 141, 203, 939, 6640, 5, 2496, 1437, 24, 95, 938, 75, 34203, 15, 127, 809, 1907, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] &#34;This swimsuit is very well made and even prettier in person. i&#39;m 5&#39;8&#34;&#34; and about 145 pounds. i usually wear a size 6. i wear a 34d bra. i found the suit to run a bit small. i returned it because despite how much i liked the style it just wasn&#39;t flattering on my body type.&#34; 2023-02-07T20:04:40Z . Query the Feature Store . In addition to transforming the data and saving in S3 bucket, the processing job populates the feature store with the transformed and balanced data. Let&#39;s query this data using Amazon Athena. . Export training, validation, and test datasets from the Feature Store . Here we will do the export only for the training dataset, as an example. . We will use the athena_query() function to create an Athena query for the defined above Feature Group. Then we can pull the table name of the Amazon Glue Data Catalog table which is auto-generated by Feature Store. . feature_store_query = feature_group.athena_query() feature_store_table = feature_store_query.table_name query_string = &quot;&quot;&quot; SELECT date, review_id, sentiment, label_id, input_ids, review_body FROM &quot;{}&quot; WHERE split_type=&#39;train&#39; LIMIT 5 &quot;&quot;&quot;.format(feature_store_table) print(&#39;Glue Catalog table name: {}&#39;.format(feature_store_table)) print(&#39;Running query: {}&#39;.format(query_string)) . Glue Catalog table name: reviews-feature-group-1675799708-1675800251 Running query: SELECT date, review_id, sentiment, label_id, input_ids, review_body FROM &#34;reviews-feature-group-1675799708-1675800251&#34; WHERE split_type=&#39;train&#39; LIMIT 5 . Now we configure the S3 location for the query results. This allows us to re-use the query results for future queries if the data has not changed. We can even share this S3 location between team members to improve query performance for common queries on data that does not change often. . output_s3_uri = &#39;s3://{}/query_results/{}/&#39;.format(bucket, feature_store_offline_prefix) print(output_s3_uri) . s3://sagemaker-us-east-1-951182689916/query_results/reviews-feature-store-1675799708/ . Let&#39;s query the feature store. . feature_store_query.run( query_string=query_string, output_location=output_s3_uri ) feature_store_query.wait() . import pandas as pd pd.set_option(&quot;max_colwidth&quot;, 100) df_feature_store = feature_store_query.as_dataframe() df_feature_store . date review_id sentiment label_id input_ids review_body . 0 2023-02-07T20:04:40Z | 3151 | 0 | 1 | [0, 17425, 27941, 181, 267, 1318, 4, 939, 33, 10, 5342, 7174, 5120, 8, 42, 10601, 15, 162, 101, ... | Definitely pj quality. i have a fairly thin frame and this hung on me like a tent. and it&#39;s very... | . 1 2023-02-07T20:04:40Z | 2313 | 0 | 1 | [0, 713, 16, 10, 182, 11962, 3588, 4, 24, 21, 1969, 137, 939, 15158, 24, 4, 5, 1272, 939, 56, 71... | This is a very cute dress. it was perfect before i washed it. the problems i had after washing i... | . 2 2023-02-07T20:04:40Z | 10378 | 1 | 2 | [0, 100, 2162, 5, 10521, 1437, 61, 16, 10, 12058, 3195, 4, 939, 101, 5, 251, 5933, 11, 5, 3701, ... | I bought the grey which is a gorgeous color. i like the long length in the arms (though i tried... | . 3 2023-02-07T20:04:40Z | 13251 | 0 | 1 | [0, 37396, 299, 804, 111, 8578, 11, 621, 4, 1237, 650, 1437, 941, 15, 2576, 23385, 1902, 4, 802,... | Pretty top online - okay in person. runs small especially on bottom hemline. thought it would h... | . 4 2023-02-07T20:04:40Z | 9286 | -1 | 0 | [0, 713, 299, 16, 2721, 804, 8, 11, 621, 4, 939, 524, 11, 117, 169, 10, 739, 455, 11464, 22101, ... | This top is beautiful online and in person. i am in no way a large full figured gal but i did o... | . Export TSV from Feature Store . Save the output as a TSV file: . df_feature_store.to_csv(&#39;./feature_store_export.tsv&#39;, sep=&#39; t&#39;, index=False, header=True) . !head -n 5 ./feature_store_export.tsv . date review_id sentiment label_id input_ids review_body 2023-02-07T20:04:40Z 3151 0 1 [0, 17425, 27941, 181, 267, 1318, 4, 939, 33, 10, 5342, 7174, 5120, 8, 42, 10601, 15, 162, 101, 10, 10178, 4, 8, 24, 18, 182, 7174, 1437, 98, 24, 1364, 25, 10, 6966, 1719, 1437, 53, 2299, 45, 10, 3588, 13, 932, 1493, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] Definitely pj quality. i have a fairly thin frame and this hung on me like a tent. and it&#39;s very thin so it works as a swim cover but definitely not a dress for anything else. 2023-02-07T20:04:40Z 2313 0 1 [0, 713, 16, 10, 182, 11962, 3588, 4, 24, 21, 1969, 137, 939, 15158, 24, 4, 5, 1272, 939, 56, 71, 14784, 24, 21, 5, 15705, 13178, 10490, 9, 5, 3588, 28704, 5933, 11036, 150, 5, 1025, 909, 14187, 222, 45, 1437, 98, 5, 909, 14187, 3723, 15390, 66, 10, 205, 10468, 50, 80, 4, 8, 187, 5, 3588, 16, 10941, 939, 64, 75, 269, 3568, 24, 396, 634, 741, 17625, 13344, 1437, 941, 13, 5, 124, 9, 5, 3588, 187, 24, 18, 10941, 89, 8, 114, 939, 18822, 81, 47, 115, 192, 960, 4, 939, 437, 98, 5779, 11, 5, 1318, 142, 24, 16, 10, 182, 11962, 1437, 4342, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] This is a very cute dress. it was perfect before i washed it. the problems i had after washing it was the outer cotton layer of the dress shrunk length wise while the inside black lining did not so the black lining peeks out a good inch or two. and since the dress is shorter i can&#39;t really wear it without using biker shorts especially for the back of the dress since it&#39;s shorter there and if i bent over you could see everything. i&#39;m so disappointed in the quality because it is a very cute ver 2023-02-07T20:04:40Z 10378 1 2 [0, 100, 2162, 5, 10521, 1437, 61, 16, 10, 12058, 3195, 4, 939, 101, 5, 251, 5933, 11, 5, 3701, 36, 18401, 939, 1381, 24, 15, 11, 430, 8089, 8, 5, 3124, 5933, 222, 182, 322, 3793, 8, 1256, 4, 939, 101, 5, 5933, 4, 5, 124, 473, 14902, 15673, 1437, 53, 939, 202, 101, 5, 6399, 4, 24, 18, 7082, 1437, 9881, 8, 34203, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] I bought the grey which is a gorgeous color. i like the long length in the arms (though i tried it on in different colors and the arm length did very). soft and pretty. i like the length. the back does wrinkle but i still like the shirt. it&#39;s loose casual and flattering. 2023-02-07T20:04:40Z 13251 0 1 [0, 37396, 299, 804, 111, 8578, 11, 621, 4, 1237, 650, 1437, 941, 15, 2576, 23385, 1902, 4, 802, 24, 74, 33, 10, 7021, 7, 24, 4, 24, 473, 45, 4, 55, 11708, 11, 621, 87, 939, 802, 24, 74, 28, 4, 14, 1979, 75, 912, 162, 31, 2396, 24, 600, 4, 24, 21, 5, 169, 24, 4976, 15, 127, 7050, 14, 21, 29747, 24203, 4, 1415, 101, 939, 21, 2498, 10, 741, 1452, 4, 939, 218, 75, 33, 10, 739, 7050, 1437, 95, 7735, 356, 15, 162, 4, 9327, 1437, 142, 24, 16, 41, 15652, 5780, 4, 299, 156, 13, 29284, 50, 10, 4716, 1459, 6429, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] Pretty top online - okay in person. runs small especially on bottom hemline. thought it would have a swing to it. it does not. more sheer in person than i thought it would be. that wouldn&#39;t stop me from keeping it though. it was the way it laid on my chest that was unflattering. looked like i was wearing a bib. i don&#39;t have a large chest just weird look on me. unfortunate because it is an adorable print. top made for thinner or a petite lady. . Upload TSV to the S3 bucket: . !aws s3 cp ./feature_store_export.tsv s3://$bucket/feature_store/feature_store_export.tsv . upload: ./feature_store_export.tsv to s3://sagemaker-us-east-1-951182689916/feature_store/feature_store_export.tsv . Check the file in the S3 bucket: . !aws s3 ls --recursive s3://$bucket/feature_store/feature_store_export.tsv . 2023-02-07 20:11:18 4714 feature_store/feature_store_export.tsv . Check that the dataset in the Feature Store is balanced by sentiment . Now we can setup an Athena query to check that the stored dataset is balanced by the target class sentiment. . We will rrite an SQL query to count the total number of the reviews per sentiment stored in the Feature Group. . feature_store_query_2 = feature_group.athena_query() query_string_count_by_sentiment = &quot;&quot;&quot; SELECT sentiment, COUNT(*) AS count_reviews FROM &quot;{}&quot; GROUP BY sentiment &quot;&quot;&quot;.format(feature_store_table) . Now we query the feature store. . feature_store_query_2.run( query_string=query_string_count_by_sentiment, output_location=output_s3_uri ) feature_store_query_2.wait() df_count_by_sentiment = feature_store_query_2.as_dataframe() df_count_by_sentiment . sentiment count_reviews . 0 0 | 2051 | . 1 -1 | 2051 | . 2 1 | 2051 | . Let&#39;s visualize the result of the query in the bar plot, showing the count of the reviews by sentiment value. . %matplotlib inline import seaborn as sns sns.barplot( data=df_count_by_sentiment, x=&#39;sentiment&#39;, y=&#39;count_reviews&#39;, color=&quot;blue&quot; ) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9c4f4c9710&gt; . Acknowledgements . I&#39;d like to express my thanks to the great Deep Learning AI Practical Data Science on AWS Specialisation Course which i completed, and acknowledge the use of some images and other materials from the training course in this article. .",
            "url": "https://www.livingdatalab.com/aws/cloud-data-science/natural-language-processing/deep-learning/2023/02/08/feature-transformation-aws-sagemaker-processing-job-feature-store.html",
            "relUrl": "/aws/cloud-data-science/natural-language-processing/deep-learning/2023/02/08/feature-transformation-aws-sagemaker-processing-job-feature-store.html",
            "date": " • Feb 8, 2023"
        }
        
    
  
    
        ,"post1": {
            "title": "Creating a Sentiment Analysis Text Classification Model using AWS SageMaker BlazingText",
            "content": "Introduction . In earlier articles we introduced AWS cloud services for data science, and showed how it can help with different stages of the data science &amp; machine learning workflow. . . In this article we will use the AWS SageMaker BlazingText built-in deep learning model to predict the sentiment for customer text reviews. The model will analyze customer feedback and classify the messages into positive (1), neutral (0) and negative (-1) sentiment. . The dataset we will use is the Women&#39;s Clothing Reviews a public dataset available on kaggle. . In my previous article we saw how you could use AWS Sagemaker Autopilot (an AutoML method) to automatically choose an appropriate model and perform all the required steps of the Data Science workflow. . But sometimes, we may need to go beyond AutoML and do more customisation and human selection for the Data Science workflow, and even between AutoML and fully customised Models, there are a range of choices in between for example from most to least automated methods we could have: . AWS Sagemaker Autopilot (AutoML) | AWS Sagemaker Built-in Algorithms | AWS Sagemaker Bring your own script (import and define your own models) | AWS Sagemaker Bring your own container (i.e. docker image with models &amp; environment) | . And of course, there are various pros and cons for each of the options for most automated to most customised. . So when would we use built-in algorithms? What would be the advantages for this? . Implementations are highly-optimized and scalable | Focus more on domain-specific tasks rather than managing low-level model code and infrastructure | Trained model can be downloaded and re-used elsewhere | . So as mentioned previously we will be using the BlazingText built in deep learning language model. BlazingText is a variant of FastText which is based on word2vec created by the AWS team in 2017. . . Key aspects of BlazingText are: . Scales and accelerates Word2Vec using multiple CPUs or GPUs for training | Extends FastText to use GPU acceleration with custom CUDA kernels | Creates n-gram embeddings using CBOW and skip-gram | Saves money by early-stopping a training job when the validation accuracy stops increasing | Optimized I/O for datasets stored in Amazon S3 | . For more information on BlazingText, see the documentation here: https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html . Let&#39;s now install and import the required modules. . import boto3 import sagemaker import pandas as pd import numpy as np import botocore config = botocore.config.Config(user_agent_extra=&#39;dlai-pds/c1/w4&#39;) # low-level service client of the boto3 session sm = boto3.client(service_name=&#39;sagemaker&#39;, config=config) sm_runtime = boto3.client(&#39;sagemaker-runtime&#39;, config=config) sess = sagemaker.Session(sagemaker_client=sm, sagemaker_runtime_client=sm_runtime) bucket = sess.default_bucket() role = sagemaker.get_execution_role() region = sess.boto_region_name . import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format=&#39;retina&#39; . Prepare dataset . Let&#39;s adapt the dataset into a format that BlazingText understands. The BlazingText format is as follows: . __label__&lt;label&gt; &quot;&lt;features&gt;&quot; . Here are some examples: . __label__-1 &quot;this is bad&quot; __label__0 &quot;this is ok&quot; __label__1 &quot;this is great&quot; . Sentiment is one of three classes: negative (-1), neutral (0), or positive (1). BlazingText requires that __label__ is prepended to each sentiment value. . We will tokenize the review_body with the Natural Language Toolkit (nltk) for the model training. We will also use nltk later to tokenize reviews to use as inputs to the deployed model. . Load the dataset . Upload the dataset into the Pandas dataframe: . !aws s3 cp &#39;s3://dlai-practical-data-science/data/balanced/womens_clothing_ecommerce_reviews_balanced.csv&#39; ./ . download: s3://dlai-practical-data-science/data/balanced/womens_clothing_ecommerce_reviews_balanced.csv to ./womens_clothing_ecommerce_reviews_balanced.csv . path = &#39;./womens_clothing_ecommerce_reviews_balanced.csv&#39; df = pd.read_csv(path, delimiter=&#39;,&#39;) df.head() . sentiment review_body product_category . 0 -1 | This suit did nothing for me. the top has zero... | Swim | . 1 -1 | Like other reviewers i saw this dress on the ... | Dresses | . 2 -1 | I wish i had read the reviews before purchasin... | Knits | . 3 -1 | I ordered these pants in my usual size (xl) an... | Legwear | . 4 -1 | I noticed this top on one of the sales associa... | Knits | . Transform the dataset . Now we will prepend __label__ to each sentiment value and tokenize the review body using nltk module. Let&#39;s import the module and download the tokenizer: . import nltk nltk.download(&#39;punkt&#39;) . [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Unzipping tokenizers/punkt.zip. . True . To split a sentence into tokens we can use word_tokenize method. It will separate words, punctuation, and apply some stemming. . For example: . sentence = &quot;I&#39;m not a fan of this product!&quot; tokens = nltk.word_tokenize(sentence) print(tokens) . [&#39;I&#39;, &#34;&#39;m&#34;, &#39;not&#39;, &#39;a&#39;, &#39;fan&#39;, &#39;of&#39;, &#39;this&#39;, &#39;product&#39;, &#39;!&#39;] . The output of word tokenization can be converted into a string separated by spaces and saved in the dataframe. The transformed sentences are prepared then for better text understending by the model. . Let&#39;s define a prepare_data function which we will apply later to transform both training and validation datasets. . def tokenize(review): # delete commas and quotation marks, apply tokenization and join back into a string separating by spaces return &#39; &#39;.join([str(token) for token in nltk.word_tokenize(str(review).replace(&#39;,&#39;, &#39;&#39;).replace(&#39;&quot;&#39;, &#39;&#39;).lower())]) def prepare_data(df): df[&#39;sentiment&#39;] = df[&#39;sentiment&#39;].map(lambda sentiment : &#39;__label__{}&#39;.format(str(sentiment).replace(&#39;__label__&#39;, &#39;&#39;))) df[&#39;review_body&#39;] = df[&#39;review_body&#39;].map(lambda review : tokenize(review)) return df . Test the prepared function and examine the result. . df_example = pd.DataFrame({ &#39;sentiment&#39;:[-1, 0, 1], &#39;review_body&#39;:[ &quot;I don&#39;t like this product!&quot;, &quot;this product is ok&quot;, &quot;I do like this product!&quot;] }) # test the prepare_data function print(prepare_data(df_example)) . sentiment review_body 0 __label__-1 i do n&#39;t like this product ! 1 __label__0 this product is ok 2 __label__1 i do like this product ! . Let&#39;s apply the prepare_data function to the dataset. . df_blazingtext = df[[&#39;sentiment&#39;, &#39;review_body&#39;]].reset_index(drop=True) df_blazingtext = prepare_data(df_blazingtext) df_blazingtext.head() . sentiment review_body . 0 __label__-1 | this suit did nothing for me . the top has zer... | . 1 __label__-1 | like other reviewers i saw this dress on the c... | . 2 __label__-1 | i wish i had read the reviews before purchasin... | . 3 __label__-1 | i ordered these pants in my usual size ( xl ) ... | . 4 __label__-1 | i noticed this top on one of the sales associa... | . Split the dataset into train and validation sets . We will now split and visualize a pie chart of the train (90%) and validation (10%) sets. . from sklearn.model_selection import train_test_split # Split all data into 90% train and 10% holdout df_train, df_validation = train_test_split(df_blazingtext, test_size=0.10, stratify=df_blazingtext[&#39;sentiment&#39;]) labels = [&#39;train&#39;, &#39;validation&#39;] sizes = [len(df_train.index), len(df_validation.index)] explode = (0.1, 0) fig1, ax1 = plt.subplots() ax1.pie(sizes, explode=explode, labels=labels, autopct=&#39;%1.1f%%&#39;, startangle=90) # Equal aspect ratio ensures that pie is drawn as a circle. ax1.axis(&#39;equal&#39;) plt.show() print(len(df_train)) . 6399 . Save the results as CSV files. . blazingtext_train_path = &#39;./train.csv&#39; df_train[[&#39;sentiment&#39;, &#39;review_body&#39;]].to_csv(blazingtext_train_path, index=False, header=False, sep=&#39; &#39;) . blazingtext_validation_path = &#39;./validation.csv&#39; df_validation[[&#39;sentiment&#39;, &#39;review_body&#39;]].to_csv(blazingtext_validation_path, index=False, header=False, sep=&#39; &#39;) . Upload the train and validation datasets to S3 bucket . We will use these to train and validate your model. Let&#39;s save them to S3 bucket. . train_s3_uri = sess.upload_data(bucket=bucket, key_prefix=&#39;blazingtext/data&#39;, path=blazingtext_train_path) validation_s3_uri = sess.upload_data(bucket=bucket, key_prefix=&#39;blazingtext/data&#39;, path=blazingtext_validation_path) . Train the model . We will now setup the BlazingText estimator. For more information on Estimators, see the SageMaker Python SDK documentation here: https://sagemaker.readthedocs.io/. . We will setup the container image to use for training with the BlazingText algorithm. . image_uri = sagemaker.image_uris.retrieve( region=region, framework=&#39;blazingtext&#39; ) . Let&#39;s now create an estimator instance passing the container image and other instance parameters. . estimator = sagemaker.estimator.Estimator( image_uri=image_uri, role=role, instance_count=1, instance_type=&#39;ml.m5.large&#39;, volume_size=30, max_run=7200, sagemaker_session=sess ) . Now we need to configure the hyper-parameters for BlazingText. In our case we are using BlazingText for a supervised classification task. . Information on the hyper-parameters can be found in the documentation here: https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext-tuning.html . The hyperparameters that have the greatest impact on word2vec objective metrics are: learning_rate and vector_dim. . estimator.set_hyperparameters(mode=&#39;supervised&#39;, # supervised (text classification) epochs=10, # number of complete passes through the dataset: 5 - 15 learning_rate=0.01, # step size for the numerical optimizer: 0.005 - 0.01 min_count=2, # discard words that appear less than this number: 0 - 100 vector_dim=300, # number of dimensions in vector space: 32-300 word_ngrams=3) # number of words in a word n-gram: 1 - 3 . To call the fit method for the created estimator instance we need to setup the input data channels. This can be organized as a dictionary . data_channels = { &#39;train&#39;: ..., # training data &#39;validation&#39;: ... # validation data } . where training and validation data are the Amazon SageMaker channels for S3 input data sources. . Let&#39;s create a train data channel. . train_data = sagemaker.inputs.TrainingInput( train_s3_uri, distribution=&#39;FullyReplicated&#39;, content_type=&#39;text/plain&#39;, s3_data_type=&#39;S3Prefix&#39; ) . Let&#39;s create a validation data channel. . validation_data = sagemaker.inputs.TrainingInput( validation_s3_uri, distribution=&#39;FullyReplicated&#39;, content_type=&#39;text/plain&#39;, s3_data_type=&#39;S3Prefix&#39; ) . Let&#39;s now organize the data channels defined above as a dictionary. . data_channels = { &#39;train&#39;: train_data, &#39;validation&#39;: validation_data } . We will now start fitting the model to the dataset. . To do this we call the fit method of the estimator passing the configured train and validation inputs (data channels). . estimator.fit( inputs=..., # train and validation input wait=False # do not wait for the job to complete before continuing ) . estimator.fit( inputs=data_channels, wait=False ) training_job_name = estimator.latest_training_job.name print(&#39;Training Job Name: {}&#39;.format(training_job_name)) . Training Job Name: blazingtext-2023-02-06-12-48-14-823 . Let&#39;s setup a watcher while we wait for the training job to complete. . %%time estimator.latest_training_job.wait(logs=False) . 2023-02-06 12:48:16 Starting - Starting the training job......... 2023-02-06 12:49:15 Starting - Preparing the instances for training.. 2023-02-06 12:49:30 Downloading - Downloading input data....... 2023-02-06 12:50:10 Training - Downloading the training image.. 2023-02-06 12:50:26 Training - Training image download completed. Training in progress....... 2023-02-06 12:51:02 Uploading - Uploading generated training model.................................................................... 2023-02-06 12:56:53 Completed - Training job completed CPU times: user 470 ms, sys: 76.5 ms, total: 547 ms Wall time: 8min 28s . Let&#39;s now review the train and validation accuracy. . estimator.training_job_analytics.dataframe() . Warning: No metrics called train:mean_rho found . timestamp metric_name value . 0 0.0 | train:accuracy | 0.5456 | . 1 0.0 | validation:accuracy | 0.5021 | . Deploy the model . Now lets deploy the trained model as an Endpoint. . %%time text_classifier = estimator.deploy(initial_instance_count=1, instance_type=&#39;ml.m5.large&#39;, serializer=sagemaker.serializers.JSONSerializer(), deserializer=sagemaker.deserializers.JSONDeserializer()) print() print(&#39;Endpoint name: {}&#39;.format(text_classifier.endpoint_name)) . --! Endpoint name: blazingtext-2023-02-06-12-56-55-806 CPU times: user 124 ms, sys: 4.38 ms, total: 128 ms Wall time: 2min 32s . Test the model . Let&#39;s now test the model to see if it makes reasonable predictions. . We need to import the nltk library to convert the raw reviews into tokens that BlazingText recognizes. . import nltk nltk.download(&#39;punkt&#39;) . [nltk_data] Downloading package punkt to /root/nltk_data... [nltk_data] Package punkt is already up-to-date! . True . Then we need to specify sample reviews to predict the sentiment. . reviews = [&#39;This product is great!&#39;, &#39;OK, but not great&#39;, &#39;This is not the right product.&#39;] . Next we tokenize the reviews and specify the payload to use when calling the REST API. . tokenized_reviews = [&#39; &#39;.join(nltk.word_tokenize(review)) for review in reviews] payload = {&quot;instances&quot; : tokenized_reviews} print(payload) . {&#39;instances&#39;: [&#39;This product is great !&#39;, &#39;OK , but not great&#39;, &#39;This is not the right product .&#39;]} . Now we can predict the sentiment for each review. Calling the predict method of the text classifier passing the tokenized sentence instances (payload) into the data argument. . predictions = text_classifier.predict(data=payload) for prediction in predictions: print(&#39;Predicted class: {}&#39;.format(prediction[&#39;label&#39;][0].lstrip(&#39;__label__&#39;))) . Predicted class: 1 Predicted class: -1 Predicted class: -1 . Acknowledgements . I&#39;d like to express my thanks to the great Deep Learning AI Practical Data Science on AWS Specialisation Course which i completed, and acknowledge the use of some images and other materials from the training course in this article. .",
            "url": "https://www.livingdatalab.com/aws/cloud-data-science/natural-language-processing/deep-learning/2023/02/06/creating-text-classifier-using-aws-sagemaker-blazingtext.html",
            "relUrl": "/aws/cloud-data-science/natural-language-processing/deep-learning/2023/02/06/creating-text-classifier-using-aws-sagemaker-blazingtext.html",
            "date": " • Feb 6, 2023"
        }
        
    
  
    
        ,"post2": {
            "title": "Train a model quickly with Amazon SageMaker Autopilot",
            "content": "Introduction . In an earlier article we introduced AWS cloud services for data science, and how it can help with different stages of the data science &amp; machine learning workflow. . . In this article, we will use Amazon Sagemaker Autopilot to train a natural language processing (NLP) model. The model will analyze customer feedback and classify the messages into positive (1), neutral (0) and negative (-1) sentiment. . Amazon SageMaker Autopilot automatically trains and tunes the best machine learning models for classification or regression, based on your data while allowing to maintain full control and visibility. . SageMaker Autopilot is an example of AutoML, much like Pycaret which I have written about previously. In comparison, not only is Autopilot even more automated than Pycaret, it is also designed to work at large scale as is possible with cloud data science solutions. . . SageMaker Autopilot will inspect the raw dataset, apply feature processors, pick the best set of algorithms, train and tune multiple models, and then rank the models based on performance - all with just a few clicks. Autopilot transparently generates a set of Python scripts and notebooks for a complete end-to-end pipeline including data analysis, candidate generation, feature engineering, and model training/tuning. . . SageMaker Autopilot job consists of the following high-level steps: . Data analysis where the data is summarized and analyzed to determine which feature engineering techniques, hyper-parameters, and models to explore. | Feature engineering where the data is scrubbed, balanced, combined, and split into train and validation. | Model training and tuning where the top performing features, hyper-parameters, and models are selected and trained. | . These re-usable scripts and notebooks give us full visibility into how the model candidates were created. Since Autopilot integrates natively with SageMaker Studio, we can visually explore the different models generated by SageMaker Autopilot. . . SageMaker Autopilot can be used by people without machine learning experience to automatically train a model from a dataset. Additionally, experienced developers can use Autopilot to train a baseline model from which they can iterate and manually improve. . Autopilot is available through the SageMaker Studio UI and AWS Python SDK. In this project, we will use the AWS Python SDK to train a series of text-classification models and deploy the model with the highest accuracy. . For more details on Autopilot, please refer to this Amazon Science Publication. . Use case: Analyze Customer Sentiment . Customer feedback appears across many channels including social media and partner websites. As a company, you want to capture this valuable product feedback to spot negative trends and improve the situation, if needed. Here we will train a model to classify the feedback messages into positive (1), neutral (0) and negative (-1) sentiment. . First, let&#39;s install and import required modules. . import boto3 import sagemaker import pandas as pd import numpy as np import botocore import time import json config = botocore.config.Config(user_agent_extra=&#39;dlai-pds/c1/w3&#39;) # low-level service client of the boto3 session sm = boto3.client(service_name=&#39;sagemaker&#39;, config=config) sm_runtime = boto3.client(&#39;sagemaker-runtime&#39;, config=config) sess = sagemaker.Session(sagemaker_client=sm, sagemaker_runtime_client=sm_runtime) bucket = sess.default_bucket() role = sagemaker.get_execution_role() region = sess.boto_region_name . import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format=&#39;retina&#39; . Review transformed dataset . Let&#39;s transform the dataset into a format that Autopilot recognizes. Specifically, a comma-separated file of label,features as shown here: . sentiment,review_body -1,&quot;this is bad&quot; 0,&quot;this is ok&quot; 1,&quot;this is great&quot; ... . Sentiment is one of three classes: negative (-1), neutral (0), or positive (1). Autopilot requires that the target variable, sentiment is first and the set of features, just review_body in this case, come next. . !aws s3 cp &#39;s3://dlai-practical-data-science/data/balanced/womens_clothing_ecommerce_reviews_balanced.csv&#39; ./ . download: s3://dlai-practical-data-science/data/balanced/womens_clothing_ecommerce_reviews_balanced.csv to ./womens_clothing_ecommerce_reviews_balanced.csv . path = &#39;./womens_clothing_ecommerce_reviews_balanced.csv&#39; df = pd.read_csv(path, delimiter=&#39;,&#39;) df.head() . sentiment review_body product_category . 0 -1 | This suit did nothing for me. the top has zero... | Swim | . 1 -1 | Like other reviewers i saw this dress on the ... | Dresses | . 2 -1 | I wish i had read the reviews before purchasin... | Knits | . 3 -1 | I ordered these pants in my usual size (xl) an... | Legwear | . 4 -1 | I noticed this top on one of the sales associa... | Knits | . path_autopilot = &#39;./womens_clothing_ecommerce_reviews_balanced_for_autopilot.csv&#39; df[[&#39;sentiment&#39;, &#39;review_body&#39;]].to_csv(path_autopilot, sep=&#39;,&#39;, index=False) . Configure the Autopilot job . Upload data to S3 bucket . autopilot_train_s3_uri = sess.upload_data(bucket=bucket, key_prefix=&#39;autopilot/data&#39;, path=path_autopilot) autopilot_train_s3_uri . &#39;s3://sagemaker-us-east-1-491783890788/autopilot/data/womens_clothing_ecommerce_reviews_balanced_for_autopilot.csv&#39; . Check the existence of the dataset in this S3 bucket folder: . !aws s3 ls $autopilot_train_s3_uri . 2023-02-05 14:47:43 2253749 womens_clothing_ecommerce_reviews_balanced_for_autopilot.csv . S3 output for generated assets . Set the S3 output path for the Autopilot outputs. This includes Jupyter notebooks (analysis), Python scripts (feature engineering), and trained models. . model_output_s3_uri = &#39;s3://{}/autopilot&#39;.format(bucket) print(model_output_s3_uri) . s3://sagemaker-us-east-1-491783890788/autopilot . Configure the Autopilot job . Let&#39;s now create the Autopilot job name. . import time timestamp = int(time.time()) auto_ml_job_name = &#39;automl-dm-{}&#39;.format(timestamp) . When configuring our Autopilot job, we need to specify the maximum number of candidates, max_candidates, to explore as well as the input/output S3 locations and target column to predict. In this case, we want to predict sentiment from the review text. . We will create an instance of the sagemaker.automl.automl.AutoML estimator class passing the required configuration parameters. Target attribute for predictions here is sentiment. . max_candidates = 3 automl = sagemaker.automl.automl.AutoML( target_attribute_name=&#39;sentiment&#39;, base_job_name=auto_ml_job_name, output_path=model_output_s3_uri, max_candidates=max_candidates, sagemaker_session=sess, role=role, max_runtime_per_training_job_in_seconds=1200, total_job_runtime_in_seconds=7200 ) . Launch the Autopilot job . Now we call the fit function of the configured estimator passing the S3 bucket input data path and the Autopilot job name. . automl.fit( autopilot_train_s3_uri, job_name=auto_ml_job_name, wait=False, logs=False ) . Track Autopilot job progress . Once the Autopilot job has been launched, we can track the job progress directly from the notebook using the SDK capabilities. . Autopilot job description . Function describe_auto_ml_job of the Amazon SageMaker service returns the information about the AutoML job in dictionary format. We can review the response syntax and response elements in the documentation. . job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) . Autopilot job status . To track the job progress we can use two response elements: AutoMLJobStatus and AutoMLJobSecondaryStatus, which correspond to the primary (Completed | InProgress | Failed | Stopped | Stopping) and secondary (AnalyzingData | FeatureEngineering | ModelTuning etc.) job states respectively. To see if the AutoML job has started, we can check the existence of the AutoMLJobStatus and AutoMLJobSecondaryStatus elements in the job description response. . We will use the following scheme to track the job progress: . # check if the job is still at certain stage while [check &#39;AutoMLJobStatus&#39; and &#39;AutoMLJobSecondaryStatus&#39;] in job_description_response: # update the job description response job_description_response = automl.describe_auto_ml_job(AutoMLJobName=auto_ml_job_name) # print the message the Autopilot job is in the stage ... print([message]) # get a time step to check the status again sleep(15) print(&quot;Autopilot job complete...&quot;) . while &#39;AutoMLJobStatus&#39; not in job_description_response.keys() and &#39;AutoMLJobSecondaryStatus&#39; not in job_description_response.keys(): job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) print(&#39;[INFO] Autopilot job has not yet started. Please wait. &#39;) # function `json.dumps` encodes JSON string for printing. print(json.dumps(job_description_response, indent=4, sort_keys=True, default=str)) print(&#39;[INFO] Waiting for Autopilot job to start...&#39;) sleep(15) print(&#39;[OK] AutoML job started.&#39;) . [OK] AutoML job started. . Review the SageMaker processing jobs . The Autopilot creates the required SageMaker processing jobs during the run: . First processing job (data splitter) checks the data sanity, performs stratified shuffling and splits the data into training and validation. | Second processing job (candidate generator) first streams through the data to compute statistics for the dataset. Then, uses these statistics to identify the problem type, and possible types of every column-predictor: numeric, categorical, natural language, etc. | . Wait for the data analysis step to finish . Here we will use the same scheme as above to check the completion of the data analysis step. This step can be identified with the (primary) job status value InProgress and secondary job status values Starting and then AnalyzingData. . %%time job_status = job_description_response[&#39;AutoMLJobStatus&#39;] job_sec_status = job_description_response[&#39;AutoMLJobSecondaryStatus&#39;] if job_status not in (&#39;Stopped&#39;, &#39;Failed&#39;): while job_status in (&#39;InProgress&#39;) and job_sec_status in (&#39;Starting&#39;, &#39;AnalyzingData&#39;): job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) job_status = job_description_response[&#39;AutoMLJobStatus&#39;] job_sec_status = job_description_response[&#39;AutoMLJobSecondaryStatus&#39;] print(job_status, job_sec_status) time.sleep(15) print(&#39;[OK] Data analysis phase completed. n&#39;) print(json.dumps(job_description_response, indent=4, sort_keys=True, default=str)) . InProgress FeatureEngineering [OK] Data analysis phase completed. { &#34;AutoMLJobArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:automl-job/automl-dm-1675608463&#34;, &#34;AutoMLJobArtifacts&#34;: { &#34;CandidateDefinitionNotebookLocation&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/sagemaker-automl-candidates/automl-dm-1675608463-pr-1-210c7900f5854fdc89ce01c59579c034fb883/notebooks/SageMakerAutopilotCandidateDefinitionNotebook.ipynb&#34;, &#34;DataExplorationNotebookLocation&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/sagemaker-automl-candidates/automl-dm-1675608463-pr-1-210c7900f5854fdc89ce01c59579c034fb883/notebooks/SageMakerAutopilotDataExplorationNotebook.ipynb&#34; }, &#34;AutoMLJobConfig&#34;: { &#34;CompletionCriteria&#34;: { &#34;MaxAutoMLJobRuntimeInSeconds&#34;: 7200, &#34;MaxCandidates&#34;: 3, &#34;MaxRuntimePerTrainingJobInSeconds&#34;: 1200 }, &#34;SecurityConfig&#34;: { &#34;EnableInterContainerTrafficEncryption&#34;: false } }, &#34;AutoMLJobName&#34;: &#34;automl-dm-1675608463&#34;, &#34;AutoMLJobSecondaryStatus&#34;: &#34;FeatureEngineering&#34;, &#34;AutoMLJobStatus&#34;: &#34;InProgress&#34;, &#34;CreationTime&#34;: &#34;2023-02-05 14:47:43.853000+00:00&#34;, &#34;GenerateCandidateDefinitionsOnly&#34;: false, &#34;InputDataConfig&#34;: [ { &#34;ChannelType&#34;: &#34;training&#34;, &#34;ContentType&#34;: &#34;text/csv;header=present&#34;, &#34;DataSource&#34;: { &#34;S3DataSource&#34;: { &#34;S3DataType&#34;: &#34;S3Prefix&#34;, &#34;S3Uri&#34;: &#34;s3://sagemaker-us-east-1-491783890788/auto-ml-input-data/womens_clothing_ecommerce_reviews_balanced_for_autopilot.csv&#34; } }, &#34;TargetAttributeName&#34;: &#34;sentiment&#34; } ], &#34;LastModifiedTime&#34;: &#34;2023-02-05 14:56:15.134000+00:00&#34;, &#34;OutputDataConfig&#34;: { &#34;S3OutputPath&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot&#34; }, &#34;ResolvedAttributes&#34;: { &#34;AutoMLJobObjective&#34;: { &#34;MetricName&#34;: &#34;Accuracy&#34; }, &#34;CompletionCriteria&#34;: { &#34;MaxAutoMLJobRuntimeInSeconds&#34;: 7200, &#34;MaxCandidates&#34;: 3, &#34;MaxRuntimePerTrainingJobInSeconds&#34;: 1200 }, &#34;ProblemType&#34;: &#34;MulticlassClassification&#34; }, &#34;ResponseMetadata&#34;: { &#34;HTTPHeaders&#34;: { &#34;content-length&#34;: &#34;1815&#34;, &#34;content-type&#34;: &#34;application/x-amz-json-1.1&#34;, &#34;date&#34;: &#34;Sun, 05 Feb 2023 14:56:16 GMT&#34;, &#34;x-amzn-requestid&#34;: &#34;0faeba6e-7645-46d4-a41d-658ebc1167e8&#34; }, &#34;HTTPStatusCode&#34;: 200, &#34;RequestId&#34;: &#34;0faeba6e-7645-46d4-a41d-658ebc1167e8&#34;, &#34;RetryAttempts&#34;: 0 }, &#34;RoleArn&#34;: &#34;arn:aws:iam::491783890788:role/sagemaker-studio-vpc-firewall-us-east-1-sagemaker-execution-role&#34; } CPU times: user 26.6 ms, sys: 43 µs, total: 26.7 ms Wall time: 15.2 s . View generated notebooks . Once data analysis is complete, SageMaker AutoPilot generates two notebooks: . Data exploration | Candidate definition | . Notebooks are included in the AutoML job artifacts generated during the run. Before checking the existence of the notebooks, we can check if the artifacts have been generated. . We will use the status check scheme described above. The generation of artifacts can be identified by existence of AutoMLJobArtifacts element in the keys of the job description response. . job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) # keep in the while loop until the Autopilot job artifacts will be generated while &#39;AutoMLJobArtifacts&#39; not in job_description_response.keys(): # update the information about the running Autopilot job job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) print(&#39;[INFO] Autopilot job has not yet generated the artifacts. Please wait. &#39;) print(json.dumps(job_description_response, indent=4, sort_keys=True, default=str)) print(&#39;[INFO] Waiting for AutoMLJobArtifacts...&#39;) time.sleep(15) print(&#39;[OK] AutoMLJobArtifacts generated.&#39;) . [OK] AutoMLJobArtifacts generated. . We need to wait for Autopilot to make the notebooks available. . We will again use the status check scheme described above. Notebooks creation can be identified by existence of DataExplorationNotebookLocation element in the keys of the job_description_response[&#39;AutoMLJobArtifacts&#39;] dictionary. . job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) # keep in the while loop until the notebooks will be created while &#39;DataExplorationNotebookLocation&#39; not in job_description_response[&#39;AutoMLJobArtifacts&#39;].keys(): # update the information about the running Autopilot job job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) print(&#39;[INFO] Autopilot job has not yet generated the notebooks. Please wait. &#39;) print(json.dumps(job_description_response, indent=4, sort_keys=True, default=str)) print(&#39;[INFO] Waiting for DataExplorationNotebookLocation...&#39;) time.sleep(15) print(&#39;[OK] DataExplorationNotebookLocation found.&#39;) . [OK] DataExplorationNotebookLocation found. . We could review the generated resources in S3 directly. We can find the notebooks in the folder notebooks and download them by clicking on object Actions/Object actions -&gt; Download as/Download. . Feature engineering . We will use the status check scheme described above. The feature engineering step can be identified with the (primary) job status value InProgress and secondary job status value FeatureEngineering. . %%time job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) job_status = job_description_response[&#39;AutoMLJobStatus&#39;] job_sec_status = job_description_response[&#39;AutoMLJobSecondaryStatus&#39;] print(job_status) print(job_sec_status) if job_status not in (&#39;Stopped&#39;, &#39;Failed&#39;): while job_status in (&#39;InProgress&#39;) and job_sec_status in (&#39;FeatureEngineering&#39;): job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) job_status = job_description_response[&#39;AutoMLJobStatus&#39;] job_sec_status = job_description_response[&#39;AutoMLJobSecondaryStatus&#39;] print(job_status, job_sec_status) time.sleep(5) print(&#39;[OK] Feature engineering phase completed. n&#39;) print(json.dumps(job_description_response, indent=4, sort_keys=True, default=str)) . InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress FeatureEngineering InProgress ModelTuning [OK] Feature engineering phase completed. { &#34;AutoMLJobArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:automl-job/automl-dm-1675608463&#34;, &#34;AutoMLJobArtifacts&#34;: { &#34;CandidateDefinitionNotebookLocation&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/sagemaker-automl-candidates/automl-dm-1675608463-pr-1-210c7900f5854fdc89ce01c59579c034fb883/notebooks/SageMakerAutopilotCandidateDefinitionNotebook.ipynb&#34;, &#34;DataExplorationNotebookLocation&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/sagemaker-automl-candidates/automl-dm-1675608463-pr-1-210c7900f5854fdc89ce01c59579c034fb883/notebooks/SageMakerAutopilotDataExplorationNotebook.ipynb&#34; }, &#34;AutoMLJobConfig&#34;: { &#34;CompletionCriteria&#34;: { &#34;MaxAutoMLJobRuntimeInSeconds&#34;: 7200, &#34;MaxCandidates&#34;: 3, &#34;MaxRuntimePerTrainingJobInSeconds&#34;: 1200 }, &#34;SecurityConfig&#34;: { &#34;EnableInterContainerTrafficEncryption&#34;: false } }, &#34;AutoMLJobName&#34;: &#34;automl-dm-1675608463&#34;, &#34;AutoMLJobSecondaryStatus&#34;: &#34;ModelTuning&#34;, &#34;AutoMLJobStatus&#34;: &#34;InProgress&#34;, &#34;CreationTime&#34;: &#34;2023-02-05 14:47:43.853000+00:00&#34;, &#34;GenerateCandidateDefinitionsOnly&#34;: false, &#34;InputDataConfig&#34;: [ { &#34;ChannelType&#34;: &#34;training&#34;, &#34;ContentType&#34;: &#34;text/csv;header=present&#34;, &#34;DataSource&#34;: { &#34;S3DataSource&#34;: { &#34;S3DataType&#34;: &#34;S3Prefix&#34;, &#34;S3Uri&#34;: &#34;s3://sagemaker-us-east-1-491783890788/auto-ml-input-data/womens_clothing_ecommerce_reviews_balanced_for_autopilot.csv&#34; } }, &#34;TargetAttributeName&#34;: &#34;sentiment&#34; } ], &#34;LastModifiedTime&#34;: &#34;2023-02-05 15:04:28.632000+00:00&#34;, &#34;OutputDataConfig&#34;: { &#34;S3OutputPath&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot&#34; }, &#34;ResolvedAttributes&#34;: { &#34;AutoMLJobObjective&#34;: { &#34;MetricName&#34;: &#34;Accuracy&#34; }, &#34;CompletionCriteria&#34;: { &#34;MaxAutoMLJobRuntimeInSeconds&#34;: 7200, &#34;MaxCandidates&#34;: 3, &#34;MaxRuntimePerTrainingJobInSeconds&#34;: 1200 }, &#34;ProblemType&#34;: &#34;MulticlassClassification&#34; }, &#34;ResponseMetadata&#34;: { &#34;HTTPHeaders&#34;: { &#34;content-length&#34;: &#34;1808&#34;, &#34;content-type&#34;: &#34;application/x-amz-json-1.1&#34;, &#34;date&#34;: &#34;Sun, 05 Feb 2023 15:04:28 GMT&#34;, &#34;x-amzn-requestid&#34;: &#34;eecffe9b-ef5e-4e69-b4ca-d0b0b3a95be7&#34; }, &#34;HTTPStatusCode&#34;: 200, &#34;RequestId&#34;: &#34;eecffe9b-ef5e-4e69-b4ca-d0b0b3a95be7&#34;, &#34;RetryAttempts&#34;: 0 }, &#34;RoleArn&#34;: &#34;arn:aws:iam::491783890788:role/sagemaker-studio-vpc-firewall-us-east-1-sagemaker-execution-role&#34; } CPU times: user 378 ms, sys: 49.3 ms, total: 427 ms Wall time: 7min 7s . Model training and tuning . We can use the status check scheme described above. the model tuning step can be identified with the (primary) job status value InProgress and secondary job status value ModelTuning. . %%time job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) job_status = job_description_response[&#39;AutoMLJobStatus&#39;] job_sec_status = job_description_response[&#39;AutoMLJobSecondaryStatus&#39;] print(job_status) print(job_sec_status) if job_status not in (&#39;Stopped&#39;, &#39;Failed&#39;): while job_status in (&#39;InProgress&#39;) and job_sec_status in (&#39;ModelTuning&#39;): job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) job_status = job_description_response[&#39;AutoMLJobStatus&#39;] job_sec_status = job_description_response[&#39;AutoMLJobSecondaryStatus&#39;] print(job_status, job_sec_status) time.sleep(5) print(&#39;[OK] Model tuning phase completed. n&#39;) print(json.dumps(job_description_response, indent=4, sort_keys=True, default=str)) . InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress ModelTuning InProgress MaxCandidatesReached [OK] Model tuning phase completed. { &#34;AutoMLJobArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:automl-job/automl-dm-1675608463&#34;, &#34;AutoMLJobArtifacts&#34;: { &#34;CandidateDefinitionNotebookLocation&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/sagemaker-automl-candidates/automl-dm-1675608463-pr-1-210c7900f5854fdc89ce01c59579c034fb883/notebooks/SageMakerAutopilotCandidateDefinitionNotebook.ipynb&#34;, &#34;DataExplorationNotebookLocation&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/sagemaker-automl-candidates/automl-dm-1675608463-pr-1-210c7900f5854fdc89ce01c59579c034fb883/notebooks/SageMakerAutopilotDataExplorationNotebook.ipynb&#34; }, &#34;AutoMLJobConfig&#34;: { &#34;CompletionCriteria&#34;: { &#34;MaxAutoMLJobRuntimeInSeconds&#34;: 7200, &#34;MaxCandidates&#34;: 3, &#34;MaxRuntimePerTrainingJobInSeconds&#34;: 1200 }, &#34;SecurityConfig&#34;: { &#34;EnableInterContainerTrafficEncryption&#34;: false } }, &#34;AutoMLJobName&#34;: &#34;automl-dm-1675608463&#34;, &#34;AutoMLJobSecondaryStatus&#34;: &#34;MaxCandidatesReached&#34;, &#34;AutoMLJobStatus&#34;: &#34;InProgress&#34;, &#34;BestCandidate&#34;: { &#34;CandidateName&#34;: &#34;automl-dm-1675608463sujxUg8wYQX0-002-657fba80&#34;, &#34;CandidateProperties&#34;: { &#34;CandidateMetrics&#34;: [ { &#34;MetricName&#34;: &#34;F1macro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;F1macro&#34;, &#34;Value&#34;: 0.6152600049972534 }, { &#34;MetricName&#34;: &#34;PrecisionMacro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;PrecisionMacro&#34;, &#34;Value&#34;: 0.6158699989318848 }, { &#34;MetricName&#34;: &#34;Accuracy&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;Accuracy&#34;, &#34;Value&#34;: 0.6150500178337097 }, { &#34;MetricName&#34;: &#34;BalancedAccuracy&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;BalancedAccuracy&#34;, &#34;Value&#34;: 0.6150500178337097 }, { &#34;MetricName&#34;: &#34;LogLoss&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;LogLoss&#34;, &#34;Value&#34;: 0.843940019607544 }, { &#34;MetricName&#34;: &#34;RecallMacro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;RecallMacro&#34;, &#34;Value&#34;: 0.6150500178337097 } ] }, &#34;CandidateStatus&#34;: &#34;Completed&#34;, &#34;CandidateSteps&#34;: [ { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:processing-job/automl-dm-1675608463-db-1-ec0fb37f4b964d1a9485854c252aa8f0683f5&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-db-1-ec0fb37f4b964d1a9485854c252aa8f0683f5&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::ProcessingJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:training-job/automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TrainingJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:transform-job/automl-dm-1675608463-dpp2-rpb-1-fd31c8d697b34a02a3472ce6d7557cd&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-dpp2-rpb-1-fd31c8d697b34a02a3472ce6d7557cd&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TransformJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:training-job/automl-dm-1675608463sujxUg8wYQX0-002-657fba80&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463sujxUg8wYQX0-002-657fba80&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TrainingJob&#34; } ], &#34;CreationTime&#34;: &#34;2023-02-05 15:06:01+00:00&#34;, &#34;EndTime&#34;: &#34;2023-02-05 15:07:54+00:00&#34;, &#34;FinalAutoMLJobObjectiveMetric&#34;: { &#34;MetricName&#34;: &#34;validation:accuracy&#34;, &#34;Value&#34;: 0.6150500178337097 }, &#34;InferenceContainers&#34;: [ { &#34;Environment&#34;: { &#34;AUTOML_SPARSE_ENCODE_RECORDIO_PROTOBUF&#34;: &#34;1&#34;, &#34;AUTOML_TRANSFORM_MODE&#34;: &#34;feature-transform&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;application/x-recordio-protobuf&#34;, &#34;SAGEMAKER_PROGRAM&#34;: &#34;sagemaker_serve&#34;, &#34;SAGEMAKER_SUBMIT_DIRECTORY&#34;: &#34;/opt/ml/model/code&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/data-processor-models/automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a/output/model.tar.gz&#34; }, { &#34;Environment&#34;: { &#34;MAX_CONTENT_LENGTH&#34;: &#34;20971520&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;text/csv&#34;, &#34;SAGEMAKER_INFERENCE_OUTPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_SUPPORTED&#34;: &#34;predicted_label,probability,probabilities&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/tuning/automl-dm--dpp2-xgb/automl-dm-1675608463sujxUg8wYQX0-002-657fba80/output/model.tar.gz&#34; }, { &#34;Environment&#34;: { &#34;AUTOML_TRANSFORM_MODE&#34;: &#34;inverse-label-transform&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;text/csv&#34;, &#34;SAGEMAKER_INFERENCE_INPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_OUTPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_SUPPORTED&#34;: &#34;predicted_label,probability,labels,probabilities&#34;, &#34;SAGEMAKER_PROGRAM&#34;: &#34;sagemaker_serve&#34;, &#34;SAGEMAKER_SUBMIT_DIRECTORY&#34;: &#34;/opt/ml/model/code&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/data-processor-models/automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a/output/model.tar.gz&#34; } ], &#34;LastModifiedTime&#34;: &#34;2023-02-05 15:09:06.585000+00:00&#34;, &#34;ObjectiveStatus&#34;: &#34;Succeeded&#34; }, &#34;CreationTime&#34;: &#34;2023-02-05 14:47:43.853000+00:00&#34;, &#34;GenerateCandidateDefinitionsOnly&#34;: false, &#34;InputDataConfig&#34;: [ { &#34;ChannelType&#34;: &#34;training&#34;, &#34;ContentType&#34;: &#34;text/csv;header=present&#34;, &#34;DataSource&#34;: { &#34;S3DataSource&#34;: { &#34;S3DataType&#34;: &#34;S3Prefix&#34;, &#34;S3Uri&#34;: &#34;s3://sagemaker-us-east-1-491783890788/auto-ml-input-data/womens_clothing_ecommerce_reviews_balanced_for_autopilot.csv&#34; } }, &#34;TargetAttributeName&#34;: &#34;sentiment&#34; } ], &#34;LastModifiedTime&#34;: &#34;2023-02-05 15:09:06.661000+00:00&#34;, &#34;OutputDataConfig&#34;: { &#34;S3OutputPath&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot&#34; }, &#34;ResolvedAttributes&#34;: { &#34;AutoMLJobObjective&#34;: { &#34;MetricName&#34;: &#34;Accuracy&#34; }, &#34;CompletionCriteria&#34;: { &#34;MaxAutoMLJobRuntimeInSeconds&#34;: 7200, &#34;MaxCandidates&#34;: 3, &#34;MaxRuntimePerTrainingJobInSeconds&#34;: 1200 }, &#34;ProblemType&#34;: &#34;MulticlassClassification&#34; }, &#34;ResponseMetadata&#34;: { &#34;HTTPHeaders&#34;: { &#34;content-length&#34;: &#34;5731&#34;, &#34;content-type&#34;: &#34;application/x-amz-json-1.1&#34;, &#34;date&#34;: &#34;Sun, 05 Feb 2023 15:09:06 GMT&#34;, &#34;x-amzn-requestid&#34;: &#34;d6af6156-cd79-4bf4-8025-52c85f36afa3&#34; }, &#34;HTTPStatusCode&#34;: 200, &#34;RequestId&#34;: &#34;d6af6156-cd79-4bf4-8025-52c85f36afa3&#34;, &#34;RetryAttempts&#34;: 0 }, &#34;RoleArn&#34;: &#34;arn:aws:iam::491783890788:role/sagemaker-studio-vpc-firewall-us-east-1-sagemaker-execution-role&#34; } CPU times: user 241 ms, sys: 24.9 ms, total: 266 ms Wall time: 4min 12s . Finally, we can check the completion of the Autopilot job looking for the Completed job status. . %%time from pprint import pprint job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) pprint(job_description_response) job_status = job_description_response[&#39;AutoMLJobStatus&#39;] job_sec_status = job_description_response[&#39;AutoMLJobSecondaryStatus&#39;] print(&#39;Job status: {}&#39;.format(job_status)) print(&#39;Secondary job status: {}&#39;.format(job_sec_status)) if job_status not in (&#39;Stopped&#39;, &#39;Failed&#39;): while job_status not in (&#39;Completed&#39;): job_description_response = automl.describe_auto_ml_job(job_name=auto_ml_job_name) job_status = job_description_response[&#39;AutoMLJobStatus&#39;] job_sec_status = job_description_response[&#39;AutoMLJobSecondaryStatus&#39;] print(&#39;Job status: {}&#39;.format(job_status)) print(&#39;Secondary job status: {}&#39;.format(job_sec_status)) time.sleep(10) print(&#39;[OK] Autopilot job completed. n&#39;) else: print(&#39;Job status: {}&#39;.format(job_status)) print(&#39;Secondary job status: {}&#39;.format(job_status)) . {&#39;AutoMLJobArn&#39;: &#39;arn:aws:sagemaker:us-east-1:491783890788:automl-job/automl-dm-1675608463&#39;, &#39;AutoMLJobArtifacts&#39;: {&#39;CandidateDefinitionNotebookLocation&#39;: &#39;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/sagemaker-automl-candidates/automl-dm-1675608463-pr-1-210c7900f5854fdc89ce01c59579c034fb883/notebooks/SageMakerAutopilotCandidateDefinitionNotebook.ipynb&#39;, &#39;DataExplorationNotebookLocation&#39;: &#39;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/sagemaker-automl-candidates/automl-dm-1675608463-pr-1-210c7900f5854fdc89ce01c59579c034fb883/notebooks/SageMakerAutopilotDataExplorationNotebook.ipynb&#39;}, &#39;AutoMLJobConfig&#39;: {&#39;CompletionCriteria&#39;: {&#39;MaxAutoMLJobRuntimeInSeconds&#39;: 7200, &#39;MaxCandidates&#39;: 3, &#39;MaxRuntimePerTrainingJobInSeconds&#39;: 1200}, &#39;SecurityConfig&#39;: {&#39;EnableInterContainerTrafficEncryption&#39;: False}}, &#39;AutoMLJobName&#39;: &#39;automl-dm-1675608463&#39;, &#39;AutoMLJobSecondaryStatus&#39;: &#39;MergingAutoMLTaskReports&#39;, &#39;AutoMLJobStatus&#39;: &#39;InProgress&#39;, &#39;BestCandidate&#39;: {&#39;CandidateName&#39;: &#39;automl-dm-1675608463sujxUg8wYQX0-002-657fba80&#39;, &#39;CandidateProperties&#39;: {&#39;CandidateMetrics&#39;: [{&#39;MetricName&#39;: &#39;F1macro&#39;, &#39;Set&#39;: &#39;Validation&#39;, &#39;StandardMetricName&#39;: &#39;F1macro&#39;, &#39;Value&#39;: 0.6152600049972534}, {&#39;MetricName&#39;: &#39;PrecisionMacro&#39;, &#39;Set&#39;: &#39;Validation&#39;, &#39;StandardMetricName&#39;: &#39;PrecisionMacro&#39;, &#39;Value&#39;: 0.6158699989318848}, {&#39;MetricName&#39;: &#39;Accuracy&#39;, &#39;Set&#39;: &#39;Validation&#39;, &#39;StandardMetricName&#39;: &#39;Accuracy&#39;, &#39;Value&#39;: 0.6150500178337097}, {&#39;MetricName&#39;: &#39;BalancedAccuracy&#39;, &#39;Set&#39;: &#39;Validation&#39;, &#39;StandardMetricName&#39;: &#39;BalancedAccuracy&#39;, &#39;Value&#39;: 0.6150500178337097}, {&#39;MetricName&#39;: &#39;LogLoss&#39;, &#39;Set&#39;: &#39;Validation&#39;, &#39;StandardMetricName&#39;: &#39;LogLoss&#39;, &#39;Value&#39;: 0.843940019607544}, {&#39;MetricName&#39;: &#39;RecallMacro&#39;, &#39;Set&#39;: &#39;Validation&#39;, &#39;StandardMetricName&#39;: &#39;RecallMacro&#39;, &#39;Value&#39;: 0.6150500178337097}]}, &#39;CandidateStatus&#39;: &#39;Completed&#39;, &#39;CandidateSteps&#39;: [{&#39;CandidateStepArn&#39;: &#39;arn:aws:sagemaker:us-east-1:491783890788:processing-job/automl-dm-1675608463-db-1-ec0fb37f4b964d1a9485854c252aa8f0683f5&#39;, &#39;CandidateStepName&#39;: &#39;automl-dm-1675608463-db-1-ec0fb37f4b964d1a9485854c252aa8f0683f5&#39;, &#39;CandidateStepType&#39;: &#39;AWS::SageMaker::ProcessingJob&#39;}, {&#39;CandidateStepArn&#39;: &#39;arn:aws:sagemaker:us-east-1:491783890788:training-job/automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a&#39;, &#39;CandidateStepName&#39;: &#39;automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a&#39;, &#39;CandidateStepType&#39;: &#39;AWS::SageMaker::TrainingJob&#39;}, {&#39;CandidateStepArn&#39;: &#39;arn:aws:sagemaker:us-east-1:491783890788:transform-job/automl-dm-1675608463-dpp2-rpb-1-fd31c8d697b34a02a3472ce6d7557cd&#39;, &#39;CandidateStepName&#39;: &#39;automl-dm-1675608463-dpp2-rpb-1-fd31c8d697b34a02a3472ce6d7557cd&#39;, &#39;CandidateStepType&#39;: &#39;AWS::SageMaker::TransformJob&#39;}, {&#39;CandidateStepArn&#39;: &#39;arn:aws:sagemaker:us-east-1:491783890788:training-job/automl-dm-1675608463sujxUg8wYQX0-002-657fba80&#39;, &#39;CandidateStepName&#39;: &#39;automl-dm-1675608463sujxUg8wYQX0-002-657fba80&#39;, &#39;CandidateStepType&#39;: &#39;AWS::SageMaker::TrainingJob&#39;}], &#39;CreationTime&#39;: datetime.datetime(2023, 2, 5, 15, 6, 1, tzinfo=tzlocal()), &#39;EndTime&#39;: datetime.datetime(2023, 2, 5, 15, 7, 54, tzinfo=tzlocal()), &#39;FinalAutoMLJobObjectiveMetric&#39;: {&#39;MetricName&#39;: &#39;validation:accuracy&#39;, &#39;Value&#39;: 0.6150500178337097}, &#39;InferenceContainers&#39;: [{&#39;Environment&#39;: {&#39;AUTOML_SPARSE_ENCODE_RECORDIO_PROTOBUF&#39;: &#39;1&#39;, &#39;AUTOML_TRANSFORM_MODE&#39;: &#39;feature-transform&#39;, &#39;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#39;: &#39;application/x-recordio-protobuf&#39;, &#39;SAGEMAKER_PROGRAM&#39;: &#39;sagemaker_serve&#39;, &#39;SAGEMAKER_SUBMIT_DIRECTORY&#39;: &#39;/opt/ml/model/code&#39;}, &#39;Image&#39;: &#39;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3&#39;, &#39;ModelDataUrl&#39;: &#39;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/data-processor-models/automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a/output/model.tar.gz&#39;}, {&#39;Environment&#39;: {&#39;MAX_CONTENT_LENGTH&#39;: &#39;20971520&#39;, &#39;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#39;: &#39;text/csv&#39;, &#39;SAGEMAKER_INFERENCE_OUTPUT&#39;: &#39;predicted_label&#39;, &#39;SAGEMAKER_INFERENCE_SUPPORTED&#39;: &#39;predicted_label,probability,probabilities&#39;}, &#39;Image&#39;: &#39;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1-cpu-py3&#39;, &#39;ModelDataUrl&#39;: &#39;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/tuning/automl-dm--dpp2-xgb/automl-dm-1675608463sujxUg8wYQX0-002-657fba80/output/model.tar.gz&#39;}, {&#39;Environment&#39;: {&#39;AUTOML_TRANSFORM_MODE&#39;: &#39;inverse-label-transform&#39;, &#39;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#39;: &#39;text/csv&#39;, &#39;SAGEMAKER_INFERENCE_INPUT&#39;: &#39;predicted_label&#39;, &#39;SAGEMAKER_INFERENCE_OUTPUT&#39;: &#39;predicted_label&#39;, &#39;SAGEMAKER_INFERENCE_SUPPORTED&#39;: &#39;predicted_label,probability,labels,probabilities&#39;, &#39;SAGEMAKER_PROGRAM&#39;: &#39;sagemaker_serve&#39;, &#39;SAGEMAKER_SUBMIT_DIRECTORY&#39;: &#39;/opt/ml/model/code&#39;}, &#39;Image&#39;: &#39;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3&#39;, &#39;ModelDataUrl&#39;: &#39;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/data-processor-models/automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a/output/model.tar.gz&#39;}], &#39;LastModifiedTime&#39;: datetime.datetime(2023, 2, 5, 15, 9, 6, 585000, tzinfo=tzlocal()), &#39;ObjectiveStatus&#39;: &#39;Succeeded&#39;}, &#39;CreationTime&#39;: datetime.datetime(2023, 2, 5, 14, 47, 43, 853000, tzinfo=tzlocal()), &#39;GenerateCandidateDefinitionsOnly&#39;: False, &#39;InputDataConfig&#39;: [{&#39;ChannelType&#39;: &#39;training&#39;, &#39;ContentType&#39;: &#39;text/csv;header=present&#39;, &#39;DataSource&#39;: {&#39;S3DataSource&#39;: {&#39;S3DataType&#39;: &#39;S3Prefix&#39;, &#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-491783890788/auto-ml-input-data/womens_clothing_ecommerce_reviews_balanced_for_autopilot.csv&#39;}}, &#39;TargetAttributeName&#39;: &#39;sentiment&#39;}], &#39;LastModifiedTime&#39;: datetime.datetime(2023, 2, 5, 15, 9, 7, 862000, tzinfo=tzlocal()), &#39;OutputDataConfig&#39;: {&#39;S3OutputPath&#39;: &#39;s3://sagemaker-us-east-1-491783890788/autopilot&#39;}, &#39;ResolvedAttributes&#39;: {&#39;AutoMLJobObjective&#39;: {&#39;MetricName&#39;: &#39;Accuracy&#39;}, &#39;CompletionCriteria&#39;: {&#39;MaxAutoMLJobRuntimeInSeconds&#39;: 7200, &#39;MaxCandidates&#39;: 3, &#39;MaxRuntimePerTrainingJobInSeconds&#39;: 1200}, &#39;ProblemType&#39;: &#39;MulticlassClassification&#39;}, &#39;ResponseMetadata&#39;: {&#39;HTTPHeaders&#39;: {&#39;content-length&#39;: &#39;5735&#39;, &#39;content-type&#39;: &#39;application/x-amz-json-1.1&#39;, &#39;date&#39;: &#39;Sun, 05 Feb 2023 15:09:27 GMT&#39;, &#39;x-amzn-requestid&#39;: &#39;5577738e-56f0-40ea-8ae0-9f4f512ecae8&#39;}, &#39;HTTPStatusCode&#39;: 200, &#39;RequestId&#39;: &#39;5577738e-56f0-40ea-8ae0-9f4f512ecae8&#39;, &#39;RetryAttempts&#39;: 0}, &#39;RoleArn&#39;: &#39;arn:aws:iam::491783890788:role/sagemaker-studio-vpc-firewall-us-east-1-sagemaker-execution-role&#39;} Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: InProgress Secondary job status: MergingAutoMLTaskReports Job status: Completed Secondary job status: Completed [OK] Autopilot job completed. CPU times: user 719 ms, sys: 63.7 ms, total: 783 ms Wall time: 7min 59s . Compare model candidates . Once model tuning is complete, we can view all the candidates (pipeline evaluations with different hyperparameter combinations) that were explored by AutoML and sort them by their final performance metric. . We will list candidates generated by Autopilot sorted by accuracy from highest to lowest. . To do this we will use the list_candidates function passing the Autopilot job name auto_ml_job_name with the accuracy field FinalObjectiveMetricValue. It returns the list of candidates with the information about them. . candidates = automl.list_candidates( job_name=..., # Autopilot job name sort_by=&#39;...&#39; # accuracy field name ) . candidates = automl.list_candidates( job_name=auto_ml_job_name, sort_by=&#39;FinalObjectiveMetricValue&#39; ) . We can review the response syntax and response elements of the function list_candidates in the documentation. Now let&#39;s put the candidate existence check into the loop: . while candidates == []: candidates = automl.list_candidates(job_name=auto_ml_job_name) print(&#39;[INFO] Autopilot job is generating the candidates. Please wait.&#39;) time.sleep(10) print(&#39;[OK] Candidates generated.&#39;) . [OK] Candidates generated. . The information about each of the candidates is in the dictionary with the following keys: . print(candidates[0].keys()) . dict_keys([&#39;CandidateName&#39;, &#39;FinalAutoMLJobObjectiveMetric&#39;, &#39;ObjectiveStatus&#39;, &#39;CandidateSteps&#39;, &#39;CandidateStatus&#39;, &#39;InferenceContainers&#39;, &#39;CreationTime&#39;, &#39;EndTime&#39;, &#39;LastModifiedTime&#39;, &#39;CandidateProperties&#39;]) . CandidateName contains the candidate name and the FinalAutoMLJobObjectiveMetric element contains the metric information which can be used to identify the best candidate later. Let&#39;s check that they were generated. . while &#39;CandidateName&#39; not in candidates[0]: candidates = automl.list_candidates(job_name=auto_ml_job_name) print(&#39;[INFO] Autopilot job is generating CandidateName. Please wait. &#39;) sleep(10) print(&#39;[OK] CandidateName generated.&#39;) . [OK] CandidateName generated. . while &#39;FinalAutoMLJobObjectiveMetric&#39; not in candidates[0]: candidates = automl.list_candidates(job_name=auto_ml_job_name) print(&#39;[INFO] Autopilot job is generating FinalAutoMLJobObjectiveMetric. Please wait. &#39;) sleep(10) print(&#39;[OK] FinalAutoMLJobObjectiveMetric generated.&#39;) . [OK] FinalAutoMLJobObjectiveMetric generated. . print(json.dumps(candidates, indent=4, sort_keys=True, default=str)) . [ { &#34;CandidateName&#34;: &#34;automl-dm-1675608463sujxUg8wYQX0-002-657fba80&#34;, &#34;CandidateProperties&#34;: { &#34;CandidateArtifactLocations&#34;: { &#34;Explainability&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/documentation/explainability/output&#34;, &#34;ModelInsights&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/documentation/model_monitor/output&#34; }, &#34;CandidateMetrics&#34;: [ { &#34;MetricName&#34;: &#34;F1macro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;F1macro&#34;, &#34;Value&#34;: 0.6152600049972534 }, { &#34;MetricName&#34;: &#34;PrecisionMacro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;PrecisionMacro&#34;, &#34;Value&#34;: 0.6158699989318848 }, { &#34;MetricName&#34;: &#34;Accuracy&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;Accuracy&#34;, &#34;Value&#34;: 0.6150500178337097 }, { &#34;MetricName&#34;: &#34;BalancedAccuracy&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;BalancedAccuracy&#34;, &#34;Value&#34;: 0.6150500178337097 }, { &#34;MetricName&#34;: &#34;LogLoss&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;LogLoss&#34;, &#34;Value&#34;: 0.843940019607544 }, { &#34;MetricName&#34;: &#34;RecallMacro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;RecallMacro&#34;, &#34;Value&#34;: 0.6150500178337097 } ] }, &#34;CandidateStatus&#34;: &#34;Completed&#34;, &#34;CandidateSteps&#34;: [ { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:processing-job/automl-dm-1675608463-db-1-ec0fb37f4b964d1a9485854c252aa8f0683f5&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-db-1-ec0fb37f4b964d1a9485854c252aa8f0683f5&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::ProcessingJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:training-job/automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TrainingJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:transform-job/automl-dm-1675608463-dpp2-rpb-1-fd31c8d697b34a02a3472ce6d7557cd&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-dpp2-rpb-1-fd31c8d697b34a02a3472ce6d7557cd&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TransformJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:training-job/automl-dm-1675608463sujxUg8wYQX0-002-657fba80&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463sujxUg8wYQX0-002-657fba80&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TrainingJob&#34; } ], &#34;CreationTime&#34;: &#34;2023-02-05 15:06:01+00:00&#34;, &#34;EndTime&#34;: &#34;2023-02-05 15:07:54+00:00&#34;, &#34;FinalAutoMLJobObjectiveMetric&#34;: { &#34;MetricName&#34;: &#34;validation:accuracy&#34;, &#34;Value&#34;: 0.6150500178337097 }, &#34;InferenceContainers&#34;: [ { &#34;Environment&#34;: { &#34;AUTOML_SPARSE_ENCODE_RECORDIO_PROTOBUF&#34;: &#34;1&#34;, &#34;AUTOML_TRANSFORM_MODE&#34;: &#34;feature-transform&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;application/x-recordio-protobuf&#34;, &#34;SAGEMAKER_PROGRAM&#34;: &#34;sagemaker_serve&#34;, &#34;SAGEMAKER_SUBMIT_DIRECTORY&#34;: &#34;/opt/ml/model/code&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/data-processor-models/automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a/output/model.tar.gz&#34; }, { &#34;Environment&#34;: { &#34;MAX_CONTENT_LENGTH&#34;: &#34;20971520&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;text/csv&#34;, &#34;SAGEMAKER_INFERENCE_OUTPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_SUPPORTED&#34;: &#34;predicted_label,probability,probabilities&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/tuning/automl-dm--dpp2-xgb/automl-dm-1675608463sujxUg8wYQX0-002-657fba80/output/model.tar.gz&#34; }, { &#34;Environment&#34;: { &#34;AUTOML_TRANSFORM_MODE&#34;: &#34;inverse-label-transform&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;text/csv&#34;, &#34;SAGEMAKER_INFERENCE_INPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_OUTPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_SUPPORTED&#34;: &#34;predicted_label,probability,labels,probabilities&#34;, &#34;SAGEMAKER_PROGRAM&#34;: &#34;sagemaker_serve&#34;, &#34;SAGEMAKER_SUBMIT_DIRECTORY&#34;: &#34;/opt/ml/model/code&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/data-processor-models/automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a/output/model.tar.gz&#34; } ], &#34;LastModifiedTime&#34;: &#34;2023-02-05 15:09:06.585000+00:00&#34;, &#34;ObjectiveStatus&#34;: &#34;Succeeded&#34; }, { &#34;CandidateName&#34;: &#34;automl-dm-1675608463sujxUg8wYQX0-001-5d775b4b&#34;, &#34;CandidateProperties&#34;: { &#34;CandidateMetrics&#34;: [ { &#34;MetricName&#34;: &#34;F1macro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;F1macro&#34;, &#34;Value&#34;: 0.6157000064849854 }, { &#34;MetricName&#34;: &#34;PrecisionMacro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;PrecisionMacro&#34;, &#34;Value&#34;: 0.6168199777603149 }, { &#34;MetricName&#34;: &#34;Accuracy&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;Accuracy&#34;, &#34;Value&#34;: 0.6149100065231323 }, { &#34;MetricName&#34;: &#34;BalancedAccuracy&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;BalancedAccuracy&#34;, &#34;Value&#34;: 0.6149100065231323 }, { &#34;MetricName&#34;: &#34;LogLoss&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;LogLoss&#34;, &#34;Value&#34;: 0.8395400047302246 }, { &#34;MetricName&#34;: &#34;RecallMacro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;RecallMacro&#34;, &#34;Value&#34;: 0.6149100065231323 } ] }, &#34;CandidateStatus&#34;: &#34;Completed&#34;, &#34;CandidateSteps&#34;: [ { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:processing-job/automl-dm-1675608463-db-1-ec0fb37f4b964d1a9485854c252aa8f0683f5&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-db-1-ec0fb37f4b964d1a9485854c252aa8f0683f5&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::ProcessingJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:training-job/automl-dm-1675608463-dpp0-1-b325f697683a4300957f609440a1906660e&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-dpp0-1-b325f697683a4300957f609440a1906660e&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TrainingJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:transform-job/automl-dm-1675608463-dpp0-rpb-1-57a73878e9f24b9dbe23bf82b200317&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-dpp0-rpb-1-57a73878e9f24b9dbe23bf82b200317&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TransformJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:training-job/automl-dm-1675608463sujxUg8wYQX0-001-5d775b4b&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463sujxUg8wYQX0-001-5d775b4b&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TrainingJob&#34; } ], &#34;CreationTime&#34;: &#34;2023-02-05 15:05:53+00:00&#34;, &#34;EndTime&#34;: &#34;2023-02-05 15:07:46+00:00&#34;, &#34;FinalAutoMLJobObjectiveMetric&#34;: { &#34;MetricName&#34;: &#34;validation:accuracy&#34;, &#34;Value&#34;: 0.6149100065231323 }, &#34;InferenceContainers&#34;: [ { &#34;Environment&#34;: { &#34;AUTOML_SPARSE_ENCODE_RECORDIO_PROTOBUF&#34;: &#34;1&#34;, &#34;AUTOML_TRANSFORM_MODE&#34;: &#34;feature-transform&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;application/x-recordio-protobuf&#34;, &#34;SAGEMAKER_PROGRAM&#34;: &#34;sagemaker_serve&#34;, &#34;SAGEMAKER_SUBMIT_DIRECTORY&#34;: &#34;/opt/ml/model/code&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/data-processor-models/automl-dm-1675608463-dpp0-1-b325f697683a4300957f609440a1906660e/output/model.tar.gz&#34; }, { &#34;Environment&#34;: { &#34;MAX_CONTENT_LENGTH&#34;: &#34;20971520&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;text/csv&#34;, &#34;SAGEMAKER_INFERENCE_OUTPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_SUPPORTED&#34;: &#34;predicted_label,probability,probabilities&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/tuning/automl-dm--dpp0-xgb/automl-dm-1675608463sujxUg8wYQX0-001-5d775b4b/output/model.tar.gz&#34; }, { &#34;Environment&#34;: { &#34;AUTOML_TRANSFORM_MODE&#34;: &#34;inverse-label-transform&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;text/csv&#34;, &#34;SAGEMAKER_INFERENCE_INPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_OUTPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_SUPPORTED&#34;: &#34;predicted_label,probability,labels,probabilities&#34;, &#34;SAGEMAKER_PROGRAM&#34;: &#34;sagemaker_serve&#34;, &#34;SAGEMAKER_SUBMIT_DIRECTORY&#34;: &#34;/opt/ml/model/code&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/data-processor-models/automl-dm-1675608463-dpp0-1-b325f697683a4300957f609440a1906660e/output/model.tar.gz&#34; } ], &#34;LastModifiedTime&#34;: &#34;2023-02-05 15:09:06.515000+00:00&#34;, &#34;ObjectiveStatus&#34;: &#34;Succeeded&#34; }, { &#34;CandidateName&#34;: &#34;automl-dm-1675608463sujxUg8wYQX0-003-a2d5723e&#34;, &#34;CandidateProperties&#34;: { &#34;CandidateMetrics&#34;: [ { &#34;MetricName&#34;: &#34;F1macro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;F1macro&#34;, &#34;Value&#34;: 0.39879000186920166 }, { &#34;MetricName&#34;: &#34;PrecisionMacro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;PrecisionMacro&#34;, &#34;Value&#34;: 0.39879998564720154 }, { &#34;MetricName&#34;: &#34;Accuracy&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;Accuracy&#34;, &#34;Value&#34;: 0.3990600109100342 }, { &#34;MetricName&#34;: &#34;BalancedAccuracy&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;BalancedAccuracy&#34;, &#34;Value&#34;: 0.3990600109100342 }, { &#34;MetricName&#34;: &#34;LogLoss&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;LogLoss&#34;, &#34;Value&#34;: 1.2047499418258667 }, { &#34;MetricName&#34;: &#34;RecallMacro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;RecallMacro&#34;, &#34;Value&#34;: 0.3990600109100342 } ] }, &#34;CandidateStatus&#34;: &#34;Completed&#34;, &#34;CandidateSteps&#34;: [ { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:processing-job/automl-dm-1675608463-db-1-ec0fb37f4b964d1a9485854c252aa8f0683f5&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-db-1-ec0fb37f4b964d1a9485854c252aa8f0683f5&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::ProcessingJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:training-job/automl-dm-1675608463-dpp1-1-8b1885df2d2546b0abb07a329d1fb466b29&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-dpp1-1-8b1885df2d2546b0abb07a329d1fb466b29&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TrainingJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:transform-job/automl-dm-1675608463-dpp1-csv-1-24672b27ae4440179a3b7b3070f05ec&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-dpp1-csv-1-24672b27ae4440179a3b7b3070f05ec&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TransformJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:training-job/automl-dm-1675608463sujxUg8wYQX0-003-a2d5723e&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463sujxUg8wYQX0-003-a2d5723e&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TrainingJob&#34; } ], &#34;CreationTime&#34;: &#34;2023-02-05 15:06:13+00:00&#34;, &#34;EndTime&#34;: &#34;2023-02-05 15:08:50+00:00&#34;, &#34;FinalAutoMLJobObjectiveMetric&#34;: { &#34;MetricName&#34;: &#34;validation:accuracy&#34;, &#34;Value&#34;: 0.3990600109100342 }, &#34;InferenceContainers&#34;: [ { &#34;Environment&#34;: { &#34;AUTOML_TRANSFORM_MODE&#34;: &#34;feature-transform&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;application/x-recordio-protobuf&#34;, &#34;SAGEMAKER_PROGRAM&#34;: &#34;sagemaker_serve&#34;, &#34;SAGEMAKER_SUBMIT_DIRECTORY&#34;: &#34;/opt/ml/model/code&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/data-processor-models/automl-dm-1675608463-dpp1-1-8b1885df2d2546b0abb07a329d1fb466b29/output/model.tar.gz&#34; }, { &#34;Environment&#34;: { &#34;MAX_CONTENT_LENGTH&#34;: &#34;20971520&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;text/csv&#34;, &#34;SAGEMAKER_INFERENCE_OUTPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_SUPPORTED&#34;: &#34;predicted_label,probability,probabilities&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/tuning/automl-dm--dpp1-xgb/automl-dm-1675608463sujxUg8wYQX0-003-a2d5723e/output/model.tar.gz&#34; }, { &#34;Environment&#34;: { &#34;AUTOML_TRANSFORM_MODE&#34;: &#34;inverse-label-transform&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;text/csv&#34;, &#34;SAGEMAKER_INFERENCE_INPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_OUTPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_SUPPORTED&#34;: &#34;predicted_label,probability,labels,probabilities&#34;, &#34;SAGEMAKER_PROGRAM&#34;: &#34;sagemaker_serve&#34;, &#34;SAGEMAKER_SUBMIT_DIRECTORY&#34;: &#34;/opt/ml/model/code&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/data-processor-models/automl-dm-1675608463-dpp1-1-8b1885df2d2546b0abb07a329d1fb466b29/output/model.tar.gz&#34; } ], &#34;LastModifiedTime&#34;: &#34;2023-02-05 15:09:06.513000+00:00&#34;, &#34;ObjectiveStatus&#34;: &#34;Succeeded&#34; } ] . You can print the names of the candidates with their metric values: . print(&quot;metric &quot; + str(candidates[0][&#39;FinalAutoMLJobObjectiveMetric&#39;][&#39;MetricName&#39;])) for index, candidate in enumerate(candidates): print(str(index) + &quot; &quot; + candidate[&#39;CandidateName&#39;] + &quot; &quot; + str(candidate[&#39;FinalAutoMLJobObjectiveMetric&#39;][&#39;Value&#39;])) . metric validation:accuracy 0 automl-dm-1675608463sujxUg8wYQX0-002-657fba80 0.6150500178337097 1 automl-dm-1675608463sujxUg8wYQX0-001-5d775b4b 0.6149100065231323 2 automl-dm-1675608463sujxUg8wYQX0-003-a2d5723e 0.3990600109100342 . Review best candidate . Now that we have successfully completed the Autopilot job on the dataset and visualized the trials, we can get the information about the best candidate model and review it. . We can use the best_candidate function passing the Autopilot job name. Note: This function will give an error if candidates have not been generated. . candidates = automl.list_candidates(job_name=auto_ml_job_name) if candidates != []: best_candidate = automl.best_candidate( job_name=auto_ml_job_name ) print(json.dumps(best_candidate, indent=4, sort_keys=True, default=str)) . { &#34;CandidateName&#34;: &#34;automl-dm-1675608463sujxUg8wYQX0-002-657fba80&#34;, &#34;CandidateProperties&#34;: { &#34;CandidateArtifactLocations&#34;: { &#34;Explainability&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/documentation/explainability/output&#34;, &#34;ModelInsights&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/documentation/model_monitor/output&#34; }, &#34;CandidateMetrics&#34;: [ { &#34;MetricName&#34;: &#34;F1macro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;F1macro&#34;, &#34;Value&#34;: 0.6152600049972534 }, { &#34;MetricName&#34;: &#34;PrecisionMacro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;PrecisionMacro&#34;, &#34;Value&#34;: 0.6158699989318848 }, { &#34;MetricName&#34;: &#34;Accuracy&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;Accuracy&#34;, &#34;Value&#34;: 0.6150500178337097 }, { &#34;MetricName&#34;: &#34;BalancedAccuracy&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;BalancedAccuracy&#34;, &#34;Value&#34;: 0.6150500178337097 }, { &#34;MetricName&#34;: &#34;LogLoss&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;LogLoss&#34;, &#34;Value&#34;: 0.843940019607544 }, { &#34;MetricName&#34;: &#34;RecallMacro&#34;, &#34;Set&#34;: &#34;Validation&#34;, &#34;StandardMetricName&#34;: &#34;RecallMacro&#34;, &#34;Value&#34;: 0.6150500178337097 } ] }, &#34;CandidateStatus&#34;: &#34;Completed&#34;, &#34;CandidateSteps&#34;: [ { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:processing-job/automl-dm-1675608463-db-1-ec0fb37f4b964d1a9485854c252aa8f0683f5&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-db-1-ec0fb37f4b964d1a9485854c252aa8f0683f5&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::ProcessingJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:training-job/automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TrainingJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:transform-job/automl-dm-1675608463-dpp2-rpb-1-fd31c8d697b34a02a3472ce6d7557cd&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463-dpp2-rpb-1-fd31c8d697b34a02a3472ce6d7557cd&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TransformJob&#34; }, { &#34;CandidateStepArn&#34;: &#34;arn:aws:sagemaker:us-east-1:491783890788:training-job/automl-dm-1675608463sujxUg8wYQX0-002-657fba80&#34;, &#34;CandidateStepName&#34;: &#34;automl-dm-1675608463sujxUg8wYQX0-002-657fba80&#34;, &#34;CandidateStepType&#34;: &#34;AWS::SageMaker::TrainingJob&#34; } ], &#34;CreationTime&#34;: &#34;2023-02-05 15:06:01+00:00&#34;, &#34;EndTime&#34;: &#34;2023-02-05 15:07:54+00:00&#34;, &#34;FinalAutoMLJobObjectiveMetric&#34;: { &#34;MetricName&#34;: &#34;validation:accuracy&#34;, &#34;Value&#34;: 0.6150500178337097 }, &#34;InferenceContainers&#34;: [ { &#34;Environment&#34;: { &#34;AUTOML_SPARSE_ENCODE_RECORDIO_PROTOBUF&#34;: &#34;1&#34;, &#34;AUTOML_TRANSFORM_MODE&#34;: &#34;feature-transform&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;application/x-recordio-protobuf&#34;, &#34;SAGEMAKER_PROGRAM&#34;: &#34;sagemaker_serve&#34;, &#34;SAGEMAKER_SUBMIT_DIRECTORY&#34;: &#34;/opt/ml/model/code&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/data-processor-models/automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a/output/model.tar.gz&#34; }, { &#34;Environment&#34;: { &#34;MAX_CONTENT_LENGTH&#34;: &#34;20971520&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;text/csv&#34;, &#34;SAGEMAKER_INFERENCE_OUTPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_SUPPORTED&#34;: &#34;predicted_label,probability,probabilities&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.3-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/tuning/automl-dm--dpp2-xgb/automl-dm-1675608463sujxUg8wYQX0-002-657fba80/output/model.tar.gz&#34; }, { &#34;Environment&#34;: { &#34;AUTOML_TRANSFORM_MODE&#34;: &#34;inverse-label-transform&#34;, &#34;SAGEMAKER_DEFAULT_INVOCATIONS_ACCEPT&#34;: &#34;text/csv&#34;, &#34;SAGEMAKER_INFERENCE_INPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_OUTPUT&#34;: &#34;predicted_label&#34;, &#34;SAGEMAKER_INFERENCE_SUPPORTED&#34;: &#34;predicted_label,probability,labels,probabilities&#34;, &#34;SAGEMAKER_PROGRAM&#34;: &#34;sagemaker_serve&#34;, &#34;SAGEMAKER_SUBMIT_DIRECTORY&#34;: &#34;/opt/ml/model/code&#34; }, &#34;Image&#34;: &#34;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-sklearn-automl:2.5-1-cpu-py3&#34;, &#34;ModelDataUrl&#34;: &#34;s3://sagemaker-us-east-1-491783890788/autopilot/automl-dm-1675608463/data-processor-models/automl-dm-1675608463-dpp2-1-f8c17915c5bd4efbb7862d503ce9d50304a/output/model.tar.gz&#34; } ], &#34;LastModifiedTime&#34;: &#34;2023-02-05 15:09:06.585000+00:00&#34;, &#34;ObjectiveStatus&#34;: &#34;Succeeded&#34; } . Check the existence of the candidate name for the best candidate. . while &#39;CandidateName&#39; not in best_candidate: best_candidate = automl.best_candidate(job_name=auto_ml_job_name) print(&#39;[INFO] Autopilot Job is generating BestCandidate CandidateName. Please wait. &#39;) print(json.dumps(best_candidate, indent=4, sort_keys=True, default=str)) sleep(10) print(&#39;[OK] BestCandidate CandidateName generated.&#39;) . [OK] BestCandidate CandidateName generated. . Check the existence of the metric value for the best candidate. . while &#39;FinalAutoMLJobObjectiveMetric&#39; not in best_candidate: best_candidate = automl.best_candidate(job_name=auto_ml_job_name) print(&#39;[INFO] Autopilot Job is generating BestCandidate FinalAutoMLJobObjectiveMetric. Please wait. &#39;) print(json.dumps(best_candidate, indent=4, sort_keys=True, default=str)) sleep(10) print(&#39;[OK] BestCandidate FinalAutoMLJobObjectiveMetric generated.&#39;) . [OK] BestCandidate FinalAutoMLJobObjectiveMetric generated. . Print the information about the best candidate: . best_candidate_identifier = best_candidate[&#39;CandidateName&#39;] print(&quot;Candidate name: &quot; + best_candidate_identifier) print(&quot;Metric name: &quot; + best_candidate[&#39;FinalAutoMLJobObjectiveMetric&#39;][&#39;MetricName&#39;]) print(&quot;Metric value: &quot; + str(best_candidate[&#39;FinalAutoMLJobObjectiveMetric&#39;][&#39;Value&#39;])) . Candidate name: automl-dm-1675608463sujxUg8wYQX0-002-657fba80 Metric name: validation:accuracy Metric value: 0.6150500178337097 . Review all output in S3 bucket . We can see the artifacts generated by Autopilot including the following: . data-processor-models/ # &quot;models&quot; learned to transform raw data into features documentation/ # explainability and other documentation about your model preprocessed-data/ # data for train and validation sagemaker-automl-candidates/ # candidate models which autopilot compares transformed-data/ # candidate-specific data for train and validation tuning/ # candidate-specific tuning results validations/ # validation results . Deploy and test best candidate model . Deploy best candidate model . While batch transformations are supported, we will deploy our model as a REST Endpoint in this example. . First, we need to customize the inference response. The inference containers generated by SageMaker Autopilot allow you to select the response content for predictions. By default the inference containers are configured to generate the predicted_label. But we can add probability into the list of inference response keys. . inference_response_keys = [&#39;predicted_label&#39;, &#39;probability&#39;] . Now we will create a SageMaker endpoint from the best candidate generated by Autopilot. Wait for SageMaker to deploy the endpoint. . autopilot_model = automl.deploy( initial_instance_count=1, instance_type=&#39;ml.m5.large&#39;, candidate=best_candidate, inference_response_keys=inference_response_keys, predictor_cls=sagemaker.predictor.Predictor, serializer=sagemaker.serializers.JSONSerializer(), deserializer=sagemaker.deserializers.JSONDeserializer() ) print(&#39; nEndpoint name: {}&#39;.format(autopilot_model.endpoint_name)) . -! Endpoint name: sagemaker-sklearn-automl-2023-02-05-15-18-52-694 . Test the model . Let&#39;s invoke a few predictions for the actual reviews using the deployed endpoint to test our model. . review_list = [&#39;This product is great!&#39;, &#39;OK, but not great.&#39;, &#39;This is not the right product.&#39;] for review in review_list: # remove commas from the review since we&#39;re passing the inputs as a CSV review = review.replace(&quot;,&quot;, &quot;&quot;) response = sm_runtime.invoke_endpoint( EndpointName=autopilot_model.endpoint_name, # endpoint name ContentType=&#39;text/csv&#39;, # type of input data Accept=&#39;text/csv&#39;, # type of the inference in the response Body=review # review text ) response_body=response[&#39;Body&#39;].read().decode(&#39;utf-8&#39;).strip().split(&#39;,&#39;) print(&#39;Review: &#39;, review, &#39; Predicated class: {}&#39;.format(response_body[0])) print(&quot;(-1 = Negative, 0=Neutral, 1=Positive)&quot;) . Review: This product is great! Predicated class: 1 Review: OK but not great. Predicated class: 0 Review: This is not the right product. Predicated class: -1 (-1 = Negative, 0=Neutral, 1=Positive) . So we used Amazon SageMaker Autopilot to automatically find the best model, hyper-parameters, and feature-engineering scripts for our dataset. Autopilot uses a uniquely-transparent approach to AutoML by generating re-usable Python scripts and notebooks. . Acknowledgements . I&#39;d like to express my thanks to the great Deep Learning AI Practical Data Science on AWS Specialisation Course which i completed, and acknowledge the use of some images and other materials from the training course in this article. .",
            "url": "https://www.livingdatalab.com/aws/cloud-data-science/natural-language-processing/2023/02/05/train-model-aws-sagemaker-autopilot.html",
            "relUrl": "/aws/cloud-data-science/natural-language-processing/2023/02/05/train-model-aws-sagemaker-autopilot.html",
            "date": " • Feb 5, 2023"
        }
        
    
  
    
        ,"post3": {
            "title": "Detect data bias with Amazon SageMaker Clarify",
            "content": "Introduction . In Data Science and machine learning, bias can be present in data before any model training occurs. Inspecting a dataset for bias can help detect collection gaps, inform your feature engineering, and understand biases the dataset may reflect. In this article we will analyze bias on a dataset, generate and analyze bias reports, and prepare the dataset for the model training. . Setup AWS Sagemaker . In an earlier article we introduced AWS cloud services for data science, and how it can help with different stages of the data science &amp; machine learning workflow. . . In this project, we will be using AWS Sagemaker Clarify to explore the bias in a dataset. . Let&#39;s now set up AWS sagemaker for this new project. . import boto3 import sagemaker import pandas as pd import numpy as np import botocore config = botocore.config.Config(user_agent_extra=&#39;dlai-pds/c1/w2&#39;) # low-level service client of the boto3 session sm = boto3.client(service_name=&#39;sagemaker&#39;, config=config) sess = sagemaker.Session(sagemaker_client=sm) bucket = sess.default_bucket() role = sagemaker.get_execution_role() region = sess.boto_region_name . import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format=&#39;retina&#39; . Analyze the dataset . As with our earlier article using AWS we will be using the Women&#39;s Clothing Reviews a public dataset available on kaggle. . It is shared in a public Amazon S3 bucket, and is available as a comma-separated value (CSV) text format: . Create a pandas data frame from the CSV file . Let&#39;s create a pandas dataframe from each of the product categories and concatenate them into one. . !aws s3 cp &#39;s3://dlai-practical-data-science/data/transformed/womens_clothing_ecommerce_reviews_transformed.csv&#39; ./ . download: s3://dlai-practical-data-science/data/transformed/womens_clothing_ecommerce_reviews_transformed.csv to ./womens_clothing_ecommerce_reviews_transformed.csv . path = &#39;./womens_clothing_ecommerce_reviews_transformed.csv&#39; df = pd.read_csv(path) df.head() . sentiment review_body product_category . 0 1 | If this product was in petite i would get the... | Blouses | . 1 1 | Love this dress! it&#39;s sooo pretty. i happene... | Dresses | . 2 0 | I had such high hopes for this dress and reall... | Dresses | . 3 1 | I love love love this jumpsuit. it&#39;s fun fl... | Pants | . 4 1 | This shirt is very flattering to all due to th... | Blouses | . As we saw in the earlier article, there are way more positive reviews than negative or neutral. Such a dataset is called unbalanced. . In this case, using a relatively small data subset we could visualize the occurring unbalances. At scale, we would need to perform bias analysis. Let&#39;s use this dataset as an example. . import seaborn as sns sns.countplot(data=df, x=&#39;sentiment&#39;, hue=&#39;product_category&#39;) plt.legend(loc=&#39;upper right&#39;,bbox_to_anchor=(1.3, 1.1)) . &lt;matplotlib.legend.Legend at 0x7fc9f6a32090&gt; . Upload the dataset to S3 bucket . Now we will upload the dataset to a private S3 bucket in a folder called bias/unbalanced. . data_s3_uri_unbalanced = sess.upload_data(bucket=bucket, key_prefix=&#39;bias/unbalanced&#39;, path=&#39;./womens_clothing_ecommerce_reviews_transformed.csv&#39;) data_s3_uri_unbalanced . &#39;s3://sagemaker-us-east-1-763519884484/bias/unbalanced/womens_clothing_ecommerce_reviews_transformed.csv&#39; . Analyze class imbalance on the dataset with Amazon SageMaker Clarify . Let&#39;s analyze bias in sentiment with respect to the product_category facet on the dataset. . Configure a DataConfig . Information about the input data needs to be provided to the processor. This can be done with the DataConfig of the Clarify container. It stores information about the dataset to be analyzed, for example the dataset file, its format, headers and labels. . We can use DataConfig to configure the target column (&#39;sentiment&#39; label), data input (data_s3_uri_unbalanced) and output paths (bias_report_unbalanced_output_path) with their formats (header names and the dataset type): . from sagemaker import clarify bias_report_unbalanced_output_path = &#39;s3://{}/bias/generated_bias_report/unbalanced&#39;.format(bucket) data_config_unbalanced = clarify.DataConfig( s3_data_input_path=data_s3_uri_unbalanced, s3_output_path=bias_report_unbalanced_output_path, label=&#39;sentiment&#39;, headers=df.columns.to_list(), dataset_type=&#39;text/csv&#39; ) . Configure BiasConfig . Bias is measured by calculating a metric and comparing it across groups. To compute it, we will specify the required information in the BiasConfig API. SageMaker Clarify needs the sensitive columns (facet_name) and the desirable outcomes (label_values_or_threshold). Here product_category is the sensitive facet and the desired outcome is with the sentiment==1. . SageMaker Clarify can handle both categorical and continuous data for label_values_or_threshold. In this case we are using categorical data. . bias_config_unbalanced = clarify.BiasConfig( label_values_or_threshold=[1], # desired sentiment facet_name=&#39;product_category&#39; # sensitive column (facet) ) . Configure Amazon SageMaker Clarify as a processing job . Now we need to construct an object called SageMakerClarifyProcessor. This allows you to scale the process of data bias detection using two parameters, instance_count and instance_type. . Instance_count represents how many nodes you want in the distributor cluster during the data detection. Instance_type specifies the processing capability (compute capacity, memory capacity) available for each one of those nodes. . clarify_processor_unbalanced = clarify.SageMakerClarifyProcessor(role=role, instance_count=1, instance_type=&#39;ml.m5.large&#39;, sagemaker_session=sess) . Run the Amazon SageMaker Clarify processing job . Let&#39;s run the configured processing job to compute the requested bias methods of the input data. . We will apply the run_pre_training_bias method to the configured Clarify processor, passing the configured input/output data (data_config_unbalanced), configuration of sensitive groups (bias_config_unbalanced) with the other job setup parameters. . clarify_processor_unbalanced.run_pre_training_bias( data_config=data_config_unbalanced, data_bias_config=bias_config_unbalanced, methods=[&quot;CI&quot;, &quot;DPL&quot;, &quot;KL&quot;, &quot;JS&quot;, &quot;LP&quot;, &quot;TVD&quot;, &quot;KS&quot;], wait=False, logs=False ) . Job Name: Clarify-Pretraining-Bias-2023-02-04-18-19-13-642 Inputs: [{&#39;InputName&#39;: &#39;dataset&#39;, &#39;AppManaged&#39;: False, &#39;S3Input&#39;: {&#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-763519884484/bias/unbalanced/womens_clothing_ecommerce_reviews_transformed.csv&#39;, &#39;LocalPath&#39;: &#39;/opt/ml/processing/input/data&#39;, &#39;S3DataType&#39;: &#39;S3Prefix&#39;, &#39;S3InputMode&#39;: &#39;File&#39;, &#39;S3DataDistributionType&#39;: &#39;FullyReplicated&#39;, &#39;S3CompressionType&#39;: &#39;None&#39;}}, {&#39;InputName&#39;: &#39;analysis_config&#39;, &#39;AppManaged&#39;: False, &#39;S3Input&#39;: {&#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-763519884484/bias/generated_bias_report/unbalanced/analysis_config.json&#39;, &#39;LocalPath&#39;: &#39;/opt/ml/processing/input/config&#39;, &#39;S3DataType&#39;: &#39;S3Prefix&#39;, &#39;S3InputMode&#39;: &#39;File&#39;, &#39;S3DataDistributionType&#39;: &#39;FullyReplicated&#39;, &#39;S3CompressionType&#39;: &#39;None&#39;}}] Outputs: [{&#39;OutputName&#39;: &#39;analysis_result&#39;, &#39;AppManaged&#39;: False, &#39;S3Output&#39;: {&#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-763519884484/bias/generated_bias_report/unbalanced&#39;, &#39;LocalPath&#39;: &#39;/opt/ml/processing/output&#39;, &#39;S3UploadMode&#39;: &#39;EndOfJob&#39;}}] . run_unbalanced_bias_processing_job_name = clarify_processor_unbalanced.latest_job.job_name print(run_unbalanced_bias_processing_job_name) . Clarify-Pretraining-Bias-2023-02-04-18-19-13-642 . Run the Amazon SageMaker Clarify processing job on the unbalanced dataset . running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=run_unbalanced_bias_processing_job_name, sagemaker_session=sess) . %%time running_processor.wait(logs=False) . .............................................................................!CPU times: user 304 ms, sys: 55.6 ms, total: 360 ms Wall time: 6min 30s . Analyze unbalanced bias report . In this run, we analyzed bias for sentiment relative to the product_category for the unbalanced data. Let&#39;s have a look at the bias report. . List the files in the output path bias_report_unbalanced_output_path: . !aws s3 ls $bias_report_unbalanced_output_path/ . 2023-02-04 18:25:39 31732 analysis.json 2023-02-04 18:19:14 346 analysis_config.json 2023-02-04 18:25:39 607108 report.html 2023-02-04 18:25:39 346473 report.ipynb 2023-02-04 18:25:39 326001 report.pdf . Download generated bias report from S3 bucket: . !aws s3 cp --recursive $bias_report_unbalanced_output_path ./generated_bias_report/unbalanced/ . download: s3://sagemaker-us-east-1-763519884484/bias/generated_bias_report/unbalanced/analysis_config.json to generated_bias_report/unbalanced/analysis_config.json download: s3://sagemaker-us-east-1-763519884484/bias/generated_bias_report/unbalanced/analysis.json to generated_bias_report/unbalanced/analysis.json download: s3://sagemaker-us-east-1-763519884484/bias/generated_bias_report/unbalanced/report.pdf to generated_bias_report/unbalanced/report.pdf download: s3://sagemaker-us-east-1-763519884484/bias/generated_bias_report/unbalanced/report.ipynb to generated_bias_report/unbalanced/report.ipynb download: s3://sagemaker-us-east-1-763519884484/bias/generated_bias_report/unbalanced/report.html to generated_bias_report/unbalanced/report.html . You can view the bias report here. . The bias report shows a number of metrics, but here we will focus on just two of them: . Class Imbalance (CI). Measures the imbalance in the number of members between different facet values. Answers the question, does a product_category have disproportionately more reviews than others? Values of CI will become equal for even distribution between facets. Here, different CI values show the existence of imbalance. | Difference in Positive Proportions in Labels (DPL). Measures the imbalance of positive outcomes between different facet values. Answers the question, does a product_category have disproportionately higher ratings than others? With the range over the interval from -1 to 1, if there is no bias, you want to see this value as close as possible to zero. Here, non-zero values indicate the imbalances. | . Balance the dataset by product_category and sentiment . Let&#39;s balance the dataset by product_category and sentiment. Then we can configure and run SageMaker Clarify processing job to analyze the bias of it. Which metrics values do we expect to see in the bias report? . df_grouped_by = df.groupby([&#39;product_category&#39;, &#39;sentiment&#39;]) df_balanced = df_grouped_by.apply(lambda x: x.sample(df_grouped_by.size().min()).reset_index(drop=True)) . df_balanced . sentiment review_body product_category . product_category sentiment . Blouses -1 0 -1 | I bought this top in the store which was good... | Blouses | . 1 -1 | Wow this is huge! i&#39;m all for the tent-look wi... | Blouses | . 2 -1 | If you have anything larger than an a cup thi... | Blouses | . 3 -1 | Like another reviewer mentioned this shirt is... | Blouses | . 4 -1 | I did not like this top at all-but had i looke... | Blouses | . ... ... ... ... | ... | ... | . Trend 1 4 1 | Never spent this much on a dress so it needs t... | Trend | . 5 1 | I love this sweatshirt! i truly did not pay mu... | Trend | . 6 1 | I am waist-challenged. i like a narrowly cut s... | Trend | . 7 1 | I love the style and look oft this blouse but ... | Trend | . 8 1 | I love this top it is a cool style mix betwee... | Trend | . 486 rows × 3 columns . Let&#39;s now visualize the distribution of review sentiment in the balanced dataset. . import seaborn as sns sns.countplot(data=df_balanced, x=&#39;sentiment&#39;, hue=&#39;product_category&#39;) plt.legend(loc=&#39;upper right&#39;,bbox_to_anchor=(1.3, 1.1)) . &lt;matplotlib.legend.Legend at 0x7fc9f52ca4d0&gt; . Analyze bias on balanced dataset with AWS SageMaker Clarify . Let&#39;s now analyze bias in sentiment with respect to the product_category facet on the balanced dataset. . We need to save and upload the balanced data to the S3 bucket. . path_balanced = &#39;./womens_clothing_ecommerce_reviews_balanced.csv&#39; df_balanced.to_csv(path_balanced, index=False, header=True) data_s3_uri_balanced = sess.upload_data(bucket=bucket, key_prefix=&#39;bias/balanced&#39;, path=path_balanced) data_s3_uri_balanced . &#39;s3://sagemaker-us-east-1-763519884484/bias/balanced/womens_clothing_ecommerce_reviews_balanced.csv&#39; . Configure a DataConfig . We need to configure a DataConfig for Clarify to analyze bias on the balanced dataset. To do this we pass the S3 object path containing the balanced dataset, the path to store the output (bias_report_balanced_output_path) and the target column. . from sagemaker import clarify bias_report_balanced_output_path = &#39;s3://{}/bias/generated_bias_report/balanced&#39;.format(bucket) data_config_balanced = clarify.DataConfig( s3_data_input_path=data_s3_uri_balanced, s3_output_path=bias_report_balanced_output_path, label=&#39;sentiment&#39;, headers=df_balanced.columns.to_list(), dataset_type=&#39;text/csv&#39; ) . Configure BiasConfig . BiasConfig for the balanced dataset will have the same settings as before. . bias_config_balanced = clarify.BiasConfig( label_values_or_threshold=[1], # desired sentiment facet_name=&#39;product_category&#39; # sensitive column (facet) ) . Configure SageMaker Clarify as a processing job . SageMakerClarifyProcessor object will also have the same parameters. . clarify_processor_balanced = clarify.SageMakerClarifyProcessor(role=role, instance_count=1, instance_type=&#39;ml.m5.large&#39;, sagemaker_session=sess) . Run the Amazon SageMaker Clarify processing job . Let&#39;s run the configured processing job for the balanced dataset. . We will apply the run_pre_training_bias method to the configured Clarify processor, passing the input/output data, configuration of sensitive groups with the other job setup parameters. . clarify_processor_balanced.run_pre_training_bias( data_config=data_config_balanced, data_bias_config=bias_config_balanced, methods=[&quot;CI&quot;, &quot;DPL&quot;, &quot;KL&quot;, &quot;JS&quot;, &quot;LP&quot;, &quot;TVD&quot;, &quot;KS&quot;], wait=False, logs=False ) . Job Name: Clarify-Pretraining-Bias-2023-02-04-18-25-47-825 Inputs: [{&#39;InputName&#39;: &#39;dataset&#39;, &#39;AppManaged&#39;: False, &#39;S3Input&#39;: {&#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-763519884484/bias/balanced/womens_clothing_ecommerce_reviews_balanced.csv&#39;, &#39;LocalPath&#39;: &#39;/opt/ml/processing/input/data&#39;, &#39;S3DataType&#39;: &#39;S3Prefix&#39;, &#39;S3InputMode&#39;: &#39;File&#39;, &#39;S3DataDistributionType&#39;: &#39;FullyReplicated&#39;, &#39;S3CompressionType&#39;: &#39;None&#39;}}, {&#39;InputName&#39;: &#39;analysis_config&#39;, &#39;AppManaged&#39;: False, &#39;S3Input&#39;: {&#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-763519884484/bias/generated_bias_report/balanced/analysis_config.json&#39;, &#39;LocalPath&#39;: &#39;/opt/ml/processing/input/config&#39;, &#39;S3DataType&#39;: &#39;S3Prefix&#39;, &#39;S3InputMode&#39;: &#39;File&#39;, &#39;S3DataDistributionType&#39;: &#39;FullyReplicated&#39;, &#39;S3CompressionType&#39;: &#39;None&#39;}}] Outputs: [{&#39;OutputName&#39;: &#39;analysis_result&#39;, &#39;AppManaged&#39;: False, &#39;S3Output&#39;: {&#39;S3Uri&#39;: &#39;s3://sagemaker-us-east-1-763519884484/bias/generated_bias_report/balanced&#39;, &#39;LocalPath&#39;: &#39;/opt/ml/processing/output&#39;, &#39;S3UploadMode&#39;: &#39;EndOfJob&#39;}}] . run_balanced_bias_processing_job_name = clarify_processor_balanced.latest_job.job_name print(run_balanced_bias_processing_job_name) . Clarify-Pretraining-Bias-2023-02-04-18-25-47-825 . running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=run_balanced_bias_processing_job_name, sagemaker_session=sess) . %%time running_processor.wait(logs=False) . ...........................................................................!CPU times: user 312 ms, sys: 46.6 ms, total: 359 ms Wall time: 6min 20s . Analyze balanced bias report . Let&#39;s see again the files created by the report. . !aws s3 ls $bias_report_balanced_output_path/ . 2023-02-04 18:32:02 29889 analysis.json 2023-02-04 18:25:48 346 analysis_config.json 2023-02-04 18:32:02 592454 report.html 2023-02-04 18:32:02 331819 report.ipynb 2023-02-04 18:32:02 320692 report.pdf . We can view the report here. . In this run, we analyzed bias for sentiment relative to the product_category for the balanced data. Note that the Class Imbalance (CI) metric is equal across all product categories for the target label, sentiment. And Difference in Positive Proportions in Labels (DPL) metric values are zero. . Acknowledgements . I&#39;d like to express my thanks to the great Deep Learning AI Practical Data Science on AWS Specialisation Course which i completed, and acknowledge the use of some images and other materials from the training course in this article. .",
            "url": "https://www.livingdatalab.com/aws/cloud-data-science/natural-language-processing/2023/02/04/detecting-bias-with-aws-sagemaker-clarify.html",
            "relUrl": "/aws/cloud-data-science/natural-language-processing/2023/02/04/detecting-bias-with-aws-sagemaker-clarify.html",
            "date": " • Feb 4, 2023"
        }
        
    
  
    
        ,"post4": {
            "title": "Loading & Transforming Clothing Reviews Text Data with AWS",
            "content": "Introduction . In this project we will ingest and transform a customer product reviews dataset using AWS (Amazon Web Services) cloud services. We will then use AWS data stack services such as AWS Glue and Amazon Athena for ingesting and querying the dataset. Finally we will use AWS Data Wrangler to analyze the dataset and plot some visuals extracting insights. . This exploration could be useful for a range of tasks, including creating a sentiment analysis text classification model - which is something we will explore in future articles. . AWS &amp; Cloud Services for Data Science . Why use the Cloud for Data Science ? . Data Science can be performed in a range of devices and environments, from local machines and laptops, to dedicated server centers, to cloud services such as AWS or Azure Databricks. . Why would you want to use cloud services for Data Science? . Local machines or server centers have limited resources as they are specific machines and the only ones you have, which have limited computing power, disk space and memory which could make certain tasks and problems unfeasible to solve | Cloud services allow the storage of any amount of data | Cloud services allow you to scale up e.g. increase the processing or memory capacity of the machines you use in minutes | Cloud services allow you to scale out e.g. increase the number of machines you are able to use for a task | Cloud services provide a large range of data science tools already installed and maintained | Cloud services provide a flexible deployment platform for any products you develop, with a capacity able to scale with demand | Cloud services provide a more cost efficient and flexible solution for many tasks, you generally only pay for what you use and can increase or decrease options and capacity easily either by choice or even automatically based on need or demand | . . Using AWS for Cloud Data Science . AWS offers a range of different services that can help at different stages of the machine learning development cycle. . . In this article we will be demonstrating how we can use AWS for the Ingesting and Analysing stage, so we will be using the following services: . Amazon S3: A simple storage service | AWS Glue: An ETL service that helps prepare, extract and load data | Amazon Athena: An interactive query service that uses SQL | Amazon Sagemaker: A cloud machine learning service | . Ingest and transform Dataset . The dataset we will use is the Women&#39;s Clothing Reviews a public dataset available on kaggle. . It is shared in a public Amazon S3 bucket, and is available as a comma-separated value (CSV) text format: . s3://dlai-practical-data-science/data/raw/womens_clothing_ecommerce_reviews.csv . . List the dataset files in the public S3 bucket . The AWS Command Line Interface (CLI) is a unified tool to manage AWS services. With just one tool, you can control multiple AWS services from the command line and automate them through scripts. We will use it to list the dataset files. . aws s3 ls [bucket_name] is a function lists all objects in the S3 bucket. Let&#39;s use it to view the reviews data files in CSV format. . !aws s3 ls s3://dlai-practical-data-science/data/raw/womens_clothing_ecommerce_reviews.csv . 2021-04-30 02:21:06 8457214 womens_clothing_ecommerce_reviews.csv . Copy the data locally to the notebook . aws s3 cp [bucket_name/file_name] [file_name] is a function that copies the file from the S3 bucket into the local environment or into another S3 bucket. Let&#39;s use it to copy the file with the dataset locally. . !aws s3 cp s3://dlai-practical-data-science/data/raw/womens_clothing_ecommerce_reviews.csv ./womens_clothing_ecommerce_reviews.csv . download: s3://dlai-practical-data-science/data/raw/womens_clothing_ecommerce_reviews.csv to ./womens_clothing_ecommerce_reviews.csv . Now we will use Pandas to load and preview the data. . import pandas as pd import csv df = pd.read_csv(&#39;./womens_clothing_ecommerce_reviews.csv&#39;, index_col=0) df.shape . (23486, 10) . df . Clothing ID Age Title Review Text Rating Recommended IND Positive Feedback Count Division Name Department Name Class Name . 0 847 | 33 | Cute, crisp shirt | If this product was in petite i would get the... | 4 | 1 | 2 | General | Tops | Blouses | . 1 1080 | 34 | NaN | Love this dress! it&#39;s sooo pretty. i happene... | 5 | 1 | 4 | General | Dresses | Dresses | . 2 1077 | 60 | Some major design flaws | I had such high hopes for this dress and reall... | 3 | 0 | 0 | General | Dresses | Dresses | . 3 1049 | 50 | My favorite buy! | I love love love this jumpsuit. it&#39;s fun fl... | 5 | 1 | 0 | General Petite | Bottoms | Pants | . 4 847 | 47 | Flattering shirt | This shirt is very flattering to all due to th... | 5 | 1 | 6 | General | Tops | Blouses | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 23481 1104 | 34 | Great dress for many occasions | I was very happy to snag this dress at such a ... | 5 | 1 | 0 | General Petite | Dresses | Dresses | . 23482 862 | 48 | Wish it was made of cotton | It reminds me of maternity clothes. soft stre... | 3 | 1 | 0 | General Petite | Tops | Knits | . 23483 1104 | 31 | Cute, but see through | This fit well but the top was very see throug... | 3 | 0 | 1 | General Petite | Dresses | Dresses | . 23484 1084 | 28 | Very cute dress, perfect for summer parties an... | I bought this dress for a wedding i have this ... | 3 | 1 | 2 | General | Dresses | Dresses | . 23485 1104 | 52 | Please make more like this one! | This dress in a lovely platinum is feminine an... | 5 | 1 | 22 | General Petite | Dresses | Dresses | . 23486 rows × 10 columns . Transform the data . To simplify the task, we will transform the data into a comma-separated value (CSV) file that contains only a review_body, product_category, and sentiment derived from the original data. . df_transformed = df.rename(columns={&#39;Review Text&#39;: &#39;review_body&#39;, &#39;Rating&#39;: &#39;star_rating&#39;, &#39;Class Name&#39;: &#39;product_category&#39;}) df_transformed.drop(columns=[&#39;Clothing ID&#39;, &#39;Age&#39;, &#39;Title&#39;, &#39;Recommended IND&#39;, &#39;Positive Feedback Count&#39;, &#39;Division Name&#39;, &#39;Department Name&#39;], inplace=True) df_transformed.dropna(inplace=True) df_transformed.shape . (22628, 3) . Now lets convert the star_rating into the sentiment (positive, neutral, negative), which later on we could use for a text classification model. . def to_sentiment(star_rating): if star_rating in {1, 2}: # negative return -1 if star_rating == 3: # neutral return 0 if star_rating in {4, 5}: # positive return 1 # transform star_rating into the sentiment df_transformed[&#39;sentiment&#39;] = df_transformed[&#39;star_rating&#39;].apply(lambda star_rating: to_sentiment(star_rating=star_rating) ) # drop the star rating column df_transformed.drop(columns=[&#39;star_rating&#39;], inplace=True) # remove reviews for product_categories with &lt; 10 reviews df_transformed = df_transformed.groupby(&#39;product_category&#39;).filter(lambda reviews : len(reviews) &gt; 10)[[&#39;sentiment&#39;, &#39;review_body&#39;, &#39;product_category&#39;]] df_transformed.shape . (22626, 3) . df_transformed . sentiment review_body product_category . 0 1 | If this product was in petite i would get the... | Blouses | . 1 1 | Love this dress! it&#39;s sooo pretty. i happene... | Dresses | . 2 0 | I had such high hopes for this dress and reall... | Dresses | . 3 1 | I love love love this jumpsuit. it&#39;s fun fl... | Pants | . 4 1 | This shirt is very flattering to all due to th... | Blouses | . ... ... | ... | ... | . 23481 1 | I was very happy to snag this dress at such a ... | Dresses | . 23482 0 | It reminds me of maternity clothes. soft stre... | Knits | . 23483 0 | This fit well but the top was very see throug... | Dresses | . 23484 0 | I bought this dress for a wedding i have this ... | Dresses | . 23485 1 | This dress in a lovely platinum is feminine an... | Dresses | . 22626 rows × 3 columns . Write the data to a CSV file . df_transformed.to_csv(&#39;./womens_clothing_ecommerce_reviews_transformed.csv&#39;, index=False) . !head -n 5 ./womens_clothing_ecommerce_reviews_transformed.csv . sentiment,review_body,product_category 1,If this product was in petite i would get the petite. the regular is a little long on me but a tailor can do a simple fix on that. fits nicely! i&#39;m 5&#39;4 130lb and pregnant so i bough t medium to grow into. the tie can be front or back so provides for some nice flexibility on form fitting.,Blouses 1,&#34;Love this dress! it&#39;s sooo pretty. i happened to find it in a store and i&#39;m glad i did bc i never would have ordered it online bc it&#39;s petite. i bought a petite and am 5&#39;8&#34;&#34;. i love the length on me- hits just a little below the knee. would definitely be a true midi on someone who is truly petite.&#34;,Dresses 0,I had such high hopes for this dress and really wanted it to work for me. i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up! i reordered it in petite medium which was just ok. overall the top half was comfortable and fit nicely but the bottom half had a very tight under layer and several somewhat cheap (net) over layers. imo a major design flaw was the net over layer sewn directly into the zipper - it c,Dresses 1,I love love love this jumpsuit. it&#39;s fun flirty and fabulous! every time i wear it i get nothing but great compliments!,Pants . Register the public dataset for querying and visualizing . We will now register the public dataset into an S3-backed database table so we can query and visualize our dataset at scale. . Register S3 dataset files as a table for querying . Before we can use Amazon Athena to query our data, we first need to get our data &#39;registered&#39; so we can do this. . Let&#39;s import some key modules. . boto3 is the AWS SDK for Python to create, configure, and manage AWS services, such as Amazon Elastic Compute Cloud (Amazon EC2) and Amazon Simple Storage Service (Amazon S3). The SDK provides an object-oriented API as well as low-level access to AWS services. . sagemaker is the SageMaker Python SDK which provides several high-level abstractions for working with the Amazon SageMaker. . import boto3 import sagemaker import pandas as pd import numpy as np import botocore config = botocore.config.Config(user_agent_extra=&#39;dlai-pds/c1/w1&#39;) # low-level service client of the boto3 session sm = boto3.client(service_name=&#39;sagemaker&#39;, config=config) sess = sagemaker.Session(sagemaker_client=sm) bucket = sess.default_bucket() role = sagemaker.get_execution_role() region = sess.boto_region_name account_id = sess.account_id print(&#39;S3 Bucket: {}&#39;.format(bucket)) print(&#39;Region: {}&#39;.format(region)) print(&#39;Account ID: {}&#39;.format(account_id)) . S3 Bucket: sagemaker-us-east-1-634231958143 Region: us-east-1 Account ID: &lt;bound method Session.account_id of &lt;sagemaker.session.Session object at 0x7f987cf24490&gt;&gt; . Lets now copy the file into the S3 bucket. . !aws s3 cp ./womens_clothing_ecommerce_reviews_transformed.csv s3://$bucket/data/transformed/womens_clothing_ecommerce_reviews_transformed.csv . upload: ./womens_clothing_ecommerce_reviews_transformed.csv to s3://sagemaker-us-east-1-634231958143/data/transformed/womens_clothing_ecommerce_reviews_transformed.csv . Import AWS Data Wrangler . AWS Data Wrangler is an AWS Professional Service open source python initiative part of Amazon Sagemaker - that extends the power of Pandas library to AWS connecting dataframes and AWS data related services (Amazon Redshift, AWS Glue, Amazon Athena, Amazon EMR, Amazon QuickSight, etc). . . Built on top of other open-source projects like Pandas, Apache Arrow, Boto3, SQLAlchemy, Psycopg2 and PyMySQL, it offers abstracted functions to execute usual ETL tasks like load/unload data from data lakes, data warehouses and databases. . import awswrangler as wr . Create AWS Glue Catalog database . The data catalog features of AWS Glue and the inbuilt integration to Amazon S3 simplify the process of identifying data and deriving the schema definition out of the discovered data. Using AWS Glue crawlers within our data catalog, we can traverse the data stored in Amazon S3 and build out the metadata tables that are defined in our data catalog. . . Here we will use the wr.catalog.create_database function to create a database with the name dsoaws_deep_learning (&quot;dsoaws&quot; stands for &quot;Data Science on AWS&quot;). . wr.catalog.create_database( name=&#39;dsoaws_deep_learning&#39;, exist_ok=True ) . dbs = wr.catalog.get_databases() for db in dbs: print(&quot;Database name: &quot; + db[&#39;Name&#39;]) . Database name: dsoaws_deep_learning . Register CSV data with AWS Glue Catalog . wr.catalog.create_csv_table( database=&#39;dsoaws_deep_learning&#39;, path=&#39;s3://{}/data/transformed/&#39;.format(bucket), table=&quot;reviews&quot;, columns_types={ &#39;sentiment&#39;: &#39;int&#39;, &#39;review_body&#39;: &#39;string&#39;, &#39;product_category&#39;: &#39;string&#39; }, mode=&#39;overwrite&#39;, skip_header_line_count=1, sep=&#39;,&#39; ) . Review the table shape: . table = wr.catalog.table(database=&#39;dsoaws_deep_learning&#39;, table=&#39;reviews&#39;) table . Column Name Type Partition Comment . 0 sentiment | int | False | | . 1 review_body | string | False | | . 2 product_category | string | False | | . Create default S3 bucket for Amazon Athena . We can use Amazon Athena to query our results. Amazon Athena requires this S3 bucket to store temporary query results and improve performance of subsequent queries. . The contents of this bucket are mostly binary and human-unreadable. . wr.athena.create_athena_bucket() . &#39;s3://aws-athena-query-results-634231958143-us-east-1/&#39; . Visualising the Data and Answering Questions . Let&#39;s review the columns we have selected from our reviews dataset. . sentiment: The review&#39;s sentiment (-1, 0, 1). | product_category: Broad product category that can be used to group reviews (in this case digital videos). | review_body: The text of the review. | . Preparation for data visualization . Imports . import numpy as np import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline %config InlineBackend.figure_format=&#39;retina&#39; . Settings . We need to set an AWS Glue database and a table name. . database_name = &#39;dsoaws_deep_learning&#39; table_name = &#39;reviews&#39; . Let&#39;s also define some seaborn config for our visualisations. . sns.set_style = &#39;seaborn-whitegrid&#39; sns.set(rc={&quot;font.style&quot;:&quot;normal&quot;, &quot;axes.facecolor&quot;:&quot;white&quot;, &#39;grid.color&#39;: &#39;.8&#39;, &#39;grid.linestyle&#39;: &#39;-&#39;, &quot;figure.facecolor&quot;:&quot;white&quot;, &quot;figure.titlesize&quot;:20, &quot;text.color&quot;:&quot;black&quot;, &quot;xtick.color&quot;:&quot;black&quot;, &quot;ytick.color&quot;:&quot;black&quot;, &quot;axes.labelcolor&quot;:&quot;black&quot;, &quot;axes.grid&quot;:True, &#39;axes.labelsize&#39;:10, &#39;xtick.labelsize&#39;:10, &#39;font.size&#39;:10, &#39;ytick.labelsize&#39;:10}) . Plotting key stats using bar charts . Amazon Athena lets you query data in Amazon S3 using a standard SQL interface. It reflects the databases and tables in the AWS Glue Catalog. . . You can create interactive queries and perform any data manipulations required for further downstream processing. . . A Standard SQL query can be saved as a string and then passed as a parameter into the Athena query. For example to count the total number of reviews by sentiment, the SQL query here will take the following form: . SELECT column_name, COUNT(column_name) as new_column_name FROM table_name GROUP BY column_name ORDER BY column_name . How many reviews per sentiment? . statement_count_by_sentiment = &quot;&quot;&quot; SELECT sentiment, COUNT(sentiment) AS count_sentiment FROM reviews GROUP BY sentiment ORDER BY sentiment &quot;&quot;&quot; print(statement_count_by_sentiment) . SELECT sentiment, COUNT(sentiment) AS count_sentiment FROM reviews GROUP BY sentiment ORDER BY sentiment . Query data in Amazon Athena database cluster using the prepared SQL statement: . df_count_by_sentiment = wr.athena.read_sql_query( sql=statement_count_by_sentiment, database=database_name ) print(df_count_by_sentiment) . sentiment count_sentiment 0 -1 2370 1 0 2823 2 1 17433 . Preview the results of the query: . df_count_by_sentiment.plot(kind=&#39;bar&#39;, x=&#39;sentiment&#39;, y=&#39;count_sentiment&#39;, rot=0) . &lt;AxesSubplot:xlabel=&#39;sentiment&#39;&gt; . So we can see the positive sentiment (1) category has by far the most reviews. . Calculate total number of reviews per product category . Using an Amazon Athena query with the standard SQL statement passed as a parameter, we can calculate the total number of reviews per product_category in the table reviews. . We can create an SQL statement of the form . SELECT category_column, COUNT(column_name) AS new_column_name FROM table_name GROUP BY category_column ORDER BY new_column_name DESC . as a triple quote string into the variable statement_count_by_category. We will also use the column sentiment in the COUNT function and give it a new name count_sentiment. . statement_count_by_category = &quot;&quot;&quot; SELECT product_category, COUNT(sentiment) AS count_sentiment FROM reviews GROUP BY product_category ORDER BY count_sentiment DESC &quot;&quot;&quot; print(statement_count_by_category) . SELECT product_category, COUNT(sentiment) AS count_sentiment FROM reviews GROUP BY product_category ORDER BY count_sentiment DESC . Let&#39;s query data in Amazon Athena database passing the prepared SQL statement: . %%time df_count_by_category = wr.athena.read_sql_query( sql=statement_count_by_category, database=database_name ) df_count_by_category . CPU times: user 320 ms, sys: 24.5 ms, total: 345 ms Wall time: 3.27 s . product_category count_sentiment . 0 Dresses | 6145 | . 1 Knits | 4626 | . 2 Blouses | 2983 | . 3 Sweaters | 1380 | . 4 Pants | 1350 | . 5 Jeans | 1104 | . 6 Fine gauge | 1059 | . 7 Skirts | 903 | . 8 Jackets | 683 | . 9 Lounge | 669 | . 10 Swim | 332 | . 11 Outerwear | 319 | . 12 Shorts | 304 | . 13 Sleep | 214 | . 14 Legwear | 158 | . 15 Intimates | 147 | . 16 Layering | 132 | . 17 Trend | 118 | . Which product categories are highest rated by average sentiment? . We will set the SQL statement to find the average sentiment per product category, showing the results in the descending order. . statement_avg_by_category = &quot;&quot;&quot; SELECT product_category, AVG(sentiment) AS avg_sentiment FROM {} GROUP BY product_category ORDER BY avg_sentiment DESC &quot;&quot;&quot;.format(table_name) print(statement_avg_by_category) . SELECT product_category, AVG(sentiment) AS avg_sentiment FROM reviews GROUP BY product_category ORDER BY avg_sentiment DESC . Lets query data in Amazon Athena database passing the prepared SQL statement: . %%time df_avg_by_category = wr.athena.read_sql_query( sql=statement_avg_by_category, database=database_name ) . CPU times: user 462 ms, sys: 16.5 ms, total: 478 ms Wall time: 3.74 s . Let&#39;s now preview the query results in the temporary S3 bucket: s3://aws-athena-query-results-ACCOUNT-REGION/ . df_avg_by_category . product_category avg_sentiment . 0 Layering | 0.780303 | . 1 Jeans | 0.746377 | . 2 Lounge | 0.745889 | . 3 Sleep | 0.710280 | . 4 Shorts | 0.707237 | . 5 Pants | 0.705185 | . 6 Intimates | 0.700680 | . 7 Jackets | 0.699854 | . 8 Skirts | 0.696567 | . 9 Legwear | 0.696203 | . 10 Fine gauge | 0.692162 | . 11 Outerwear | 0.683386 | . 12 Knits | 0.653913 | . 13 Swim | 0.644578 | . 14 Dresses | 0.643287 | . 15 Sweaters | 0.641304 | . 16 Blouses | 0.641301 | . 17 Trend | 0.483051 | . Visualization . def show_values_barplot(axs, space): def _show_on_plot(ax): for p in ax.patches: _x = p.get_x() + p.get_width() + float(space) _y = p.get_y() + p.get_height() value = round(float(p.get_width()),2) ax.text(_x, _y, value, ha=&quot;left&quot;) if isinstance(axs, np.ndarray): for idx, ax in np.ndenumerate(axs): _show_on_plot(ax) else: _show_on_plot(axs) . barplot = sns.barplot( data = df_avg_by_category, y=&#39;product_category&#39;, x=&#39;avg_sentiment&#39;, color=&quot;b&quot;, saturation=1 ) # Set the size of the figure sns.set(rc={&#39;figure.figsize&#39;:(15.0, 10.0)}) # Set title and x-axis ticks plt.title(&#39;Average sentiment by product category&#39;) #plt.xticks([-1, 0, 1], [&#39;Negative&#39;, &#39;Neutral&#39;, &#39;Positive&#39;]) # Helper code to show actual values afters bars show_values_barplot(barplot, 0.1) plt.xlabel(&quot;Average sentiment&quot;) plt.ylabel(&quot;Product category&quot;) plt.tight_layout() # Show graphic plt.show(barplot) . Which product categories have the most reviews? . Let&#39;s create an SQL statement to find the count of sentiment per product category, showing the results in the descending order. . statement_count_by_category_desc = &quot;&quot;&quot; SELECT product_category, COUNT(*) AS count_reviews FROM {} GROUP BY product_category ORDER BY count_reviews DESC &quot;&quot;&quot;.format(table_name) print(statement_count_by_category_desc) . SELECT product_category, COUNT(*) AS count_reviews FROM reviews GROUP BY product_category ORDER BY count_reviews DESC . Now lets query data in Amazon Athena database passing the prepared SQL statement. . %%time df_count_by_category_desc = wr.athena.read_sql_query( sql=statement_count_by_category_desc, database=database_name ) . CPU times: user 360 ms, sys: 22.6 ms, total: 382 ms Wall time: 4.38 s . Let&#39;s store maximum number of sentiment for the visualization plot. . max_sentiment = df_count_by_category_desc[&#39;count_reviews&#39;].max() print(&#39;Highest number of reviews (in a single category): {}&#39;.format(max_sentiment)) . Highest number of reviews (in a single category): 6145 . Let&#39;s now plot this as a bar chart. . barplot = sns.barplot( data=df_count_by_category_desc, y=&#39;product_category&#39;, x=&#39;count_reviews&#39;, color=&quot;b&quot;, saturation=1 ) # Set the size of the figure sns.set(rc={&#39;figure.figsize&#39;:(15.0, 10.0)}) # Set title plt.title(&quot;Number of reviews per product category&quot;) plt.xlabel(&quot;Number of reviews&quot;) plt.ylabel(&quot;Product category&quot;) plt.tight_layout() # Show the barplot plt.show(barplot) . What is the breakdown of sentiments per product category? . Let&#39;s set the SQL statement to find the count of sentiment per product category and sentiment. . statement_count_by_category_and_sentiment = &quot;&quot;&quot; SELECT product_category, sentiment, COUNT(*) AS count_reviews FROM {} GROUP BY product_category, sentiment ORDER BY product_category ASC, sentiment DESC, count_reviews &quot;&quot;&quot;.format(table_name) print(statement_count_by_category_and_sentiment) . SELECT product_category, sentiment, COUNT(*) AS count_reviews FROM reviews GROUP BY product_category, sentiment ORDER BY product_category ASC, sentiment DESC, count_reviews . Now we query the data in Amazon Athena database passing the prepared SQL statement. . %%time df_count_by_category_and_sentiment = wr.athena.read_sql_query( sql=statement_count_by_category_and_sentiment, database=database_name ) . CPU times: user 482 ms, sys: 22 ms, total: 504 ms Wall time: 3.56 s . Let&#39;s prepare for a stacked percentage horizontal bar plot showing proportion of sentiments per product category. . grouped_category = df_count_by_category_and_sentiment.groupby(&#39;product_category&#39;) grouped_star = df_count_by_category_and_sentiment.groupby(&#39;sentiment&#39;) # Create sum of sentiments per star sentiment df_sum = df_count_by_category_and_sentiment.groupby([&#39;sentiment&#39;]).sum() # Calculate total number of sentiments total = df_sum[&#39;count_reviews&#39;].sum() print(&#39;Total number of reviews: {}&#39;.format(total)) . Total number of reviews: 22626 . And create a dictionary of product categories and array of star rating distribution per category. . distribution = {} count_reviews_per_star = [] i=0 for category, sentiments in grouped_category: count_reviews_per_star = [] for star in sentiments[&#39;sentiment&#39;]: count_reviews_per_star.append(sentiments.at[i, &#39;count_reviews&#39;]) i=i+1; distribution[category] = count_reviews_per_star . Now let&#39;s build an array per star across all categories. . distribution . {&#39;Blouses&#39;: [2256, 384, 343], &#39;Dresses&#39;: [4634, 830, 681], &#39;Fine gauge&#39;: [837, 118, 104], &#39;Intimates&#39;: [117, 16, 14], &#39;Jackets&#39;: [550, 61, 72], &#39;Jeans&#39;: [909, 110, 85], &#39;Knits&#39;: [3523, 605, 498], &#39;Layering&#39;: [113, 9, 10], &#39;Legwear&#39;: [126, 16, 16], &#39;Lounge&#39;: [545, 78, 46], &#39;Outerwear&#39;: [254, 29, 36], &#39;Pants&#39;: [1074, 154, 122], &#39;Shorts&#39;: [240, 39, 25], &#39;Skirts&#39;: [714, 104, 85], &#39;Sleep&#39;: [175, 16, 23], &#39;Sweaters&#39;: [1036, 193, 151], &#39;Swim&#39;: [252, 42, 38], &#39;Trend&#39;: [78, 19, 21]} . df_distribution_pct = pd.DataFrame(distribution).transpose().apply( lambda num_sentiments: num_sentiments/sum(num_sentiments)*100, axis=1 ) df_distribution_pct.columns=[&#39;1&#39;, &#39;0&#39;, &#39;-1&#39;] df_distribution_pct . 1 0 -1 . Blouses 75.628562 | 12.872947 | 11.498491 | . Dresses 75.410903 | 13.506916 | 11.082181 | . Fine gauge 79.036827 | 11.142587 | 9.820585 | . Intimates 79.591837 | 10.884354 | 9.523810 | . Jackets 80.527086 | 8.931186 | 10.541728 | . Jeans 82.336957 | 9.963768 | 7.699275 | . Knits 76.156507 | 13.078253 | 10.765240 | . Layering 85.606061 | 6.818182 | 7.575758 | . Legwear 79.746835 | 10.126582 | 10.126582 | . Lounge 81.464873 | 11.659193 | 6.875934 | . Outerwear 79.623824 | 9.090909 | 11.285266 | . Pants 79.555556 | 11.407407 | 9.037037 | . Shorts 78.947368 | 12.828947 | 8.223684 | . Skirts 79.069767 | 11.517165 | 9.413068 | . Sleep 81.775701 | 7.476636 | 10.747664 | . Sweaters 75.072464 | 13.985507 | 10.942029 | . Swim 75.903614 | 12.650602 | 11.445783 | . Trend 66.101695 | 16.101695 | 17.796610 | . Let&#39;s plot the distributions of sentiments per product category. . categories = df_distribution_pct.index # Plot bars plt.figure(figsize=(10,5)) df_distribution_pct.plot(kind=&quot;barh&quot;, stacked=True, edgecolor=&#39;white&#39;, width=1.0, color=[&#39;green&#39;, &#39;orange&#39;, &#39;blue&#39;]) plt.title(&quot;Distribution of reviews per sentiment per category&quot;, fontsize=&#39;16&#39;) plt.legend(bbox_to_anchor=(1.04,1), loc=&quot;upper left&quot;, labels=[&#39;Positive&#39;, &#39;Neutral&#39;, &#39;Negative&#39;]) plt.xlabel(&quot;% Breakdown of sentiments&quot;, fontsize=&#39;14&#39;) plt.gca().invert_yaxis() plt.tight_layout() plt.show() . &lt;Figure size 1000x500 with 0 Axes&gt; . Analyze the distribution of review word counts . Let&#39;s now set the SQL statement to count the number of the words in each of the reviews. . statement_num_words = &quot;&quot;&quot; SELECT CARDINALITY(SPLIT(review_body, &#39; &#39;)) as num_words FROM {} &quot;&quot;&quot;.format(table_name) print(statement_num_words) . SELECT CARDINALITY(SPLIT(review_body, &#39; &#39;)) as num_words FROM reviews . Now query the data in Amazon Athena database passing the SQL statement. . %%time df_num_words = wr.athena.read_sql_query( sql=statement_num_words, database=database_name ) . CPU times: user 286 ms, sys: 20.9 ms, total: 307 ms Wall time: 3.25 s . Let&#39;s print out and analyse some descriptive statistics. . summary = df_num_words[&quot;num_words&quot;].describe(percentiles=[0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1.00]) summary . count 22626.000000 mean 62.709847 std 29.993735 min 2.000000 10% 22.000000 20% 33.000000 30% 42.000000 40% 51.000000 50% 61.000000 60% 72.000000 70% 86.000000 80% 97.000000 90% 103.000000 100% 122.000000 max 122.000000 Name: num_words, dtype: float64 . Now we will plot the distribution of the words number per review. . df_num_words[&quot;num_words&quot;].plot.hist(xticks=[0, 16, 32, 64, 128, 256], bins=100, range=[0, 256]).axvline( x=summary[&quot;100%&quot;], c=&quot;red&quot; ) plt.xlabel(&quot;Words number&quot;, fontsize=&#39;14&#39;) plt.ylabel(&quot;Frequency&quot;, fontsize=&#39;14&#39;) plt.savefig(&#39;distribution_num_words_per_review.png&#39;, dpi=300) plt.show() . Acknowledgements . I&#39;d like to express my thanks to the great Deep Learning AI Practical Data Science on AWS Specialisation Course which i completed, and acknowledge the use of some images and other materials from the training course in this article. .",
            "url": "https://www.livingdatalab.com/aws/cloud-data-science/natural-language-processing/2023/02/03/loading-transforming-clothes-reviews-text-aws.html",
            "relUrl": "/aws/cloud-data-science/natural-language-processing/2023/02/03/loading-transforming-clothes-reviews-text-aws.html",
            "date": " • Feb 3, 2023"
        }
        
    
  
    
        ,"post5": {
            "title": "Using Satellite Images and Deep Learning to Track Deforestation in the Amazon",
            "content": "Introduction . In this project we will be using a deep learning model to help classify satellite images of the amazon rain forest. Here the main objective is not actually to get the best results for this task, rather to use this dataset to illustrate the use of the Fastai deep learning library - in particular to demonstrate the uses of the high-level api as well as the mid-level api and show how this can be used to configure different types of datasets for different types of problems. . Using Fastai to prepare data for the Amazon Image classification task . The amazon dataset comes from the Understanding the Amazon from Space project, which aims: . &#39;...to label satellite image chips with atmospheric conditions and various classes of land cover/land use. Resulting algorithms will help the global community better understand where, how, and why deforestation happens all over the world - and ultimately how to respond.&#39; . Key aspects of this task include. . Our data consists of images as well as multiple labels for each image | Our task is Multi-label Classification i.e. to be able to predict one or more labels for a given image | . While the main dataset has over 40,000 images - we will be using a small subset of this of just 200 images. . In an earlier project I looked at a different dataset of satellite images, in this case for an image segmentation task rather than classification. . Loading and examining the data . Let&#39;s see how we can use the Fastai library to prepare our data to perform this task, and start by loading the data. . path = untar_data(URLs.PLANET_TINY) path.ls() . (#3) [Path(&#39;/root/.fastai/data/planet_tiny/labels.csv&#39;),Path(&#39;/root/.fastai/data/planet_tiny/models&#39;),Path(&#39;/root/.fastai/data/planet_tiny/train&#39;)] . So we have a folder called &#39;train&#39; which we assume has the images, lets take a look to check. . (path/&quot;train&quot;).ls()[:5] . (#5) [Path(&#39;/root/.fastai/data/planet_tiny/train/train_39223.jpg&#39;),Path(&#39;/root/.fastai/data/planet_tiny/train/train_5302.jpg&#39;),Path(&#39;/root/.fastai/data/planet_tiny/train/train_34793.jpg&#39;),Path(&#39;/root/.fastai/data/planet_tiny/train/train_28156.jpg&#39;),Path(&#39;/root/.fastai/data/planet_tiny/train/train_15839.jpg&#39;)] . We also have a labels.csv file, which would normally have the image names and their associated labels, lets verify this. . df = pd.read_csv(path/&quot;labels.csv&quot;) df.head() . image_name tags . 0 train_31112 | clear primary | . 1 train_4300 | partly_cloudy primary water | . 2 train_39539 | clear primary water | . 3 train_12498 | agriculture clear primary road | . 4 train_9320 | clear primary | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Let&#39;s check how many images we have. . df.shape . (200, 2) . So this is a multi-label classification task, each image has one or more labels which we hope to predict. Lets get an idea of how many example images we have for each label. . new_df = df[&#39;tags&#39;].str.split(expand=True).stack().value_counts().reset_index() new_df.columns = [&#39;Word&#39;, &#39;Frequency&#39;] print(new_df.shape) new_df.head(20) . (14, 2) . Word Frequency . 0 primary | 190 | . 1 clear | 139 | . 2 agriculture | 61 | . 3 partly_cloudy | 42 | . 4 road | 41 | . 5 water | 31 | . 6 cultivation | 28 | . 7 habitation | 19 | . 8 haze | 11 | . 9 cloudy | 8 | . 10 bare_ground | 5 | . 11 artisinal_mine | 4 | . 12 blooming | 3 | . 13 selective_logging | 2 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; So we can see this is a very imbalanced dataset, some labels such as primary occur alot, wheras other labels such as selective_logging only occur twice. . As we are mainly focussing on the use of fastai not making the best model, we will be using the fastest method of creating a training &amp; validation datasets using the random split method. Given we have some categories that don&#39;t have many examples, if we do a random split its possible we could have some labels only in the training or valdiation sets, and this will create an error as we can&#39;t have labels in the validation set that are not in the training set. . Let&#39;s deal with this by removing the images that have low-frequency labels, to try to reduce the risk of this error so we can focus on how to use the fastai library. . df = df.copy() df = df[df[&quot;tags&quot;].str.contains(&quot;haze|cloudy|bare_ground|artisinal_mine|blooming|selective_logging&quot;) == False] new_df = df[&#39;tags&#39;].str.split(expand=True).stack().value_counts().reset_index() new_df.columns = [&#39;Word&#39;, &#39;Frequency&#39;] print(new_df.shape) new_df.head(20) . (7, 2) . Word Frequency . 0 clear | 127 | . 1 primary | 126 | . 2 agriculture | 38 | . 3 road | 26 | . 4 water | 18 | . 5 cultivation | 14 | . 6 habitation | 10 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We now have a second issue to deal with, the image names in our labels.csv is not a complete file name, this will make it more difficult to read in the image files. Lets create a new column that has the complete image file name. . df[&#39;filename&#39;] = df[&#39;image_name&#39;] + &#39;.jpg&#39; df.head() . image_name tags filename . 0 train_31112 | clear primary | train_31112.jpg | . 2 train_39539 | clear primary water | train_39539.jpg | . 3 train_12498 | agriculture clear primary road | train_12498.jpg | . 4 train_9320 | clear primary | train_9320.jpg | . 5 train_28430 | agriculture clear cultivation primary road | train_28430.jpg | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; The Fastai layered API . In a previous article i gave an introduction to the Fastai layered API . . In this article we will make use of the High &amp; Mid level API. . Using the High Level API . This level API is the simplest to use, having many preset defaults that make it easy to load and setup data for a range of deep learning tasks. . Let&#39;s use it now to set up our amazon image data. . dls = ImageDataLoaders.from_df(df, path, fn_col=2, folder=&#39;train&#39;, label_delim=&#39; &#39;, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224)) dls.show_batch() . So a few things to note which the Fastai high level api has done: . It&#39;s used our dataframe to load the data | It uses the path variable to know which file path the images are located | The &#39;fn_col&#39; parameter tells it which column to use for the filenames, in this case column 2 is the new column we created for the complete filename | The folder parameter tells it where the images are located under path | The label_delim parameter tells it how to split the labels, in this case separated by spaces | item_tfms a list of one or several transforms applied to the items before batching them for model training | batch_tfms a list of one or several transforms applied to batches of images once they are formed during model training | . So we can see we have a good level of configurability even at the high level api. . There are many other high level api functions for Fastai vision applications for loading different types of data. . It will also be helpful to set up some metrics to measure our progress during training, specific to being a multi labelled classification task, and having an unbalanced dataset. A Good metric for this situation would be an F1 score for multiple classes, so lets set up some metrics for this now. . f1_macro = F1ScoreMulti(thresh=0.5, average=&#39;macro&#39;) f1_macro.name = &#39;F1(macro)&#39; f1_samples = F1ScoreMulti(thresh=0.5, average=&#39;samples&#39;) f1_samples.name = &#39;F1(samples)&#39; . So we are almost ready to create our model and start training. . One consideration we have when creating a model is which model to use? as of date of this article, there are many pre-trained deep learning vision models, and many new ones being added. Which should we use? . Jeremy Howard, one of the co-founders of FastAI completed a project where he looked at a number recent of vision models and evaluated and ranked them by different criteria. . These were based on Ros Wightmanns list of Pytorch state of the art image models library timm. . Looking at these models and considering this use case: i&#39;d like the best performing model but the best smallest model as we are not focussing here on getting the best results, rather to just demonstrate the usage of the Fastai library. . So looking with this criteria, i&#39;ve selected the &#39;convnext_small_in22k&#39; pre-trained image model to use. . Let&#39;s now create the model using the high-level api function vision_learner. . learn = vision_learner(dls, &#39;convnext_small_in22k&#39;, metrics=[partial(accuracy_multi, thresh=0.5), f1_macro, f1_samples]) . So we have created our model, using our data, and added the metrics to use. . But what about the model learning rate? for this we can use another great Fastai api function lr_find(). . For more information on this concept and the research behind it, including discriminative learning rates this is a great article. . learn.lr_find() . SuggestedLRs(valley=0.0008317637839354575) . So this gives us a good idea of the a good learning rate to use, lets set this and train the model for 2 epochs. . learn.fine_tune(2, 3e-2) . epoch train_loss valid_loss accuracy_multi F1(macro) F1(samples) time . 0 | 1.044040 | 2.020892 | 0.497143 | 0.312559 | 0.430190 | 00:02 | . epoch train_loss valid_loss accuracy_multi F1(macro) F1(samples) time . 0 | 0.955900 | 1.813314 | 0.411429 | 0.347462 | 0.411784 | 00:04 | . 1 | 0.914945 | 1.890064 | 0.554286 | 0.363607 | 0.453518 | 00:04 | . We can see our model is slowly starting to improve. . Let&#39;s see how our model is predicting labels for our satellite images. . learn.show_results() . We can also get an idea of which images the model finds hardest to predict by using the plot_top_losses() function. . interp = Interpretation.from_learner(learn) interp.plot_top_losses(9) . target predicted probabilities loss . 0 clear;habitation;primary;road;water | agriculture;cultivation;habitation;road;water | TensorBase([1.0000e+00, 5.3429e-10, 9.1896e-01, 5.3812e-01, 1.8748e-02, n 9.9999e-01, 9.9779e-01]) | 5.9950785636901855 | . 1 agriculture;clear;habitation;primary;road | agriculture;cultivation;habitation;primary;road;water | TensorBase([1.0000e+00, 1.3865e-08, 9.7993e-01, 9.4586e-01, 5.2794e-01, n 9.9999e-01, 9.9923e-01]) | 4.266438961029053 | . 2 clear;primary;water | agriculture;habitation;primary;road;water | TensorBase([9.9979e-01, 5.7836e-05, 1.7540e-01, 7.1101e-01, 5.7885e-01, n 9.9740e-01, 9.9980e-01]) | 3.7381298542022705 | . 3 clear;cultivation;primary | agriculture;road;water | TensorBase([9.9726e-01, 3.5533e-04, 2.8459e-01, 3.0627e-01, 3.5213e-01, n 9.9678e-01, 9.3701e-01]) | 3.573106050491333 | . 4 agriculture;clear;habitation;primary;road;water | agriculture;habitation;primary;road;water | TensorBase([9.9999e-01, 6.4912e-11, 1.6498e-01, 8.6925e-01, 8.6978e-01, n 1.0000e+00, 9.9922e-01]) | 3.4169580936431885 | . 5 agriculture;clear;primary;road | agriculture;cultivation;habitation;road;water | TensorBase([9.9999e-01, 3.5587e-06, 6.8011e-01, 5.0741e-01, 3.6172e-02, n 9.9992e-01, 9.7514e-01]) | 3.058271884918213 | . 6 clear;primary;water | agriculture;habitation;primary;road;water | TensorBase([9.9094e-01, 1.3812e-04, 4.5300e-01, 6.2815e-01, 6.4152e-01, n 7.7717e-01, 8.5307e-01]) | 2.4697818756103516 | . 7 clear;primary | agriculture;habitation;primary;road;water | TensorBase([0.9377, 0.0013, 0.1471, 0.7862, 0.9317, 0.9659, 0.6219]) | 2.221360206604004 | . 8 clear;primary | agriculture;road | TensorBase([6.0217e-01, 3.6376e-04, 3.3483e-02, 3.6663e-01, 4.4091e-01, n 5.4576e-01, 9.7288e-02]) | 1.5774089097976685 | . Using the Mid Level API - Version 1 . Using the mid-level api can give us more control over how the dataset is constructed, which will be determined by the task. . The Fastai data block tutorial is a great way to understand the methodology behind what the mid level api can do. . So there are many ways we could construct the data using the mid level api, however JH encourages us to consider a list of questions that can be helpful for choosing the best method which are: . what are the types of our inputs and targets? Images and multiple labels. | where is the data? In a dataframe. | how do we know if a sample is in the training or the validation set? A column of our dataframe. | how do we get an image? By looking at the column &#39;filename&#39;. | how do we know the label of an image? By looking at the column &#39;tags&#39;. | do we want to apply a function to a given sample? Yes, we need to resize everything to a given size. | do we want to apply a function to a batch after it’s created? Yes, we want data augmentation. | . So while our model input (x -images) and outputs (y - labels) are in the dataframe, we need to do need to do a little processing on these dataframe columns before being able to use them, for example the filenames need filepaths added, and the labels need splitting. . We can create a datablock this way to address these needs. . planet = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x=ColReader(&#39;filename&#39;, pref=str(path/&#39;train&#39;) + os.path.sep), get_y=ColReader(&#39;tags&#39;, label_delim=&#39; &#39;), item_tfms = Resize(460), batch_tfms=aug_transforms(size=224)) dls = planet.dataloaders(df) dls.show_batch() . We can see we used the get_x &amp; get_y parameters to process the images and labels columns using the ColReader() function. We can also see how the answers to those questions directly translates to different parameters in the DataBlock function. . Using the Mid Level API - Version 2 . Another way we could approach this, for getting our images and labels correctly processed is by defining our own functions for doing this using a lambda function. . planet = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x=lambda x:path/&quot;train&quot;/f&#39;{x[2]}&#39;, get_y=lambda x:x[1].split(&#39; &#39;), item_tfms = Resize(460), batch_tfms=aug_transforms(size=224)) dls = planet.dataloaders(df) dls.show_batch() . Using the Mid Level API - Version 3 . Alternatively, for our lambda functions we could use the column names rather than the indexes. . planet = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x=lambda o:f&#39;{path}/train/&#39;+o.filename, get_y=lambda o:o.tags.split(), item_tfms = Resize(460), batch_tfms=aug_transforms(size=224)) dls = planet.dataloaders(df) dls.show_batch() . Using the Mid Level API - Version 4 . Both of these previous methods would involve iterating over all the rows of the dataframe. For large datasets &amp; dataframes, this could prove very costly in terms of time - not the ideal way for Fastai ! . A better and faster way would be to use the from_columns() Datablock method. This uses a user-defined function passed in the get_items parameter to convert the columns into numpy arrays and work with these which would be quicker. . def _amazon_items(x): return ( f&#39;{path}/train/&#39;+x.filename, x.tags.str.split()) planet = DataBlock.from_columns(blocks=(ImageBlock, MultiCategoryBlock), get_items=_amazon_items, item_tfms = Resize(460), batch_tfms=aug_transforms(size=224)) dls = planet.dataloaders(df) dls.show_batch() . Training our Model More . Let&#39;s now train our model for a few more epochs and observe the progress. . learn.fine_tune(12, 1e-2) . epoch train_loss valid_loss accuracy_multi F1(macro) F1(samples) time . 0 | 0.839966 | 1.319353 | 0.628571 | 0.423056 | 0.526470 | 00:02 | . epoch train_loss valid_loss accuracy_multi F1(macro) F1(samples) time . 0 | 0.816123 | 0.863024 | 0.662857 | 0.434543 | 0.519232 | 00:04 | . 1 | 0.743988 | 0.717785 | 0.714286 | 0.561080 | 0.625896 | 00:04 | . 2 | 0.748999 | 0.740482 | 0.645714 | 0.488145 | 0.593423 | 00:04 | . 3 | 0.726016 | 0.943211 | 0.605714 | 0.451780 | 0.529645 | 00:04 | . 4 | 0.710094 | 1.014733 | 0.622857 | 0.514764 | 0.472312 | 00:04 | . 5 | 0.707066 | 0.860917 | 0.697143 | 0.643097 | 0.563126 | 00:04 | . 6 | 0.692620 | 0.711039 | 0.702857 | 0.556803 | 0.558268 | 00:04 | . 7 | 0.679113 | 0.690488 | 0.691429 | 0.542517 | 0.570459 | 00:04 | . 8 | 0.668592 | 0.613841 | 0.720000 | 0.580288 | 0.608078 | 00:04 | . 9 | 0.664969 | 0.561042 | 0.748571 | 0.617624 | 0.648078 | 00:04 | . 10 | 0.652281 | 0.525281 | 0.760000 | 0.630952 | 0.665602 | 00:04 | . 11 | 0.635785 | 0.508053 | 0.754286 | 0.626052 | 0.635316 | 00:04 | . Conclusion . In this article we used the Amazon images dataset to illustrate the different ways we can use the Fastai library to prepare the data for the task. We used both the high &amp; mid level api, and in particular explored the many options the mid level api offers to make it easy and fast to prepare data for deep learning model training. .",
            "url": "https://www.livingdatalab.com/fastai/deep-learning/2023/01/15/using-satellite-images-and-deep-learning-to-track-deforestation-in-the-amazon.html",
            "relUrl": "/fastai/deep-learning/2023/01/15/using-satellite-images-and-deep-learning-to-track-deforestation-in-the-amazon.html",
            "date": " • Jan 15, 2023"
        }
        
    
  
    
        ,"post6": {
            "title": "NLP and Text Classification Without Deep Learning for Business Applications",
            "content": "Introduction . Deep Learning and AI is powering some of the most recent amazing advances in text &amp; natural language processing (NLP) applications, such as GPT-3, Chat-GPT and Dall-E, but these often require specialist resources such as GPU servers that many businesses new to this technology don&#39;t have or can&#39;t yet justify these resources. With traditional Machine Learning (ML) its possible to create useful NLP applications such as text classification without using AI and Deep Learning, and in this article we will look at some examples of how these can provide useful business applications. . Business Applications of NLP . NLP (Natural Language Processing) is a branch of Artificial Intelligence (AI) and Data Science that is having a huge effect on all areas of society, including business. . In essence, Natural language processing helps computers communicate with humans in their own language and scales other language-related tasks. For example, NLP makes it possible for computers to read text, hear speech, interpret it, measure sentiment and determine which parts are important. . A recent article by the Harvard Business Review highlighted some of the huge potential NLP has for businesses. . Until recently, the conventional wisdom was that while AI was better than humans at data-driven decision making tasks, it was still inferior to humans for cognitive and creative ones. But in the past two years language-based AI has advanced by leaps and bounds, changing common notions of what this technology can do. The most visible advances have been in what’s called “natural language processing” (NLP), the branch of AI focused on how computers can process language like humans do. It has been used to write an article for The Guardian, and AI-authored blog posts have gone viral — feats that weren’t possible a few years ago. AI even excels at cognitive tasks like programming where it is able to generate programs for simple video games from human instructions. . A recent article on LinkedIn highlighted some of the top business applications of NLP these include: . Market Intelligence . Marketers can utilize natural language processing to understand their clients better and use those insights to develop more effective tactics. They can analyze subjects and keywords and make effective use of unstructured data thanks to the power of NLP. It can also determine your consumers pain points and maintain track of your competition. . Sentiment Analysis . Companies can regularly use sentiment analysis to acquire a better knowledge of their business. Humans can be sarcastic and sardonic during conversations. You may keep an eye on social media mentions and use real-time sentiment analysis to intervene before things get out of hand. Your company may sense the pulse of its customers with this NLP application. It also allows you to evaluate how your clients reacted to your most recent digital marketing campaign. . Text Classification . Text classification, is a text analysis task that also includes sentiment analysis, involves automatically understanding, processing, and categorizing unstructured text. . Let’s say you want to analyze hundreds of open-ended responses to your recent NPS survey. Doing it manually would take you a lot of time and end up being too expensive. But what if you could train a natural language processing model to automatically tag your data in just seconds, using predefined categories and applying your own criteria. . Topic Modelling . Topic modeling is an approach that can scan a series of documents, find word and phrase patterns within them, and automatically cluster word groupings and related expressions that best represent the set. . Topic Modeling doesn&#39;t require a preexisting list of tags or training data that has been previously categorized by humans, it can &#39;discover&#39; what seem the most appropriate categories for a given set of documents for itself, based on which documents seem the most similar or different. . Recruiting And Hiring . We can all agree that picking the right staff is one of the most important duties performed by the HR department. However, HR has so much data in the current situation that sifting resumes and shortlisting prospects become overwhelming. . Natural Language Processing can help to make this work more accessible. HR experts can use information extraction and named entity recognition to extract information from candidates, such as their names, talents, locations, and educational histories. This enables unbiased resume filtering and the selection of the best candidate for the job. . Text Summarization . This NLP application extracts the most crucial information from a text and summarises it. The primary purpose is to speed up sifting through massive volumes of data in news articles, legal documents, and scientific studies. Text summarization can be done in two ways: extraction-based summarization, which selects crucial words and provides a summary without adding further information, and abstraction-based summarization, which paraphrases the original content to produce new terms. . Survey Analysis . Surveys are an essential tool for businesses to use in evaluating their performance. Survey analysis is crucial in finding defects and supporting companies in improving their goods, whether gathering input on a new product launch or analyzing how effectively a company’s customer service is doing. When many clients complete these surveys, the issue emerges, resulting in massive data. The human brain is unable to comprehend everything. At this time, natural language processing is introduced. These methods help organisations get accurate information about their consumers’ opinions and improve their performance. . Machine Learning vs Deep Learning for NLP and Business . The most powerful and useful applications of NLP use Deep Learning and AI which is a sub-branch of Machine Learning. All the the most recent and most powerful applications of NLP such as GPT-3, Chat-GPT and Dall-E all use Deep Learning. Many would argue Deep Learning is perfect for NLP. . In fact, most of my own recent projects in NLP over the last few years have almost exclusively used Deep Learning. . However before Deep Learning and AI existed and was developed recently, NLP still existed for many years and has its origins in work in the 1950&#39;s. It just used different methods and techniques, that while not as powerful as Deep Learning and AI, still provided useful business applications and benefits at the time they were developed and used. These include the use of traddtional machine learning for NLP. . In a recent article i covered in more detail the differences between tradditonal machine learning and deep learning. . Also, Deep Learning requires the use of specialist resources - namely GPU servers. Many businesses starting to explore the potental benefit of Data, Data Science, Machine Learning and AI don&#39;t always have the rescources or infrastructure setup to develop this technology. . Furthermore, some businesses may feel much more cautious to adopt this technology and the associated cost of resources, and may need a more gradual approach that takes them on a journey as much about education, learning what this technology can do to help solve business problems, as much as gradually using more and more advanced technology. . Some businesses, especially older &amp; established businesses with exisiting business practices, may need to learn slowly how to walk first before running with the most advanced technology! . With this in mind, it&#39;s good to know it is actually possible to develop useful and valuable NLP business applications - without the use of Deep Learning and the specialist resources that requires. While you might not get the best or near state of the art results for your solution, businesses can still gain huge value and benefit by using these slightly older methods compared to none at all. . Pycaret and NLP . NLP often requires a significant amount of code and steps to solve business problems. Pycaret is a low code machine learning library, that allows you to perform common tasks in Data Science and Machine Learning with very little code, and has been listed in a recent article by Forbes as one of the 10 Best Examples Of Low-Code And No-Code AI . I&#39;ve been using Pycaret myself professionally in my role as a Data Scientist as well as for personal projects for over a year now and have found it incredibily useful to enable me to work much more quickly and efficiently. I&#39;ve also written about how Pycaret is actually a Data Science Power Tool. . In this project I will be using Pycaret for the NLP tasks we will be doing to solve certain business problems using machine learning. . Text Classification Without Deep Learning . Remembering our common uses of NLP, we are going to solve 2 different business problems to illustrate these methods: . Topic Modelling: We will use this method to try to discover what the hidden categories are for a dataset from kiva - a crowdfunder for loans which includes text data of each loan application. Or put another way - what kind of hidden topics would best describe peoples loan applications? For most busineses, it might be really useful to understand using customer text, such as customer contact form text etc, and discover what kind of topics customers were talking about without us knowing or assuming we know what they are before hand. | Sentiment Analysis &amp; Classification: We will use this method to learn to predict the sentiment of amazon customer product reviews using the review text, and each of the positive or negative labels they have been assigned in the dataset. In other words, given a customer review text - to predict if this is a positive or negative review. This could be very useful for a business to understand if a product or service was succesful or not, by analysing thousands or even millions of customer reviews automatically and efficiently. | . Note, with Topic Modelling we are actually trying to discover new categories for a given set of texts, wheras with Sentiment Analysis &amp; Classification we are using an exisiting category. These are known as unsupervised machine learning and supervised machine learning respectively. In both cases, we produce something called a model which is something that we can then use on new text to predict what category that text is. . Topic modelling - Discovering hidden categories in Kiva loan applications . Pycaret comes with some ready to use datasets such as Kiva. Kiva is a non-profit that allows individuals to lend money to low-income entrepreneurs and students around the world. The kiva dataset is data on individual loan applications which include the text of the application. Lets load and view the data. . kiva = get_data(&#39;kiva&#39;) . country en gender loan_amount nonpayment sector status . 0 Dominican Republic | &quot;Banco Esperanza&quot; is a group of 10 women looking to receive a small loan. Each of them has taken out a very small loan already, so this would be their second. With this loan the group is going to try and expand their small businesses and start generating more income. &lt;P&gt; n nEduviges is the group representative and leader of the group. Eduviges has a lot on the line because she has 6 children that she has to take care of. She told me that those children are the reason she wants to be successful. She wants to be able to provide a different life for them and show them that they can be successful as well. &lt;P&gt; n nEduviges has a very small business selling shoes and Avon products. She plans to expand using this loan and dreams of success. The whole group is ready for this new challenge and a... | F | 1225 | partner | Retail | 0 | . 1 Dominican Republic | &quot;Caminemos Hacia Adelante&quot; or &quot;Walking Forward&quot; is a group of ten entrepreneurs seeking their second loan from Esperanza International. The groups past loan has been successfully repaid and the group hopes to use additional loan funds for further business expansion. n nEstella is one of the coordinators for this group in Santiago. Estella sells undergarments to her community and neighboring communities. Estella used her first loan, which has now been completely repaid, to buy additional products and Estela was able to increase the return on her business by adding inventory. Estella wants to use her second loan to buy more undergarments to sell to her customers. n nEstella lives with her mother and sister and dreams of improving the house they live in and plans to use her business ... | F | 1975 | lender | Clothing | 0 | . 2 Dominican Republic | &quot;Creciendo Por La Union&quot; is a group of 10 people hoping to start their own businesses. This group is looking to receive loans to either start a small business or to try and increase their business. Everyone in this group is living in extreme poverty, and they see this as a chance to improve their lives and the lives of their families. n n&quot;Dalina&quot; is the group representative and was chosen because she is a very hardworking women. She is a young mother of two children, and she realized that she wanted a better life for her and her family. She is hoping to start a small business of selling clothes to people in her barrio. She hopes to someday have a thriving business and be able to provide for her family. On behalf of Dalina, the rest of the group, and Esperanza International: Thank you ... | F | 2175 | partner | Clothing | 0 | . 3 Dominican Republic | &quot;Cristo Vive&quot; (&quot;Christ lives&quot; is a group of 10 women who are looking to receive their first loans. This is a very young group of women, and they all want to start changing their lives right away. Riquena is the group representative and leader of this group, and she is only 18 years old. She is also married, but has no children. She told me that once she has kids she wants to be able to provide them with a good life, and that is the main reason she is trying to start her own business. She plans on selling used clothes in her area, and hopes to one day have a big clothing store, and also design clothes. She is a very motivated person, and you can see it when you speak with her. She speaks Spanish and Creole fluently, and is studying English. This whole group is ready for this next step, ... | F | 1425 | partner | Clothing | 0 | . 4 Dominican Republic | &quot;Cristo Vive&quot; is a large group of 35 people, 20 of which are hoping to take out a loan. For many of them this is their second loan, and a loan they hope to use to increase their business. The business range from clothing sales to salons. Miline is the chosen group representative due to her hard work and dedication. Miline is a hardworking mother of 5 very young children, the oldest being only 10 years old. She took her first loan and started a small business of selling chicken and other types of food. With this next loan she feels like she can increase her business greatly and start making money to support her family. Her dream is to have her own store someday, and be able to provide her family with comfortable life. On behalf of Miline, the group, and Esperanza International, thank yo... | F | 4025 | partner | Food | 0 | . Let&#39;s check how big the dataset is. . kiva.shape[0] . 6818 . So we have around 7,000 loan applications. Lets now process and prepare the data. . %time experiment1 = setup(data=kiva, target=&#39;en&#39;) . Description Value . session_id | 2214 | . Documents | 6818 | . Vocab Size | 12383 | . Custom Stopwords | False | . CPU times: user 1min 14s, sys: 295 ms, total: 1min 15s Wall time: 1min 15s . This single line of code has actually performed a large number of tasks that would normally take many lines of code, but in Pycaret is a single line of code. You can find out more about what this line does for NLP text pre-processing here. . Now our data is prepared, lets create our topic model. . For topic modelling we will be using the Latent Dirichlet Allocation (LDA) technique. I&#39;ve written previously about the mathemetics behind two other techniques called Non-negative Matrix Factorization (NMF) and Singular Value Decomposition (SVD). . lda_topic_model = create_model(&#39;lda&#39;, num_topics=4) . So we now have our topic model. Notice we have set &#39;num_topics=4&#39; - this means the model tries to discover the 4 topics that seem most relevant to the loan applications. We could set this to a different number if we wanted to. . Now we have discovered our 4 topics for the loan applications and trained a model to recognise them, we can use this model to predict each of these 4 topics for all our applications using the assign_model() function. . lda_results = assign_model(lda_topic_model) lda_results.head() . country en gender loan_amount nonpayment sector status Topic_0 Topic_1 Topic_2 Topic_3 Dominant_Topic Perc_Dominant_Topic . 0 Dominican Republic | group woman look receive small loan take small loan already second loan group go try expand small business start generate income group representative leader group eduvige lot line child tell child reason want successful want able provide different life show successful well eduvige small business selling shoe avon product plan expand use loan dream success whole group ready new challenge road better live behalf eduvige thank support | F | 1225 | partner | Retail | 0 | 0.410590 | 0.044232 | 0.001707 | 0.543472 | Topic 3 | 0.54 | . 1 Dominican Republic | caminemos walk forward group entrepreneur seek second loan esperanza_international group loan successfully_repaid group hope use additional loan fund business expansion coordinator group sell undergarment community neighboring community use first loan completely repay buy additional product estela able increase return business add inventory estella want use second loan buy undergarment sell customer live mother sister dream improve house live plan use business profit member art juice ice_cream fry food cake sale behalf esperanza group business entrepreneur like thank support | F | 1975 | lender | Clothing | 0 | 0.608610 | 0.084845 | 0.001478 | 0.305067 | Topic 0 | 0.61 | . 2 Dominican Republic | por la_union group people hope start business group look receive loan start small business try increase business group poverty see chance improve life live family representative choose hardworke woman young mother child realize want well life family hope start small business sell clothe people barrio hope someday thrive business able provide family behalf thank support | F | 2175 | partner | Clothing | 0 | 0.486984 | 0.012169 | 0.002022 | 0.498825 | Topic 3 | 0.50 | . 3 Dominican Republic | vive live group woman look receive first loan young group woman want start change life right away riquena group representative leader group year old also marry child tell kid want able provide good life main reason try start business plan sell use clothe area hope day big clothing store also design clothe motivated person see speak speak spanish creole fluently study english whole group ready next step excited_opportunity behalf thank support | F | 1425 | partner | Clothing | 0 | 0.289351 | 0.071750 | 0.001620 | 0.637279 | Topic 3 | 0.64 | . 4 Dominican Republic | cristo vive large group people hope take loan many second loan hope use increase business business range clothing sale salon miline choose group representative due hard work dedication miline hardworke mother young child old year old take first loan start small business sell chicken type food next loan feel increase business greatly start make money support family dream store someday able provide family comfortable life behalf miline thank support | F | 4025 | partner | Food | 0 | 0.562529 | 0.032050 | 0.001672 | 0.403749 | Topic 0 | 0.56 | . We can see the topic model has given us several new things. Firstly, for each loan application it has given us a measure of how much of each of the 4 topics that loan application scores for - which would be a value between 0 and 1. Secondly, for each loan application Dominant_Topic tells us which is the most important topic. Finally, Perc_Dominant_Topic tells hows how highly that loan application scores for its dominant topic. . Lets have a look at how many loan applications are within each of the 4 topics, Pycaret makes this very easy using the plot_model() function. . plot_model(lda_topic_model, plot = &#39;topic_distribution&#39;) . . So we can see that topic 0 covers most of the loan applications, and the other topics much less, with topic 1 having very few examples. . What are topics actually about ? Word counts . How can we find out what these hidden topics are about? We can look at the top 100 words in the text of each topic to give us some idea. . Again, Pycaret makes this very easy again using the plot_model() function. . plot_model(lda_topic_model, plot = &#39;frequency&#39;, topic_num = &#39;Topic 0&#39;) . . So we can see for topic 0 the top 4 words are: . Business | Year | Child | Old | . You could imagine perhaps the loan applications for this topic might emphasise for example how these loans would have a benefit in a specific year, or would benefit perhaps both older and younger people in the community? . Lets have a look at topic 1. . plot_model(lda_topic_model, plot = &#39;frequency&#39;, topic_num = &#39;Topic 1&#39;) . . So we can see for topic 1 the top 4 words are: . Year | Loan | Community | Clinic | . Perhaps applications under this topic tend to emphasise how the loan might benefit the local community, including healthcare services specifically? . Lets examine topic 2. . plot_model(lda_topic_model, plot = &#39;frequency&#39;, topic_num = &#39;Topic 2&#39;) . . So we can see for topic 2 the top 4 words are: . Rice | Farmer | Use | Sector | . For this topic it might be the case that these loan applications could be for projects more relating to agriculture and food production. . Finally lets explore topic 3. . plot_model(lda_topic_model, plot = &#39;frequency&#39;, topic_num = &#39;Topic 3&#39;) . . The top 4 words for topic 3 are: . Loan | Child | School | Sell | . You could imagine that perhaps loans under this topic might be related to education and schools, and perhaps also the buying and selling of products for schools or children. . So this have given us some good indications as to what the different hidden topics might be about regarding these loan applications. . How similar or different are topics? Dimensionality Reduction . Another thing we can do is look at these loan applicaton texts spatially. We can convert these texts into numbers that represent these texts in terms of their meaning, then plot these numbers as points in 3D space. Each point will then represent an individual loan application, and points that are closer will be applications that are more similar, and points further away applications more different. . This general approach of reducing data down into simplified numbers is called Dimenstionality Reduction and you can find more about these methods in an earlier project i did on this. We will use a method for this called TSNE. . Again Pycaret makes this very easy to do using the plot_model() function. . plot_model(lda_topic_model, plot = &#39;tsne&#39;) . . We can tell a few things from this view of the loan applications and topics: . All topics seem to be fairly distinct with little overlap | Topic 0, 1 &amp; 3 seem to meet at the edges suggesting there are a few cases that could be in either topic | Topic 2 seems to be the most unique, its the most separated from the others spatially | . This seems to confirm what we found when we looked at the top words from each topic, topic 2 was about farming and agriculture which really was much more unique compared to the other topics, which had a little more overlap between them. . So we can see that topic modelling can be a very useful technique for businesses to provide insight on a group of text that we may know nothing about. It can help us discover hidden categories among these texts, how many are under each of these categories, how closely related or distinct these categories are - and much more. This could easily be applied to customer queries, survey responses, transcripts of customer conversations or emails, and more - to help businesses gain useful insights from their textual data. . Sentiment Analysis &amp; Classification - Predict if Amazon product reviews are positive or negative . Pycaret also comes with a dataset of amazon product reviews, lets load these and have a look. . amazon_reviews = get_data(&#39;amazon&#39;) . reviewText Positive . 0 This is a one of the best apps acording to a bunch of people and I agree it has bombs eggs pigs TNT king pigs and realustic stuff | 1 | . 1 This is a pretty good version of the game for being free. There are LOTS of different levels to play. My kids enjoy it a lot too. | 1 | . 2 this is a really cool game. there are a bunch of levels and you can find golden eggs. super fun. | 1 | . 3 This is a silly game and can be frustrating, but lots of fun and definitely recommend just as a fun time. | 1 | . 4 This is a terrific game on any pad. Hrs of fun. My grandkids love it. Great entertainment when waiting in long lines | 1 | . So we can see we have just a column for the text of the review, and another called &#39;Positive&#39; which is a label to indicate if the review was positive or not i.e. 1 or 0. Let&#39;s see how many reviews we have. . amazon_reviews.shape[0] . So we have around 20,000 reviews. Lets get a count of how many positive and negative reviews we have. . amazon_reviews[&#39;Positive&#39;].value_counts() . 1 15233 0 4767 Name: Positive, dtype: int64 . So around 75% of the reviews are positive, and 25% negative reviews. . To create a classification model, we will first need to create some features. These are essentially numbers that represent something we are trying to predict, so given we are trying to predict if a review is positive or negative, these features need to represent something about the text that will help us predict that. . There are many methods of turning text into numeric features, but we are actually going to use topic modelling to create some topics to describe our text, and use these as features to help our classfier model to predict positive or negative sentiment. . Lets set up and process our review data for topic modelling. . %time experiment2 = setup(data=amazon_reviews, target=&#39;reviewText&#39;) . Description Value . session_id | 497 | . Documents | 20000 | . Vocab Size | 12771 | . Custom Stopwords | False | . CPU times: user 1min 28s, sys: 1.51 s, total: 1min 30s Wall time: 1min 35s . As before we will create a topic model to create some new categories. . lda_topic_model2 = create_model(&#39;lda&#39;) . Let&#39;s now predict these categories for our reviews. . lda_results = assign_model(lda_topic_model2) lda_results.head() . reviewText Positive Topic_0 Topic_1 Topic_2 Topic_3 Dominant_Topic Perc_Dominant_Topic . 0 good app acorde bunch people agree bomb egg pig king pig realustic stuff | 1 | 0.081603 | 0.309925 | 0.227132 | 0.381340 | Topic 3 | 0.38 | . 1 pretty good version game free lot different level play kid enjoy lot | 1 | 0.070119 | 0.200039 | 0.249249 | 0.480594 | Topic 3 | 0.48 | . 2 really cool game bunch level find golden egg super fun | 1 | 0.116654 | 0.263965 | 0.197222 | 0.422159 | Topic 3 | 0.42 | . 3 silly game frustrating lot fun definitely recommend fun time | 1 | 0.077698 | 0.148072 | 0.309584 | 0.464646 | Topic 3 | 0.46 | . 4 terrific game pad fun grandkid love great entertainment wait long line | 1 | 0.072539 | 0.138212 | 0.424701 | 0.364547 | Topic 2 | 0.42 | . So our data is almost ready. Our classification model does&#39;nt need the text data now as we have represented the text using values for our new categories created by our topic model. We also don&#39;t need the Dominant or Perc topic fields, so lets drop these columns. . lda_results.drop([&#39;reviewText&#39;, &#39;Dominant_Topic&#39;, &#39;Perc_Dominant_Topic&#39;], axis=1, inplace=True) lda_results.head() . Positive Topic_0 Topic_1 Topic_2 Topic_3 . 0 1 | 0.081603 | 0.309925 | 0.227132 | 0.381340 | . 1 1 | 0.070119 | 0.200039 | 0.249249 | 0.480594 | . 2 1 | 0.116654 | 0.263965 | 0.197222 | 0.422159 | . 3 1 | 0.077698 | 0.148072 | 0.309584 | 0.464646 | . 4 1 | 0.072539 | 0.138212 | 0.424701 | 0.364547 | . It&#39;s common practice when training classification models to split the data, some to train the model on, and some to test the model later. Let&#39;s split this data of 20,000 reviews, to give is a small test data set. . train, test = split_data(lda_results) . Let&#39;s now set the data up, this time to prepare it for classification model training using our training data. . %time experiment3 = setup(data=train, target=&#39;Positive&#39;) . &nbsp; Description Value . 0 Session id | 227 | . 1 Target | Positive | . 2 Target type | classification | . 3 Data shape | (19980, 5) | . 4 Train data shape | (13985, 5) | . 5 Test data shape | (5995, 5) | . 6 Numeric features | 4 | . 7 Preprocess | True | . 8 Imputation type | simple | . 9 Numeric imputation | mean | . 10 Categorical imputation | constant | . 11 Fold Generator | StratifiedKFold | . 12 Fold Number | 10 | . 13 CPU Jobs | -1 | . 14 Log Experiment | False | . 15 Experiment Name | clf-default-name | . 16 USI | 22b0 | . CPU times: user 259 ms, sys: 8.99 ms, total: 267 ms Wall time: 269 ms . Let&#39;s now train a range of different models to predict the positive or negative sentiment, and choose the best one. . Again Pycaret makes this very easy to do something that would normally take many lines of code to do. . compare_models(exclude=&#39;dummy&#39;) . &nbsp; Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) . svm SVM - Linear Kernel | 0.7618 | 0.0000 | 1.0000 | 0.7618 | 0.8648 | 0.0000 | 0.0000 | 0.0220 | . lr Logistic Regression | 0.7617 | 0.6472 | 0.9981 | 0.7625 | 0.8645 | 0.0053 | 0.0294 | 0.0290 | . ridge Ridge Classifier | 0.7617 | 0.0000 | 0.9992 | 0.7620 | 0.8646 | 0.0019 | 0.0150 | 0.0160 | . lda Linear Discriminant Analysis | 0.7616 | 0.6474 | 0.9948 | 0.7637 | 0.8641 | 0.0156 | 0.0512 | 0.0210 | . gbc Gradient Boosting Classifier | 0.7610 | 0.6559 | 0.9965 | 0.7626 | 0.8640 | 0.0065 | 0.0282 | 0.8190 | . ada Ada Boost Classifier | 0.7602 | 0.6476 | 0.9937 | 0.7631 | 0.8633 | 0.0103 | 0.0318 | 0.2600 | . catboost CatBoost Classifier | 0.7600 | 0.6468 | 0.9868 | 0.7658 | 0.8624 | 0.0316 | 0.0690 | 6.6620 | . lightgbm Light Gradient Boosting Machine | 0.7583 | 0.6380 | 0.9829 | 0.7661 | 0.8610 | 0.0332 | 0.0675 | 0.1940 | . nb Naive Bayes | 0.7540 | 0.6470 | 0.9608 | 0.7720 | 0.8561 | 0.0727 | 0.1019 | 0.0250 | . xgboost Extreme Gradient Boosting | 0.7495 | 0.6231 | 0.9590 | 0.7692 | 0.8537 | 0.0528 | 0.0750 | 0.8160 | . qda Quadratic Discriminant Analysis | 0.7439 | 0.6441 | 0.9504 | 0.7712 | 0.8465 | 0.0333 | 0.0493 | 0.0190 | . rf Random Forest Classifier | 0.7233 | 0.5970 | 0.8956 | 0.7758 | 0.8314 | 0.0819 | 0.0892 | 1.3430 | . knn K Neighbors Classifier | 0.7171 | 0.5745 | 0.8887 | 0.7737 | 0.8272 | 0.0683 | 0.0737 | 0.0930 | . et Extra Trees Classifier | 0.7058 | 0.5801 | 0.8628 | 0.7760 | 0.8171 | 0.0756 | 0.0786 | 0.6430 | . dt Decision Tree Classifier | 0.6556 | 0.5333 | 0.7667 | 0.7780 | 0.7723 | 0.0657 | 0.0658 | 0.0740 | . SGDClassifier(alpha=0.0001, average=False, class_weight=None, early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty=&#39;l2&#39;, power_t=0.5, random_state=227, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False) . The F1 score is a good measure of how well a model is predicting both positive and negative sentiment, the best model for this is &#39;svm&#39;. . Lets use this model on our test data to see if it seems to be predicting correct sentiment for our reviews. . best_model = create_model(&#39;svm&#39;, verbose=False) new_predictions = predict_model(best_model, data=test) new_predictions = new_predictions.join(amazon_reviews) new_predictions = new_predictions[[&#39;reviewText&#39;, &#39;Topic_0&#39;, &#39;Topic_0&#39;, &#39;Topic_0&#39;, &#39;Topic_0&#39;, &#39;Positive&#39;, &#39;Label&#39;]] new_predictions.head() . reviewText Topic_0 Topic_0 Topic_0 Topic_0 Positive Label . 60 who doesn&#39;t like angrybirds?but the paid version is better as it doesn&#39;t have all those annoying adds. blocking your shots! | 0.085445 | 0.085445 | 0.085445 | 0.085445 | 1 | 1 | . 159 Free and fun, what could be better? The birds are angry, it&#39;s everything I expected, and anyway, those pigs had it coming! | 0.079090 | 0.079090 | 0.079090 | 0.079090 | 1 | 1 | . 1294 I downloaded this to my tablet, as my phone is out of space. Very easy to read the latest tweets that way | 0.118320 | 0.118320 | 0.118320 | 0.118320 | 1 | 1 | . 4352 I love this App and also use Out Of Milk via the website. It makes creating my lists and sharing it with others, quick and easy! It also keeps track of my cost as I add to is, making budgeting a breeze. | 0.081643 | 0.081643 | 0.081643 | 0.081643 | 1 | 1 | . 7016 its actualy saying wat I&#39;m going through. its very fun and creative. I will be sure to use it everyday. no complaints. good job guys. :) | 0.104748 | 0.104748 | 0.104748 | 0.104748 | 1 | 1 | . &#39;Positive&#39; is our original sentiment for our reviews, and &#39;Label&#39; is the sentiment predicted by the model. Looking at the first few reviews seems to confirm that our model is able to predict the sentiment of reviews quite well. . This type of text classification or sentiment analysis model could be used for many different types of business application, for example on customer requests to identify complaints. A customer complaints prediction model could be used to classify thousands of customer requests, which could then be used to prioritise customer requests that are flagged as complaints by the model, or pass these on to a specialist team. This could ensure customer complaints were dealt with quickly regardless of how many total customer messages were incoming. . Conclusion . In this article we have looked at the huge benefits NLP applications can bring to businesses. Most state of the art NLP applications use deep learning which often require specialist resources not all businesses will be able or willing initially to support. . We have shown here some examples of how NLP applications without deep learning - such as topic modelling or sentiment analysis and text classification, can bring huge benefits to businesses despite not being state of the art methods, especially for businesses new to Data Science, Machine Learning and AI. .",
            "url": "https://www.livingdatalab.com/pycaret/natural-language-processing/2023/01/08/nlp-text-classification-without-deep-learning-for-business-applications.html",
            "relUrl": "/pycaret/natural-language-processing/2023/01/08/nlp-text-classification-without-deep-learning-for-business-applications.html",
            "date": " • Jan 8, 2023"
        }
        
    
  
    
        ,"post7": {
            "title": "From Machine Learning to Deep Learning From Scratch",
            "content": "Introduction . In this series of articles I will be re-visiting the FastAI Practical Deep Learning for Coders course for this year 2022 which I have completed in previous years. This article covers lesson 5 of this years course, where we will look at the fundemental details and differences between machine learning (ml) and deep learning (dl). . If you don&#39;t understand the difference between ml and dl or were too afraid to ask - this is the article for you! . Machine Learning vs Deep Learning . Machine Learning is a branch of computer science that seeks to create systems (often called &#39;models&#39;) that learn how to perform a task, without being given explicit instructions of how to perform that task. These models learn for themselves how to perform a task. Machine Learning includes a wide range of different types of models, for example linear regression, random forrests, and more. . Deep learning is a sub-branch of machine learning, which uses multi-layered artifical neural networks as models that learn how to perform a task, without being given explicit instructions of how to perform that task. . Other notable differences between machine learning and deep learning include: . Machine learning models tend to be easier to understand and explain why they do what they do, deep learning models tend to be more difficult to understand the reasons for their behaviour | Machine learning models tend to require the data they use to be more carefully constructed, deep learning models tend to be able to work with data that does not need to be so carefully created | Deep learning models are much more powerful and succesful than machine learning models at solving problems that use images or text | . This article also further explains these differences. . In this project we will construct from scratch a very simple machine learning model called linear regression. We will then gradually develop a deep learning model from scratch, and we will illustrate the technical differences between these types of models, which also demonstrates the reasons for the differences between the two types of models highlighted above. . We will not use any machine learning libraries, which often obscure the details of how these models are implemented. In this project, we will expose the fundemental details of these models by coding them manually and illustrating the mathematics behind them. . The Dataset: The Kaggle Titanic passenger suvival dataset . For our project we will use the famous Titanic - Machine Learning from Disaster dataset. This is a dataset of the passengers from the Titanic disaster, and the task is to predict which of these passengers died and which survived. . This is a very simple and well known dataset, and is chosen not because it&#39;s an especially challenging task, but more to allow us to understand the differences between machine learning and deep learning. . Import Libraries . First we will import the required libraries. . import pandas as pd import numpy as np import torch from torch import tensor from fastai.data.transforms import RandomSplitter import sympy import torch.nn.functional as F # Set some useful display settings np.set_printoptions(linewidth=140) torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7) pd.set_option(&#39;display.width&#39;, 140) . Get &amp; Clean Data . Let&#39;s now extract the data and examine what it looks like. . !unzip titanic.zip !ls . Archive: titanic.zip inflating: gender_submission.csv inflating: test.csv inflating: train.csv drive gender_submission.csv sample_data test.csv titanic.zip train.csv . df = pd.read_csv(&#39;train.csv&#39;) df.head() . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Here we can see the different columns in our passenger dataset, for example Name, Sex, Age etc. The Survived column tells us if that passenger survived the disaster, with a value of 1 if they did and a value of 0 if they died. This is the value we want our model to predict, given the other data in the dataset. In other words, we want to create a model to predict Survived based on Name, Age, Ticket, Fare etc. . Machine learning models require the data to be all numbers, they can&#39;t work with missing values. Let&#39;s check to see if we have any missing values in our dataet the textual columns of the data. The isna() function will do this for us in python. . df.isna().sum() . PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 . We can see that the Age, Cabin and Embarked columns have missing values, so we will need to do something about these. Let&#39;s replace the missing values with the most common value in that column, this is known in statistics as the mode. . Lets calculate the mode for each column. . modes = df.mode().iloc[0] modes . PassengerId 1 Survived 0.0 Pclass 3.0 Name Abbing, Mr. Anthony Sex male Age 24.0 SibSp 0.0 Parch 0.0 Ticket 1601 Fare 8.05 Cabin B96 B98 Embarked S Name: 0, dtype: object . Now that we have the mode of each column, we can use these to fill in the missing values of any column using the fillna() function. . df.fillna(modes, inplace=True) . Let&#39;s check to see we no longer have any missing values. . df.isna().sum() . PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 0 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 0 Embarked 0 dtype: int64 . As mentioned earlier, machine learning models require numbers as inputs - so we will need to convert our text fields into numeric fields. We can do this using a standard technique called one-hot encoding which creates a numeric column for each text value which are called dummy variables which has a value of 1 or zero depending if that text/category value is present or not. We can create these fields using the get_dummies() method. . df = pd.get_dummies(df, columns=[&quot;Sex&quot;,&quot;Pclass&quot;,&quot;Embarked&quot;]) df.columns . Index([&#39;PassengerId&#39;, &#39;Survived&#39;, &#39;Name&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Ticket&#39;, &#39;Fare&#39;, &#39;Cabin&#39;, &#39;LogFare&#39;, &#39;Sex_female&#39;, &#39;Sex_male&#39;, &#39;Pclass_1&#39;, &#39;Pclass_2&#39;, &#39;Pclass_3&#39;, &#39;Embarked_C&#39;, &#39;Embarked_Q&#39;, &#39;Embarked_S&#39;], dtype=&#39;object&#39;) . Let&#39;s see what these dummy variable columns look like. . added_cols = [&#39;Sex_male&#39;, &#39;Sex_female&#39;, &#39;Pclass_1&#39;, &#39;Pclass_2&#39;, &#39;Pclass_3&#39;, &#39;Embarked_C&#39;, &#39;Embarked_Q&#39;, &#39;Embarked_S&#39;] df[added_cols].head() . Sex_male Sex_female Pclass_1 Pclass_2 Pclass_3 Embarked_C Embarked_Q Embarked_S . 0 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | . 1 0 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | . 2 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | . 3 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | . 4 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; So we will need to convert our model variables into Pytorch tensors, which will enable us to use our data for both machine learning and deep learning later on. . t_dep = tensor(df.Survived) . indep_cols = [&#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;LogFare&#39;] + added_cols t_indep = tensor(df[indep_cols].values, dtype=torch.float) t_indep . tensor([[22.0000, 1.0000, 0.0000, 2.1102, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [38.0000, 1.0000, 0.0000, 4.2806, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000], [26.0000, 0.0000, 0.0000, 2.1889, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [35.0000, 1.0000, 0.0000, 3.9908, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000], [35.0000, 0.0000, 0.0000, 2.2028, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [24.0000, 0.0000, 0.0000, 2.2469, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000], [54.0000, 0.0000, 0.0000, 3.9677, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000], ..., [25.0000, 0.0000, 0.0000, 2.0857, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [39.0000, 0.0000, 5.0000, 3.4054, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000], [27.0000, 0.0000, 0.0000, 2.6391, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000], [19.0000, 0.0000, 0.0000, 3.4340, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000], [24.0000, 1.0000, 2.0000, 3.1966, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [26.0000, 0.0000, 0.0000, 3.4340, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000], [32.0000, 0.0000, 0.0000, 2.1691, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000]]) . t_indep.shape . torch.Size([891, 12]) . Creating a Linear Model . A simple linear regression model attempts to capture a linear relationship betweeen one independant variable and a dependant variable, so that you can predict the latter using the former. In our example below, the independant variable model coefficient is $b_{1}$. A constant value is also added, in this case $b_{0}$. This is basically the equation of a line. . A multiple linear regression model attempts to capture a linear relationship betweeen multiple independant variables and a dependant variable, so that you can predict the latter using the former. In our example below, the independant variable model coefficients are $b_{0}$ to $b_{n}$. This is basically the equation of a hyperplane which is a line in multiple dimensions, in this case that number is the number of independant variables. . The values of the independant variables themselves are represented by $x_{1}$ to $x_{n}$. . Linear models generate their predictions by multiplying the values of each variable by its coefficient, then summing the values. So for our multiple linear regression model that would mean summing $b_{1}$ $x_{1}$ to $b_{n}$ $x_{n}$ then adding the constant term $b_{0}$ to get the value for the dependant variable y. . You can read more about linear regression here. . . For our titanic dataset, we have multiple independant variables such as passenger id, name, fare etc - so we will need to use a multiple linear regression model, which will have a coefficient for each variable we have. . Let&#39;s set up some coefficient&#39;s for each variable with some random initial values. . torch.manual_seed(442) n_coeff = t_indep.shape[1] coeffs = torch.rand(n_coeff)-0.5 coeffs . tensor([-0.4629, 0.1386, 0.2409, -0.2262, -0.2632, -0.3147, 0.4876, 0.3136, 0.2799, -0.4392, 0.2103, 0.3625]) . Interestingly we don&#39;t need to add a constant term as per the linear regression model equation. Why? because our dummy variables already cover the whole dataset, everyone is already within one existing value eg male or female. So we don&#39;t need a separate constant term to cover any rows not included. . As mentioned, a linear model will calculate its predictions by multiplying the independant variables by their corresponding coefficients so lets see what that looks like. Remember we have multiple values of our independant variables, one row per passenger, so a matrix. So we will expect from linear algebra, when we multiply a vector (coefficients) by a matrix we should end up with a new matrix. . t_indep*coeffs . tensor([[-10.1838, 0.1386, 0.0000, -0.4772, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-17.5902, 0.1386, 0.0000, -0.9681, -0.0000, -0.3147, 0.4876, 0.0000, 0.0000, -0.4392, 0.0000, 0.0000], [-12.0354, 0.0000, 0.0000, -0.4950, -0.0000, -0.3147, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-16.2015, 0.1386, 0.0000, -0.9025, -0.0000, -0.3147, 0.4876, 0.0000, 0.0000, -0.0000, 0.0000, 0.3625], [-16.2015, 0.0000, 0.0000, -0.4982, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-11.1096, 0.0000, 0.0000, -0.5081, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.2103, 0.0000], [-24.9966, 0.0000, 0.0000, -0.8973, -0.2632, -0.0000, 0.4876, 0.0000, 0.0000, -0.0000, 0.0000, 0.3625], ..., [-11.5725, 0.0000, 0.0000, -0.4717, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-18.0531, 0.0000, 1.2045, -0.7701, -0.0000, -0.3147, 0.0000, 0.0000, 0.2799, -0.0000, 0.2103, 0.0000], [-12.4983, 0.0000, 0.0000, -0.5968, -0.2632, -0.0000, 0.0000, 0.3136, 0.0000, -0.0000, 0.0000, 0.3625], [ -8.7951, 0.0000, 0.0000, -0.7766, -0.0000, -0.3147, 0.4876, 0.0000, 0.0000, -0.0000, 0.0000, 0.3625], [-11.1096, 0.1386, 0.4818, -0.7229, -0.0000, -0.3147, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-12.0354, 0.0000, 0.0000, -0.7766, -0.2632, -0.0000, 0.4876, 0.0000, 0.0000, -0.4392, 0.0000, 0.0000], [-14.8128, 0.0000, 0.0000, -0.4905, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.2103, 0.0000]]) . So there is a bit of an issue here, we notice the first column has much bigger values? this is for the column age, which has bigger numbers than all other numeric columns. This can create problems for machine learning, as many models will treat the column with bigger numbers as more important than other columns. . We can address this issue by normalising all the values i.e. dividing each column by its maximum value. This will result in all values being bewteen 1 and 0 and so all variables being treated with equal importance. . vals,indices = t_indep.max(dim=0) t_indep = t_indep / vals . t_indep*coeffs . tensor([[-0.1273, 0.0173, 0.0000, -0.0765, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-0.2199, 0.0173, 0.0000, -0.1551, -0.0000, -0.3147, 0.4876, 0.0000, 0.0000, -0.4392, 0.0000, 0.0000], [-0.1504, 0.0000, 0.0000, -0.0793, -0.0000, -0.3147, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-0.2025, 0.0173, 0.0000, -0.1446, -0.0000, -0.3147, 0.4876, 0.0000, 0.0000, -0.0000, 0.0000, 0.3625], [-0.2025, 0.0000, 0.0000, -0.0798, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-0.1389, 0.0000, 0.0000, -0.0814, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.2103, 0.0000], [-0.3125, 0.0000, 0.0000, -0.1438, -0.2632, -0.0000, 0.4876, 0.0000, 0.0000, -0.0000, 0.0000, 0.3625], ..., [-0.1447, 0.0000, 0.0000, -0.0756, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-0.2257, 0.0000, 0.2008, -0.1234, -0.0000, -0.3147, 0.0000, 0.0000, 0.2799, -0.0000, 0.2103, 0.0000], [-0.1562, 0.0000, 0.0000, -0.0956, -0.2632, -0.0000, 0.0000, 0.3136, 0.0000, -0.0000, 0.0000, 0.3625], [-0.1099, 0.0000, 0.0000, -0.1244, -0.0000, -0.3147, 0.4876, 0.0000, 0.0000, -0.0000, 0.0000, 0.3625], [-0.1389, 0.0173, 0.0803, -0.1158, -0.0000, -0.3147, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-0.1504, 0.0000, 0.0000, -0.1244, -0.2632, -0.0000, 0.4876, 0.0000, 0.0000, -0.4392, 0.0000, 0.0000], [-0.1852, 0.0000, 0.0000, -0.0786, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.2103, 0.0000]]) . We can now create predictions from our linear model, by adding up the rows of the product: . preds = (t_indep*coeffs).sum(axis=1) . Let&#39;s take a look at the first few: . preds[:10] . tensor([ 0.1927, -0.6239, 0.0979, 0.2056, 0.0968, 0.0066, 0.1306, 0.3476, 0.1613, -0.6285]) . How our Linear Model Learns - Adding Gradient Descent . So currently we have a basic linear model, but it is&#39;nt predicting very well because the model coefficients are still random values. How can make these coefficients better so our model predictions can get better? we can use a algorithm called Gradient Descent (or GD). . This article explains the fundamentals of GD. And this article as well as this one explain more the mathematics of GD. . In essence, Gradient Descent is an algorithm that can be used to find values for the coefficients of a function that reduce a separate loss function. So as long as we can define an appropriate loss function, we can use this algorithm. . What would be an appropriate loss function that we would want to minimise the value of? Well we would like our predictions ultimately to be as close to the actual values we want to predict. So here the loss would be a measure of how wrong our predictions are. A high loss value would mean many mistakes, and a low loss value would mean fewer mistakes. This would then be a good function for us to minimise using Gradient Descent. . So in our case, a good loss function might be: . Loss = predictions - values we want to predict . So we will have a different loss value for each value and its prediction, so if we took the mean value of all of these different loss values, that would be a way to capture the overall loss for all predictions. It would also be helpful for these differences to be always positive values. . Lets calculate what this loss would be on our current predictions. . loss = torch.abs(preds-t_dep).mean() loss . tensor(0.5382) . Since for Gradient Descent we will need to repeatedly use this loss function, lets define some functions to calculate our predictions as well as the loss. . def calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1) def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean() . Gradient Descent requires us to calculate gradients. These are the values of the derivatives of the functions that generate the predictions so in our case the derviatives of the multiple linear regression function seen earlier. The Pytorch module can calculate these gradients for us every time the linear regression function is used if we set requires_grad() on the model coefficients. Lets do that now. . coeffs.requires_grad_() . tensor([-0.4629, 0.1386, 0.2409, -0.2262, -0.2632, -0.3147, 0.4876, 0.3136, 0.2799, -0.4392, 0.2103, 0.3625], requires_grad=True) . Let&#39;s now calculate the loss for our current predictions again using our new function. . loss = calc_loss(coeffs, t_indep, t_dep) loss . tensor(0.5382, grad_fn=&lt;MeanBackward0&gt;) . We can now ask Pytorch to calculate our gradients now using backward(). . loss.backward() . Let&#39;s have a look at the gradients calculated for our model coefficients. . coeffs.grad . tensor([-0.0106, 0.0129, -0.0041, -0.0484, 0.2099, -0.2132, -0.1212, -0.0247, 0.1425, -0.1886, -0.0191, 0.2043]) . These gradients tell us how much we need to change each model coefficient to reduce the loss function i.e. to improve the predictions. . So putting these steps together: . loss = calc_loss(coeffs, t_indep, t_dep) loss.backward() coeffs.grad . tensor([-0.0212, 0.0258, -0.0082, -0.0969, 0.4198, -0.4265, -0.2424, -0.0494, 0.2851, -0.3771, -0.0382, 0.4085]) . We can see our gradient values have doubled? this ie because every time backward() is called it adds the new gradients to the previous ones. We don&#39;t want this, as we only want the gradients that pertain to the current model coefficients, not the previous ones. . So what we really want to do is reset the gradient values to zero after each step of the gradient descent process. . Lets define some code to put this all together, and print our current loss value. . # Calculate loss loss = calc_loss(coeffs, t_indep, t_dep) # Calculate gradients of linear model e.g. coeffs * inputs loss.backward() # Don&#39;t calculate any gradients here with torch.no_grad(): # Subtract the gradients from the model coeffcients to improve them, but scale this update by 0.1 called the &#39;learning rate&#39; coeffs.sub_(coeffs.grad * 0.1) # Set gradients to zero coeffs.grad.zero_() # Print current loss print(calc_loss(coeffs, t_indep, t_dep)) . tensor(0.4945) . The learning rate i used to ensure we take small steps of improvement for the cofficients, rather than big steps. To better understand why and how gradient decent works in more detail this article explains the fundamentals of GD. And this article as well as this one explain more the mathematics of GD. . Training the Linear Model . Before we can train our model we need to split our data into training and validation sets. We can use RandomSplitter() to do this. . trn_split,val_split=RandomSplitter(seed=42)(df) . trn_indep,val_indep = t_indep[trn_split],t_indep[val_split] trn_dep,val_dep = t_dep[trn_split],t_dep[val_split] len(trn_indep),len(val_indep) . (713, 178) . We&#39;ll also create functions for the three things we did manually above: updating coeffs, doing one full gradient descent step, and initilising coeffs to random numbers. . def update_coeffs(coeffs, lr): coeffs.sub_(coeffs.grad * lr) coeffs.grad.zero_() def one_epoch(coeffs, lr): loss = calc_loss(coeffs, trn_indep, trn_dep) loss.backward() with torch.no_grad(): update_coeffs(coeffs, lr) print(f&quot;{loss:.3f}&quot;, end=&quot;; &quot;) def init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_() . Let&#39;s now create a function do train the model. We will initialise the model coefficients to random values, then loop through one epoch to calculate the loss and gradients, and update the coefficients. An epoch is the model generating precdictions for the entire training dataet. So the training process is multiple epochs/loops over the training data, updating the model coefficients in each loop. This is the gradient descent algorithm. . def train_model(epochs=30, lr=0.01): torch.manual_seed(442) coeffs = init_coeffs() for i in range(epochs): one_epoch(coeffs, lr=lr) return coeffs . Lets choose a learning rate of 0.2 and train our model for 18 epochs. What we hope to see is out loss value go down in each epoch, as the model coefficients are updated to get better and improve the predictions. . coeffs = train_model(18, lr=0.2) . 0.536; 0.502; 0.477; 0.454; 0.431; 0.409; 0.388; 0.367; 0.349; 0.336; 0.330; 0.326; 0.329; 0.304; 0.314; 0.296; 0.300; 0.289; . We can see here as expected, the loss is going down and the predictions are improving with each epoch. . This means that the model coefficients for each of the input variables is getting better, or more accurate. Lets have a look at the improved coefficients so far. . def show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False))) show_coeffs() . {&#39;Age&#39;: tensor(-0.2694), &#39;SibSp&#39;: tensor(0.0901), &#39;Parch&#39;: tensor(0.2359), &#39;LogFare&#39;: tensor(0.0280), &#39;Sex_male&#39;: tensor(-0.3990), &#39;Sex_female&#39;: tensor(0.2345), &#39;Pclass_1&#39;: tensor(0.7232), &#39;Pclass_2&#39;: tensor(0.4112), &#39;Pclass_3&#39;: tensor(0.3601), &#39;Embarked_C&#39;: tensor(0.0955), &#39;Embarked_Q&#39;: tensor(0.2395), &#39;Embarked_S&#39;: tensor(0.2122)} . Checking Model Accuracy . So the loss value is giving us a good indication of how well our model is improving. But it&#39;s not perhaps what we want as our ultimate measure of the model performance. For the kaggle competition, the desire measure of performance is accuracy i.e. . Accuracy = Correct Predictions / Total Predictions . Lets first get the predictions. . preds = calc_preds(coeffs, val_indep) . We want a simple category of True if the passenger died, and False if they survived. To convert our predictions into these values we will use a threshold of 0.5 to decide which converts to which. . results = val_dep.bool()==(preds&gt;0.5) results[:16] . tensor([ True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, False]) . Let&#39;s now calculate the accuracy. . def acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)&gt;0.5)).float().mean() acc(coeffs) . tensor(0.7865) . Improving Model Predictions with a Sigmoid Function . If we look at our predictions, they could easily have values bigger that 1 or less than zero. . preds[:28] . tensor([ 0.8160, 0.1295, -0.0148, 0.1831, 0.1520, 0.1350, 0.7279, 0.7754, 0.3222, 0.6740, 0.0753, 0.0389, 0.2216, 0.7631, 0.0678, 0.3997, 0.3324, 0.8278, 0.1078, 0.7126, 0.1023, 0.3627, 0.9937, 0.8050, 0.1153, 0.1455, 0.8652, 0.3425]) . We want these predictions to be only from 0-1. If we pass these predictions through a sigmoid function that will achieve this. . sympy.plot(&quot;1/(1+exp(-x))&quot;, xlim=(-5,5)); . Let&#39;s now improve our predictions function using this. . def calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1)) . And now lets train the model again. . coeffs = train_model(lr=100) . 0.510; 0.327; 0.294; 0.207; 0.201; 0.199; 0.198; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; . This has really improved the loss which is falling much more. Let&#39;s check the accuracy. . acc(coeffs) . tensor(0.8258) . This has also improved a lot. . Lets look at the model coefficients. . show_coeffs() . {&#39;Age&#39;: tensor(-1.5061), &#39;SibSp&#39;: tensor(-1.1575), &#39;Parch&#39;: tensor(-0.4267), &#39;LogFare&#39;: tensor(0.2543), &#39;Sex_male&#39;: tensor(-10.3320), &#39;Sex_female&#39;: tensor(8.4185), &#39;Pclass_1&#39;: tensor(3.8389), &#39;Pclass_2&#39;: tensor(2.1398), &#39;Pclass_3&#39;: tensor(-6.2331), &#39;Embarked_C&#39;: tensor(1.4771), &#39;Embarked_Q&#39;: tensor(2.1168), &#39;Embarked_S&#39;: tensor(-4.7958)} . Do these values make sense? these coefficients suggest what are the most important features useful for predicting survival. We can see that Sex_male has a big negative value, which implies a negative association. We can also see age is negatively associated. Taken together, these two coefficients suggest that males and older people were less likely to survive the titantic disaster. . Improving the Maths - Using Matrix Multiplications . Is there a way we can improve the calculations to make things more efficient? if we look again at the biggest calculation to make predictions. . (val_indep*coeffs).sum(axis=1) . tensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3512, -13.6469, 3.6248, 5.3429, -22.0878, 3.1233, -21.8742, -15.6421, -21.5504, 3.9393, -21.9190, -12.0010, -12.3775, 5.3550, -13.5880, -3.1015, -21.7237, -12.2081, 12.9767, 4.7427, -21.6525, -14.9135, -2.7433, -12.3210, -21.5886, 3.9387, 5.3890, -3.6196, -21.6296, -21.8454, 12.2159, -3.2275, -12.0289, 13.4560, -21.7230, -3.1366, -13.2462, -21.7230, -13.6831, 13.3092, -21.6477, -3.5868, -21.6854, -21.8316, -14.8158, -2.9386, -5.3103, -22.2384, -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818, -5.4439, -21.7407, -12.6551, -21.6671, 4.9238, -11.5777, -13.3323, -21.9638, -15.3030, 5.0243, -21.7614, 3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652, -13.2382, -13.7599, -13.2170, 13.1347, -21.7049, -21.7268, 4.9207, -7.3198, -5.3081, 7.1065, 11.4948, -13.3135, -21.8723, -21.7230, 13.3603, -15.5670, 3.4105, -7.2857, -13.7197, 3.6909, 3.9763, -14.7227, -21.8268, 3.9387, -21.8743, -21.8367, -11.8518, -13.6712, -21.8299, 4.9440, -5.4471, -21.9666, 5.1333, -3.2187, -11.6008, 13.7920, -21.7230, 12.6369, -3.7268, -14.8119, -22.0637, 12.9468, -22.1610, -6.1827, -14.8119, -3.2838, -15.4540, -11.6950, -2.9926, -3.0110, -21.5664, -13.8268, 7.3426, -21.8418, 5.0744, 5.2582, 13.3415, -21.6289, -13.9898, -21.8112, -7.3316, 5.2296, -13.4453, 12.7891, -22.1235, -14.9625, -3.4339, 6.3089, -21.9839, 3.1968, 7.2400, 2.8558, -3.1187, 3.7965, 5.4667, -15.1101, -15.0597, -22.9391, -21.7230, -3.0346, -13.5206, -21.7011, 13.4425, -7.2690, -21.8335, -12.0582, 13.0489, 6.7993, 5.2160, 5.0794, -12.6957, -12.1838, -3.0873, -21.6070, 7.0744, -21.7170, -22.1001, 6.8159, -11.6002, -21.6310]) . So we are multiplying elements together then summing accross rows. This is identical to the linear algebra operation of a matrix-vector product. This operation has been implemented in Pytorch and uses the &#39;@&#39; symbol, so we can write the above in a simpler way as: . val_indep@coeffs . tensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3511, -13.6468, 3.6248, 5.3429, -22.0878, 3.1233, -21.8742, -15.6421, -21.5504, 3.9393, -21.9190, -12.0010, -12.3775, 5.3550, -13.5880, -3.1015, -21.7237, -12.2081, 12.9767, 4.7427, -21.6525, -14.9135, -2.7433, -12.3210, -21.5886, 3.9387, 5.3890, -3.6196, -21.6296, -21.8454, 12.2159, -3.2275, -12.0289, 13.4560, -21.7230, -3.1366, -13.2462, -21.7230, -13.6831, 13.3092, -21.6477, -3.5868, -21.6854, -21.8316, -14.8158, -2.9386, -5.3103, -22.2384, -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818, -5.4439, -21.7407, -12.6551, -21.6671, 4.9238, -11.5777, -13.3323, -21.9638, -15.3030, 5.0243, -21.7614, 3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652, -13.2382, -13.7599, -13.2170, 13.1347, -21.7049, -21.7268, 4.9207, -7.3198, -5.3081, 7.1065, 11.4948, -13.3135, -21.8723, -21.7230, 13.3603, -15.5670, 3.4105, -7.2857, -13.7197, 3.6909, 3.9763, -14.7227, -21.8268, 3.9387, -21.8743, -21.8367, -11.8518, -13.6712, -21.8299, 4.9440, -5.4471, -21.9666, 5.1333, -3.2187, -11.6008, 13.7920, -21.7230, 12.6369, -3.7268, -14.8119, -22.0637, 12.9468, -22.1610, -6.1827, -14.8119, -3.2838, -15.4540, -11.6950, -2.9926, -3.0110, -21.5664, -13.8268, 7.3426, -21.8418, 5.0744, 5.2582, 13.3415, -21.6289, -13.9898, -21.8112, -7.3316, 5.2296, -13.4453, 12.7891, -22.1235, -14.9625, -3.4339, 6.3089, -21.9839, 3.1968, 7.2400, 2.8558, -3.1187, 3.7965, 5.4667, -15.1101, -15.0597, -22.9391, -21.7230, -3.0346, -13.5206, -21.7011, 13.4425, -7.2690, -21.8335, -12.0582, 13.0489, 6.7993, 5.2160, 5.0794, -12.6957, -12.1838, -3.0873, -21.6070, 7.0744, -21.7170, -22.1001, 6.8159, -11.6002, -21.6310]) . Not only is this simpler, but matrix-vector products in PyTorch have been highly optimised to make them much faster. So not only is the code for this more compact, this actually runs much faster than using the normal multiplication and sum. . Let&#39;s update our predictions function with this. . def calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs) . Creating a Neural Network Model . We will now transition to creating a simple neural network model, which will build on what we have used to make our linear model. . For this type of model we will need to perform matrix-matrix products and to do this we will need to turn the coefficients into a column vector i.e. a matrix with a single column which we can do by passing a second argument 1 to torch.rand(), indicating that we want our coefficients to have one column. . def init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_() . We&#39;ll also need to turn our dependent variable into a column vector, which we can do by indexing the column dimension with the special value None, which tells PyTorch to add a new dimension in this position: . trn_dep = trn_dep[:,None] val_dep = val_dep[:,None] . We can now train our model as before and confirm we get identical outputs... . coeffs = train_model(lr=100) . 0.512; 0.323; 0.290; 0.205; 0.200; 0.198; 0.197; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; . ...and identical accuracy: . acc(coeffs) . tensor(0.8258) . So what is a Neural Network? In simple terms . Artificial neural networks (ANNs), usually simply called neural networks (NNs) or neural nets are computing systems inspired by the biological neural networks that constitute animal brains. An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain . One key difference between Neural Networks (NN) and Linear Regression (LR), is that while LR has model parameters/coefficients one for each input variable, NN&#39;s have many model parameters, many of which do not correspond to specific input variables which are often called &#39;hidden layers&#39;. . You can read more about Neural Networks here. . To create a Neural Network we&#39;ll need to create coefficients for each of our layers. Our first set of coefficients will take our n_coeff inputs, and create n_hidden outputs for our hidden layers. We can choose whatever n_hidden we like -- a higher number gives our network more flexibility, but makes it slower and harder to train. So we need a matrix of size n_coeff by n_hidden. We&#39;ll divide these coefficients by n_hidden so that when we sum them up in the next layer we&#39;ll end up with similar magnitude numbers to what we started with. . Then our second layer will need to take the n_hidden inputs and create a single output, so that means we need a n_hidden by 1 matrix there. The second layer will also need a constant term added. . def init_coeffs(n_hidden=20): layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden layer2 = torch.rand(n_hidden, 1)-0.3 const = torch.rand(1)[0] return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_() . Now we have our coefficients, we can create our neural net. The key steps are the two matrix products, indeps@l1 and res@l2 (where res is the output of the first layer). The first layer output is passed to F.relu (that&#39;s our non-linearity), and the second is passed to torch.sigmoid as before. . def calc_preds(coeffs, indeps): l1,l2,const = coeffs res = F.relu(indeps@l1) res = res@l2 + const return torch.sigmoid(res) . Finally, now that we have more than one set of coefficients, we need to add a loop to update each one: . def update_coeffs(coeffs, lr): for layer in coeffs: layer.sub_(layer.grad * lr) layer.grad.zero_() . Let&#39;s train our model. . coeffs = train_model(lr=1.4) . 0.543; 0.532; 0.520; 0.505; 0.487; 0.466; 0.439; 0.407; 0.373; 0.343; 0.319; 0.301; 0.286; 0.274; 0.264; 0.256; 0.250; 0.245; 0.240; 0.237; 0.234; 0.231; 0.229; 0.227; 0.226; 0.224; 0.223; 0.222; 0.221; 0.220; . coeffs = train_model(lr=20) . 0.543; 0.400; 0.260; 0.390; 0.221; 0.211; 0.197; 0.195; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; . acc(coeffs) . tensor(0.8258) . In this case our neural net isn&#39;t showing better results than the linear model. That&#39;s not surprising; this dataset is very small and very simple, and isn&#39;t the kind of thing we&#39;d expect to see neural networks excel at. Furthermore, our validation set is too small to reliably see much accuracy difference. But the key thing is that we now know exactly what a real neural net looks like, and can see how it relates to a linear regression model. . Creating a Deep Learning Model . The neural net in the previous section only uses one hidden layer, so it doesn&#39;t count as &quot;deep&quot; learning. But we can use the exact same technique to make our neural net deep, by adding more &#39;hidden layers&#39;. . First, we&#39;ll need to create additional coefficients for each layer: . def init_coeffs(): hiddens = [10, 10] # &lt;-- set this to the size of each hidden layer you want sizes = [n_coeff] + hiddens + [1] n = len(sizes) layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)] consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)] for l in layers+consts: l.requires_grad_() return layers,consts . You&#39;ll notice here that there&#39;s a lot of messy constants to get the random numbers in just the right ranges. When we train the model in a moment, you&#39;ll see that the tiniest changes to these initialisations can cause our model to fail to train at all. . This is a key reason that deep learning failed to make much progress in the early days - it&#39;s very finicky to get a good starting point for our coefficients. Nowadays, we have better ways to deal with that. . Our deep learning calc_preds looks much the same as before, but now we loop through each layer, instead of listing them separately: . def calc_preds(coeffs, indeps): layers,consts = coeffs n = len(layers) res = indeps for i,l in enumerate(layers): res = res@l + consts[i] if i!=n-1: res = F.relu(res) return torch.sigmoid(res) . We also need a minor update to update_coeffs since we&#39;ve got layers and consts separated now: . def update_coeffs(coeffs, lr): layers,consts = coeffs for layer in layers+consts: layer.sub_(layer.grad * lr) layer.grad.zero_() . Let&#39;s train our model... . coeffs = train_model(lr=4) . 0.521; 0.483; 0.427; 0.379; 0.379; 0.379; 0.379; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.377; 0.376; 0.371; 0.333; 0.239; 0.224; 0.208; 0.204; 0.203; 0.203; 0.207; 0.197; 0.196; 0.195; . acc(coeffs) . tensor(0.8258) . The &quot;real&quot; deep learning models that are used in research and industry look very similar to this, and in fact if you look inside the source code of any deep learning model you&#39;ll recognise the basic steps are the same. . The biggest differences in practical models to what we have above are: . How initialisation and normalisation is done to ensure the model trains correctly every time | Regularization (to avoid over-fitting) | Modifying the neural net itself to take advantage of knowledge of the problem domain | Doing gradient descent steps on smaller batches, rather than the whole dataset | .",
            "url": "https://www.livingdatalab.com/fastai/fastai-2022/deep-learning/mathematics/2022/12/17/machine-learning-to-deep-learning-from-scratch.html",
            "relUrl": "/fastai/fastai-2022/deep-learning/mathematics/2022/12/17/machine-learning-to-deep-learning-from-scratch.html",
            "date": " • Dec 17, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "US Patent Phrase to Phrase Matching",
            "content": "Introduction . In this series of articles I will be re-visiting the FastAI Practical Deep Learning for Coders course for this year 2022 which I have completed in previous years. This article covers lesson 4 of this years course, which I will use to create model that can associate short phrases with the correct US patent classification. . While this is based on a fastai training course, in this particular project we will not actually be using the fastai library, we will be using the Hugging Face Transformers Library which is a python library of state of the art deep learning models, including the very powerful transformers model architecture behind so many of the recent advances in AI. Fastai does also integrate transfomer models as well. . First we will import the required libraries. . Import Libraries . import pandas as pd import numpy as np from datasets import Dataset,DatasetDict import datasets from transformers import AutoModelForSequenceClassification,AutoTokenizer,TrainingArguments,Trainer . The Project: US Patent Phrase to Phrase Matching . The U.S. Patent and Trademark Office (USPTO) offers one of the largest repositories of scientific, technical, and commercial information in the world through its Open Data Portal. Patents are a form of intellectual property granted in exchange for the public disclosure of new and useful inventions. Because patents undergo an intensive vetting process prior to grant, and because the history of U.S. innovation spans over two centuries and 11 million patents, the U.S. patent archives stand as a rare combination of data volume, quality, and diversity. . In this project, I will train a model on a novel semantic similarity dataset to extract relevant information by matching key phrases in patent documents. Determining the semantic similarity between phrases is critically important during the patent search and examination process to determine if an invention has been described before. . For example, if one invention claims &quot;television set&quot; and a prior publication describes &quot;TV set&quot;, a model would ideally recognize these are the same and assist a patent attorney or examiner in retrieving relevant documents. This extends beyond paraphrase identification; if one invention claims a &quot;strong material&quot; and another uses &quot;steel&quot;, that may also be a match. What counts as a &quot;strong material&quot; varies per domain (it may be steel in one domain and ripstop fabric in another, but you wouldn&#39;t want your parachute made of steel). . We will seek to build a model to match phrases in order to extract contextual information, which could help the patent community connect the dots between millions of patent documents. . Specifically, we will be comparing two words or short phrases, and scoring them based on whether they&#39;re similar or not, based on which patent class they were used in. With a score of 1 it is considered that the two inputs have identical meaning, and 0 means they have totally different meaning. For instance, abatement and eliminating process have a score of 0.5, meaning they&#39;re somewhat similar, but not identical. . It turns out that this can be represented as a classification problem. How? By representing the question like this: . For the following text...:&quot;TEXT1: abatement; TEXT2: eliminating process&quot; ...chose a category of meaning similarity: &quot;Different; Similar; Identical&quot;. In this project we&#39;ll see how to solve the Patent Phrase Matching problem by treating it as a classification task, by representing it in a very similar way to that shown above. . The dataset comes from this kaggle project. . Get Data . Let&#39;s first download and extract our data. . !unzip us-patent-phrase-to-phrase-matching.zip !ls . Archive: us-patent-phrase-to-phrase-matching.zip inflating: sample_submission.csv inflating: test.csv inflating: train.csv drive sample_submission.csv train.csv sample_data test.csv us-patent-phrase-to-phrase-matching.zip . df = pd.read_csv(&#39;train.csv&#39;) df.head() . id anchor target context score . 0 37d61fd2272659b1 | abatement | abatement of pollution | A47 | 0.50 | . 1 7b9652b17b68b7a4 | abatement | act of abating | A47 | 0.75 | . 2 36d72442aefd8232 | abatement | active catalyst | A47 | 0.25 | . 3 5296b0c19e1ce60e | abatement | eliminating process | A47 | 0.50 | . 4 54c1e3b9184cb5b6 | abatement | forest region | A47 | 0.00 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; The dataset description gives a clearer idea of what these different fields mean. . For example: . id - a unique identifier for a pair of phrases | anchor - the first phrase | target - the second phrase | context - the CPC classification (version 2021.05), which indicates the subject within which the similarity is to be scored | score - the similarity. This is sourced from a combination of one or more manual expert ratings. | . Lets generate some basic summary stats for each field. . df.describe(include=&#39;object&#39;) . id anchor target context . count 36473 | 36473 | 36473 | 36473 | . unique 36473 | 733 | 29340 | 106 | . top 37d61fd2272659b1 | component composite coating | composition | H01 | . freq 1 | 152 | 24 | 2186 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We can see that we have far fewer anchors than targets, and that some of these anchors are very common for example &#39;component composite coating&#39; is associated with 152 different targets. . It was suggested earlier that we could represent the input to the model as something like &quot;TEXT1: abatement; TEXT2: eliminating process&quot;. We&#39;ll need to add the context to this too. In Pandas, we just use + to concatenate, like so: . df[&#39;input&#39;] = &#39;TEXT1: &#39; + df.context + &#39;; TEXT2: &#39; + df.target + &#39;; ANC1: &#39; + df.anchor df[&#39;input&#39;].head() . 0 TEXT1: A47; TEXT2: abatement of pollution; ANC... 1 TEXT1: A47; TEXT2: act of abating; ANC1: abate... 2 TEXT1: A47; TEXT2: active catalyst; ANC1: abat... 3 TEXT1: A47; TEXT2: eliminating process; ANC1: ... 4 TEXT1: A47; TEXT2: forest region; ANC1: abatement Name: input, dtype: object . Text Data Transformation . The Hugging Face transformers library uses the Dataset object to store data, lets create one for our data. . ds = Dataset.from_pandas(df) ds . Dataset({ features: [&#39;id&#39;, &#39;anchor&#39;, &#39;target&#39;, &#39;context&#39;, &#39;score&#39;, &#39;input&#39;], num_rows: 36473 }) . So we have our text data, but there is a problem. Machine learning and AI models don&#39;t actually understand text! They can only understand numbers. So we need a way to convert our text data into a numerical representation. . The branch of machine learning and AI concerned with understanding language is called Natural Language Processing or NLP. In NLP we prepare text data for machine learning by converting it into numbers, two common steps are followed: . Tokenization: Split each text up into words (or actually, as we&#39;ll see, into tokens) | Numericalization: Convert each word (or token) into a number. | . The details about how this is done actually depends on the particular model we use. So first we&#39;ll need to pick a model. There are thousands of models available, but a reasonable starting point for nearly any NLP problem is to use a smaller model, then working up to a bigger model later. . Why? It&#39;s true that in deep learning and AI, a larger model generally does better than a smaller model. However a smaller model is quicker to train and experiment with multiple times which is better when we are just trying things out at the start and need to iterate rapidly, and can give an idea of some kind of baseline we can expect to improve on with a bigger model. . We will use this small model. . model_nm = &#39;microsoft/deberta-v3-small&#39; . AutoTokenizer will create a tokenizer appropriate for a given model: . tokz = AutoTokenizer.from_pretrained(model_nm) . Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. /usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text. warnings.warn( Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. . Here&#39;s an example of how the tokenizer splits a text into &quot;tokens&quot; (which are like words, but can be sub-word pieces, as you see below): . tokz.tokenize(&quot;Hi my name is Pranath !&quot;) . [&#39;▁Hi&#39;, &#39;▁my&#39;, &#39;▁name&#39;, &#39;▁is&#39;, &#39;▁Prana&#39;, &#39;th&#39;, &#39;▁!&#39;] . Uncommon words will be split into pieces. The start of a new word is represented by ▁: . tokz.tokenize(&quot;A platypus is an ornithorhynchus anatinus.&quot;) . [&#39;▁A&#39;, &#39;▁platypus&#39;, &#39;▁is&#39;, &#39;▁an&#39;, &#39;▁or&#39;, &#39;ni&#39;, &#39;tho&#39;, &#39;rhynch&#39;, &#39;us&#39;, &#39;▁an&#39;, &#39;at&#39;, &#39;inus&#39;, &#39;.&#39;] . Here&#39;s a simple function which tokenizes our inputs: . def tok_func(x): return tokz(x[&quot;input&quot;]) tok_ds = ds.map(tok_func, batched=True) . This adds a new item to our dataset called input_ids. For instance, here is the input and IDs for the first row of our data: . row = tok_ds[0] row[&#39;input&#39;], row[&#39;input_ids&#39;] . (&#39;TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement&#39;, [1, 54453, 435, 294, 336, 5753, 346, 54453, 445, 294, 47284, 265, 6435, 346, 23702, 435, 294, 47284, 2]) . So, what are those IDs and where do they come from? The secret is that there&#39;s a list called vocab in the tokenizer which contains a unique integer for every possible token string. We can look them up like this, for instance to find the token for the word &quot;of&quot;: . tokz.vocab[&#39;▁of&#39;] . 265 . Looking above at our input IDs, we see that 265 appears as expected. . Finally, we need to prepare our labels. Transformers always assumes that your labels has the column name labels, but in our dataset it&#39;s currently called score. Therefore, we need to rename it: . tok_ds = tok_ds.rename_columns({&#39;score&#39;:&#39;labels&#39;}) . Now that we&#39;ve prepared our tokens and labels, we need to create our validation set. . Test and Validation Sets . You may have noticed that our directory contained another file for our test set. . eval_df = pd.read_csv(&#39;test.csv&#39;) eval_df.describe() . id anchor target context . count 36 | 36 | 36 | 36 | . unique 36 | 34 | 36 | 29 | . top 4112d61851461f60 | el display | inorganic photoconductor drum | G02 | . freq 1 | 2 | 1 | 3 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Transformers uses a DatasetDict for holding your training and validation sets. To create one that contains 25% of our data for the validation set, and 75% for the training set, we use train_test_split: . dds = tok_ds.train_test_split(0.25, seed=42) dds . DatasetDict({ train: Dataset({ features: [&#39;id&#39;, &#39;anchor&#39;, &#39;target&#39;, &#39;context&#39;, &#39;labels&#39;, &#39;input&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 27354 }) test: Dataset({ features: [&#39;id&#39;, &#39;anchor&#39;, &#39;target&#39;, &#39;context&#39;, &#39;labels&#39;, &#39;input&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 9119 }) }) . As you see above, the validation set here is called test and not validate, so we need to be careful we don&#39;t confuse ourselves with terminology! . We will use the separate test set at the end to check our predictions, whereas the validation set will be used during the model training to check our progress. . We&#39;ll use eval as our name for the test set, to avoid confusion with the test dataset that was created above. . eval_df[&#39;input&#39;] = &#39;TEXT1: &#39; + eval_df.context + &#39;; TEXT2: &#39; + eval_df.target + &#39;; ANC1: &#39; + eval_df.anchor eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True) . Model Training . To train our model we need to pick a batch size that fits our GPU, and small number of epochs so we can run experiments quickly. . bs = 128 epochs = 4 lr = 8e-5 . The most important hyperparameter for model training is the learning rate. Fastai provides a learning rate finder to help you figure this out, but Hugging Face Transformers doesn&#39;t, so we just have to use trial and error. The idea is to find the largest value you can, but which doesn&#39;t result in training failing. . We will also need to define some functions for our model metric, which is how we measure how well our model is performing. For this we will be using Pearsons Correlation Coefficient as a measure of similarity between the anchor and target texts. . def corr(x,y): return np.corrcoef(x,y)[0][1] def corr_d(eval_pred): return {&#39;pearson&#39;: corr(*eval_pred)} . Transformers uses the TrainingArguments class to set up model training hyper-parameter arguments. . args = TrainingArguments(&#39;outputs&#39;, learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type=&#39;cosine&#39;, fp16=True, evaluation_strategy=&quot;epoch&quot;, per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2, num_train_epochs=epochs, weight_decay=0.01, report_to=&#39;none&#39;) . We can now create our model, and Trainer, which is a class which combines the data and model together (just like Learner in fastai): . model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1) trainer = Trainer(model, args, train_dataset=dds[&#39;train&#39;], eval_dataset=dds[&#39;test&#39;], tokenizer=tokz, compute_metrics=corr_d) . Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: [&#39;mask_predictions.dense.bias&#39;, &#39;mask_predictions.LayerNorm.bias&#39;, &#39;lm_predictions.lm_head.dense.bias&#39;, &#39;lm_predictions.lm_head.bias&#39;, &#39;lm_predictions.lm_head.LayerNorm.weight&#39;, &#39;lm_predictions.lm_head.dense.weight&#39;, &#39;mask_predictions.dense.weight&#39;, &#39;mask_predictions.classifier.bias&#39;, &#39;mask_predictions.LayerNorm.weight&#39;, &#39;mask_predictions.classifier.weight&#39;, &#39;lm_predictions.lm_head.LayerNorm.bias&#39;] - This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: [&#39;pooler.dense.bias&#39;, &#39;classifier.weight&#39;, &#39;pooler.dense.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Using cuda_amp half precision backend . Let&#39;s train our model! . trainer.train(); . The following columns in the training set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input, context, anchor, target, id. If input, context, anchor, target, id are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running training ***** Num examples = 27354 Num Epochs = 4 Instantaneous batch size per device = 128 Total train batch size (w. parallel, distributed &amp; accumulation) = 128 Gradient Accumulation steps = 1 Total optimization steps = 856 Number of trainable parameters = 141895681 . . [856/856 03:39, Epoch 4/4] Epoch Training Loss Validation Loss Pearson . 1 | No log | 0.023299 | 0.827306 | . 2 | No log | 0.022970 | 0.831413 | . 3 | 0.014000 | 0.022094 | 0.831611 | . 4 | 0.014000 | 0.022278 | 0.831688 | . &lt;/div&gt; &lt;/div&gt; The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input, context, anchor, target, id. If input, context, anchor, target, id are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 9119 Batch size = 256 . . [215/856 00:52 &lt; 02:37, 4.08 it/s, Epoch 1/4] Epoch Training Loss Validation Loss . . [36/36 03:48] &lt;/div&gt; &lt;/div&gt; The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input, context, anchor, target, id. If input, context, anchor, target, id are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 9119 Batch size = 256 Saving model checkpoint to outputs/checkpoint-500 Configuration saved in outputs/checkpoint-500/config.json Model weights saved in outputs/checkpoint-500/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input, context, anchor, target, id. If input, context, anchor, target, id are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 9119 Batch size = 256 The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input, context, anchor, target, id. If input, context, anchor, target, id are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 9119 Batch size = 256 Training completed. Do not forget to share your model on huggingface.co/models =) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Lots of warning messages from Transformers -- we can ignore these. . The key thing to look at is the &quot;Pearson&quot; value in table above. As we can see, it&#39;s increasing, and is already above 0.8. It looks like we have a model that can predict with high accuracy for these patent text phrases. . Generate Predictions for US Patent Phrases . Let&#39;s get some predictions on the test set. . preds = trainer.predict(eval_ds).predictions.astype(float) preds . The following columns in the test set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input, context, anchor, target, id. If input, context, anchor, target, id are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Prediction ***** Num examples = 36 Batch size = 256 . array([[ 5.01464844e-01], [ 6.09863281e-01], [ 6.35742188e-01], [ 2.67578125e-01], [-2.59160995e-04], [ 5.31738281e-01], [ 4.78515625e-01], [-4.77981567e-03], [ 2.24121094e-01], [ 1.07910156e+00], [ 2.25463867e-01], [ 2.15087891e-01], [ 7.56347656e-01], [ 8.77929688e-01], [ 7.44628906e-01], [ 3.58642578e-01], [ 2.76855469e-01], [-7.08770752e-03], [ 6.49414062e-01], [ 3.75488281e-01], [ 4.80468750e-01], [ 2.20336914e-01], [ 2.38159180e-01], [ 1.93481445e-01], [ 5.60546875e-01], [ 1.14746094e-02], [-7.29751587e-03], [-9.97924805e-03], [-8.94165039e-03], [ 6.04492188e-01], [ 3.15673828e-01], [ 1.96685791e-02], [ 7.78808594e-01], [ 4.83886719e-01], [ 4.22363281e-01], [ 1.96655273e-01]]) . Looking at these predictions something is not quite right. The Pearson&#39;s correlation coefficient should have a value (for our case) between 0 and 1, but some values of our predictions are less than zero and bigger than 1. . This once again shows the value of remembering to actually look at your data. Let&#39;s fix those out-of-bounds predictions: . preds = np.clip(preds, 0, 1) preds . array([[0.50146484], [0.60986328], [0.63574219], [0.26757812], [0. ], [0.53173828], [0.47851562], [0. ], [0.22412109], [1. ], [0.22546387], [0.21508789], [0.75634766], [0.87792969], [0.74462891], [0.35864258], [0.27685547], [0. ], [0.64941406], [0.37548828], [0.48046875], [0.22033691], [0.23815918], [0.19348145], [0.56054688], [0.01147461], [0. ], [0. ], [0. ], [0.60449219], [0.31567383], [0.01966858], [0.77880859], [0.48388672], [0.42236328], [0.19665527]]) . We now have our predictions for the patent phrase pairs which should have a high accruacy from our results. . &lt;/div&gt; .",
            "url": "https://www.livingdatalab.com/fastai/fastai-2022/deep-learning/natural-language-processing/2022/12/10/us-patent-phrase-to-phrase-matching.html",
            "relUrl": "/fastai/fastai-2022/deep-learning/natural-language-processing/2022/12/10/us-patent-phrase-to-phrase-matching.html",
            "date": " • Dec 10, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Using AI to Identify Galaxies",
            "content": "Introduction . In this series of articles I will be re-visiting the FastAI Practical Deep Learning for Coders for this year 2022. . This article covers lesson 1 of this years course, which I will use to create model that can identify different types of galaxies. I will also highlight some notable differences from earlier versions of the fastai course and library. . First we will import the required libraries. . Import Libraries . from duckduckgo_search import ddg_images from fastdownload import download_url from fastcore.all import * from fastai.vision.all import * . The first notable difference from earlier versions of fastai is that its now much easier to download images from a search engine to create a dataset from, by default this uses the search engine duck duck go. Lets define a short function that will gather images for us. . def search_images(term, max_images=30): print(f&quot;Searching for &#39;{term}&#39;&quot;) return L(ddg_images(term, max_results=max_images)).itemgot(&#39;image&#39;) . The Project: Recognise Spiral vs Irregular Galaxies . Two of the main types of galaxies are spiral and irregular galaxies. Lets use our previous function to first download some examples of spiral galaxy images to see what they look like. . urls = search_images(&#39;spiral galaxy photos&#39;) . Let&#39;s now grab one of these images and have a look. . dest = &#39;spiral_galaxy.jpg&#39; download_url(urls[2], dest, show_progress=False) im = Image.open(dest) im.to_thumb(512,512) . So we can see spiral galaxies have a spiral structure to them, they are relatively flat and have distinctive arms, with a bulge concerntrated at the center. . Let&#39;s now download some irregular galaxies and have a look at one. . download_url(search_images(&#39;irregular galaxy photos&#39;)[3], &#39;irregular_galaxy.jpg&#39;, show_progress=False) Image.open(&#39;irregular_galaxy.jpg&#39;).to_thumb(512,512) . Searching for &#39;irregular galaxy photos&#39; . Irregular galaxies have no obvious structure, and are not flat like spiral galaxies. These are often some of the oldest galaxies in the universe, which were abundant in the early universe before spirals and other types of galaxies developed. . Download Galaxy Images . So it looks like our images correspond to the types of galaxy images we want, so we will now grab some examples of each to create our dataset. . searches = &#39;spiral galaxy&#39;,&#39;irregular galaxy&#39; path = Path(&#39;spiral_or_irregular&#39;) from time import sleep for o in searches: dest = (path/o) dest.mkdir(exist_ok=True, parents=True) download_images(dest, urls=search_images(f&#39;{o} photo&#39;)) sleep(10) # Pause between searches to avoid over-loading server resize_images(path/o, max_size=400, dest=path/o) . Searching for &#39;spiral galaxy photo&#39; Searching for &#39;irregular galaxy photo&#39; . Another nice new fastai feature is the ability to check the images we have download have valid paths and delete any that are not valid images. . failed = verify_images(get_image_files(path)) failed.map(Path.unlink) len(failed) . 0 . Create Dataset . We will now create a DataLoader object using the DataBlock object. This is very much the way it was done in fastai the last time i did this course. . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(192, method=&#39;squish&#39;)] ).dataloaders(path, bs=32) dls.show_batch(max_n=9) . We can see we have some nice examples of each type of galaxy. . Train Model . Now we have our data ready we can create our vision model and train it. We will train a ResNet18 model for just 3 epochs (or 3 complete passes over the entire dataset). . learn = vision_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(3) . epoch train_loss valid_loss error_rate time . 0 | 1.071076 | 0.766020 | 0.391304 | 00:00 | . epoch train_loss valid_loss error_rate time . 0 | 0.594808 | 0.279009 | 0.173913 | 00:00 | . 1 | 0.417826 | 0.361526 | 0.086957 | 00:00 | . 2 | 0.303060 | 0.362775 | 0.086957 | 00:00 | . Test Model . We will now test our model by picking an example image for each type of galaxy and see how well it can predict which type of galaxy it is. . dest = &#39;spiral_galaxy2.jpg&#39; download_url(urls[3], dest, show_progress=False) im = Image.open(dest) im.to_thumb(512,512) . is_spiral_galaxy,_,probs = learn.predict(PILImage.create(&#39;spiral_galaxy2.jpg&#39;)) print(f&quot;This is a: {is_spiral_galaxy}.&quot;) print(f&quot;Probability it&#39;s a spiral galaxy: {probs[1]:.4f}&quot;) . This is a: spiral galaxy. Probability it&#39;s a spiral galaxy: 0.9313 . download_url(search_images(&#39;irregular galaxy photos&#39;)[6], &#39;irregular_galaxy2.jpg&#39;, show_progress=False) Image.open(&#39;irregular_galaxy2.jpg&#39;).to_thumb(512,512) . Searching for &#39;irregular galaxy photos&#39; . is_irregular_galaxy,_,probs = learn.predict(PILImage.create(&#39;irregular_galaxy2.jpg&#39;)) print(f&quot;This is a: {is_irregular_galaxy}.&quot;) print(f&quot;Probability it&#39;s a irregular galaxy: {probs[0]:.4f}&quot;) . This is a: irregular galaxy. Probability it&#39;s a irregular galaxy: 0.8309 . After training the model for just 3 epochs the model has achieved an excellent accuracy, probably if it had trained for a few more epochs it would have had near perfect accuracy in correctly distingushing these 2 different types of galaxy. . Conclusion . It&#39;s worth stepping back for a moment just to appreciate how incredible this achievement is - with just a few lines of code, we have trained a model with around 31 million artifical neurons to recognise a galaxy with around 100 billion stars in a matter of a few seconds. . The fastai library just becomes easier and easier to use over time with continual improvements, automatically using the best methods and practices in deep learning in an easy to use library. . Lesson 2 of 2022 coming up ! .",
            "url": "https://www.livingdatalab.com/fastai/fastai-2022/deep-learning/2022/12/05/using-ai-to-identify-galaxies.html",
            "relUrl": "/fastai/fastai-2022/deep-learning/2022/12/05/using-ai-to-identify-galaxies.html",
            "date": " • Dec 5, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Predicting 10 Year Death Risk from Health Data",
            "content": "Introduction . In this article we will build a model to predict the 10-year risk of death of individuals from the NHANES I epidemiology dataset. . Topics we will cover will include: . Dealing with Missing Data Complete Case Analysis. | Imputation | . | Decision Trees Evaluation. | Regularization. | . | Random Forests Hyperparameter Tuning. | . | . Import Packages . shap is a library that explains predictions made by machine learning models. | sklearn is one of the most popular machine learning libraries. | itertools allows us to conveniently manipulate iterable objects such as lists. | pydotplus is used together with IPython.display.Image to visualize graph structures such as decision trees. | numpy is a fundamental package for scientific computing in Python. | pandas is what we&#39;ll use to manipulate our data. | seaborn is a plotting library which has some convenient functions for visualizing missing data. | matplotlib is a plotting library. | . import shap import sklearn import itertools import pydotplus import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from IPython.display import Image from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.experimental import enable_iterative_imputer from sklearn.impute import IterativeImputer, SimpleImputer # We&#39;ll also import some helper functions that will be useful later on. from util import load_data, cindex from public_tests import * . . The Dataset . This dataset contains various features of hospital patients as well as their outcomes, i.e. whether or not they died within 10 years. . X_dev, X_test, y_dev, y_test = load_data(10, &#39;data/NHANESI_subset_X.csv&#39;, &#39;data/NHANESI_subset_y.csv&#39;) . The dataset has been split into a development set (or dev set), which we will use to develop our risk models, and a test set, which we will use to test our models. . We further split the dev set into a training and validation set, respectively to train and tune our models, using a 75/25 split (note that we set a random state to make this split repeatable). . X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=0.25, random_state=10) . . Explore the Dataset . print(&quot;X_train shape: {}&quot;.format(X_train.shape)) X_train.head() . X_train shape: (5147, 18) . Age Diastolic BP Poverty index Race Red blood cells Sedimentation rate Serum Albumin Serum Cholesterol Serum Iron Serum Magnesium Serum Protein Sex Systolic BP TIBC TS White blood cells BMI Pulse pressure . 1599 43.0 | 84.0 | 637.0 | 1.0 | 49.3 | 10.0 | 5.0 | 253.0 | 134.0 | 1.59 | 7.7 | 1.0 | NaN | 490.0 | 27.3 | 9.1 | 25.803007 | 34.0 | . 2794 72.0 | 96.0 | 154.0 | 2.0 | 43.4 | 23.0 | 4.3 | 265.0 | 106.0 | 1.66 | 6.8 | 2.0 | 208.0 | 301.0 | 35.2 | 6.0 | 33.394319 | 112.0 | . 1182 54.0 | 78.0 | 205.0 | 1.0 | 43.8 | 12.0 | 4.2 | 206.0 | 180.0 | 1.67 | 6.6 | 2.0 | NaN | 363.0 | 49.6 | 5.9 | 20.278410 | 34.0 | . 6915 59.0 | 90.0 | 417.0 | 1.0 | 43.4 | 9.0 | 4.5 | 327.0 | 114.0 | 1.65 | 7.6 | 2.0 | NaN | 347.0 | 32.9 | 6.1 | 32.917744 | 78.0 | . 500 34.0 | 80.0 | 385.0 | 1.0 | 77.7 | 9.0 | 4.1 | 197.0 | 64.0 | 1.74 | 7.3 | 2.0 | NaN | 376.0 | 17.0 | 8.2 | 30.743489 | 30.0 | . Our targets y will be whether or not the target died within 10 years. . y_train.head(20) . 1599 False 2794 True 1182 False 6915 False 500 False 1188 True 9739 False 3266 False 6681 False 8822 False 5856 True 3415 False 9366 False 7975 False 1397 False 6809 False 9461 False 9374 False 1170 True 158 False Name: time, dtype: bool . i = 10 print(X_train.iloc[i,:]) print(&quot; nDied within 10 years? {}&quot;.format(y_train.loc[y_train.index[i]])) . Age 67.000000 Diastolic BP 94.000000 Poverty index 114.000000 Race 1.000000 Red blood cells 43.800000 Sedimentation rate 12.000000 Serum Albumin 3.700000 Serum Cholesterol 178.000000 Serum Iron 73.000000 Serum Magnesium 1.850000 Serum Protein 7.000000 Sex 1.000000 Systolic BP 140.000000 TIBC 311.000000 TS 23.500000 White blood cells 4.300000 BMI 17.481227 Pulse pressure 46.000000 Name: 5856, dtype: float64 Died within 10 years? True . . Dealing with Missing Data . Looking at our data in X_train, we see that some of the data is missing: some values in the output of the previous cell are marked as NaN (&quot;not a number&quot;). . Missing data is a common occurrence in data analysis, that can be due to a variety of reasons, such as measuring instrument malfunction, respondents not willing or not able to supply information, and errors in the data collection process. . Let&#39;s examine the missing data pattern. seaborn is an alternative to matplotlib that has some convenient plotting functions for data analysis. We can use its heatmap function to easily visualize the missing data pattern. . sns.heatmap(X_train.isnull(), cbar=False) plt.title(&quot;Training&quot;) plt.show() sns.heatmap(X_val.isnull(), cbar=False) plt.title(&quot;Validation&quot;) plt.show() . For each feature, represented as a column, values that are present are shown in black, and missing values are set in a light color. . From this plot, we can see that many values are missing for systolic blood pressure (Systolic BP). . We will write a function to compute the fraction of cases with missing data. This will help us decide how we handle this missing data in the future. . def fraction_rows_missing(df): &#39;&#39;&#39; Return percent of rows with any missing data in the dataframe. Input: df (dataframe): a pandas dataframe with potentially missing data Output: frac_missing (float): fraction of rows with missing data &#39;&#39;&#39; return sum(df.isnull().any(axis=1)) / len(df) . fraction_rows_missing_test(fraction_rows_missing, X_train, X_val, X_test) . Example dataframe: a b 0 NaN 1.0 1 1.0 NaN 2 1.0 0.0 3 NaN 1.0 Computed fraction missing: 0.75 Fraction of rows missing from X_train: 0.6986594132504371 Fraction of rows missing from X_val: 0.703962703962704 Fraction of rows missing from X_test: 0.0 All tests passed. . We see that our train and validation sets have missing values, but luckily our test set has complete cases. . As a first pass, we will begin with a complete case analysis, dropping all of the rows with any missing data. . X_train_dropped = X_train.dropna(axis=&#39;rows&#39;) y_train_dropped = y_train.loc[X_train_dropped.index] X_val_dropped = X_val.dropna(axis=&#39;rows&#39;) y_val_dropped = y_val.loc[X_val_dropped.index] print(&quot;X_train_dropped shape: {}&quot;.format(X_train_dropped.shape)) X_train_dropped.head() . X_train_dropped shape: (1551, 18) . Age Diastolic BP Poverty index Race Red blood cells Sedimentation rate Serum Albumin Serum Cholesterol Serum Iron Serum Magnesium Serum Protein Sex Systolic BP TIBC TS White blood cells BMI Pulse pressure . 2794 72.0 | 96.0 | 154.0 | 2.0 | 43.4 | 23.0 | 4.3 | 265.0 | 106.0 | 1.66 | 6.8 | 2.0 | 208.0 | 301.0 | 35.2 | 6.0 | 33.394319 | 112.0 | . 5856 67.0 | 94.0 | 114.0 | 1.0 | 43.8 | 12.0 | 3.7 | 178.0 | 73.0 | 1.85 | 7.0 | 1.0 | 140.0 | 311.0 | 23.5 | 4.3 | 17.481227 | 46.0 | . 9374 68.0 | 80.0 | 201.0 | 1.0 | 46.2 | 20.0 | 4.1 | 223.0 | 204.0 | 1.54 | 7.2 | 1.0 | 140.0 | 275.0 | 74.2 | 17.2 | 20.690581 | 60.0 | . 8819 68.0 | 80.0 | 651.0 | 1.0 | 47.7 | 16.0 | 4.3 | 178.0 | 168.0 | 1.97 | 7.3 | 1.0 | 102.0 | 339.0 | 49.6 | 10.2 | 27.719091 | 22.0 | . 7331 73.0 | 88.0 | 68.0 | 2.0 | 42.1 | 19.0 | 3.6 | 215.0 | 64.0 | 1.59 | 5.7 | 2.0 | 190.0 | 334.0 | 19.2 | 6.6 | 31.880432 | 102.0 | . . Decision Trees . We will use scikit-learn to build a decision tree for the hospital dataset using the train set. . dt = DecisionTreeClassifier(max_depth=None, random_state=10) dt.fit(X_train_dropped, y_train_dropped) . DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=&#39;deprecated&#39;, random_state=10, splitter=&#39;best&#39;) . Next we will evaluate our model. We&#39;ll use C-Index for evaluation. . The C-Index evaluates the ability of a model to differentiate between different classes, by quantifying how often, when considering all pairs of patients (A, B), the model says that patient A has a higher risk score than patient B when, in the observed data, patient A actually died and patient B actually lived. In our case, our model is a binary classifier, where each risk score is either 1 (the model predicts that the patient will die) or 0 (the patient will live). . More formally, defining permissible pairs of patients as pairs where the outcomes are different, concordant pairs as permissible pairs where the patient that died had a higher risk score (i.e. our model predicted 1 for the patient that died and 0 for the one that lived), and ties as permissible pairs where the risk scores were equal (i.e. our model predicted 1 for both patients or 0 for both patients), the C-Index is equal to:&gt;&gt; $$ text{C-Index} = frac{ # text{concordant pairs} + 0.5 times # text{ties}}{ # text{permissible pairs}}$$ . y_train_preds = dt.predict_proba(X_train_dropped)[:, 1] print(f&quot;Train C-Index: {cindex(y_train_dropped.values, y_train_preds)}&quot;) y_val_preds = dt.predict_proba(X_val_dropped)[:, 1] print(f&quot;Val C-Index: {cindex(y_val_dropped.values, y_val_preds)}&quot;) . Train C-Index: 1.0 Val C-Index: 0.5629321808510638 . Unfortunately the tree seems to be overfitting: it fits the training data so closely that it doesn&#39;t generalize well to other samples such as those from the validation set. . The training C-index comes out to 1.0 because, when initializing DecisionTreeClasifier, we have left max_depth and min_samples_split unspecified. The resulting decision tree will therefore keep splitting as far as it can, which pretty much guarantees a pure fit to the training data. . To handle this, we can change some of the hyperparameters of our tree. . dt_hyperparams = { &#39;max_depth&#39;: 3 } . dt_reg = DecisionTreeClassifier(**dt_hyperparams, random_state=10) dt_reg.fit(X_train_dropped, y_train_dropped) y_train_preds = dt_reg.predict_proba(X_train_dropped)[:, 1] y_val_preds = dt_reg.predict_proba(X_val_dropped)[:, 1] print(f&quot;Train C-Index: {cindex(y_train_dropped.values, y_train_preds)}&quot;) print(f&quot;Val C-Index (expected &gt; 0.6): {cindex(y_val_dropped.values, y_val_preds)}&quot;) . Train C-Index: 0.688738755448391 Val C-Index (expected &gt; 0.6): 0.6302692819148936 . As we have a low max_depth we can print the entire tree. This allows for easy interpretability. . dot_data = StringIO() export_graphviz(dt_reg, feature_names=X_train_dropped.columns, out_file=dot_data, filled=True, rounded=True, proportion=True, special_characters=True, impurity=False, class_names=[&#39;neg&#39;, &#39;pos&#39;], precision=2) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) Image(graph.create_png()) . Overfitting, underfitting, and the bias-variance tradeoff . We can see a max_depth value of 3 gives training and validation C-Indices of about 0.689 and 0.630, and that a max_depth of 2 gives better agreement with values of about 0.653 and 0.607. In the latter case, we have further reduced overfitting, at the cost of a minor loss in predictive performance. . Contrast this with a max_depth value of 1, which results in C-Indices of about 0.597 for the training set and 0.598 for the validation set:we have eliminated overfitting but with a much stronger degradation of predictive performance.&gt; &gt; Lower predictive performance on the training and validation sets is indicative of the model underfitting the data:it neither learns enough from the training data nor is able to generalize to unseen data (the validation data in our case).&gt; Finding a model that minimizes and acceptably balances underfitting and overfitting (e.g. selecting the model with a max_depth of 2 over the other values) is a common problem in machine learning that is known as the bias-variance tradeoff. . . Random Forests . No matter how you choose hyperparameters, a single decision tree is prone to overfitting. To solve this problem, we can try random forests, which combine predictions from many different trees to create a robust classifier. . As before, we will use scikit-learn to build a random forest for the data. We will use the default hyperparameters. . rf = RandomForestClassifier(n_estimators=100, random_state=10) rf.fit(X_train_dropped, y_train_dropped) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None, oob_score=False, random_state=10, verbose=0, warm_start=False) . Now we compute and report the C-Index for the random forest on the training and validation set. . y_train_rf_preds = rf.predict_proba(X_train_dropped)[:, 1] print(f&quot;Train C-Index: {cindex(y_train_dropped.values, y_train_rf_preds)}&quot;) y_val_rf_preds = rf.predict_proba(X_val_dropped)[:, 1] print(f&quot;Val C-Index: {cindex(y_val_dropped.values, y_val_rf_preds)}&quot;) . Train C-Index: 1.0 Val C-Index: 0.6660488696808511 . Training a random forest with the default hyperparameters results in a model that has better predictive performance than individual decision trees as in the previous section, but this model is overfitting. . We therefore need to tune (or optimize) the hyperparameters, to find a model that both has good predictive performance and minimizes overfitting. . The hyperparameters we choose to adjust will be: . n_estimators: the number of trees used in the forest. | max_depth: the maximum depth of each tree. | min_samples_leaf: the minimum number (if int) or proportion (if float) of samples in a leaf. | . The approach we implement to tune the hyperparameters is known as a grid search: . We define a set of possible values for each of the target hyperparameters. . | A model is trained and evaluated for every possible combination of hyperparameters. . | The best performing set of hyperparameters is returned. . | . The cell below implements a hyperparameter grid search, using the C-Index to evaluate each tested model. . def holdout_grid_search(clf, X_train_hp, y_train_hp, X_val_hp, y_val_hp, hyperparams, fixed_hyperparams={}): &#39;&#39;&#39; Conduct hyperparameter grid search on hold out validation set. Use holdout validation. Hyperparameters are input as a dictionary mapping each hyperparameter name to the range of values they should iterate over. Use the cindex function as your evaluation function. Input: clf: sklearn classifier X_train_hp (dataframe): dataframe for training set input variables y_train_hp (dataframe): dataframe for training set targets X_val_hp (dataframe): dataframe for validation set input variables y_val_hp (dataframe): dataframe for validation set targets hyperparams (dict): hyperparameter dictionary mapping hyperparameter names to range of values for grid search fixed_hyperparams (dict): dictionary of fixed hyperparameters that are not included in the grid search Output: best_estimator (sklearn classifier): fitted sklearn classifier with best performance on validation set best_hyperparams (dict): hyperparameter dictionary mapping hyperparameter names to values in best_estimator &#39;&#39;&#39; best_estimator = None best_hyperparams = {} # hold best running score best_score = 0.0 # get list of param values lists = hyperparams.values() # get all param combinations param_combinations = list(itertools.product(*lists)) total_param_combinations = len(param_combinations) # iterate through param combinations for i, params in enumerate(param_combinations, 1): # fill param dict with params param_dict = {} for param_index, param_name in enumerate(hyperparams): param_dict[param_name] = params[param_index] # create estimator with specified params estimator = clf(**param_dict, **fixed_hyperparams) # fit estimator estimator.fit(X_train_hp, y_train_hp) # get predictions on validation set preds = estimator.predict_proba(X_val_hp) # compute cindex for predictions estimator_score = cindex(y_val_hp, preds[:,1]) print(f&#39;[{i}/{total_param_combinations}] {param_dict}&#39;) print(f&#39;Val C-Index: {estimator_score} n&#39;) # if new high score, update high score, best estimator # and best params if estimator_score &gt;= best_score: best_score = estimator_score best_estimator = estimator best_hyperparams = param_dict # add fixed hyperparamters to best combination of variable hyperparameters best_hyperparams.update(fixed_hyperparams) return best_estimator, best_hyperparams . def random_forest_grid_search(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped): # Define ranges for the chosen random forest hyperparameters hyperparams = { # how many trees should be in the forest (int) &#39;n_estimators&#39;: [100, 200, 300], # the maximum depth of trees in the forest (int) &#39;max_depth&#39;: [3, 4, 5], # the minimum number of samples in a leaf as a fraction # of the total number of samples in the training set # Can be int (in which case that is the minimum number) # or float (in which case the minimum is that fraction of the # number of training set samples) &#39;min_samples_leaf&#39;: [0.25, 1, 3], } fixed_hyperparams = { &#39;random_state&#39;: 10, } rf = RandomForestClassifier best_rf, best_hyperparams = holdout_grid_search(rf, X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped, hyperparams, fixed_hyperparams) print(f&quot;Best hyperparameters: n{best_hyperparams}&quot;) y_train_best = best_rf.predict_proba(X_train_dropped)[:, 1] print(f&quot;Train C-Index: {cindex(y_train_dropped, y_train_best)}&quot;) y_val_best = best_rf.predict_proba(X_val_dropped)[:, 1] print(f&quot;Val C-Index: {cindex(y_val_dropped, y_val_best)}&quot;) # add fixed hyperparamters to best combination of variable hyperparameters best_hyperparams.update(fixed_hyperparams) return best_rf, best_hyperparams . best_rf, best_hyperparams = random_forest_grid_search(X_train_dropped, y_train_dropped, X_val_dropped, y_val_dropped) . [1/27] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 0.25} Val C-Index: 0.6639793882978723 [2/27] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1} Val C-Index: 0.6782579787234042 [3/27] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.6772273936170212 [4/27] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 0.25} Val C-Index: 0.6639793882978723 [5/27] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 1} Val C-Index: 0.668783244680851 [6/27] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.6712599734042554 [7/27] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 0.25} Val C-Index: 0.6639793882978723 [8/27] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1} Val C-Index: 0.6687666223404255 [9/27] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.6697972074468085 [10/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 0.25} Val C-Index: 0.6729637632978723 [11/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1} Val C-Index: 0.6811502659574468 [12/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.6809175531914894 [13/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 0.25} Val C-Index: 0.6729637632978723 [14/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 1} Val C-Index: 0.6758477393617022 [15/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.6752659574468085 [16/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 0.25} Val C-Index: 0.6729637632978723 [17/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1} Val C-Index: 0.6765458776595744 [18/27] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.6745844414893617 [19/27] {&#39;n_estimators&#39;: 300, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 0.25} Val C-Index: 0.6700880984042553 [20/27] {&#39;n_estimators&#39;: 300, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1} Val C-Index: 0.6796542553191489 [21/27] {&#39;n_estimators&#39;: 300, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.6793716755319149 [22/27] {&#39;n_estimators&#39;: 300, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 0.25} Val C-Index: 0.6700880984042553 [23/27] {&#39;n_estimators&#39;: 300, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 1} Val C-Index: 0.6776761968085107 [24/27] {&#39;n_estimators&#39;: 300, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.6777260638297873 [25/27] {&#39;n_estimators&#39;: 300, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 0.25} Val C-Index: 0.6700880984042553 [26/27] {&#39;n_estimators&#39;: 300, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 1} Val C-Index: 0.6775764627659574 [27/27] {&#39;n_estimators&#39;: 300, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.6730385638297872 Best hyperparameters: {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 1, &#39;random_state&#39;: 10} Train C-Index: 0.7801762032829453 Val C-Index: 0.6811502659574468 . Finally, we will evaluate the model on the test set. This is a crucial step, as trying out many combinations of hyperparameters and evaluating them on the validation set could result in a model that ends up overfitting the validation set. We therefore need to check if the model performs well on unseen data, which is the role of the test set, which we have held out until now. . y_test_best = best_rf.predict_proba(X_test)[:, 1] print(f&quot;Test C-Index: {cindex(y_test.values, y_test_best)}&quot;) . Test C-Index: 0.7019872579216067 . . Imputation . We&#39;ve now built and optimized a random forest model on our data. However, there was still a drop in test C-Index. This might be because we threw away more than half of the data of our data because of missing values for systolic blood pressure. Instead, we can try filling in, or imputing, these values. . First, let&#39;s explore to see if our data is missing at random or not. Let&#39;s plot histograms of the dropped rows against each of the covariates (aside from systolic blood pressure) to see if there is a trend. Compare these to the histograms of the feature in the entire dataset. Lets see if one of the covariates has a signficantly different distribution in the two subsets. . dropped_rows = X_train[X_train.isnull().any(axis=1)] columns_except_Systolic_BP = [col for col in X_train.columns if col not in [&#39;Systolic BP&#39;]] for col in columns_except_Systolic_BP: sns.distplot(X_train.loc[:, col], norm_hist=True, kde=False, label=&#39;full data&#39;) sns.distplot(dropped_rows.loc[:, col], norm_hist=True, kde=False, label=&#39;without missing data&#39;) plt.legend() plt.show() . Most of the covariates are distributed similarly whether or not we have discarded rows with missing data. In other words missingness of the data is independent of these covariates. . If this had been true across all covariates, then the data would have been said to be missing completely at random (MCAR). . But when considering the age covariate, we see that much more data tends to be missing for patients over 65. The reason could be that blood pressure was measured less frequently for old people to avoid placing additional burden on them. . As missingness is related to one or more covariates, the missing data is said to be missing at random (MAR). . Based on the information we have, there is however no reason to believe that the values of the missing data — or specifically the values of the missing systolic blood pressures — are related to the age of the patients. If this was the case, then this data would be said to be missing not at random (MNAR). . def bad_subset(forest, X_test, y_test): # define mask to select large subset with poor performance # currently mask defines the entire set mask = X_test[&#39;Age&#39;] &lt;= 30 X_subgroup = X_test[mask] y_subgroup = y_test[mask] subgroup_size = len(X_subgroup) y_subgroup_preds = forest.predict_proba(X_subgroup)[:, 1] performance = cindex(y_subgroup.values, y_subgroup_preds) return performance, subgroup_size . performance, subgroup_size = bad_subset(best_rf, X_test, y_test) print(&quot;Subgroup size should greater than 250, performance should be less than 0.69&quot;) print(f&quot;Subgroup size: {subgroup_size}, and your C-Index: {performance}&quot;) . Subgroup size should greater than 250, performance should be less than 0.69 Your Subgroup size: 294, and your C-Index: 0.5225123355263158 . . Imputation Approaches . Seeing that our data is not missing completely at random, we can handle the missing values by replacing them with substituted values based on the other values that we have. This is known as imputation. . The first imputation strategy that we will use is mean substitution: we will replace the missing values for each feature with the mean of the available values. In the next cell, use the SimpleImputer from sklearn to use mean imputation for the missing values. . imputer = SimpleImputer(strategy=&#39;mean&#39;) imputer.fit(X_train) X_train_mean_imputed = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns) X_val_mean_imputed = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns) . hyperparams = { # how many trees should be in the forest (int) &#39;n_estimators&#39;: [150, 200], # the maximum depth of trees in the forest (int) &#39;max_depth&#39;: [3, 4, 5], # the minimum number of samples in a leaf as a fraction # of the total number of samples in the training set # Can be int (in which case that is the minimum number) # or float (in which case the minimum is that fraction of the # number of training set samples) &#39;min_samples_leaf&#39;: [3, 4], } . rf = RandomForestClassifier rf_mean_imputed, best_hyperparams_mean_imputed = holdout_grid_search(rf, X_train_mean_imputed, y_train, X_val_mean_imputed, y_val, hyperparams, {&#39;random_state&#39;: 10}) print(&quot;Performance for best hyperparameters:&quot;) y_train_best = rf_mean_imputed.predict_proba(X_train_mean_imputed)[:, 1] print(f&quot;- Train C-Index: {cindex(y_train, y_train_best):.4f}&quot;) y_val_best = rf_mean_imputed.predict_proba(X_val_mean_imputed)[:, 1] print(f&quot;- Val C-Index: {cindex(y_val, y_val_best):.4f}&quot;) y_test_imp = rf_mean_imputed.predict_proba(X_test)[:, 1] print(f&quot;- Test C-Index: {cindex(y_test, y_test_imp):.4f}&quot;) . [1/12] {&#39;n_estimators&#39;: 150, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.737671510990383 [2/12] {&#39;n_estimators&#39;: 150, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7375510000238851 [3/12] {&#39;n_estimators&#39;: 150, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.745231131348268 [4/12] {&#39;n_estimators&#39;: 150, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7450291940530552 [5/12] {&#39;n_estimators&#39;: 150, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.7483622451084491 [6/12] {&#39;n_estimators&#39;: 150, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7477325481663877 [7/12] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.7396604847797906 [8/12] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7393901493684574 [9/12] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.745559008031893 [10/12] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7454830101250925 [11/12] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.7495499838233027 [12/12] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7489767424691502 Performance for best hyperparameters: - Train C-Index: 0.8109 - Val C-Index: 0.7495 - Test C-Index: 0.7805 . Next, we will apply another imputation strategy, known as multivariate feature imputation, using scikit-learn&#39;s IterativeImputer class (see the documentation). . With this strategy, for each feature that is missing values, a regression model is trained to predict observed values based on all of the other features, and the missing values are inferred using this model. As a single iteration across all features may not be enough to impute all missing values, several iterations may be performed, hence the name of the class IterativeImputer. . imputer = IterativeImputer(random_state=0, sample_posterior=False, max_iter=1, min_value=0) imputer.fit(X_train) X_train_imputed = pd.DataFrame(imputer.transform(X_train), columns=X_train.columns) X_val_imputed = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns) . hyperparams = { # how many trees should be in the forest (int) &#39;n_estimators&#39;: [100, 150, 200], # the maximum depth of trees in the forest (int) &#39;max_depth&#39;: [3, 4, 5], # the minimum number of samples in a leaf as a fraction # of the total number of samples in the training set # Can be int (in which case that is the minimum number) # or float (in which case the minimum is that fraction of the # number of training set samples) &#39;min_samples_leaf&#39;: [3, 4], } . rf = RandomForestClassifier rf_imputed, best_hyperparams_imputed = holdout_grid_search(rf, X_train_imputed, y_train, X_val_imputed, y_val, hyperparams, {&#39;random_state&#39;: 10}) print(&quot;Performance for best hyperparameters:&quot;) y_train_best = rf_imputed.predict_proba(X_train_imputed)[:, 1] print(f&quot;- Train C-Index: {cindex(y_train, y_train_best):.4f}&quot;) y_val_best = rf_imputed.predict_proba(X_val_imputed)[:, 1] print(f&quot;- Val C-Index: {cindex(y_val, y_val_best):.4f}&quot;) y_test_imp = rf_imputed.predict_proba(X_test)[:, 1] print(f&quot;- Test C-Index: {cindex(y_test, y_test_imp):.4f}&quot;) . [1/18] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.7329770117188772 [2/18] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7325264526999885 [3/18] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.7406224011430085 [4/18] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7401512141208454 [5/18] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.7439022536636419 [6/18] {&#39;n_estimators&#39;: 100, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7433290123094896 [7/18] {&#39;n_estimators&#39;: 150, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.7338140743780657 [8/18] {&#39;n_estimators&#39;: 150, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7336707640395276 [9/18] {&#39;n_estimators&#39;: 150, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.7409926195175653 [10/18] {&#39;n_estimators&#39;: 150, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7403889790006927 [11/18] {&#39;n_estimators&#39;: 150, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.7430380488948819 [12/18] {&#39;n_estimators&#39;: 150, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7422932694082369 [13/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.7356792801478268 [14/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.735444772321128 [15/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.7429316518253611 [16/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 4, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7425451481850615 [17/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 3} Val C-Index: 0.7453787844243376 [18/18] {&#39;n_estimators&#39;: 200, &#39;max_depth&#39;: 5, &#39;min_samples_leaf&#39;: 4} Val C-Index: 0.7451247342787473 Performance for best hyperparameters: - Train C-Index: 0.8131 - Val C-Index: 0.7454 - Test C-Index: 0.7797 . . Comparison . For good measure, lets retest on the subgroup from before to see if our new models do better. . performance, subgroup_size = bad_subset(best_rf, X_test, y_test) print(f&quot;C-Index (no imputation): {performance}&quot;) performance, subgroup_size = bad_subset(rf_mean_imputed, X_test, y_test) print(f&quot;C-Index (mean imputation): {performance}&quot;) performance, subgroup_size = bad_subset(rf_imputed, X_test, y_test) print(f&quot;C-Index (multivariate feature imputation): {performance}&quot;) . C-Index (no imputation): 0.5225123355263158 C-Index (mean imputation): 0.5373149671052632 C-Index (multivariate feature imputation): 0.5447162828947368 . We see that avoiding complete case analysis (i.e. analysis only on observations for which there is no missing data) allows our model to generalize a bit better. . . Explanations: SHAP . Using a random forest has improved results, but we&#39;ve lost some of the natural interpretability of trees. In this section we&#39;ll try to explain the predictions using slightly more sophisticated techniques. . SHAP (SHapley Additive exPlanations), is a cutting edge method that explains predictions made by black-box machine learning models (i.e. models which are too complex to be understandable by humans as is). . Given a prediction made by a machine learning model, SHAP values explain the prediction by quantifying the additive importance of each feature to the prediction. SHAP values have their roots in cooperative game theory, where Shapley values are used to quantify the contribution of each player to the game. . Although it is computationally expensive to compute SHAP values for general black-box models, in the case of trees and forests there exists a fast polynomial-time algorithm. For more details, see the TreeShap paper. We&#39;ll use the shap library to do this for our random forest model. . X_test_risk = X_test.copy(deep=True) X_test_risk.loc[:, &#39;risk&#39;] = rf_imputed.predict_proba(X_test_risk)[:, 1] X_test_risk = X_test_risk.sort_values(by=&#39;risk&#39;, ascending=False) X_test_risk.head() . Age Diastolic BP Poverty index Race Red blood cells Sedimentation rate Serum Albumin Serum Cholesterol Serum Iron Serum Magnesium Serum Protein Sex Systolic BP TIBC TS White blood cells BMI Pulse pressure risk . 5493 67.0 | 80.0 | 30.0 | 1.0 | 77.7 | 59.0 | 3.4 | 231.0 | 36.0 | 1.40 | 6.3 | 1.0 | 170.0 | 202.0 | 17.8 | 8.4 | 17.029470 | 90.0 | 0.619022 | . 1017 65.0 | 98.0 | 16.0 | 1.0 | 49.4 | 30.0 | 3.4 | 124.0 | 129.0 | 1.59 | 7.7 | 1.0 | 184.0 | 293.0 | 44.0 | 5.9 | 30.858853 | 86.0 | 0.545443 | . 2050 66.0 | 100.0 | 69.0 | 2.0 | 42.9 | 47.0 | 3.8 | 233.0 | 170.0 | 1.42 | 8.6 | 1.0 | 180.0 | 411.0 | 41.4 | 7.2 | 22.129498 | 80.0 | 0.527768 | . 6337 69.0 | 80.0 | 233.0 | 1.0 | 77.7 | 48.0 | 4.2 | 159.0 | 87.0 | 1.81 | 6.9 | 1.0 | 146.0 | 291.0 | 29.9 | 15.2 | 17.931276 | 66.0 | 0.526019 | . 2608 71.0 | 80.0 | 104.0 | 1.0 | 43.8 | 23.0 | 4.0 | 201.0 | 119.0 | 1.60 | 7.0 | 1.0 | 166.0 | 311.0 | 38.3 | 6.3 | 17.760766 | 86.0 | 0.525624 | . We can use SHAP values to try and understand the model output on specific individuals using force plots. Run the cell below to see a force plot on the riskiest individual. . explainer = shap.TreeExplainer(rf_imputed) i = 0 shap_value = explainer.shap_values(X_test.loc[X_test_risk.index[i], :])[1] shap.force_plot(explainer.expected_value[1], shap_value, feature_names=X_test.columns, matplotlib=True) . How to read this chart: . The red sections on the left are features which push the model towards the final prediction in the positive direction (i.e. a higher Age increases the predicted risk). | The blue sections on the right are features that push the model towards the final prediction in the negative direction (if an increase in a feature leads to a lower risk, it will be shown in blue). | Note that the exact output of your chart will differ depending on the hyper-parameters that you choose for your model. | . We can also use SHAP values to understand the model output in aggregate. Run the next cell to initialize the SHAP values (this may take a few minutes). . shap_values = shap.TreeExplainer(rf_imputed).shap_values(X_test)[1] . Summary plot of the SHAP values for each feature on each of the test examples. The colors indicate the value of the feature. . shap.summary_plot(shap_values, X_test) . Clearly we see that being a woman (sex = 2.0, as opposed to men for which sex = 1.0) has a negative SHAP value, meaning that it reduces the risk of dying within 10 years. High age and high systolic blood pressure have positive SHAP values, and are therefore related to increased mortality. . We can see how features interact using dependence plots. These plot the SHAP value for a given feature for each data point, and color the points in using the value for another feature. This lets us begin to explain the variation in SHAP value for a single value of the main feature. . Run the next cell to see the interaction between Age and Sex. . shap.dependence_plot(&#39;Age&#39;, shap_values, X_test, interaction_index=&#39;Sex&#39;) . We see that while Age &gt; 50 is generally bad (positive SHAP value), being a woman generally reduces the impact of age. This makes sense since we know that women generally live longer than men. . Let&#39;s now look at poverty index and age. . shap.dependence_plot(&#39;Poverty index&#39;, shap_values, X_test, interaction_index=&#39;Age&#39;) . We see that the impact of poverty index drops off quickly, and for higher income individuals age begins to explain much of variation in the impact of poverty index. .",
            "url": "https://www.livingdatalab.com/health/2022/08/06/predicting-10-year-death-risk-health-data.html",
            "relUrl": "/health/2022/08/06/predicting-10-year-death-risk-health-data.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "A Prognostic Risk Score Model for Retinopathy in Diabetes Patients",
            "content": "Introduction . In this project, we will build a risk score model for retinopathy in diabetes patients using logistic regression. This will be a Prognostic model for disease rather than a Diagnostic model. A Prognostic model predicts the future risk of a disease as opposed to a Diagnositic model, which would predict the presence of a disease now. . As we develop the model, we will consider the following topics: . Data preprocessing Log transformations | Standardization | . | Basic Risk Models Logistic Regression | C-index | Interactions Terms | . | . Diabetic Retinopathy . Retinopathy is an eye condition that causes changes to the blood vessels in the part of the eye called the retina. This often leads to vision changes or blindness. Diabetic patients are known to be at high risk for retinopathy. . Logistic Regression . Logistic regression is an appropriate analysis to use for predicting the probability of a binary outcome. In our case, this would be the probability of having or not having diabetic retinopathy. Logistic Regression is one of the most commonly used algorithms for binary classification. It is used to find the best fitting model to describe the relationship between a set of features (also referred to as input, independent, predictor, or explanatory variables) and a binary outcome label (also referred to as an output, dependent, or response variable). Logistic regression has the property that the output prediction is always in the range $[0,1]$. Sometimes this output is used to represent a probability from 0%-100%, but for straight binary classification, the output is converted to either $0$ or $1$ depending on whether it is below or above a certain threshold, usually $0.5$. . . Load Data . First we will load in the dataset that we will use for training and testing our model. . from utils import load_data # This function creates randomly generated data # X, y = load_data(6000) # For stability, load data from files that were generated using the load_data X = pd.read_csv(&#39;X_data.csv&#39;,index_col=0) y_df = pd.read_csv(&#39;y_data.csv&#39;,index_col=0) y = y_df[&#39;y&#39;] . X and y are Pandas DataFrames that hold the data for 6,000 diabetic patients. . . Explore the Dataset . The features (X) include the following fields: . Age: (years) | Systolic_BP: Systolic blood pressure (mmHg) | Diastolic_BP: Diastolic blood pressure (mmHg) | Cholesterol: (mg/DL) | . We can use the head() method to display the first few records of each. . X.head() . Age Systolic_BP Diastolic_BP Cholesterol . 0 77.196340 | 85.288742 | 80.021878 | 79.957109 | . 1 63.529850 | 99.379736 | 84.852361 | 110.382411 | . 2 69.003986 | 111.349455 | 109.850616 | 100.828246 | . 3 82.638210 | 95.056128 | 79.666851 | 87.066303 | . 4 78.346286 | 109.154591 | 90.713220 | 92.511770 | . The target (y) is an indicator of whether or not the patient developed retinopathy. . y = 1 : patient has retinopathy. | y = 0 : patient does not have retinopathy. | . y.head() . 0 1.0 1 1.0 2 1.0 3 1.0 4 1.0 Name: y, dtype: float64 . Before we build a model, let&#39;s take a closer look at the distribution of our training data. To do this, we will split the data into train and test sets using a 75/25 split. . For this, we can use the built in function provided by sklearn library. . from sklearn.model_selection import train_test_split . X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=0) . Plot the histograms of each column of X_train below: . for col in X.columns: X_train_raw.loc[:, col].hist() plt.title(col) plt.show() . As we can see, the distributions have a generally bell shaped distribution, but with slight rightward skew. . Many statistical models assume that the data is normally distributed, forming a symmetric Gaussian bell shape (with no skew) more like the example below. . from scipy.stats import norm data = np.random.normal(50,12, 5000) fitting_params = norm.fit(data) norm_dist_fitted = norm(*fitting_params) t = np.linspace(0,100, 100) plt.hist(data, bins=60, density=True) plt.plot(t, norm_dist_fitted.pdf(t)) plt.title(&#39;Example of Normally Distributed Data&#39;) plt.show() . We can transform our data to be closer to a normal distribution by removing the skew. One way to remove the skew is by applying the log function to the data. . Let&#39;s plot the log of the feature variables to see that it produces the desired effect. . for col in X_train_raw.columns: np.log(X_train_raw.loc[:, col]).hist() plt.title(col) plt.show() . We can see that the data is more symmetric after taking the log. . . Mean-Normalize the Data . Let&#39;s now transform our data so that the distributions are closer to standard normal distributions. . First we will remove some of the skew from the distribution by using the log transformation. Then we will &quot;standardize&quot; the distribution so that it has a mean of zero and standard deviation of 1. Recall that a standard normal distribution has mean of zero and standard deviation of 1. . def make_standard_normal(df_train, df_test): &quot;&quot;&quot; In order to make the data closer to a normal distribution, take log transforms to reduce the skew. Then standardize the distribution with a mean of zero and standard deviation of 1. Args: df_train (dataframe): unnormalized training data. df_test (dataframe): unnormalized test data. Returns: df_train_normalized (dateframe): normalized training data. df_test_normalized (dataframe): normalized test data. &quot;&quot;&quot; # Remove skew by applying the log function to the train set, and to the test set train_cols = df_train.columns test_cols = df_test.columns df_train_unskewed = df_train[train_cols].apply(lambda x: np.log(x)) df_test_unskewed = df_test[test_cols].apply(lambda x: np.log(x)) #calculate the mean and standard deviation of the training set mean = df_train_unskewed.mean(axis=0) stdev = df_train_unskewed.std(axis=0) # standardize the training set df_train_standardized = (df_train_unskewed - mean) / stdev # standardize the test set (see instructions and hints above) df_test_standardized = (df_test_unskewed - mean) / stdev return df_train_standardized, df_test_standardized . # test tmp_train = pd.DataFrame({&#39;field1&#39;: [1,2,10], &#39;field2&#39;: [4,5,11]}) tmp_test = pd.DataFrame({&#39;field1&#39;: [1,3,10], &#39;field2&#39;: [4,6,11]}) tmp_train_transformed, tmp_test_transformed = make_standard_normal(tmp_train,tmp_test) print(f&quot;Training set transformed field1 has mean {tmp_train_transformed[&#39;field1&#39;].mean(axis=0):.4f} and standard deviation {tmp_train_transformed[&#39;field1&#39;].std(axis=0):.4f} &quot;) print(f&quot;Test set transformed, field1 has mean {tmp_test_transformed[&#39;field1&#39;].mean(axis=0):.4f} and standard deviation {tmp_test_transformed[&#39;field1&#39;].std(axis=0):.4f}&quot;) print(f&quot;Skew of training set field1 before transformation: {tmp_train[&#39;field1&#39;].skew(axis=0):.4f}&quot;) print(f&quot;Skew of training set field1 after transformation: {tmp_train_transformed[&#39;field1&#39;].skew(axis=0):.4f}&quot;) print(f&quot;Skew of test set field1 before transformation: {tmp_test[&#39;field1&#39;].skew(axis=0):.4f}&quot;) print(f&quot;Skew of test set field1 after transformation: {tmp_test_transformed[&#39;field1&#39;].skew(axis=0):.4f}&quot;) . Training set transformed field1 has mean -0.0000 and standard deviation 1.0000 Test set transformed, field1 has mean 0.1144 and standard deviation 0.9749 Skew of training set field1 before transformation: 1.6523 Skew of training set field1 after transformation: 1.0857 Skew of test set field1 before transformation: 1.3896 Skew of test set field1 after transformation: 0.1371 . Transform training and test data . X_train, X_test = make_standard_normal(X_train_raw, X_test_raw) . After transforming the training and test sets, we&#39;ll expect the training set to be centered at zero with a standard deviation of $1$. . We will avoid observing the test set during model training in order to avoid biasing the model training process, but let&#39;s have a look at the distributions of the transformed training data. . for col in X_train.columns: X_train[col].hist() plt.title(col) plt.show() . . Build the Model . Now we are ready to build the risk model by training logistic regression with our data. . def lr_model(X_train, y_train): # import the LogisticRegression class from sklearn.linear_model import LogisticRegression # create the model object model = LogisticRegression() # fit the model to the training data model.fit(X_train, y_train) #return the fitted model return model . # Test tmp_model = lr_model(X_train[0:3], y_train[0:3] ) print(tmp_model.predict(X_train[4:5])) print(tmp_model.predict(X_train[5:6])) . [1.] [1.] . Now that we&#39;ve tested our model, we can go ahead and build it. . model_X = lr_model(X_train, y_train) . . Evaluate the Model Using the C-index . Now that we have a model, we need to evaluate it. We&#39;ll do this using the c-index. . The c-index measures the discriminatory power of a risk score. | Intuitively, a higher c-index indicates that the model&#39;s prediction is in agreement with the actual outcomes of a pair of patients. | The formula for the c-index is: cindex = (concordant + 0.5 * ties) / permissible | A permissible pair is a pair of patients who have different outcomes. | A concordant pair is a permissible pair in which the patient with the higher risk score also has the worse outcome. | A tie is a permissible pair where the patients have the same risk score. | . def cindex(y_true, scores): &#39;&#39;&#39; Input: y_true (np.array): a 1-D array of true binary outcomes (values of zero or one) 0: patient does not get the disease 1: patient does get the disease scores (np.array): a 1-D array of corresponding risk scores output by the model Output: c_index (float): (concordant pairs + 0.5*ties) / number of permissible pairs &#39;&#39;&#39; n = len(y_true) assert len(scores) == n concordant = 0 permissible = 0 ties = 0 # Two nested for loops to go through all unique pairs of patients for i in range(n): for j in range(i+1, n): #choose the range of j so that j&gt;i # Check if the pair is permissible (the patient outcomes are different) if y_true[i] != y_true[j]: # Count the pair if it&#39;s permissible permissible += 1 # For permissible pairs, check if they are concordant or are ties # check for ties in the score if scores[i] == scores[j]: # count the tie ties += 1 # if it&#39;s a tie, we don&#39;t need to check patient outcomes, continue to the top of the for loop. continue # case 1: patient i doesn&#39;t get the disease, patient j does if y_true[i] == 0 and y_true[j] == 1: # Check if patient i has a lower risk score than patient j if scores[i] &lt; scores[j]: # count the concordant pair concordant += 1 # Otherwise if patient i has a higher risk score, it&#39;s not a concordant pair. # Already checked for ties earlier # case 2: patient i gets the disease, patient j does not if y_true[i] == 1 and y_true[j] == 0: # Check if patient i has a higher risk score than patient j if scores[i] &gt; scores[j]: #count the concordant pair concordant += 1 # Otherwise if patient i has a lower risk score, it&#39;s not a concordant pair. # We already checked for ties earlier # calculate the c-index using the count of permissible pairs, concordant pairs, and tied pairs. c_index = (concordant + 0.5 * ties) / permissible return c_index . # test y_true = np.array([1.0, 0.0, 0.0, 1.0]) # Case 1 scores = np.array([0, 1, 1, 0]) print(&#39;Case 1 Output: {}&#39;.format(cindex(y_true, scores))) # Case 2 scores = np.array([1, 0, 0, 1]) print(&#39;Case 2 Output: {}&#39;.format(cindex(y_true, scores))) # Case 3 scores = np.array([0.5, 0.5, 0.0, 1.0]) print(&#39;Case 3 Output: {}&#39;.format(cindex(y_true, scores))) cindex(y_true, scores) . Case 1 Output: 0.0 Case 2 Output: 1.0 Case 3 Output: 0.875 . 0.875 . . Evaluate the Model on the Test Set . Now, we can evaluate your trained model on the test set. . To get the predicted probabilities, we use the predict_proba method. This method will return the result from the model before it is converted to a binary 0 or 1. For each input case, it returns an array of two values which represent the probabilities for both the negative case (patient does not get the disease) and positive case (patient the gets the disease). . scores = model_X.predict_proba(X_test)[:, 1] c_index_X_test = cindex(y_test.values, scores) print(f&quot;c-index on test set is {c_index_X_test:.4f}&quot;) . c-index on test set is 0.8182 . Let&#39;s plot the coefficients to see which variables (patient features) are having the most effect. . coeffs = pd.DataFrame(data = model_X.coef_, columns = X_train.columns) coeffs.T.plot.bar(legend=None); . . Improve the Model . We can try to improve the model by including interaction terms. . An interaction term is the product of two variables. For example, if we have data $$ x = [x_1, x_2]$$ | We could add the product so that: $$ hat{x} = [x_1, x_2, x_1*x_2]$$ | . | . def add_interactions(X): &quot;&quot;&quot; Add interaction terms between columns to dataframe. Args: X (dataframe): Original data Returns: X_int (dataframe): Original data with interaction terms appended. &quot;&quot;&quot; features = X.columns m = len(features) X_int = X.copy(deep=True) # &#39;i&#39; loops through all features in the original dataframe X for i in range(m): # get the name of feature &#39;i&#39; feature_i_name = features[i] # get the data for feature &#39;i&#39; feature_i_data = X[feature_i_name] # choose the index of column &#39;j&#39; to be greater than column i for j in range(i+1, m): # get the name of feature &#39;j&#39; feature_j_name = features[j] # get the data for feature j&#39; feature_j_data = X[feature_j_name] # create the name of the interaction feature by combining both names # example: &quot;apple&quot; and &quot;orange&quot; are combined to be &quot;apple_x_orange&quot; feature_i_j_name = feature_i_name + &#39;_x_&#39; + feature_j_name # Multiply the data for feature &#39;i&#39; and feature &#39;j&#39; # store the result as a column in dataframe X_int X_int[feature_i_j_name] = X_int[feature_i_name] * X_int[feature_j_name] return X_int . # Test print(&quot;Original Data&quot;) print(X_train.loc[:, [&#39;Age&#39;, &#39;Systolic_BP&#39;]].head()) print(&quot;Data w/ Interactions&quot;) print(add_interactions(X_train.loc[:, [&#39;Age&#39;, &#39;Systolic_BP&#39;]].head())) . Original Data Age Systolic_BP 1824 -0.912451 -0.068019 253 -0.302039 1.719538 1114 2.576274 0.155962 3220 1.163621 -2.033931 2108 -0.446238 -0.054554 Data w/ Interactions Age Systolic_BP Age_x_Systolic_BP 1824 -0.912451 -0.068019 0.062064 253 -0.302039 1.719538 -0.519367 1114 2.576274 0.155962 0.401800 3220 1.163621 -2.033931 -2.366725 2108 -0.446238 -0.054554 0.024344 . X_train_int = add_interactions(X_train) X_test_int = add_interactions(X_test) . . Evaluate the Improved Model . Now we can train the new and improved version of the model. . model_X_int = lr_model(X_train_int, y_train) . Let&#39;s evaluate our new model on the test set. . scores_X = model_X.predict_proba(X_test)[:, 1] c_index_X_int_test = cindex(y_test.values, scores_X) scores_X_int = model_X_int.predict_proba(X_test_int)[:, 1] c_index_X_int_test = cindex(y_test.values, scores_X_int) print(f&quot;c-index on test set without interactions is {c_index_X_test:.4f}&quot;) print(f&quot;c-index on test set with interactions is {c_index_X_int_test:.4f}&quot;) . c-index on test set without interactions is 0.8182 c-index on test set with interactions is 0.8281 . We can see that the model with interaction terms performs a bit better than the model without interactions. . Now let&#39;s take another look at the model coefficients to try and see which variables made a difference. . int_coeffs = pd.DataFrame(data = model_X_int.coef_, columns = X_train_int.columns) int_coeffs.T.plot.bar(); . We can see that Age, Systolic_BP, and Cholesterol have a positive coefficient. This means that a higher value in these three features leads to a higher prediction probability for the disease. You also may notice that the interaction of Age x Cholesterol has a negative coefficient. This means that a higher value for the Age x Cholesterol product reduces the prediction probability for the disease. . To understand the effect of interaction terms, let&#39;s compare the output of the model we&#39;ve trained on sample cases with and without the interaction. . index = index = 3432 case = X_train_int.iloc[index, :] print(case) . Age 2.502061 Systolic_BP 1.713547 Diastolic_BP 0.268265 Cholesterol 2.146349 Age_x_Systolic_BP 4.287400 Age_x_Diastolic_BP 0.671216 Age_x_Cholesterol 5.370296 Systolic_BP_x_Diastolic_BP 0.459685 Systolic_BP_x_Cholesterol 3.677871 Diastolic_BP_x_Cholesterol 0.575791 Name: 5970, dtype: float64 . We can see that they have above average Age and Cholesterol. We can now see what our original model would have output by zero-ing out the value for Cholesterol and Age. . new_case = case.copy(deep=True) new_case.loc[&quot;Age_x_Cholesterol&quot;] = 0 new_case . Age 2.502061 Systolic_BP 1.713547 Diastolic_BP 0.268265 Cholesterol 2.146349 Age_x_Systolic_BP 4.287400 Age_x_Diastolic_BP 0.671216 Age_x_Cholesterol 0.000000 Systolic_BP_x_Diastolic_BP 0.459685 Systolic_BP_x_Cholesterol 3.677871 Diastolic_BP_x_Cholesterol 0.575791 Name: 5970, dtype: float64 . print(f&quot;Output with interaction: t{model_X_int.predict_proba([case.values])[:, 1][0]:.4f}&quot;) print(f&quot;Output without interaction: t{model_X_int.predict_proba([new_case.values])[:, 1][0]:.4f}&quot;) . Output with interaction: 0.9448 Output without interaction: 0.9965 . We see that the model is less confident in its prediction with the interaction term than without (the prediction value is lower when including the interaction term). With the interaction term, the model has adjusted for the fact that the effect of high cholesterol becomes less important for older patients compared to younger patients. . Conclusion . In this project, we will built a prognostic risk score model for retinopathy in diabetes patients using logistic regression. . We considered the following topics: . Data preprocessing Log transformations | Standardization | . | Basic Risk Models Logistic Regression | C-index | Interactions Terms | . | .",
            "url": "https://www.livingdatalab.com/health/2022/06/11/a-prognostic-risk-score-model-for-retinopathy-in-diabetes-patients.html",
            "relUrl": "/health/2022/06/11/a-prognostic-risk-score-model-for-retinopathy-in-diabetes-patients.html",
            "date": " • Jun 11, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Brain Tumor Auto-Segmentation for Magnetic Resonance Imaging (MRI)",
            "content": "Introduction . In an earlier project I developed a deep learning model that could predict Alzheimer&#39;s disease using 3D MRI medical images. In this project we will build a deep learning model to automatically segment tumor regions in brain using MRI (Magnetic Resonance Imaging) scans. . The MRI scan is one of the most common image modalities that we encounter in the radiology field. . Other data modalities include: . Computer Tomography (CT), | Ultrasound | X-Rays. | . We will cover: . What is in an MR image | Standard data preparation techniques for MRI datasets | Metrics and loss functions for segmentation | Visualizing and evaluating segmentation models | . . Dataset . What is an MRI? . Magnetic resonance imaging (MRI) is an advanced imaging technique that is used to observe a variety of diseases and parts of the body. . As we will see later, neural networks can analyze these images individually (as a radiologist would) or combine them into a single 3D volume to make predictions. . At a high level, MRI works by measuring the radio waves emitting by atoms subjected to a magnetic field. . In this project, we&#39;ll build a multi-class segmentation model. We&#39;ll identify 3 different abnormalities in each image: edemas, non-enhancing tumors, and enhancing tumors. . MRI Data Processing . We often encounter MR images in the DICOM format. . The DICMOM format is the output format for most commercial MRI scanners. This type of data can be processed using the pydicom Python library. | . We will be using the data from the Decathlon 10 Challenge. This data has been mostly pre-processed for the competition participants, however in real practice, MRI data needs to be significantly pre-preprocessed before we can use it to train our models. . . Exploring the Dataset . Our dataset is stored in the NifTI-1 format and we will be using the NiBabel library to interact with the files. Each training sample is composed of two separate files: . The first file is an image file containing a 4D array of MR image in the shape of (240, 240, 155, 4). . The first 3 dimensions are the X, Y, and Z values for each point in the 3D volume, which is commonly called a voxel. | The 4th dimension is the values for 4 different sequences 0: FLAIR: &quot;Fluid Attenuated Inversion Recovery&quot; (FLAIR) | 1: T1w: &quot;T1-weighted&quot; | 2: t1gd: &quot;T1-weighted with gadolinium contrast enhancement&quot; (T1-Gd) | 3: T2w: &quot;T2-weighted&quot; | . | . The second file in each training example is a label file containing a 3D array with the shape of (240, 240, 155). . The integer values in this array indicate the &quot;label&quot; for each voxel in the corresponding image files: 0: background | 1: edema | 2: non-enhancing tumor | 3: enhancing tumor | . | . We have access to a total of 484 training images which we will be splitting into a training (80%) and validation (20%) dataset. . Let&#39;s begin by looking at one single case and visualizing the data. . We&#39;ll use the NiBabel library to load the image and label for a case. The function is shown below to give you a sense of how it works. . # set home directory and data directory HOME_DIR = &quot;data/BraTS-Data/&quot; DATA_DIR = HOME_DIR def load_case(image_nifty_file, label_nifty_file): # load the image and label file, get the image content and return a numpy array for each image = np.array(nib.load(image_nifty_file).get_fdata()) label = np.array(nib.load(label_nifty_file).get_fdata()) return image, label . We&#39;ll now visualize an example. For this, we use a pre-defined function we have written in the util.py file that uses matplotlib to generate a summary of the image. . The colors correspond to each class. . Red is edema | Green is a non-enhancing tumor | Blue is an enhancing tumor. | . image, label = load_case(DATA_DIR + &quot;imagesTr/BRATS_003.nii.gz&quot;, DATA_DIR + &quot;labelsTr/BRATS_003.nii.gz&quot;) image = util.get_labeled_image(image, label) util.plot_image_grid(image) . We have a previously written a utility function which generates a GIF that shows what it looks like to iterate over each axis. . image, label = load_case(DATA_DIR + &quot;imagesTr/BRATS_003.nii.gz&quot;, DATA_DIR + &quot;labelsTr/BRATS_003.nii.gz&quot;) util.visualize_data_gif(util.get_labeled_image(image, label)) . . Data Preprocessing using Patches . While our dataset is provided to us post-registration and in the NIfTI format, we still have to do some minor pre-processing before feeding the data to our model. . Generate sub-volumes . We are going to first generate &quot;patches&quot; of our data which you can think of as sub-volumes of the whole MR images. . The reason that we are generating patches is because a network that can process the entire volume at once will simply not fit inside our current environment&#39;s memory/GPU. | Therefore we will be using this common technique to generate spatially consistent sub-volumes of our data, which can be fed into our network. | Specifically, we will be generating randomly sampled sub-volumes of shape [160, 160, 16] from our images. | Furthermore, given that a large portion of the MRI volumes are just brain tissue or black background without any tumors, we want to make sure that we pick patches that at least include some amount of tumor data. | Therefore, we are only going to pick patches that have at most 95% non-tumor regions (so at least 5% tumor). | We do this by filtering the volumes based on the values present in the background labels. | . Standardization (mean 0, stdev 1) . Lastly, given that the values in MR images cover a very wide range, we will standardize the values to have a mean of zero and standard deviation of 1. . This is a common technique in deep image processing since standardization makes it much easier for the network to learn. | . Let&#39;s define a function to get a sub volume/patch. . def get_sub_volume(image, label, orig_x = 240, orig_y = 240, orig_z = 155, output_x = 160, output_y = 160, output_z = 16, num_classes = 4, max_tries = 1000, background_threshold=0.95): &quot;&quot;&quot; Extract random sub-volume from original images. Args: image (np.array): original image, of shape (orig_x, orig_y, orig_z, num_channels) label (np.array): original label. labels coded using discrete values rather than a separate dimension, so this is of shape (orig_x, orig_y, orig_z) orig_x (int): x_dim of input image orig_y (int): y_dim of input image orig_z (int): z_dim of input image output_x (int): desired x_dim of output output_y (int): desired y_dim of output output_z (int): desired z_dim of output num_classes (int): number of class labels max_tries (int): maximum trials to do when sampling background_threshold (float): limit on the fraction of the sample which can be the background returns: X (np.array): sample of original image of dimension (num_channels, output_x, output_y, output_z) y (np.array): labels which correspond to X, of dimension (num_classes, output_x, output_y, output_z) &quot;&quot;&quot; # Initialize features and labels with `None` X = None y = None tries = 0 while tries &lt; max_tries: # randomly sample sub-volume by sampling the corner voxel start_x = np.random.randint(0, orig_x - output_x + 1) start_y = np.random.randint(0, orig_y - output_y + 1) start_z = np.random.randint(0, orig_z - output_z + 1) # extract relevant area of label y = label[start_x: start_x + output_x, start_y: start_y + output_y, start_z: start_z + output_z] # One-hot encode the categories. # This adds a 4th dimension, &#39;num_classes&#39; # (output_x, output_y, output_z, num_classes) y = keras.utils.to_categorical(y, num_classes=num_classes) # compute the background ratio bgrd_ratio = np.sum(y[:, :, :, 0])/(output_x * output_y * output_z) # increment tries counter tries += 1 # if background ratio is below the desired threshold, # use that sub-volume. # otherwise continue the loop and try another random sub-volume if bgrd_ratio &lt; background_threshold: # make copy of the sub-volume X = np.copy(image[start_x: start_x + output_x, start_y: start_y + output_y, start_z: start_z + output_z, :]) # change dimension of X # from (x_dim, y_dim, z_dim, num_channels) # to (num_channels, x_dim, y_dim, z_dim) X = np.moveaxis(X, -1, 0) # change dimension of y # from (x_dim, y_dim, z_dim, num_classes) # to (num_classes, x_dim, y_dim, z_dim) y = np.moveaxis(y, -1, 0) # take a subset of y that excludes the background class # in the &#39;num_classes&#39; dimension y = y[1:, :, :, :] return X, y # if we&#39;ve tried max_tries number of samples # Give up in order to avoid looping forever. print(f&quot;Tried {tries} times to find a sub-volume. Giving up...&quot;) . ### Test get_sub_volume_test(get_sub_volume) . Image: z = 0 [[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] z = 1 [[0. 0. 0. 0.] [0. 1. 2. 3.] [0. 2. 4. 6.] [0. 3. 6. 9.]] z = 2 [[ 0. 0. 0. 0.] [ 0. 2. 4. 6.] [ 0. 4. 8. 12.] [ 0. 6. 12. 18.]] Label: z = 0 [[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] z = 1 [[1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.] [1. 1. 1. 1.]] z = 2 [[2. 2. 2. 2.] [2. 2. 2. 2.] [2. 2. 2. 2.] [2. 2. 2. 2.]] Extracting (2, 2, 2) sub-volume All tests passed. Sampled Image: z = 0 [[1. 2.] [2. 4.]] z = 1 [[2. 4.] [4. 8.]] Sampled Label: class = 0 z = 0 [[1. 1.] [1. 1.]] z = 1 [[0. 0.] [0. 0.]] class = 1 z = 0 [[0. 0.] [0. 0.]] z = 1 [[1. 1.] [1. 1.]] . We&#39;ll now look at the enhancing tumor part of the label. . image, label = load_case(DATA_DIR + &quot;imagesTr/BRATS_001.nii.gz&quot;, DATA_DIR + &quot;labelsTr/BRATS_001.nii.gz&quot;) X, y = get_sub_volume(image, label) # enhancing tumor is channel 2 in the class label # change indexer for y to look at different classes util.visualize_patch(X[0, :, :, :], y[2]) . Next, lets define a function that given a patch (sub-volume), standardizes the values across each channel and each Z plane to have a mean of zero and standard deviation of 1. . def standardize(image): &quot;&quot;&quot; Standardize mean and standard deviation of each channel and z_dimension. Args: image (np.array): input image, shape (num_channels, dim_x, dim_y, dim_z) Returns: standardized_image (np.array): standardized version of input image &quot;&quot;&quot; # initialize to array of zeros, with same shape as the image standardized_image = np.zeros(image.shape) # iterate over channels for c in range(image.shape[0]): # iterate over the `z` dimension for z in range(image.shape[3]): # get a slice of the image # at channel c and z-th dimension `z` image_slice = image[c,:,:,z] # subtract the mean from image_slice centered = image_slice - np.mean(image_slice) # divide by the standard deviation (only if it is different from zero) if np.std(centered) != 0: centered_scaled = centered / np.std(centered) # update the slice of standardized image # with the scaled centered and scaled image standardized_image[c, :, :, z] = centered_scaled return standardized_image . ### test standardize_test(standardize, X) . stddv for X_norm[0, :, :, 0]: 0.9999999999999999 All tests passed. . X_norm = standardize(X) util.visualize_patch(X_norm[0, :, :, :], y[2]) . . 3D U-Net Model . Now let&#39;s build our model. In this project we will be building a 3D U-net. . This architecture will take advantage of the volumetric shape of MR images and is one of the best performing models for this task. . . Metrics . . Dice Similarity Coefficient . Aside from the architecture, one of the most important elements of any deep learning method is the choice of our loss function. . A natural choice with is the cross-entropy loss function. . However, this loss function is not ideal for segmentation tasks due to heavy class imbalance (there are typically not many positive regions). | . A much more common loss for segmentation tasks is the Dice similarity coefficient, which is a measure of how well two contours overlap. . The Dice index ranges from 0 (complete mismatch) | To 1 (perfect match). | . In general, for two sets $A$ and $B$, the Dice similarity coefficient is defined as: $$ text{DSC}(A, B) = frac{2 times |A cap B|}{|A| + |B|}.$$ . Here we can interpret $A$ and $B$ as sets of voxels, $A$ being the predicted tumor region and $B$ being the ground truth. . Our model will map each voxel to 0 or 1 . 0 means it is a background voxel | 1 means it is part of the segmented region. | . In the dice coefficient, the variables in the formula are: . $x$ : the input image | $f(x)$ : the model output (prediction) | $y$ : the label (actual ground truth) | . The dice coefficient &quot;DSC&quot; is: . $$ text{DSC}(f, x, y) = frac{2 times sum_{i, j} f(x)_{ij} times y_{ij} + epsilon}{ sum_{i,j} f(x)_{ij} + sum_{i, j} y_{ij} + epsilon}$$ . $ epsilon$ is a small number that is added to avoid division by zero | . Let&#39;s define a function for the dice coefficient. . def single_class_dice_coefficient(y_true, y_pred, axis=(0, 1, 2), epsilon=0.00001): &quot;&quot;&quot; Compute dice coefficient for single class. Args: y_true (Tensorflow tensor): tensor of ground truth values for single class. shape: (x_dim, y_dim, z_dim) y_pred (Tensorflow tensor): tensor of predictions for single class. shape: (x_dim, y_dim, z_dim) axis (tuple): spatial axes to sum over when computing numerator and denominator of dice coefficient. Hint: pass this as the &#39;axis&#39; argument to the K.sum function. epsilon (float): small constant added to numerator and denominator to avoid divide by 0 errors. Returns: dice_coefficient (float): computed value of dice coefficient. &quot;&quot;&quot; intersection = K.sum(y_true * y_pred) dice_numerator = 2 * intersection + epsilon dice_denominator = K.sum(y_true) + K.sum(y_pred) + epsilon dice_coefficient = dice_numerator / dice_denominator return dice_coefficient . ### test # test with a large epsilon in order to catch errors. # In order to pass the tests, set epsilon = 1 epsilon = 1 sess = K.get_session() single_class_dice_coefficient_test(single_class_dice_coefficient, epsilon, sess) . Test Case 1: Pred: [[1. 0.] [0. 1.]] Label: [[1. 1.] [0. 0.]] Dice coefficient: 0.6 - Test Case 2: Pred: [[1. 0.] [0. 1.]] Label: [[1. 1.] [0. 1.]] Dice coefficient: 0.8333333333333334 All tests passed. . . Dice Coefficient for Multiple Classes . Now that we have the single class case, we can think about how to approach the multi class context. . Remember, we want segmentations for each of the 3 classes of abnormality (edema, enhancing tumor, non-enhancing tumor). | This will give us 3 different dice coefficients (one for each abnormality class). | To combine these, we can just take the average. We can write that the overall dice coefficient is: | . $$DC(f, x, y) = frac{1}{3} left ( DC_{1}(f, x, y) + DC_{2}(f, x, y) + DC_{3}(f, x, y) right )$$ . $DC_{1}$, $DC_{2}$ and $DC_{3}$ are edema, enhancing tumor, and non-enhancing tumor dice coefficients. | . For any number of classes $C$, the equation becomes: $$DC(f, x, y) = frac{1}{C} sum_{c=1}^{C} left ( DC_{c}(f, x, y) right )$$ . In this case, with three categories, $C = 3$ . Lets implement the mean dice coefficient. . def dice_coefficient(y_true, y_pred, axis=(1, 2, 3), epsilon=0.00001): &quot;&quot;&quot; Compute mean dice coefficient over all abnormality classes. Args: y_true (Tensorflow tensor): tensor of ground truth values for all classes. shape: (num_classes, x_dim, y_dim, z_dim) y_pred (Tensorflow tensor): tensor of predictions for all classes. shape: (num_classes, x_dim, y_dim, z_dim) axis (tuple): spatial axes to sum over when computing numerator and denominator of dice coefficient. Hint: pass this as the &#39;axis&#39; argument to the K.sum function. epsilon (float): small constant add to numerator and denominator to avoid divide by 0 errors. Returns: dice_coefficient (float): computed value of dice coefficient. &quot;&quot;&quot; intersection = K.sum(y_true * y_pred, axis=axis) dice_numerator = 2 * intersection + epsilon dice_denominator = K.sum(y_true, axis=axis) + K.sum(y_pred, axis=axis) + epsilon dice_coefficient = K.mean(dice_numerator / dice_denominator) return dice_coefficient . ### test # test with a large epsilon in order to catch errors. # In order to pass the tests, set epsilon = 1 epsilon = 1 sess = K.get_session() dice_coefficient_test(dice_coefficient, epsilon, sess) . Test Case 1: Pred: [[1. 0.] [0. 1.]] Label: [[1. 1.] [0. 0.]] Dice coefficient: 0.6 - Test Case 2: Pred: [[1. 0.] [0. 1.]] Label: [[1. 1.] [0. 1.]] Dice coefficient: 0.8333333333333334 - Test Case 3: Pred: class = 0 [[1. 0.] [0. 1.]] class = 1 [[1. 0.] [0. 1.]] Label: class = 0 [[1. 1.] [0. 0.]] class = 1 [[1. 1.] [0. 1.]] Dice coefficient: 0.7166666666666667 All tests passed. . . Soft Dice Loss . While the Dice Coefficient makes intuitive sense, it is not the best for training. . This is because it takes in discrete values (zeros and ones). | The model outputs probabilities that each pixel is, say, a tumor or not, and we want to be able to backpropagate through those outputs. | . Therefore, we need an analogue of the Dice loss which takes real valued input. This is where the Soft Dice loss comes in. The formula is: . $$ mathcal{L}_{Dice}(p, q) = 1 - frac{2 times sum_{i, j} p_{ij}q_{ij} + epsilon}{ left( sum_{i, j} p_{ij}^2 right) + left( sum_{i, j} q_{ij}^2 right) + epsilon}$$ . $p$ is our predictions | $q$ is the ground truth | In practice each $q_i$ will either be 0 or 1. | $ epsilon$ is a small number that is added to avoid division by zero | . The soft Dice loss ranges between . 0: perfectly matching the ground truth distribution $q$ | 1: complete mismatch with the ground truth. | . You can also check that if $p_i$ and $q_i$ are each 0 or 1, then the soft Dice loss is just one minus the dice coefficient. . Lets mplement the soft dice loss. . def soft_dice_loss(y_true, y_pred, axis=(1, 2, 3), epsilon=0.00001): &quot;&quot;&quot; Compute mean soft dice loss over all abnormality classes. Args: y_true (Tensorflow tensor): tensor of ground truth values for all classes. shape: (num_classes, x_dim, y_dim, z_dim) y_pred (Tensorflow tensor): tensor of soft predictions for all classes. shape: (num_classes, x_dim, y_dim, z_dim) axis (tuple): spatial axes to sum over when computing numerator and denominator in formula for dice loss. Hint: pass this as the &#39;axis&#39; argument to the K.sum function. epsilon (float): small constant added to numerator and denominator to avoid divide by 0 errors. Returns: dice_loss (float): computed value of dice loss. &quot;&quot;&quot; intersection = K.sum(y_true * y_pred, axis=axis) dice_numerator = 2. * intersection + epsilon dice_denominator = K.sum(K.square(y_true), axis=axis) + K.sum(K.square(y_pred), axis=axis) + epsilon dice_loss = 1 - K.mean(dice_numerator / dice_denominator) return dice_loss . ### test # test with a large epsilon in order to catch errors. # In order to pass the tests, set epsilon = 1 epsilon = 1 sess = K.get_session() soft_dice_loss_test(soft_dice_loss, epsilon, sess) . Test Case 1: Pred: [[1. 0.] [0. 1.]] Label: [[1. 1.] [0. 0.]] Soft Dice Loss: 0.4 - Test Case 2: Pred: [[0.5 0. ] [0. 0.5]] Label: [[1. 1.] [0. 0.]] Soft Dice Loss: 0.4285714285714286 - Test Case 3: Pred: [[1. 0.] [0. 1.]] Label: [[1. 1.] [0. 1.]] Soft Dice Loss: 0.16666666666666663 - Test Case 4: Pred: [[1. 0.8] [0. 1. ]] Label: [[1. 1.] [0. 1.]] Soft Dice Loss: 0.006024096385542355 - Test Case 5: Pred: class = 0 [[0.5 0. ] [0. 0.5]] class = 1 [[1. 0.8] [0. 1. ]] Label: class = 0 [[1. 1.] [0. 0.]] class = 1 [[1. 1.] [0. 1.]] Soft Dice Loss: 0.21729776247848553 - Test Case 6: Soft Dice Loss: 0.4375 All tests passed. . . Create and Train the Model . We can now create the model. . model = util.unet_model_3d(loss_function=soft_dice_loss, metrics=[dice_coefficient]) . . Load a Pre-Trained Model . base_dir = HOME_DIR + &quot;processed/&quot; with open(base_dir + &quot;config.json&quot;) as json_file: config = json.load(json_file) # Get generators for training and validation sets train_generator = util.VolumeDataGenerator(config[&quot;train&quot;], base_dir + &quot;train/&quot;, batch_size=3, dim=(160, 160, 16), verbose=0) valid_generator = util.VolumeDataGenerator(config[&quot;valid&quot;], base_dir + &quot;valid/&quot;, batch_size=3, dim=(160, 160, 16), verbose=0) . model.load_weights(HOME_DIR + &quot;model_pretrained.hdf5&quot;) . . Evaluation . Now that we have a trained model, we&#39;ll learn to extract its predictions and evaluate its performance on scans from our validation set. . . Patch-level Predictions . When applying the model, we&#39;ll want to look at segmentations for individual scans (entire scans, not just the sub-volumes) . This will be a bit complicated because of our sub-volume approach. | First let&#39;s keep things simple and extract model predictions for sub-volumes. | We can use the sub-volume which we extracted at the beginning | . util.visualize_patch(X_norm[0, :, :, :], y[2]) . Add a &#39;batch&#39; dimension . We can extract predictions by calling model.predict on the patch. . X_norm_with_batch_dimension = np.expand_dims(X_norm, axis=0) patch_pred = model.predict(X_norm_with_batch_dimension) . Convert prediction from probability into a category . Currently, each element of patch_pred is a number between 0.0 and 1.0. . Each number is the model&#39;s confidence that a voxel is part of a given class. | We will convert these to discrete 0 and 1 integers by using a threshold. | We&#39;ll use a threshold of 0.5. | In real applications, we would tune this to achieve your required level of sensitivity or specificity. | . # set threshold. threshold = 0.5 # use threshold to get hard predictions patch_pred[patch_pred &gt; threshold] = 1.0 patch_pred[patch_pred &lt;= threshold] = 0.0 . Now let&#39;s visualize the original patch and ground truth alongside our thresholded predictions. . print(&quot;Patch and ground truth&quot;) util.visualize_patch(X_norm[0, :, :, :], y[2]) plt.show() print(&quot;Patch and prediction&quot;) util.visualize_patch(X_norm[0, :, :, :], patch_pred[0, 2, :, :, :]) plt.show() . Patch and ground truth . Patch and prediction . . Sensitivity and Specificity . The model is covering some of the relevant areas, but it&#39;s definitely not perfect. . To quantify its performance, we can use per-pixel sensitivity and specificity. | . Recall that in terms of the true positives, true negatives, false positives, and false negatives, . $$ text{sensitivity} = frac{ text{true positives}}{ text{true positives} + text{false negatives}}$$ . $$ text{specificity} = frac{ text{true negatives}}{ text{true negatives} + text{false positives}}$$ . Let&#39;s write a function to compute the sensitivity and specificity per output class. . def compute_class_sens_spec(pred, label, class_num): &quot;&quot;&quot; Compute sensitivity and specificity for a particular example for a given class. Args: pred (np.array): binary arrary of predictions, shape is (num classes, height, width, depth). label (np.array): binary array of labels, shape is (num classes, height, width, depth). class_num (int): number between 0 - (num_classes -1) which says which prediction class to compute statistics for. Returns: sensitivity (float): for a given class_num. specificity (float): for a given class_num. &quot;&quot;&quot; # extract sub-array for specified class class_pred = pred[class_num] class_label = label[class_num] # compute: # true positives tp = np.sum(np.logical_and(class_pred == 1, class_label == 1)) # true negatives tn = np.sum(np.logical_and(class_pred == 0, class_label == 0)) #false positives fp = np.sum(np.logical_and(class_pred == 1, class_label == 0)) # false negatives fn = np.sum(np.logical_and(class_pred == 0, class_label == 1)) # compute sensitivity and specificity sensitivity = tp / (tp + fn) specificity = tn / (tn + fp) return sensitivity, specificity . ### test compute_class_sens_spec_test(compute_class_sens_spec) . Test Case 1: Pred: [[1. 0.] [0. 1.]] Label: [[1. 1.] [0. 0.]] Sensitivity: 0.5 Specificity: 0.5 - Test Case 2: Pred: [[1. 0.] [0. 1.]] Label: [[1. 1.] [0. 1.]] Sensitivity: 0.6666666666666666 Specificity: 1.0 - Test Case 3: . y_test preds_test category . 0 1 | 1 | TP | . 1 1 | 1 | TP | . 2 0 | 0 | TN | . 3 0 | 0 | TN | . 4 0 | 0 | TN | . 5 0 | 1 | FP | . 6 0 | 1 | FP | . 7 0 | 1 | FP | . 8 0 | 1 | FP | . 9 1 | 0 | FN | . 10 1 | 0 | FN | . 11 1 | 0 | FN | . 12 1 | 0 | FN | . 13 1 | 0 | FN | . Sensitivity: 0.2857142857142857 Specificity: 0.42857142857142855 All tests passed. . Sensitivity and Specificity for the patch prediction . Next let&#39;s compute the sensitivity and specificity on that patch for expanding tumors. . sensitivity, specificity = compute_class_sens_spec(patch_pred[0], y, 2) print(f&quot;Sensitivity: {sensitivity:.4f}&quot;) print(f&quot;Specificity: {specificity:.4f}&quot;) . Sensitivity: 0.7891 Specificity: 0.9960 . We can also display the sensitivity and specificity for each class. . def get_sens_spec_df(pred, label): patch_metrics = pd.DataFrame( columns = [&#39;Edema&#39;, &#39;Non-Enhancing Tumor&#39;, &#39;Enhancing Tumor&#39;], index = [&#39;Sensitivity&#39;, &#39;Specificity&#39;]) for i, class_name in enumerate(patch_metrics.columns): sens, spec = compute_class_sens_spec(pred, label, i) patch_metrics.loc[&#39;Sensitivity&#39;, class_name] = round(sens,4) patch_metrics.loc[&#39;Specificity&#39;, class_name] = round(spec,4) return patch_metrics . df = get_sens_spec_df(patch_pred[0], y) print(df) . Edema Non-Enhancing Tumor Enhancing Tumor Sensitivity 0.9085 0.9505 0.7891 Specificity 0.9848 0.9961 0.996 . . Running on Entire Scans . As of now, our model just runs on patches, but what we really want to see is our model&#39;s result on a whole MRI scan. . To do this, we generate patches for the scan. | Then we run the model on the patches. | Then combine the results together to get a fully labeled MR image. | . The output of our model will be a 4D array with 3 probability values for each voxel in our data. . We then can use a threshold (which you can find by a calibration process) to decide whether or not to report a label for each voxel. | . We have a function that stitches the patches together: predict_and_viz(image, label, model, threshold) . Inputs: an image, label and model. | Ouputs: the model prediction over the whole image, and a visual of the ground truth and prediction. | . image, label = load_case(DATA_DIR + &quot;imagesTr/BRATS_003.nii.gz&quot;, DATA_DIR + &quot;labelsTr/BRATS_003.nii.gz&quot;) pred = util.predict_and_viz(image, label, model, .5, loc=(130, 130, 77)) . Check how well the predictions do . We can see some of the discrepancies between the model and the ground truth visually. . We can also use the functions we wrote previously to compute sensitivity and specificity for each class over the whole scan. | First we need to format the label and prediction to match our functions expect. | . whole_scan_label = keras.utils.to_categorical(label, num_classes = 4) whole_scan_pred = pred # move axis to match shape expected in functions whole_scan_label = np.moveaxis(whole_scan_label, 3 ,0)[1:4] whole_scan_pred = np.moveaxis(whole_scan_pred, 3, 0)[1:4] . Now we can compute sensitivity and specificity for each class just like before. . whole_scan_df = get_sens_spec_df(whole_scan_pred, whole_scan_label) print(whole_scan_df) . Edema Non-Enhancing Tumor Enhancing Tumor Sensitivity 0.902 0.2617 0.8496 Specificity 0.9894 0.9998 0.9982 . Conclusion . In this project we will built a deep learning model to automatically segment tumor regions in brain using MRI (Magnetic Resonance Imaging) scans. . We looked at: . What is in an MR image | Standard data preparation techniques for MRI datasets | Metrics and loss functions for segmentation | Visualizing and evaluating segmentation models | .",
            "url": "https://www.livingdatalab.com/health/deep-learning/2022/06/05/segment-brain-tumor-mri.html",
            "relUrl": "/health/deep-learning/2022/06/05/segment-brain-tumor-mri.html",
            "date": " • Jun 5, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Evaluating Healthcare Diagnostic Models",
            "content": "Introduction . In my previous article I developed a deep learning model able to classify 14 different diseases using chest x-rays. There are various meterics available to help evaluate model performance, but there are specific metrics that are of particular relevance to evaluating models for medical diagnosis. These metrics which we will be covering in this article are: . Accuracy | Prevalence | Specificity &amp; Sensitivity | PPV and NPV | ROC curve and AUCROC (c-statistic) | Confidence Intervals | Overview . Let&#39;s take a look at our dataset. The data is stored in two CSV files called train_preds.csv and valid_preds.csv. We have precomputed the model outputs for our test cases. We&#39;ll work with these predictions and the true class labels throughout this article. . train_results = pd.read_csv(&quot;data/train_preds.csv&quot;) valid_results = pd.read_csv(&quot;data/valid_preds.csv&quot;) # the labels in our dataset class_labels = [&#39;Cardiomegaly&#39;, &#39;Emphysema&#39;, &#39;Effusion&#39;, &#39;Hernia&#39;, &#39;Infiltration&#39;, &#39;Mass&#39;, &#39;Nodule&#39;, &#39;Atelectasis&#39;, &#39;Pneumothorax&#39;, &#39;Pleural_Thickening&#39;, &#39;Pneumonia&#39;, &#39;Fibrosis&#39;, &#39;Edema&#39;, &#39;Consolidation&#39;] # the labels for prediction values in our dataset pred_labels = [l + &quot;_pred&quot; for l in class_labels] . Extract the labels (y) and the predictions (pred). . y = valid_results[class_labels].values pred = valid_results[pred_labels].values . Run the next cell to view them side by side. . # let&#39;s take a peek at our dataset valid_results[np.concatenate([class_labels, pred_labels])].head() . Cardiomegaly Emphysema Effusion Hernia Infiltration Mass Nodule Atelectasis Pneumothorax Pleural_Thickening ... Infiltration_pred Mass_pred Nodule_pred Atelectasis_pred Pneumothorax_pred Pleural_Thickening_pred Pneumonia_pred Fibrosis_pred Edema_pred Consolidation_pred . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0.256020 | 0.266928 | 0.312440 | 0.460342 | 0.079453 | 0.271495 | 0.276861 | 0.398799 | 0.015867 | 0.156320 | . 1 0 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | ... | 0.382199 | 0.176825 | 0.465807 | 0.489424 | 0.084595 | 0.377318 | 0.363582 | 0.638024 | 0.025948 | 0.144419 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0.427727 | 0.115513 | 0.249030 | 0.035105 | 0.238761 | 0.167095 | 0.166389 | 0.262463 | 0.007758 | 0.125790 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0.158596 | 0.259460 | 0.334870 | 0.266489 | 0.073371 | 0.229834 | 0.191281 | 0.344348 | 0.008559 | 0.119153 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0.536762 | 0.198797 | 0.273110 | 0.186771 | 0.242122 | 0.309786 | 0.411771 | 0.244666 | 0.126930 | 0.342409 | . 5 rows × 28 columns . To further understand our dataset details, here&#39;s a histogram of the number of samples for each label in the validation dataset: . plt.xticks(rotation=90) plt.bar(x = class_labels, height= y.sum(axis=0)); . It seem like our dataset has an imbalanced population of samples. Specifically, our dataset has a small number of patients diagnosed with a Hernia. . Metrics . True Positives, False Positives, True Negatives and False Negatives . The most basic statistics to compute from the model predictions are the true positives, true negatives, false positives, and false negatives. . As the name suggests . True Positive (TP): The model classifies the example as positive, and the actual label also positive. | False Positive (FP): The model classifies the example as positive, but the actual label is negative. | True Negative (TN): The model classifies the example as negative, and the actual label is also negative. | False Negative (FN): The model classifies the example as negative, but the label is actually positive. | . We will count the number of TP, FP, TN and FN in the given data. All of our metrics can be built off of these four statistics. . Recall that the model outputs real numbers between 0 and 1. . To compute binary class predictions, we need to convert these to either 0 or 1. | We&#39;ll do this using a threshold value $th$. | Any model outputs above $th$ are set to 1, and below $th$ are set to 0. | . All of our metrics (except for AUC at the end) will depend on the choice of this threshold. . Let&#39;s define some functions for computing each of these basic statistics. . def true_positives(y, pred, th=0.5): &quot;&quot;&quot; Count true positives. Args: y (np.array): ground truth, size (n_examples) pred (np.array): model output, size (n_examples) th (float): cutoff value for positive prediction from model Returns: TP (int): true positives &quot;&quot;&quot; TP = 0 # get thresholded predictions thresholded_preds = pred &gt;= th # compute TP TP = np.sum((y == 1) &amp; (thresholded_preds == 1)) return TP def true_negatives(y, pred, th=0.5): &quot;&quot;&quot; Count true negatives. Args: y (np.array): ground truth, size (n_examples) pred (np.array): model output, size (n_examples) th (float): cutoff value for positive prediction from model Returns: TN (int): true negatives &quot;&quot;&quot; TN = 0 # get thresholded predictions thresholded_preds = pred &gt;= th # compute TN TN = np.sum((y == 0) &amp; (thresholded_preds == 0)) return TN def false_positives(y, pred, th=0.5): &quot;&quot;&quot; Count false positives. Args: y (np.array): ground truth, size (n_examples) pred (np.array): model output, size (n_examples) th (float): cutoff value for positive prediction from model Returns: FP (int): false positives &quot;&quot;&quot; FP = 0 # get thresholded predictions thresholded_preds = pred &gt;= th # compute FP FP = np.sum((y == 0) &amp; (thresholded_preds == 1)) return FP def false_negatives(y, pred, th=0.5): &quot;&quot;&quot; Count false positives. Args: y (np.array): ground truth, size (n_examples) pred (np.array): model output, size (n_examples) th (float): cutoff value for positive prediction from model Returns: FN (int): false negatives &quot;&quot;&quot; FN = 0 # get thresholded predictions thresholded_preds = pred &gt;= th # compute FN FN = np.sum((y == 1) &amp; (thresholded_preds == 0)) return FN . # Test functions get_tp_tn_fp_fn_test(true_positives, true_negatives, false_positives, false_negatives) . y_test preds_test category . 0 1 | 0.8 | TP | . 1 1 | 0.7 | TP | . 2 0 | 0.4 | TN | . 3 0 | 0.3 | TN | . 4 0 | 0.2 | TN | . 5 0 | 0.5 | FP | . 6 0 | 0.6 | FP | . 7 0 | 0.7 | FP | . 8 0 | 0.8 | FP | . 9 1 | 0.1 | FN | . 10 1 | 0.2 | FN | . 11 1 | 0.3 | FN | . 12 1 | 0.4 | FN | . 13 1 | 0.0 | FN | . Your functions calcualted: TP: 2 TN: 3 FP: 4 FN: 5 All tests passed. All tests passed. All tests passed. All tests passed. . Expected output . Your functions calcualted: TP: 2 TN: 3 FP: 4 FN: 5 . All tests passed. All tests passed. All tests passed. All tests passed. . # Add these to a table for each disease util.get_performance_metrics(y, pred, class_labels) . TP TN FP FN Accuracy Prevalence Sensitivity Specificity PPV NPV AUC F1 Threshold . . Cardiomegaly 16 | 814 | 169 | 1 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Emphysema 20 | 869 | 103 | 8 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Effusion 99 | 690 | 196 | 15 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Hernia 1 | 743 | 255 | 1 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Infiltration 114 | 543 | 265 | 78 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Mass 40 | 789 | 158 | 13 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Nodule 28 | 731 | 220 | 21 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Atelectasis 64 | 657 | 249 | 30 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Pneumothorax 24 | 785 | 183 | 8 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Pleural_Thickening 24 | 713 | 259 | 4 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Pneumonia 14 | 661 | 320 | 5 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Fibrosis 10 | 725 | 261 | 4 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Edema 15 | 767 | 213 | 5 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Consolidation 36 | 658 | 297 | 9 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Right now it only has TP, TN, FP, FN. Throughout this article we&#39;ll fill in all the other metrics to learn more about our model performance. . Accuracy . Let&#39;s use a threshold of .5 for the probability cutoff for our predictions for all classes and calculate our model&#39;s accuracy as we would normally do in a machine learning problem. . $$accuracy = frac{ text{true positives} + text{true negatives}}{ text{true positives} + text{true negatives} + text{false positives} + text{false negatives}}$$ . Let&#39;s define a function to calculate this. . def get_accuracy(y, pred, th=0.5): &quot;&quot;&quot; Compute accuracy of predictions at threshold. Args: y (np.array): ground truth, size (n_examples) pred (np.array): model output, size (n_examples) th (float): cutoff value for positive prediction from model Returns: accuracy (float): accuracy of predictions at threshold &quot;&quot;&quot; accuracy = 0.0 # get TP, FP, TN, FN using our previously defined functions TP = true_positives(y, pred, th) FP = false_positives(y, pred, th) TN = true_negatives(y, pred, th) FN = false_negatives(y, pred, th) # Compute accuracy using TP, FP, TN, FN accuracy = (TP + TN) / (TP + FP + TN + FN) return accuracy . # Test function get_accuracy_test(get_accuracy) . Test Case: Test Labels: [1 0 0 1 1] Test Predictions: [0.8 0.8 0.4 0.6 0.3] Threshold: 0.5 Computed Accuracy: 0.6 All tests passed. . Expected output: . Test Case: Test Labels: [1 0 0 1 1] Test Predictions: [0.8 0.8 0.4 0.6 0.3] Threshold: 0.5 Computed Accuracy: 0.6 . All tests passed. . Let&#39;s compute this for each disease. . util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy) . TP TN FP FN Accuracy Prevalence Sensitivity Specificity PPV NPV AUC F1 Threshold . . Cardiomegaly 16 | 814 | 169 | 1 | 0.83 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Emphysema 20 | 869 | 103 | 8 | 0.889 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Effusion 99 | 690 | 196 | 15 | 0.789 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Hernia 1 | 743 | 255 | 1 | 0.744 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Infiltration 114 | 543 | 265 | 78 | 0.657 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Mass 40 | 789 | 158 | 13 | 0.829 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Nodule 28 | 731 | 220 | 21 | 0.759 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Atelectasis 64 | 657 | 249 | 30 | 0.721 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Pneumothorax 24 | 785 | 183 | 8 | 0.809 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Pleural_Thickening 24 | 713 | 259 | 4 | 0.737 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Pneumonia 14 | 661 | 320 | 5 | 0.675 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Fibrosis 10 | 725 | 261 | 4 | 0.735 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Edema 15 | 767 | 213 | 5 | 0.782 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Consolidation 36 | 658 | 297 | 9 | 0.694 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . If we were to judge our model&#39;s performance based on the accuracy metric, we would say that our model is not very accurate for detecting the Infiltration cases (accuracy of 0.657) but pretty accurate for detecting Emphysema (accuracy of 0.889). . But is that really the case?... . Let&#39;s imagine a model that simply predicts that any patient does Not have Emphysema, regardless of patient&#39;s measurements. Let&#39;s calculate the accuracy for such a model. . get_accuracy(valid_results[&quot;Emphysema&quot;].values, np.zeros(len(valid_results))) . 0.972 . As you can see above, such a model would be 97% accurate! Even better than our deep learning based model. . But is this really a good model? Wouldn&#39;t this model be wrong 100% of the time if the patient actually had this condition? . This issue can be particularly common in cases where we have very imbalanced classes and few examples of a particular disease, in such cases accuracy can be very misleading for what we want to know. . In the following sections, we will address this concern with more advanced model measures - sensitivity and specificity - that evaluate how well the model predicts positives for patients with the condition and negatives for cases that actually do not have the condition. . Prevalence . Another important concept is prevalence. . In a medical context, prevalence is the proportion of people in the population who have the disease (or condition, etc). | In machine learning terms, this is the proportion of positive examples. | . We encountered prevalence in a previous article on measures of disease in epidemiology. . The expression for prevalence is: . $$prevalence = frac{1}{N} sum_{i} y_i$$ . where $y_i = 1$ when the example is &#39;positive&#39; (has the disease). . Let&#39;s define a function to measure prevalence for each disease. . def get_prevalence(y): &quot;&quot;&quot; Compute prevalence. Args: y (np.array): ground truth, size (n_examples) Returns: prevalence (float): prevalence of positive cases &quot;&quot;&quot; prevalence = 0.0 prevalence = np.sum(y) / y.size return prevalence . # Test function get_prevalence_test(get_prevalence) . Test Case: Test Labels: [1 0 0 1 1 0 0 0 0 1] Computed Prevalence: 0.4 All tests passed. . Expected output: . Test Case: Test Labels: [1 0 0 1 1 0 0 0 0 1] Computed Prevalence: 0.4 . All tests passed. . # Calculate this for each disease util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence) . TP TN FP FN Accuracy Prevalence Sensitivity Specificity PPV NPV AUC F1 Threshold . . Cardiomegaly 16 | 814 | 169 | 1 | 0.83 | 0.017 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Emphysema 20 | 869 | 103 | 8 | 0.889 | 0.028 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Effusion 99 | 690 | 196 | 15 | 0.789 | 0.114 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Hernia 1 | 743 | 255 | 1 | 0.744 | 0.002 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Infiltration 114 | 543 | 265 | 78 | 0.657 | 0.192 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Mass 40 | 789 | 158 | 13 | 0.829 | 0.053 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Nodule 28 | 731 | 220 | 21 | 0.759 | 0.049 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Atelectasis 64 | 657 | 249 | 30 | 0.721 | 0.094 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Pneumothorax 24 | 785 | 183 | 8 | 0.809 | 0.032 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Pleural_Thickening 24 | 713 | 259 | 4 | 0.737 | 0.028 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Pneumonia 14 | 661 | 320 | 5 | 0.675 | 0.019 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Fibrosis 10 | 725 | 261 | 4 | 0.735 | 0.014 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Edema 15 | 767 | 213 | 5 | 0.782 | 0.02 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Consolidation 36 | 658 | 297 | 9 | 0.694 | 0.045 | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Hernia has a prevalence 0.002, which is the rarest among the studied conditions in our dataset. . Sensitivity and Specificity . Sensitivity and specificity are two of the most prominent numbers that are used to measure diagnostics tests. . Sensitivity is the probability that our test outputs positive given that the case is actually positive. | Specificity is the probability that the test outputs negative given that the case is actually negative. | . We can phrase this easily in terms of true positives, true negatives, false positives, and false negatives: . $$sensitivity = frac{ text{true positives}}{ text{true positives} + text{false negatives}}$$ . $$specificity = frac{ text{true negatives}}{ text{true negatives} + text{false positives}}$$ . Let&#39;s calculate sensitivity and specificity for our model. . def get_sensitivity(y, pred, th=0.5): &quot;&quot;&quot; Compute sensitivity of predictions at threshold. Args: y (np.array): ground truth, size (n_examples) pred (np.array): model output, size (n_examples) th (float): cutoff value for positive prediction from model Returns: sensitivity (float): probability that our test outputs positive given that the case is actually positive &quot;&quot;&quot; sensitivity = 0.0 # get TP and FN using our previously defined functions TP = true_positives(y, pred, th) FN = false_negatives(y, pred, th) # use TP and FN to compute sensitivity sensitivity = TP / (TP + FN) return sensitivity def get_specificity(y, pred, th=0.5): &quot;&quot;&quot; Compute specificity of predictions at threshold. Args: y (np.array): ground truth, size (n_examples) pred (np.array): model output, size (n_examples) th (float): cutoff value for positive prediction from model Returns: specificity (float): probability that the test outputs negative given that the case is actually negative &quot;&quot;&quot; specificity = 0.0 # get TN and FP using our previously defined functions TN = true_negatives(y, pred, th) FP = false_positives(y, pred, th) # use TN and FP to compute specificity specificity = TN / (TN + FP) return specificity . # Test function get_sensitivity_specificity_test(get_sensitivity, get_specificity) . Test Case: Test Labels: [1 0 0 1 1] Test Predictions: [1 0 0 1 1] Threshold: 0.5 Computed Sensitivity: 0.6666666666666666 Computed Specificity: 0.5 All tests passed. All tests passed. . Expected output: . Test Case: Test Labels: [1 0 0 1 1] Test Predictions: [1 0 0 1 1] Threshold: 0.5 Computed Sensitivity: 0.6666666666666666 Computed Specificity: 0.5 . All tests passed. All tests passed. . # Calculate for all diseases util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, sens=get_sensitivity, spec=get_specificity) . TP TN FP FN Accuracy Prevalence Sensitivity Specificity PPV NPV AUC F1 Threshold . . Cardiomegaly 16 | 814 | 169 | 1 | 0.83 | 0.017 | 0.941 | 0.828 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Emphysema 20 | 869 | 103 | 8 | 0.889 | 0.028 | 0.714 | 0.894 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Effusion 99 | 690 | 196 | 15 | 0.789 | 0.114 | 0.868 | 0.779 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Hernia 1 | 743 | 255 | 1 | 0.744 | 0.002 | 0.5 | 0.744 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Infiltration 114 | 543 | 265 | 78 | 0.657 | 0.192 | 0.594 | 0.672 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Mass 40 | 789 | 158 | 13 | 0.829 | 0.053 | 0.755 | 0.833 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Nodule 28 | 731 | 220 | 21 | 0.759 | 0.049 | 0.571 | 0.769 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Atelectasis 64 | 657 | 249 | 30 | 0.721 | 0.094 | 0.681 | 0.725 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Pneumothorax 24 | 785 | 183 | 8 | 0.809 | 0.032 | 0.75 | 0.811 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Pleural_Thickening 24 | 713 | 259 | 4 | 0.737 | 0.028 | 0.857 | 0.734 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Pneumonia 14 | 661 | 320 | 5 | 0.675 | 0.019 | 0.737 | 0.674 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Fibrosis 10 | 725 | 261 | 4 | 0.735 | 0.014 | 0.714 | 0.735 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Edema 15 | 767 | 213 | 5 | 0.782 | 0.02 | 0.75 | 0.783 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Consolidation 36 | 658 | 297 | 9 | 0.694 | 0.045 | 0.8 | 0.689 | Not Defined | Not Defined | Not Defined | Not Defined | 0.5 | . Note that specificity and sensitivity do not depend on the prevalence of the positive class in the dataset. . This is because the statistics are only computed within people of the same class | Sensitivity only considers output on people in the positive class | Similarly, specificity only considers output on people in the negative class. | . . PPV and NPV . Diagnostically, however, sensitivity and specificity are not helpful. Sensitivity, for example, tells us the probability our test outputs positive given that the person already has the condition. Here, we are conditioning on the thing we would like to find out (whether the patient has the condition)! . What would be more helpful is the probability that the person has the disease given that our test outputs positive. That brings us to positive predictive value (PPV) and negative predictive value (NPV). . Positive predictive value (PPV) is the probability that subjects with a positive screening test truly have the disease. | Negative predictive value (NPV) is the probability that subjects with a negative screening test truly don&#39;t have the disease. | . Again, we can formulate these in terms of true positives, true negatives, false positives, and false negatives: . $$PPV = frac{ text{true positives}}{ text{true positives} + text{false positives}}$$ . $$NPV = frac{ text{true negatives}}{ text{true negatives} + text{false negatives}}$$ . We also encountered PPV and NPV in a previous article on measures of disease in epidemiology. . Let&#39;s calculate PPV &amp; NPV for our model. . def get_ppv(y, pred, th=0.5): &quot;&quot;&quot; Compute PPV of predictions at threshold. Args: y (np.array): ground truth, size (n_examples) pred (np.array): model output, size (n_examples) th (float): cutoff value for positive prediction from model Returns: PPV (float): positive predictive value of predictions at threshold &quot;&quot;&quot; PPV = 0.0 # get TP and FP using our previously defined functions TP = true_positives(y, pred, th) FP = false_positives(y, pred, th) # use TP and FP to compute PPV PPV = TP / (TP + FP) return PPV def get_npv(y, pred, th=0.5): &quot;&quot;&quot; Compute NPV of predictions at threshold. Args: y (np.array): ground truth, size (n_examples) pred (np.array): model output, size (n_examples) th (float): cutoff value for positive prediction from model Returns: NPV (float): negative predictive value of predictions at threshold &quot;&quot;&quot; NPV = 0.0 # get TN and FN using our previously defined functions TN = true_negatives(y, pred, th) FN = false_negatives(y, pred, th) # use TN and FN to compute NPV NPV = TN / (TN + FN) return NPV . # Test function get_ppv_npv_test(get_ppv, get_npv) . Test Case: Test Labels: [1 0 0 1 1] Test Predictions: [1 0 0 1 1] Threshold: 0.5 Computed PPV: 0.6666666666666666 Computed NPV: 0.5 All tests passed. All tests passed. . Expected output: . Test Case: Test Labels: [1 0 0 1 1] Test Predictions: [1 0 0 1 1] Threshold: 0.5 Computed PPV: 0.6666666666666666 Computed NPV: 0.5 . All tests passed. All tests passed. . # Calculate for all diseases util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv) . TP TN FP FN Accuracy Prevalence Sensitivity Specificity PPV NPV AUC F1 Threshold . . Cardiomegaly 16 | 814 | 169 | 1 | 0.83 | 0.017 | 0.941 | 0.828 | 0.086 | 0.999 | Not Defined | Not Defined | 0.5 | . Emphysema 20 | 869 | 103 | 8 | 0.889 | 0.028 | 0.714 | 0.894 | 0.163 | 0.991 | Not Defined | Not Defined | 0.5 | . Effusion 99 | 690 | 196 | 15 | 0.789 | 0.114 | 0.868 | 0.779 | 0.336 | 0.979 | Not Defined | Not Defined | 0.5 | . Hernia 1 | 743 | 255 | 1 | 0.744 | 0.002 | 0.5 | 0.744 | 0.004 | 0.999 | Not Defined | Not Defined | 0.5 | . Infiltration 114 | 543 | 265 | 78 | 0.657 | 0.192 | 0.594 | 0.672 | 0.301 | 0.874 | Not Defined | Not Defined | 0.5 | . Mass 40 | 789 | 158 | 13 | 0.829 | 0.053 | 0.755 | 0.833 | 0.202 | 0.984 | Not Defined | Not Defined | 0.5 | . Nodule 28 | 731 | 220 | 21 | 0.759 | 0.049 | 0.571 | 0.769 | 0.113 | 0.972 | Not Defined | Not Defined | 0.5 | . Atelectasis 64 | 657 | 249 | 30 | 0.721 | 0.094 | 0.681 | 0.725 | 0.204 | 0.956 | Not Defined | Not Defined | 0.5 | . Pneumothorax 24 | 785 | 183 | 8 | 0.809 | 0.032 | 0.75 | 0.811 | 0.116 | 0.99 | Not Defined | Not Defined | 0.5 | . Pleural_Thickening 24 | 713 | 259 | 4 | 0.737 | 0.028 | 0.857 | 0.734 | 0.085 | 0.994 | Not Defined | Not Defined | 0.5 | . Pneumonia 14 | 661 | 320 | 5 | 0.675 | 0.019 | 0.737 | 0.674 | 0.042 | 0.992 | Not Defined | Not Defined | 0.5 | . Fibrosis 10 | 725 | 261 | 4 | 0.735 | 0.014 | 0.714 | 0.735 | 0.037 | 0.995 | Not Defined | Not Defined | 0.5 | . Edema 15 | 767 | 213 | 5 | 0.782 | 0.02 | 0.75 | 0.783 | 0.066 | 0.994 | Not Defined | Not Defined | 0.5 | . Consolidation 36 | 658 | 297 | 9 | 0.694 | 0.045 | 0.8 | 0.689 | 0.108 | 0.987 | Not Defined | Not Defined | 0.5 | . Notice that despite having very high sensitivity and accuracy, the PPV of the predictions could still be very low. . This is the case with Edema, for example. . The sensitivity for Edema is 0.75. | However, given that the model predicted positive, the probability that a person has Edema (its PPV) is only 0.066! | . ROC Curve . So far we have been operating under the assumption that our model&#39;s prediction of 0.5 and above should be treated as positive and otherwise it should be treated as negative. This however was a rather arbitrary choice. One way to see this, is to look at a very informative visualization called the receiver operating characteristic (ROC) curve. . The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The ideal point is at the top left, with a true positive rate of 1 and a false positive rate of 0. The various points on the curve are generated by gradually changing the threshold. . Let&#39;s look at this curve for our model: . util.get_curve(y, pred, class_labels) . The area under the ROC curve is also called AUCROC or C-statistic and is a measure of goodness of fit. In medical literature this number also gives the probability that a randomly selected patient who experienced a condition had a higher risk score than a patient who had not experienced the event. This summarizes the model output across all thresholds, and provides a good sense of the discriminative power of a given model. . One important caveat to bear in mind with the ROC curve is that it implicitly assumes roughly equal numbers of positive and negative cases for each disease. This is because the false positive rate includes true negatives in its calculation. For a disease with many examples without the disease and few with the disease - this could lead to a misleading indication of model performance. In such cases, the precision-recall curve can be a better indication of performance, which we will cover shortly. . Let&#39;s use the sklearn metric function of roc_auc_score to add this score to our metrics table. . from sklearn.metrics import roc_auc_score util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score) . TP TN FP FN Accuracy Prevalence Sensitivity Specificity PPV NPV AUC F1 Threshold . . Cardiomegaly 16 | 814 | 169 | 1 | 0.83 | 0.017 | 0.941 | 0.828 | 0.086 | 0.999 | 0.933 | Not Defined | 0.5 | . Emphysema 20 | 869 | 103 | 8 | 0.889 | 0.028 | 0.714 | 0.894 | 0.163 | 0.991 | 0.935 | Not Defined | 0.5 | . Effusion 99 | 690 | 196 | 15 | 0.789 | 0.114 | 0.868 | 0.779 | 0.336 | 0.979 | 0.891 | Not Defined | 0.5 | . Hernia 1 | 743 | 255 | 1 | 0.744 | 0.002 | 0.5 | 0.744 | 0.004 | 0.999 | 0.644 | Not Defined | 0.5 | . Infiltration 114 | 543 | 265 | 78 | 0.657 | 0.192 | 0.594 | 0.672 | 0.301 | 0.874 | 0.696 | Not Defined | 0.5 | . Mass 40 | 789 | 158 | 13 | 0.829 | 0.053 | 0.755 | 0.833 | 0.202 | 0.984 | 0.888 | Not Defined | 0.5 | . Nodule 28 | 731 | 220 | 21 | 0.759 | 0.049 | 0.571 | 0.769 | 0.113 | 0.972 | 0.745 | Not Defined | 0.5 | . Atelectasis 64 | 657 | 249 | 30 | 0.721 | 0.094 | 0.681 | 0.725 | 0.204 | 0.956 | 0.781 | Not Defined | 0.5 | . Pneumothorax 24 | 785 | 183 | 8 | 0.809 | 0.032 | 0.75 | 0.811 | 0.116 | 0.99 | 0.826 | Not Defined | 0.5 | . Pleural_Thickening 24 | 713 | 259 | 4 | 0.737 | 0.028 | 0.857 | 0.734 | 0.085 | 0.994 | 0.868 | Not Defined | 0.5 | . Pneumonia 14 | 661 | 320 | 5 | 0.675 | 0.019 | 0.737 | 0.674 | 0.042 | 0.992 | 0.762 | Not Defined | 0.5 | . Fibrosis 10 | 725 | 261 | 4 | 0.735 | 0.014 | 0.714 | 0.735 | 0.037 | 0.995 | 0.801 | Not Defined | 0.5 | . Edema 15 | 767 | 213 | 5 | 0.782 | 0.02 | 0.75 | 0.783 | 0.066 | 0.994 | 0.856 | Not Defined | 0.5 | . Consolidation 36 | 658 | 297 | 9 | 0.694 | 0.045 | 0.8 | 0.689 | 0.108 | 0.987 | 0.799 | Not Defined | 0.5 | . Confidence Intervals . Of course our dataset is only a sample of the real world, and our calculated values for all above metrics is an estimate of the real world values. It would be good to quantify this uncertainty due to the sampling of our dataset. We&#39;ll do this through the use of confidence intervals. A 95 % confidence interval for an estimate $ hat{s}$ of a parameter $s$ is an interval $I = (a, b)$ such that 95 % of the time when the experiment is run, the true value $s$ is contained in $I$. More concretely, if we were to run the experiment many times, then the fraction of those experiments for which $I$ contains the true parameter would tend towards 95 %. . While some estimates come with methods for computing the confidence interval analytically, more complicated statistics, such as the AUC for example, are difficult. For these we can use a method called the bootstrap. The bootstrap estimates the uncertainty by resampling the dataset with replacement. For each resampling $i$, we will get a new estimate, $ hat{s}_i$. We can then estimate the distribution of $ hat{s}$ by using the distribution of $ hat{s}_i$ for our bootstrap samples. . The Bootstrap method has many advantages, one of which is that it does not assume the underlying distribution is normal. . In the code below, we create bootstrap samples and compute sample AUCs from those samples. Note that we use stratified random sampling (sampling from the positive and negative classes separately) to make sure that members of each class are represented. . def bootstrap_auc(y, pred, classes, bootstraps = 100, fold_size = 1000): statistics = np.zeros((len(classes), bootstraps)) for c in range(len(classes)): df = pd.DataFrame(columns=[&#39;y&#39;, &#39;pred&#39;]) df.loc[:, &#39;y&#39;] = y[:, c] df.loc[:, &#39;pred&#39;] = pred[:, c] # get positive examples for stratified sampling df_pos = df[df.y == 1] df_neg = df[df.y == 0] prevalence = len(df_pos) / len(df) for i in range(bootstraps): # stratified sampling of positive and negative examples pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True) neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True) y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values]) pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values]) score = roc_auc_score(y_sample, pred_sample) statistics[c][i] = score return statistics statistics = bootstrap_auc(y, pred, class_labels) . Now we can compute confidence intervals from the sample statistics that we computed. . util.print_confidence_intervals(class_labels, statistics) . Mean AUC (CI 5%-95%) . Cardiomegaly 0.93 (0.90-0.96) | . Emphysema 0.93 (0.91-0.96) | . Effusion 0.89 (0.87-0.91) | . Hernia 0.62 (0.29-0.98) | . Infiltration 0.70 (0.66-0.74) | . Mass 0.89 (0.85-0.92) | . Nodule 0.75 (0.69-0.80) | . Atelectasis 0.79 (0.75-0.83) | . Pneumothorax 0.83 (0.76-0.90) | . Pleural_Thickening 0.87 (0.82-0.91) | . Pneumonia 0.77 (0.68-0.84) | . Fibrosis 0.80 (0.73-0.86) | . Edema 0.86 (0.82-0.89) | . Consolidation 0.80 (0.74-0.86) | . As you can see, our confidence intervals are much wider for some classes than for others. Hernia, for example, has an interval around (0.30 - 0.98), indicating that we can&#39;t be certain it is better than chance (at 0.5). . Precision-Recall Curve . Precision-Recall are informative prediction metrics when significant class imbalance are present in the data. . In information retrieval . Precision is a measure of result relevancy and that is equivalent to our previously defined PPV. | Recall is a measure of how many truly relevant results are returned and that is equivalent to our previously defined sensitivity measure. | . The precision-recall curve (PRC) shows the trade-off between precision and recall for different thresholds. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. . High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall). . util.get_curve(y, pred, class_labels, curve=&#39;prc&#39;) . F1 Score . F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. The harmonic mean differs from the more common arithmetic mean, in that it gives more weight to the lower value. This means the F1 score leads to a more modest score than would be given by the arithmetic mean, which can be skewed by extremely high values. . Again, we can simply use sklearn&#39;s utility metric function of f1_score to add this measure to our performance table. . from sklearn.metrics import f1_score util.get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score,f1=f1_score) . TP TN FP FN Accuracy Prevalence Sensitivity Specificity PPV NPV AUC F1 Threshold . . Cardiomegaly 16 | 814 | 169 | 1 | 0.83 | 0.017 | 0.941 | 0.828 | 0.086 | 0.999 | 0.933 | 0.158 | 0.5 | . Emphysema 20 | 869 | 103 | 8 | 0.889 | 0.028 | 0.714 | 0.894 | 0.163 | 0.991 | 0.935 | 0.265 | 0.5 | . Effusion 99 | 690 | 196 | 15 | 0.789 | 0.114 | 0.868 | 0.779 | 0.336 | 0.979 | 0.891 | 0.484 | 0.5 | . Hernia 1 | 743 | 255 | 1 | 0.744 | 0.002 | 0.5 | 0.744 | 0.004 | 0.999 | 0.644 | 0.008 | 0.5 | . Infiltration 114 | 543 | 265 | 78 | 0.657 | 0.192 | 0.594 | 0.672 | 0.301 | 0.874 | 0.696 | 0.399 | 0.5 | . Mass 40 | 789 | 158 | 13 | 0.829 | 0.053 | 0.755 | 0.833 | 0.202 | 0.984 | 0.888 | 0.319 | 0.5 | . Nodule 28 | 731 | 220 | 21 | 0.759 | 0.049 | 0.571 | 0.769 | 0.113 | 0.972 | 0.745 | 0.189 | 0.5 | . Atelectasis 64 | 657 | 249 | 30 | 0.721 | 0.094 | 0.681 | 0.725 | 0.204 | 0.956 | 0.781 | 0.314 | 0.5 | . Pneumothorax 24 | 785 | 183 | 8 | 0.809 | 0.032 | 0.75 | 0.811 | 0.116 | 0.99 | 0.826 | 0.201 | 0.5 | . Pleural_Thickening 24 | 713 | 259 | 4 | 0.737 | 0.028 | 0.857 | 0.734 | 0.085 | 0.994 | 0.868 | 0.154 | 0.5 | . Pneumonia 14 | 661 | 320 | 5 | 0.675 | 0.019 | 0.737 | 0.674 | 0.042 | 0.992 | 0.762 | 0.079 | 0.5 | . Fibrosis 10 | 725 | 261 | 4 | 0.735 | 0.014 | 0.714 | 0.735 | 0.037 | 0.995 | 0.801 | 0.07 | 0.5 | . Edema 15 | 767 | 213 | 5 | 0.782 | 0.02 | 0.75 | 0.783 | 0.066 | 0.994 | 0.856 | 0.121 | 0.5 | . Consolidation 36 | 658 | 297 | 9 | 0.694 | 0.045 | 0.8 | 0.689 | 0.108 | 0.987 | 0.799 | 0.19 | 0.5 | . Calibration . When performing classification we often want not only to predict the class label, but also obtain a probability of each label. This probability would ideally give us some kind of confidence on the prediction. In order to observe how our model&#39;s generated probabilities are aligned with the real probabilities, we can plot what&#39;s called a calibration curve. . In order to generate a calibration plot, we first bucketize our predictions to a fixed number of separate bins (e.g. 5) between 0 and 1. We then calculate a point for each bin: the x-value for each point is the mean for the probability that our model has assigned to these points and the y-value for each point fraction of true positives in that bin. We then plot these points in a linear plot. A well-calibrated model has a calibration curve that almost aligns with the y=x line. . The sklearn library has a utility calibration_curve for generating a calibration plot. Let&#39;s use it and take a look at our model&#39;s calibration: . from sklearn.calibration import calibration_curve def plot_calibration_curve(y, pred): plt.figure(figsize=(20, 20)) for i in range(len(class_labels)): plt.subplot(4, 4, i + 1) fraction_of_positives, mean_predicted_value = calibration_curve(y[:,i], pred[:,i], n_bins=20) plt.plot([0, 1], [0, 1], linestyle=&#39;--&#39;) plt.plot(mean_predicted_value, fraction_of_positives, marker=&#39;.&#39;) plt.xlabel(&quot;Predicted Value&quot;) plt.ylabel(&quot;Fraction of Positives&quot;) plt.title(class_labels[i]) plt.tight_layout() plt.show() . plot_calibration_curve(y, pred) . As the above plots show, for most predictions our model&#39;s calibration plot does not resemble a well calibrated plot. How can we fix that?... . Thankfully, there is a very useful method called Platt scaling which works by fitting a logistic regression model to our model&#39;s scores. To build this model, we will be using the training portion of our dataset to generate the linear model and then will use the model to calibrate the predictions for our test portion. . from sklearn.linear_model import LogisticRegression as LR y_train = train_results[class_labels].values pred_train = train_results[pred_labels].values pred_calibrated = np.zeros_like(pred) for i in range(len(class_labels)): lr = LR(solver=&#39;liblinear&#39;, max_iter=10000) lr.fit(pred_train[:, i].reshape(-1, 1), y_train[:, i]) pred_calibrated[:, i] = lr.predict_proba(pred[:, i].reshape(-1, 1))[:,1] . plot_calibration_curve(y[:,], pred_calibrated) . Conclusion . In this article we covered specific metrics that are of particular relevance to evaluating models for medical diagnosis. These metrics were: . Accuracy | Prevalence | Specificity &amp; Sensitivity | PPV and NPV | ROC curve and AUCROC (c-statistic) | Confidence Intervals |",
            "url": "https://www.livingdatalab.com/health/deep-learning/2022/05/22/evaluation-diagnostic-medical-models.html",
            "relUrl": "/health/deep-learning/2022/05/22/evaluation-diagnostic-medical-models.html",
            "date": " • May 22, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "Medical Diagnosis of 14 Diseases Using Chest X-Rays",
            "content": "Introduction . In an earlier project I developed a deep learning model that could detect and diagnose Pneumonia from Chest X-Rays. In this project we will explore medical image diagnosis further by building a state-of-the-art chest X-ray classifier using Keras that can classify and diagnose 14 different diseases. . In particular, we will: . Pre-process and prepare a real-world X-ray dataset. | Use transfer learning to retrain a DenseNet model for X-ray image classification. | Learn a technique to handle class imbalance | Measure diagnostic performance by computing the AUC (Area Under the Curve) for the ROC (Receiver Operating Characteristic) curve. | Visualize model activity using GradCAMs. | . In completing this project we will cover the following key topics in the use of deep learning in medical diagnosis: . Data preparation Visualizing data. | Preventing data leakage. | . | Model Development Addressing class imbalance. | Leveraging pre-trained models using transfer learning. | . | Evaluation AUC and ROC curves. | . | . Load the Datasets . I will be using the ChestX-ray8 dataset which contains 108,948 frontal-view X-ray images of 32,717 unique patients. . Each image in the data set contains multiple text-mined labels identifying 14 different pathological conditions. | These in turn can be used by physicians to diagnose 8 different diseases. | We will use this data to develop a single model that will provide binary classification predictions for each of the 14 labeled pathologies. | In other words it will predict &#39;positive&#39; or &#39;negative&#39; for each of the pathologies. | . You can download the entire dataset for free here. . We have taken a subset of these images of around 1000 for the purposes of this project. . This dataset has been annotated by consensus among four different radiologists for 5 of our 14 pathologies: . Consolidation | Edema | Effusion | Cardiomegaly | Atelectasis | . . Loading the Data . train_df = pd.read_csv(&quot;data/nih/train-small.csv&quot;) valid_df = pd.read_csv(&quot;data/nih/valid-small.csv&quot;) test_df = pd.read_csv(&quot;data/nih/test.csv&quot;) train_df.head() . Image Atelectasis Cardiomegaly Consolidation Edema Effusion Emphysema Fibrosis Hernia Infiltration Mass Nodule PatientId Pleural_Thickening Pneumonia Pneumothorax . 0 00008270_015.png | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 8270 | 0 | 0 | 0 | . 1 00029855_001.png | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 29855 | 0 | 0 | 0 | . 2 00001297_000.png | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1297 | 1 | 0 | 0 | . 3 00012359_002.png | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 12359 | 0 | 0 | 0 | . 4 00017951_001.png | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 17951 | 0 | 0 | 0 | . labels = [&#39;Cardiomegaly&#39;, &#39;Emphysema&#39;, &#39;Effusion&#39;, &#39;Hernia&#39;, &#39;Infiltration&#39;, &#39;Mass&#39;, &#39;Nodule&#39;, &#39;Atelectasis&#39;, &#39;Pneumothorax&#39;, &#39;Pleural_Thickening&#39;, &#39;Pneumonia&#39;, &#39;Fibrosis&#39;, &#39;Edema&#39;, &#39;Consolidation&#39;] . . Preventing Data Leakage . It is worth noting that our dataset contains multiple images for each patient. This could be the case, for example, when a patient has taken multiple X-ray images at different times during their hospital visits. In our data splitting, we have ensured that the split is done on the patient level so that there is no data &quot;leakage&quot; between the train, validation, and test datasets. . . Check for Leakage . We will write a function to check whether there is leakage between two datasets. We&#39;ll use this to make sure there are no patients in the test set that are also present in either the train or validation sets. . def check_for_leakage(df1, df2, patient_col): &quot;&quot;&quot; Return True if there any patients are in both df1 and df2. Args: df1 (dataframe): dataframe describing first dataset df2 (dataframe): dataframe describing second dataset patient_col (str): string name of column with patient IDs Returns: leakage (bool): True if there is leakage, otherwise False &quot;&quot;&quot; # Extract patient id&#39;s for df1 ids_df1 = df1[patient_col].values # Extract patient id&#39;s for df2 ids_df2 = df2[patient_col].values # Create sets for both df1_patients_unique = set(ids_df1) df2_patients_unique = set(ids_df2) # Find the interesction of sets patients_in_both_groups = list(df1_patients_unique.intersection(df2_patients_unique)) # If non empty then we have patients in both df if len(patients_in_both_groups) &gt; 0: leakage = True else: leakage = False return leakage . # Run test check_for_leakage_test(check_for_leakage) . Test Case 1 df1 patient_id 0 0 1 1 2 2 df2 patient_id 0 2 1 3 2 4 leakage output: True - Test Case 2 df1 patient_id 0 0 1 1 2 2 df2 patient_id 0 3 1 4 2 5 leakage output: False All tests passed. . Expected output . Test Case 1 df1 patient_id 0 0 1 1 2 2 df2 patient_id 0 2 1 3 2 4 leakage output: True - Test Case 2 df1 patient_id 0 0 1 1 2 2 df2 patient_id 0 3 1 4 2 5 leakage output: False . All tests passed. . print(&quot;leakage between train and valid: {}&quot;.format(check_for_leakage(train_df, valid_df, &#39;PatientId&#39;))) print(&quot;leakage between train and test: {}&quot;.format(check_for_leakage(train_df, test_df, &#39;PatientId&#39;))) print(&quot;leakage between valid and test: {}&quot;.format(check_for_leakage(valid_df, test_df, &#39;PatientId&#39;))) . leakage between train and valid: True leakage between train and test: False leakage between valid and test: False . Expected output . leakage between train and valid: True leakage between train and test: False leakage between valid and test: False . . Preparing Images . With our dataset splits ready, we can now proceed with setting up our model to consume them. . For this we will use the off-the-shelf ImageDataGenerator class from the Keras framework, which allows us to build a &quot;generator&quot; for images specified in a dataframe. | This class also provides support for basic data augmentation such as random horizontal flipping of images. | We also use the generator to transform the values in each batch so that their mean is $0$ and their standard deviation is 1. This will facilitate model training by standardizing the input distribution. | . | The generator also converts our single channel X-ray images (gray-scale) to a three-channel format by repeating the values in the image across all channels. We will want this because the pre-trained model that we&#39;ll use requires three-channel inputs. | . | . def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=8, seed=1, target_w = 320, target_h = 320): &quot;&quot;&quot; Return generator for training set, normalizing using batch statistics. Args: train_df (dataframe): dataframe specifying training data. image_dir (str): directory where image files are held. x_col (str): name of column in df that holds filenames. y_cols (list): list of strings that hold y labels for images. batch_size (int): images per batch to be fed into model during training. seed (int): random seed. target_w (int): final width of input images. target_h (int): final height of input images. Returns: train_generator (DataFrameIterator): iterator over training set &quot;&quot;&quot; print(&quot;getting train generator...&quot;) # normalize images image_generator = ImageDataGenerator( samplewise_center=True, samplewise_std_normalization= True) # flow from directory with specified batch size # and target image size generator = image_generator.flow_from_dataframe( dataframe=df, directory=image_dir, x_col=x_col, y_col=y_cols, class_mode=&quot;raw&quot;, batch_size=batch_size, shuffle=shuffle, seed=seed, target_size=(target_w,target_h)) return generator . Build a separate generator for valid and test sets . Now we need to build a new generator for validation and testing data. . Why can&#39;t we use the same generator as for the training data? . Look back at the generator we wrote for the training data. . It normalizes each image per batch, meaning that it uses batch statistics. | We should not do this with the test and validation data, since in a real life scenario we don&#39;t process incoming images a batch at a time (we process one image at a time). | Knowing the average per batch of test data would effectively give our model an advantage. The model should not have any information about the test data. | . | . What we need to do is normalize incoming test data using the statistics computed from the training set. . We implement this in the function below. | There is one technical note. Ideally, we would want to compute our sample mean and standard deviation using the entire training set. | However, since this is extremely large, that would be very time consuming. | In the interest of time, we&#39;ll take a random sample of the dataset and calcualte the sample mean and sample standard deviation. | . def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=100, batch_size=8, seed=1, target_w = 320, target_h = 320): &quot;&quot;&quot; Return generator for validation set and test set using normalization statistics from training set. Args: valid_df (dataframe): dataframe specifying validation data. test_df (dataframe): dataframe specifying test data. train_df (dataframe): dataframe specifying training data. image_dir (str): directory where image files are held. x_col (str): name of column in df that holds filenames. y_cols (list): list of strings that hold y labels for images. sample_size (int): size of sample to use for normalization statistics. batch_size (int): images per batch to be fed into model during training. seed (int): random seed. target_w (int): final width of input images. target_h (int): final height of input images. Returns: test_generator (DataFrameIterator) and valid_generator: iterators over test set and validation set respectively &quot;&quot;&quot; print(&quot;getting train and valid generators...&quot;) # get generator to sample dataset raw_train_generator = ImageDataGenerator().flow_from_dataframe( dataframe=train_df, directory=IMAGE_DIR, x_col=&quot;Image&quot;, y_col=labels, class_mode=&quot;raw&quot;, batch_size=sample_size, shuffle=True, target_size=(target_w, target_h)) # get data sample batch = raw_train_generator.next() data_sample = batch[0] # use sample to fit mean and std for test set generator image_generator = ImageDataGenerator( featurewise_center=True, featurewise_std_normalization= True) # fit generator to sample from training data image_generator.fit(data_sample) # get test generator valid_generator = image_generator.flow_from_dataframe( dataframe=valid_df, directory=image_dir, x_col=x_col, y_col=y_cols, class_mode=&quot;raw&quot;, batch_size=batch_size, shuffle=False, seed=seed, target_size=(target_w,target_h)) test_generator = image_generator.flow_from_dataframe( dataframe=test_df, directory=image_dir, x_col=x_col, y_col=y_cols, class_mode=&quot;raw&quot;, batch_size=batch_size, shuffle=False, seed=seed, target_size=(target_w,target_h)) return valid_generator, test_generator . With our generator function ready, let&#39;s make one generator for our training data and one each of our test and validation datasets. . IMAGE_DIR = &quot;data/nih/images-small/&quot; train_generator = get_train_generator(train_df, IMAGE_DIR, &quot;Image&quot;, labels) valid_generator, test_generator= get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, &quot;Image&quot;, labels) . getting train generator... Found 1000 validated image filenames. getting train and valid generators... Found 1000 validated image filenames. Found 200 validated image filenames. Found 420 validated image filenames. . Let&#39;s peek into what the generator gives our model during training and validation. We can do this by calling the __get_item__(index) function: . x, y = train_generator.__getitem__(0) plt.imshow(x[0]); . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . . Model Development . Now we&#39;ll move on to model training and development. We have a few practical challenges to deal with before actually training a neural network, though. The first is class imbalance. . . Addressing Class Imbalance . One of the challenges with working with medical diagnostic datasets is the large class imbalance present in such datasets. Let&#39;s plot the frequency of each of the labels in our dataset: . plt.xticks(rotation=90) plt.bar(x=labels, height=np.mean(train_generator.labels, axis=0)) plt.title(&quot;Frequency of Each Class&quot;) plt.show() . We can see from this plot that the prevalance of positive cases varies significantly across the different pathologies. (These trends mirror the ones in the full dataset as well.) . The Hernia pathology has the greatest imbalance with the proportion of positive training cases being about 0.2%. | But even the Infiltration pathology, which has the least amount of imbalance, has only 17.5% of the training cases labelled positive. | . Ideally, we would train our model using an evenly balanced dataset so that the positive and negative training cases would contribute equally to the loss. . If we use a normal cross-entropy loss function with a highly unbalanced dataset, as we are seeing here, then the algorithm will be incentivized to prioritize the majority class (i.e negative in our case), since it contributes more to the loss. . Impact of class imbalance on loss function . Let&#39;s take a closer look at this. Assume we would have used a normal cross-entropy loss for each pathology. We recall that the cross-entropy loss contribution from the $i^{th}$ training data case is: . $$ mathcal{L}_{cross-entropy}(x_i) = -(y_i log(f(x_i)) + (1-y_i) log(1-f(x_i))),$$ . where $x_i$ and $y_i$ are the input features and the label, and $f(x_i)$ is the output of the model, i.e. the probability that it is positive. . Note that for any training case, either $y_i=0$ or else $(1-y_i)=0$, so only one of these terms contributes to the loss (the other term is multiplied by zero, and becomes zero). . We can rewrite the overall average cross-entropy loss over the entire training set $ mathcal{D}$ of size $N$ as follows: . $$ mathcal{L}_{cross-entropy}( mathcal{D}) = - frac{1}{N} big( sum_{ text{positive examples}} log (f(x_i)) + sum_{ text{negative examples}} log(1-f(x_i)) big).$$ . Using this formulation, we can see that if there is a large imbalance with very few positive training cases, for example, then the loss will be dominated by the negative class. Summing the contribution over all the training cases for each class (i.e. pathological condition), we see that the contribution of each class (i.e. positive or negative) is: . $$freq_{p} = frac{ text{number of positive examples}}{N} $$ . $$ text{and}$$ . $$freq_{n} = frac{ text{number of negative examples}}{N}.$$ . . Compute Class Frequencies . Let&#39;s write a function to calculate these frequences for each label in our dataset. . def compute_class_freqs(labels): &quot;&quot;&quot; Compute positive and negative frequences for each class. Args: labels (np.array): matrix of labels, size (num_examples, num_classes) Returns: positive_frequencies (np.array): array of positive frequences for each class, size (num_classes) negative_frequencies (np.array): array of negative frequences for each class, size (num_classes) &quot;&quot;&quot; # total number of patients (rows) N = labels.shape[0] positive_frequencies = np.sum(labels, axis=0) / N negative_frequencies = 1 - positive_frequencies return positive_frequencies, negative_frequencies . ### Compute frequencies compute_class_freqs_test(compute_class_freqs) . Labels: [[1 0 0] [0 1 1] [1 0 1] [1 1 1] [1 0 1]] Pos Freqs: [0.8 0.4 0.8] Neg Freqs: [0.2 0.6 0.2] All tests passed. . Expected output . Labels: [[1 0 0] [0 1 1] [1 0 1] [1 1 1] [1 0 1]] Pos Freqs: [0.8 0.4 0.8] Neg Freqs: [0.2 0.6 0.2] . All tests passed. . Now we&#39;ll compute frequencies for our training data. . freq_pos, freq_neg = compute_class_freqs(train_generator.labels) freq_pos . array([0.02 , 0.013, 0.128, 0.002, 0.175, 0.045, 0.054, 0.106, 0.038, 0.021, 0.01 , 0.014, 0.016, 0.033]) . Expected output . array([0.02 , 0.013, 0.128, 0.002, 0.175, 0.045, 0.054, 0.106, 0.038, 0.021, 0.01 , 0.014, 0.016, 0.033]) . Let&#39;s visualize these two contribution ratios next to each other for each of the pathologies: . data = pd.DataFrame({&quot;Class&quot;: labels, &quot;Label&quot;: &quot;Positive&quot;, &quot;Value&quot;: freq_pos}) data = data.append([{&quot;Class&quot;: labels[l], &quot;Label&quot;: &quot;Negative&quot;, &quot;Value&quot;: v} for l,v in enumerate(freq_neg)], ignore_index=True) plt.xticks(rotation=90) f = sns.barplot(x=&quot;Class&quot;, y=&quot;Value&quot;, hue=&quot;Label&quot; ,data=data) . As we see in the above plot, the contributions of positive cases is significantly lower than that of the negative ones. However, we want the contributions to be equal. One way of doing this is by multiplying each example from each class by a class-specific weight factor, $w_{pos}$ and $w_{neg}$, so that the overall contribution of each class is the same. . To have this, we want . $$w_{pos} times freq_{p} = w_{neg} times freq_{n},$$ . which we can do simply by taking . $$w_{pos} = freq_{neg}$$ $$w_{neg} = freq_{pos}$$ . This way, we will be balancing the contribution of positive and negative labels. . pos_weights = freq_neg neg_weights = freq_pos pos_contribution = freq_pos * pos_weights neg_contribution = freq_neg * neg_weights . Let&#39;s verify this by graphing the two contributions next to each other again: . data = pd.DataFrame({&quot;Class&quot;: labels, &quot;Label&quot;: &quot;Positive&quot;, &quot;Value&quot;: pos_contribution}) data = data.append([{&quot;Class&quot;: labels[l], &quot;Label&quot;: &quot;Negative&quot;, &quot;Value&quot;: v} for l,v in enumerate(neg_contribution)], ignore_index=True) plt.xticks(rotation=90) sns.barplot(x=&quot;Class&quot;, y=&quot;Value&quot;, hue=&quot;Label&quot; ,data=data); . As the above figure shows, by applying these weightings the positive and negative labels within each class would have the same aggregate contribution to the loss function. Now let&#39;s implement such a loss function. . After computing the weights, our final weighted loss for each training case will be . $$ mathcal{L}_{cross-entropy}^{w}(x) = - (w_{p} y log(f(x)) + w_{n}(1-y) log( 1 - f(x) ) ).$$ . . Get Weighted Loss . We will write a weighted_loss function to return a loss function that calculates the weighted loss for each batch. Recall that for the multi-class loss, we add up the average loss for each individual class. Note that we also want to add a small value, $ epsilon$, to the predicted values before taking their logs. This is simply to avoid a numerical error that would otherwise occur if the predicted value happens to be zero. . def get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7): &quot;&quot;&quot; Return weighted loss function given negative weights and positive weights. Args: pos_weights (np.array): array of positive weights for each class, size (num_classes) neg_weights (np.array): array of negative weights for each class, size (num_classes) Returns: weighted_loss (function): weighted loss function &quot;&quot;&quot; def weighted_loss(y_true, y_pred): &quot;&quot;&quot; Return weighted loss value. Args: y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes) y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes) Returns: loss (float): overall scalar loss summed across all classes &quot;&quot;&quot; # initialize loss to zero loss = 0.0 for i in range(len(pos_weights)): # for each class, add average weighted loss for that class loss_pos = -1 * K.mean(pos_weights[i] * y_true[:, i] * K.log(y_pred[:, i] + epsilon)) loss_neg = -1 * K.mean( neg_weights[i] * (1 - y_true[:, i]) * K.log(1 - y_pred[:, i] + epsilon)) loss += loss_pos + loss_neg return loss return weighted_loss . # In order to pass the tests, set epsilon = 1 epsilon = 1 sess = K.get_session() get_weighted_loss_test(get_weighted_loss, epsilon, sess) . y_true: [[1. 1. 1.] [1. 1. 0.] [0. 1. 0.] [1. 0. 1.]] w_p: [0.25 0.25 0.5 ] w_n: [0.75 0.75 0.5 ] y_pred_1: [[0.7 0.7 0.7] [0.7 0.7 0.7] [0.7 0.7 0.7] [0.7 0.7 0.7]] y_pred_2: [[0.3 0.3 0.3] [0.3 0.3 0.3] [0.3 0.3 0.3] [0.3 0.3 0.3]] If you weighted them correctly, you&#39;d expect the two losses to be the same. With epsilon = 1, your losses should be, L(y_pred_1) = -0.4956203 and L(y_pred_2) = -0.4956203 Your outputs: L(y_pred_1) = -0.4956203 L(y_pred_2) = -0.4956203 Difference: L(y_pred_1) - L(y_pred_2) = 0.0 All tests passed. . Expected output . with epsilon = 1 . Outputs: L(y_pred_1) = -0.4956203 L(y_pred_2) = -0.4956203 Difference: L(y_pred_1) - L(y_pred_2) = 0.0 . All tests passed. . . DenseNet121 . Next, we will use a pre-trained DenseNet121 model which we can load directly from Keras and then add two layers on top of it: . A GlobalAveragePooling2D layer to get the average of the last convolution layers from DenseNet121. | A Dense layer with sigmoid activation to get the prediction logits for each of our classes. | We can set our custom loss function for the model by specifying the loss parameter in the compile() function. . # create the base pre-trained model base_model = DenseNet121(weights=&#39;models/nih/densenet.hdf5&#39;, include_top=False) x = base_model.output # add a global spatial average pooling layer x = GlobalAveragePooling2D()(x) # and a logistic layer predictions = Dense(len(labels), activation=&quot;sigmoid&quot;)(x) model = Model(inputs=base_model.input, outputs=predictions) model.compile(optimizer=&#39;adam&#39;, loss=get_weighted_loss(pos_weights, neg_weights)) . . Training . With our model ready for training, we could use the model.fit() function in Keras to train our model. . We are training on a small subset of the dataset (~1%). | So what we care about at this point is to make sure that the loss on the training set is decreasing. | . If we were going to train this model we could use the following code to do this: . history = model.fit_generator(train_generator, validation_data=valid_generator, steps_per_epoch=100, validation_steps=25, epochs = 3) plt.plot(history.history[&#39;loss&#39;]) plt.ylabel(&quot;loss&quot;) plt.xlabel(&quot;epoch&quot;) plt.title(&quot;Training Loss Curve&quot;) plt.show() . In our case, we will alternatively load a pre-trained model. . . Training on the Larger Dataset . Given that the original dataset is 40GB+ in size and the training process on the full dataset takes a few hours, I have access to a pre-trained the model on a GPU-equipped machine which provides the weights file from our model (with a batch size of 32 instead) to be used for the rest of the project. . Let&#39;s load our pre-trained weights into the model now: . model.load_weights(&quot;models/nih/pretrained_model.h5&quot;) . . Prediction and Evaluation . Now that we have a model, let&#39;s evaluate it using our test set. . predicted_vals = model.predict_generator(test_generator, steps = len(test_generator)) . . ROC Curve and AUROC . For evaluating this model we will use a metric called the AUC (Area Under the Curve) from the ROC (Receiver Operating Characteristic) curve. This is also referred to as the AUROC value. . The key insight for interpreting this plot is that a curve that the more to the left and the top has more &quot;area&quot; under it, this indicates that the model is performing better. . roc_curve | roc_auc_score | . auc_rocs = util.get_roc_curve(labels, predicted_vals, test_generator) . For details about the best performing methods and their performance on this dataset, please read the following papers: . CheXNet | CheXpert | ChexNeXt | . . Visualizing Learning with GradCAM . One of the challenges of using deep learning in medicine is that the complex architecture used for neural networks makes them much harder to interpret compared to traditional machine learning models (e.g. linear models). There are no easily interpretable model coeffcients. . One of the most common approaches aimed at increasing the interpretability of models for computer vision tasks is to use Class Activation Maps (CAM). . In this section we will use a GradCAM&#39;s technique to produce a heatmap highlighting the important regions in the image for predicting the pathological condition. . This is done by extracting the gradients of each predicted class, flowing into our model&#39;s final convolutional layer. . Indeed I used this method previously in an earlier article using fastai&#39;s deep learning library as an alternative to Keras. . It is worth mentioning that GradCAM does not provide a full explanation of the reasoning for each classification probability. However, it is still a useful tool for &quot;debugging&quot; our model and augmenting our prediction so that an expert could validate that a prediction is indeed due to the model focusing on the right regions of the image. . First we will load the small training set and setup to look at the 4 classes with the highest performing AUC measures. . df = pd.read_csv(&quot;data/nih/train-small.csv&quot;) IMAGE_DIR = &quot;data/nih/images-small/&quot; # only show the labels with top 4 AUC labels_to_show = np.take(labels, np.argsort(auc_rocs)[::-1])[:4] . Now let&#39;s look at a few specific images. . util.compute_gradcam(model, &#39;00008270_015.png&#39;, IMAGE_DIR, df, labels, labels_to_show) . Loading original image Generating gradcam for class Cardiomegaly Generating gradcam for class Mass Generating gradcam for class Pneumothorax Generating gradcam for class Edema . util.compute_gradcam(model, &#39;00011355_002.png&#39;, IMAGE_DIR, df, labels, labels_to_show) . Loading original image Generating gradcam for class Cardiomegaly Generating gradcam for class Mass Generating gradcam for class Pneumothorax Generating gradcam for class Edema . util.compute_gradcam(model, &#39;00029855_001.png&#39;, IMAGE_DIR, df, labels, labels_to_show) . Loading original image Generating gradcam for class Cardiomegaly Generating gradcam for class Mass Generating gradcam for class Pneumothorax Generating gradcam for class Edema . util.compute_gradcam(model, &#39;00005410_000.png&#39;, IMAGE_DIR, df, labels, labels_to_show) . Loading original image Generating gradcam for class Cardiomegaly Generating gradcam for class Mass Generating gradcam for class Pneumothorax Generating gradcam for class Edema . Conclusion . In this project we looked at medical image diagnosis by building a state-of-the-art chest X-ray classifier using Keras. . In particular, looked at the following: . Pre-processed and prepare a real-world X-ray dataset. | Used transfer learning to retrain a DenseNet model for X-ray image classification. | Learned a technique to handle class imbalance | Measured diagnostic performance by computing the AUC (Area Under the Curve) for the ROC (Receiver Operating Characteristic) curve. | Visualized model activity using GradCAMs. | .",
            "url": "https://www.livingdatalab.com/health/deep-learning/2022/05/15/medical-diagnosis-chest-xrays.html",
            "relUrl": "/health/deep-learning/2022/05/15/medical-diagnosis-chest-xrays.html",
            "date": " • May 15, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "The International Classification of Disease System (ICD)",
            "content": "Introduction . In an earlier article we looked at how we can extract clinical outcomes and patient level data from the MIMIC-III EHR (Electronic Health Record) database. In this article we will look at the history of the International Classification of Diseases (ICD) system, which has been developed collaboratively so that the medical terms and information in death certificates can be grouped together for statistical purposes. In practical examples we will look at how to extract ICD-9 codes from MIMIC III database and visualise them. . The International Classification of Disease System (ICD) . The World Health Organization is an agency that works on behalf of 194 member states. The aim of the organization is to promote the best standards in health for all people, regardless social and economic condition. As well as regardless of race, gender, religion and political beliefs. The main principle behind the organization work, is that access to affordable and articulate healthcare is a human right. For this reason, it promotes the fusion of universal health coverage. . There are several determinants that influence human health. This can be biomedical and genetic factors. Health behaviors, socioeconomic factors and environmental factors. The organization recognizes that we need to have common metrics to measure health and wellbeing. And some of those metrics are related to life expectancy, as well as mortality. Other metrics are subjective, and it depends of how well the person feelings. Disability, as well as illness and comorbidity are also measures of health and wellbeing. . The World Health Organisation aims to coordinate action among the member states, in order to intervene and improve health globally. To achieve this, it is required to collect data from patients, and this data will be analyzed from researchers, statisticians and clinicians to estimate indices of health and wellbeing. Technological and machine learning advances can promote healthcare and narrow the gap between rich and poor countries. . In order to collect data that can be compared across different locations and times. We need to have common notations and definitions. For this reason, the World Health Organization maintains a family of international classification schemes. In other words, there is a set of integrated classifications that provide a common language for health information across the world. The International Classification of Diseases, is the international standard diagnostic tool. For epidemiology, health management and clinical purposes. . The International Classification of Diseases, have been designed with the aim to describe various aspects of the health and the health systems in a consistent manner. In this way, it helps the development of reliable statistical systems at local, national and international levels. With the aim of improving status and health care. In practice, this process is used to translate diagnosis of diseases and other health problems from words into an alphanumeric code. The usage of the International Classification of Disease system. Provides a systematic way for storage, retrieval and analysis of the data. . The first type of users exposed in these classifications is in a clinic and it includes physician nurses and health workers. They integrate this information and they used it to support decision making for their patients. The second type of users are in administration and this can be health information managers, policymakers, insurers and national health program managers. . This data are also of paramount importance for population, health and epidemiology as well as research. They allow quantifying disability, diseases and risk factors in a global level. And they enable research in decision support system, based on artificial intelligence. . Summarizing, the International Classification of Diseases is one of the oldest and most important classification in medicine. It enables the optimal application of computer technology in the processing and retrieval of clinical information. Importantly, it is recognized internationally. Which enables sound statistical comparison of data, from different regions in different times. . The Evolution of the ICD System . The first effort to systematically classify diseases goes back in the 17th century. John Graunt, who was an epidemiologist and statistician, was looking into the death of children who&#39;re born alive, but died before the age of six. He recognized the need to organize mortality data into some logical form and therefore develop the first statistical study of disease called the London Bills of Mortality. . William Farr is considered as the first medical statistician of the general Register Office of England and Wales. He submitted his report on Nomenclature and Statistical Classification of Diseases in 1855. In this report, he included most of those fatal diseases that affect health. In fact, in mid 80s, it was recognized the need of classification of diseases that was uniform and internationally accepted. Farr pointed out that medicine has progressed by that time and many diseases could affect particular organs, pointing out for a classification of diseases related to the organic systems they affect. He also considered previous classifications as largely symptomatic and the arrangements could not be used for statistical analysis. . The beginning of modern classification can be considered as the 1893. The chief of statistical services of Paris prepared a classification based on the principle of distinguishing between general diseases and those localized to a particular organ or anatomical site. Bertillon presented his report on causes of death and incapacity for work, including hospital admissions. Bertillon&#39;s main headings included general diseases, diseases of nervous systems and sense organs, circulatory system, respiratory system, digestive system, and many others. The International Statistical Institute adapted the first edition of international classification system, the so-called the Internationally List of Causes of Death in 1893. . The ICD-10 coding system was endorsed by the 43rd World Health Assembly in May 1990. It came into use in World Health Organization member states as from 1994. ICD-10 involved a thorough rethinking of its structure and an effort to devise a stable and flexible classification which won&#39;t require fundamental changes. Also, the structure of codes have changed from numeric to alphanumeric, which allows for significant expansion. The ICD-11 coding has been adopted by the 72nd World Health Assembly in 2019, and it comes into effect in January 2022. ICD-11 has been designed for digital use and it&#39;s fully electronic. It aims to assist implementation and reduce error in diagnosis while it makes it more adaptable in local countries. The system has an improved ability to code for the quality and safety of health care and highlights socioeconomic factors that directly and indirectly contribute to people&#39;s health. Finally, it also tries to simplify diagnostic descriptions, particularly in relation to mental health. . Summarizing, the need to organize disease data systematically was recognized in the 17th century. However, it wasn&#39;t until the late 80s where the first international list of causes of death was founded. ICD codes are ubiquitously used in medicine and they are necessary to be able to compare statistics across different countries and across different times. . ICD-9 and MIMIC-III . ICD-9 is the disease classification system used in MIMIC-III. We will review its main structure, and we are going to see how the ICD codes can help us extract summary statistics from MIMIC-III database for example, to the number and distribution of patients across age which are diagnosed with a specific disease. We&#39;re going to also see how we&#39;re going to be able to put together queries to extract data with relation to the most common ICD codes in the MIMIC database and how these codes are distributed across ICU units. . . The main structure of the ICD-9 coding system consists of three digits that reflect a category and two digits that reflect the cause or the location. The World Health Organization requires a minimum of three-character categories level for international reporting and comparison. Therefore, these three digits always need to be provided with the corresponding number. Whereas the fourth digit is filled with X when there is no further information about the sub-division. . . Here, we see a more detailed overview of the ICD-9 categories. In the first column, we see the codes related to the three first digits of the ICD-9 code. On the right column, we see the description of each of these categories. We start here with epidemic diseases and then we see diseases like neoplasm, endocrine, nutritional, and metabolic diseases and immunity disorders. We see here diseases of the blood and blood forming organs, mental disorders, and then we see also a number of diseases related with specific systems, such as the nervous system and sense organs, the circulatory system, the respiratory system, the digestive system, the genitourinary system, and so on. . Subsequently, we see developmental diseases, for example, congenital abnormalities. We also see injury and poisoning category. Finally, we see here that the last two categories, the first digit can be a letter. Both of this category offer a supplemental classification. We&#39;re going to see how we can extract those codes from MIMIC-III. ICD codes in MIMIC-III are under the table of Diagnoses_icd. . Extracting ICD-9 related information from MIMIC-III . 1. Number of patients diagnosed with hypertension above 30 years old . We would like to count the number of patients who were diagnosed with hypertension and are at least 30 years old. . First, we need to combine the Admissions and Patients table to obtain the age (from date of birth and admission time), and filter out all patients younger than 30 years old. ICD9 diagnoses can be found in the Diagnoses_icd table (and descriptions of each code can be found in the D_icd_diagnoses table). We select all ICD-9 codes that are starting with 401, as these are related to hypertension. . # Compose SQL query query = &quot;&quot;&quot; SELECT p.subject_id, d.icd9_code, round((EXTRACT(EPOCH FROM (a.admittime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) as age FROM public.patients p INNER JOIN public.Diagnoses_icd d ON p.subject_id = d.subject_id INNER JOIN public.Admissions a ON p.subject_id = a.subject_id WHERE round((EXTRACT(EPOCH FROM (a.admittime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) &gt; 30 AND icd9_code LIKE &#39;401%&#39; &quot;&quot;&quot; # Run query query_output = pd.read_sql_query(query,con) query_output.head() . subject_id icd9_code age . 0 10017 | 4019 | 73.6792 | . 1 10019 | 4019 | 48.9014 | . 2 10026 | 4010 | 300.0020 | . 3 10027 | 4019 | 82.4941 | . 4 10033 | 4019 | 81.6256 | . 2. Histogram of the number of patients diagnosed with hypertension . Instead of counting the number of patients diagnosed with hypertension of 30 years and older, we would also like to see the distribution of hypertension diagnoses across different age groups for all ages. Hence, we do not want to filter out any age ranges. . The approach is very similar to the previous query. However, we now do not filter on age, and also select the age for each patient, to be able to create a histogram across different age ranges. . # Compose SQL query query = &quot;&quot;&quot; SELECT p.subject_id, d.icd9_code, round((EXTRACT(EPOCH FROM (a.admittime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) as age FROM public.patients p INNER JOIN public.Diagnoses_icd d ON p.subject_id = d.subject_id INNER JOIN public.Admissions a ON p.subject_id = a.subject_id WHERE icd9_code LIKE &#39;401%&#39; &quot;&quot;&quot; # Run query query_output = pd.read_sql_query(query,con) query_output.head() . subject_id icd9_code age . 0 10017 | 4019 | 73.6792 | . 1 10019 | 4019 | 48.9014 | . 2 10026 | 4010 | 300.0020 | . 3 10027 | 4019 | 82.4941 | . 4 10033 | 4019 | 81.6256 | . # Remove outlier age df = query_output[query_output[&#39;age&#39;] &lt; 300] # Visualize distribution of age: df[&#39;age&#39;].hist(bins=200) . &lt;AxesSubplot:&gt; . 3. Most common ICD-9 codes across adults patients . We are interested in the ICD-9 codes sorted by their frequency, and want to select to five ICD-9 codes with the highest frequencies. We are only interested to see these results for adult (age &gt;= 16) patients who have been admitted to the ICU. . First, we combine the Patients and Admissions tables to obtain each patient’s age at time of hospital admission from their date of birth and hospital admission time. We also combine the Icustays tables, to filter out any patients who were not admitted to the ICU. We join the Diagnoses_icd and D_icd_diagnoses tables to get all ICD-9 codes and their descriptions. From the Diagnoses_icd table, we also take into account the priority of each ICD-9 code, as hospital admissions might correspond to multiple ICD-9 codes, but we are only interested in the primary diagnosis. . # Compose SQL query query = &quot;&quot;&quot; SELECT diag.hadm_id, diag.icd9_code, d_icd.short_title FROM public.patients p INNER JOIN public.admissions a ON p.subject_id = a.subject_id INNER JOIN public.diagnoses_icd diag ON a.hadm_id = diag.hadm_id INNER JOIN public.d_icd_diagnoses d_icd ON diag.icd9_code = d_icd.icd9_code INNER JOIN public.icustays i ON a.hadm_id = i.hadm_id WHERE round((EXTRACT(EPOCH FROM (a.admittime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) &gt;= 16 AND diag.seq_num = 1 &quot;&quot;&quot; # Run query query_output = pd.read_sql_query(query,con) query_output.head() . hadm_id icd9_code short_title . 0 142345 | 99591 | Sepsis | . 1 105331 | 570 | Acute necrosis of liver | . 2 165520 | 0389 | Septicemia NOS | . 3 199207 | 81201 | Fx surg nck humerus-clos | . 4 177759 | 0389 | Septicemia NOS | . # Print key stats print(&#39;Top 5 ICD-9 codes and their frequencies&#39;) print(query_output.drop_duplicates([&#39;hadm_id&#39;])[&#39;icd9_code&#39;].value_counts().head()) print(&#39; &#39;) print(&#39;Top 5 ICD-9 codes and their frequencies by percentage&#39;) print(query_output.drop_duplicates([&#39;hadm_id&#39;])[&#39;icd9_code&#39;].value_counts().head() / len(query_output.drop_duplicates([&#39;hadm_id&#39;])[&#39;icd9_code&#39;]) * 100) print(&#39; &#39;) print(&#39;Top 5 ICD-9 codes Disease Description&#39;) most_frequent_icd9s = np.array(query_output.drop_duplicates([&#39;hadm_id&#39;])[&#39;icd9_code&#39;].value_counts().head().index.values) query_output.loc[query_output[&#39;icd9_code&#39;].isin(most_frequent_icd9s)].drop_duplicates([&#39;icd9_code&#39;]).drop(&#39;hadm_id&#39;, axis=1) . Top 5 ICD-9 codes and their frequencies 0389 15 486 6 51881 6 41071 5 4280 4 Name: icd9_code, dtype: int64 Top 5 ICD-9 codes and their frequencies by percentage 0389 11.627907 486 4.651163 51881 4.651163 41071 3.875969 4280 3.100775 Name: icd9_code, dtype: float64 Top 5 ICD-9 codes Disease Description . icd9_code short_title . 2 0389 | Septicemia NOS | . 6 4280 | CHF NOS | . 14 41071 | Subendo infarct, initial | . 15 51881 | Acute respiratry failure | . 28 486 | Pneumonia, organism NOS | . 4. Distribution of ICD-9 codes across care units . Instead of looking at the ICD-9 codes themselves, we will now take a look at the ICD-9 categories. ICD-9 codes can be grouped into nine larger categories (or ten if we have an ‘other’ category). . (001-139): Infectious and parasitic diseases, i.e., septicemia, other infectious and parasitic diseases, etc. | (139-239): Neoplasms of digestive organs and intrathoracic organs, etc. | (240-279): Endocrine, nutritional, metabolic, and immunity. | (390-459): Diseases of the circulatory system, i.e., ischemic heart diseases, diseases of pulmonary circulation, dysrhythmias, heart failure, cerebrovascular diseases, etc. | (460-519): Pulmonary diseases, i.e., pneumonia and influenza, chronic obstructive pulmonary disease, etc. | (520-579): Diseases of the digestive system. | (580-629): Diseases of the genitourinary system, i.e., nephritis, nephrotic syndrome, nephrosis, and other diseases of the genitourinary system. | (800-959): Trauma. | (960-979): Poisoning by drugs and biological substances. | . ICD-codes can start with an m, v or e. These are supplementary classifications that we can classify under ‘Other’. We would like to have the distribution of these ICD-9 categories for all adult (age &gt;= 16) patients across different intensive care units (ICUs). . We need almost the same columns from all tables as for the previous query. However, we now also need the care unit that a patient was admitted to, from the Icustays table, and we need to extract the different ICD-9 categories from the ICD-9 codes. . # Compose SQL query query = &quot;&quot;&quot; SELECT a.hadm_id, i.first_careunit, diag.icd9_code, CASE WHEN (lower(LEFT(diag.icd9_code, 1)) = &#39;e&#39;) OR (lower(LEFT(diag.icd9_code, 1)) = &#39;v&#39;) OR (lower(LEFT(diag.icd9_code, 1)) = &#39;m&#39;) THEN 9 WHEN CAST(LEFT(diag.icd9_code, 3) AS int) &gt;= 0 AND CAST(LEFT(diag.icd9_code, 3) AS int) &lt;= 139 THEN 0 WHEN CAST(LEFT(diag.icd9_code, 3) AS int) &gt;= 140 AND CAST(LEFT(diag.icd9_code, 3) AS int) &lt;= 239 THEN 1 WHEN CAST(LEFT(diag.icd9_code, 3) AS int) &gt;= 240 AND CAST(LEFT(diag.icd9_code, 3) AS int) &lt;= 279 THEN 2 WHEN CAST(LEFT(diag.icd9_code, 3) AS int) &gt;= 390 AND CAST(LEFT(diag.icd9_code, 3) AS int) &lt;= 459 THEN 3 WHEN CAST(LEFT(diag.icd9_code, 3) AS int) &gt;= 460 AND CAST(LEFT(diag.icd9_code, 3) AS int) &lt;= 519 THEN 4 WHEN CAST(LEFT(diag.icd9_code, 3) AS int) &gt;= 520 AND CAST(LEFT(diag.icd9_code, 3) AS int) &lt;= 579 THEN 5 WHEN CAST(LEFT(diag.icd9_code, 3) AS int) &gt;= 580 AND CAST(LEFT(diag.icd9_code, 3) AS int) &lt;= 629 THEN 6 WHEN CAST(LEFT(diag.icd9_code, 3) AS int) &gt;= 800 AND CAST(LEFT(diag.icd9_code, 3) AS int) &lt;= 959 THEN 7 WHEN CAST(LEFT(diag.icd9_code, 3) AS int) &gt;= 960 AND CAST(LEFT(diag.icd9_code, 3) AS int) &lt;= 989 THEN 8 ELSE 9 END AS icd9_category FROM public.admissions a INNER JOIN public.icustays i ON a.hadm_id = i.hadm_id INNER JOIN public.patients p ON p.subject_id = a.subject_id INNER JOIN public.diagnoses_icd diag ON a.hadm_id = diag.hadm_id INNER JOIN public.d_icd_diagnoses d_icd ON diag.icd9_code = d_icd.icd9_code WHERE round((EXTRACT(EPOCH FROM (a.admittime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) &gt;= 16 AND diag.seq_num = 1 &quot;&quot;&quot; # Run query query_output = pd.read_sql_query(query,con) query_output.head() . hadm_id first_careunit icd9_code icd9_category . 0 142345 | MICU | 99591 | 9 | . 1 105331 | MICU | 570 | 5 | . 2 165520 | MICU | 0389 | 0 | . 3 199207 | CCU | 81201 | 7 | . 4 177759 | MICU | 0389 | 0 | . def icu_icd9_categories(df): # Replace category codes with names categories_dict = { 0: &#39;Infectious and parasitic diseases&#39;, 1: &#39;Neoplasms of digestive organs and intrathoracic organs, etc&#39;, 2: &#39;Endocrine, nutritional, metabolic, and immunity&#39;, 3: &#39;Diseases of the circulatory system&#39;, 4: &#39;Pulmonary diseases&#39;, 5: &#39;Diseases of the digestive system&#39;, 6: &#39;Diseases of the genitourinary system&#39;, 7: &#39;Trauma&#39;, 8: &#39;Poisoning by drugs and biological substances&#39;, 9: &#39;Other&#39;} df[&#39;icd9_category&#39;] = df[&#39;icd9_category&#39;].map(categories_dict) # Get list of ICUs icu_list = df[&#39;first_careunit&#39;].unique() # Plot pie chart for each ICU fig = plt.figure(figsize=(40,50)) subplot = 1 for icu in icu_list: icu_df = df[df[&#39;first_careunit&#39;] == icu] icu_df = icu_df.drop_duplicates() pie_df = pd.DataFrame(icu_df[&#39;icd9_category&#39;].value_counts() / icu_df.shape[0] * 100) pie_df = pie_df.reset_index() plt.subplot(5, 1, subplot) plt.pie(pie_df[&#39;icd9_category&#39;], labels=pie_df[&#39;index&#39;], autopct=&#39;%1.1f%%&#39;) plt.title(&#39;Disease categories for &#39; + icu) subplot += 1 # Print key stats icu_icd9_categories(query_output) .",
            "url": "https://www.livingdatalab.com/health/electronic-health-records/2022/03/18/the-international-classification-of-disease.html",
            "relUrl": "/health/electronic-health-records/2022/03/18/the-international-classification-of-disease.html",
            "date": " • Mar 18, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "MIMIC-III (EHR) Clinical Outcomes & Patient Level Data",
            "content": "Introduction . In an earlier article we looked at how we can extract some useful descriptive statistics from the MIMIC-III EHR (Electronic Health Record) database. In this article we will further explore the MIMIC-III Dataset, looking at how we examine clinical outcomes as well as extracting indivdual patient level data. . MIMIC-III and Clinical Outcomes . Mortality is the most commonly used outcome in prediction studies in critical care, and in fact potentially across all of medicine. Since it is a strong surrogate for realness, machine learning practitioners use this signal to infer the relationship between clinical data and patient outcome. Mortality is a deceptively simple outcome to define. In fact, the nature of retrospective data often complicates matters. For example, in MIMIC database, a number of patients have two consecutive hospitalizations in which they die as organ donors. These consecutive hospitalizations are treated as distinct hospital admissions. . However, the underlying reason is the same. Mortality is susceptible to selection bias based on the source of the information. For example, many studies report hospital mortality as it is feasible to collect this information. However, the number of deaths recorded in the hospital database might not include the number of people that have died after they have been discharged and went to the home care. This adds noise to the outcome. . Other important factors in defining mortality as an outcome are controllable by the researcher. For example, defining mortality as death within 30 days of admission will provide a stronger signal for immediate physiological abnormality, which is likely related to the patient&#39;s admission. On the other hand, one-year mortality will emphasize chronic illness in parallel conditions. . in this article we will see how we can extract the mortality numbers of adult patients who were admitted to the ICU and the distribution of this mortality numbers across the different ICUs in MIMIC dataset. If a patient&#39;s death was registered while the patient was in ICU, or six hours before being admitted to, or six hours after leaving the ICU, we will assume that the patient has died in the intensive care unit. . Extract mortality numbers . 1. Mortality numbers in ICU across care units . We would like to know the mortality numbers of adult patients who were admitted to the ICU, and the distribution of these mortality numbers across the different ICUs. If a patient’s death was registered while the patient was on the ICU, or 6 hours before being admitted to, or 6 hours after leaving the ICU, we assume that the patient has died on the ICU. . The following diagram visualizes the SQL query that is needed to obtain the ICU mortality numbers. We combine the Patients and Icustays tables based on the subject identifier, and select each patient’s date of birth and date of death, and the care unit and admission time corresponding to each ICU stay. The admission time and date of death together indicate whether or not a patient died on the ICU. The age (age &gt;= 16) is again combined from the admission time and date of birth. . . # Compose SQL query query = &quot;&quot;&quot; SELECT i.first_careunit, round((EXTRACT(EPOCH FROM (i.intime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) as age, CASE WHEN p.dod IS NOT NULL AND p.dod &gt;= i.intime - interval &#39;6 hour&#39; AND p.dod &lt;= i.outtime + interval &#39;6 hour&#39; THEN &#39;Died in ICU&#39; ELSE &#39;Not dead&#39; END AS icu_mortality FROM public.patients p INNER JOIN public.icustays i ON p.subject_id = i.subject_id WHERE round((EXTRACT(EPOCH FROM (i.intime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) &gt;= 16; &quot;&quot;&quot; # Run query query_output = pd.read_sql_query(query,con) query_output.head() . first_careunit age icu_mortality . 0 MICU | 70.6378 | Not dead | . 1 MICU | 36.1923 | Died in ICU | . 2 MICU | 87.0874 | Died in ICU | . 3 CCU | 73.6875 | Not dead | . 4 MICU | 48.9015 | Died in ICU | . # Print overall mortality print(&#39;Overall mortality - Totals&#39;) print(query_output[&#39;icu_mortality&#39;].value_counts()) print(&#39; &#39;) print(&#39;Overall mortality - Percentages&#39;) print(query_output[&#39;icu_mortality&#39;].value_counts()/query_output.shape[0]*100) # Print mortality per icu print(&#39; &#39;) print(&#39;Mortality per ICU - Totals&#39;) result = query_output.groupby([&#39;first_careunit&#39;,&#39;icu_mortality&#39;])[&#39;icu_mortality&#39;].count() print(result) print(&#39; &#39;) print(&#39;Mortality per ICU - Percentages&#39;) result = query_output.groupby([&#39;first_careunit&#39;,&#39;icu_mortality&#39;])[&#39;icu_mortality&#39;].count() / query_output.groupby([&#39;first_careunit&#39;])[&#39;icu_mortality&#39;].count() * 100 print(result) # Print mortality percentages accross all icu print(&#39; &#39;) print(&#39;Mortality accross all ICUs&#39;) dead_only_df = query_output[query_output[&#39;icu_mortality&#39;] == &#39;Died in ICU&#39;] icu_percentages_df = pd.DataFrame(dead_only_df.groupby([&#39;first_careunit&#39;])[&#39;icu_mortality&#39;].count()) icu_percentages_df = icu_percentages_df.reset_index() icu_percentages_df[&#39;icu_mortality&#39;] = icu_percentages_df[&#39;icu_mortality&#39;] / dead_only_df.shape[0] * 100 icu_percentages_df.head() fig1, ax1 = plt.subplots() ax1.pie(icu_percentages_df[&#39;icu_mortality&#39;], labels=icu_percentages_df[&#39;first_careunit&#39;], autopct=&#39;%1.1f%%&#39;) ax1.axis(&#39;equal&#39;) plt.show() . Overall mortality - Totals Not dead 105 Died in ICU 31 Name: icu_mortality, dtype: int64 Overall mortality - Percentages Not dead 77.205882 Died in ICU 22.794118 Name: icu_mortality, dtype: float64 Mortality per ICU - Totals first_careunit icu_mortality CCU Died in ICU 5 Not dead 14 CSRU Died in ICU 1 Not dead 5 MICU Died in ICU 18 Not dead 59 SICU Died in ICU 4 Not dead 19 TSICU Died in ICU 3 Not dead 8 Name: icu_mortality, dtype: int64 Mortality per ICU - Percentages first_careunit icu_mortality CCU Died in ICU 26.315789 Not dead 73.684211 CSRU Died in ICU 16.666667 Not dead 83.333333 MICU Died in ICU 23.376623 Not dead 76.623377 SICU Died in ICU 17.391304 Not dead 82.608696 TSICU Died in ICU 27.272727 Not dead 72.727273 Name: icu_mortality, dtype: float64 Mortality accross all ICUs . 2. Mortality numbers in hospital across care units . We would also like to know the mortality numbers of adult patients who were admitted to hospital, and the distribution of those numbers across different ICUs. In this case, there is a variable called ‘hospital_expire_flag’ in the Admissions table that defines if a patient has died in hospital. . The Patients and Icustays tables are combined based on the subject’s unique identifier, and the Admissions table is also joined based on the hospital admission’s identifier. We will need to select the hospital admission’s identifier, care unit, admission time and ‘hospital_expire_flag’. We also need the date of birth to obtain the age (age &gt;= 16). . . # Compose SQL query query = &quot;&quot;&quot; SELECT i.first_careunit, a.hospital_expire_flag, round((EXTRACT(EPOCH FROM (a.admittime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) as age FROM public.Icustays i INNER JOIN public.patients p ON i.subject_id = p.subject_id INNER JOIN public.Admissions a ON i.hadm_id = a.hadm_id WHERE round((EXTRACT(EPOCH FROM (a.admittime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) &gt;= 16 &quot;&quot;&quot; # Run query query_output = pd.read_sql_query(query,con) query_output.head() . first_careunit hospital_expire_flag age . 0 MICU | 0 | 70.6378 | . 1 MICU | 1 | 36.1923 | . 2 MICU | 1 | 87.0874 | . 3 CCU | 0 | 73.6792 | . 4 MICU | 1 | 48.9014 | . # Print overall mortality print(&#39;Overall mortality - Totals&#39;) print(query_output[&#39;hospital_expire_flag&#39;].value_counts()) print(&#39; &#39;) print(&#39;Overall mortality - Percentages&#39;) print(query_output[&#39;hospital_expire_flag&#39;].value_counts()/query_output.shape[0]*100) # Print mortality per icu print(&#39; &#39;) print(&#39;Mortality per ICU - Totals&#39;) result = query_output.groupby([&#39;first_careunit&#39;,&#39;hospital_expire_flag&#39;])[&#39;hospital_expire_flag&#39;].count() print(result) print(&#39; &#39;) print(&#39;Mortality per ICU - Percentages&#39;) result = query_output.groupby([&#39;first_careunit&#39;,&#39;hospital_expire_flag&#39;])[&#39;hospital_expire_flag&#39;].count() / query_output.groupby([&#39;first_careunit&#39;])[&#39;hospital_expire_flag&#39;].count() * 100 print(result) # Print mortality percentages accross all icu print(&#39; &#39;) print(&#39;Mortality accross all ICUs&#39;) dead_only_df = query_output[query_output[&#39;hospital_expire_flag&#39;] == 1] icu_percentages_df = pd.DataFrame(dead_only_df.groupby([&#39;first_careunit&#39;])[&#39;hospital_expire_flag&#39;].count()) icu_percentages_df = icu_percentages_df.reset_index() icu_percentages_df[&#39;hospital_expire_flag&#39;] = icu_percentages_df[&#39;hospital_expire_flag&#39;] / dead_only_df.shape[0] * 100 icu_percentages_df.head() fig1, ax1 = plt.subplots() ax1.pie(icu_percentages_df[&#39;hospital_expire_flag&#39;], labels=icu_percentages_df[&#39;first_careunit&#39;], autopct=&#39;%1.1f%%&#39;) ax1.axis(&#39;equal&#39;) plt.show() . Overall mortality - Totals 0 90 1 46 Name: hospital_expire_flag, dtype: int64 Overall mortality - Percentages 0 66.176471 1 33.823529 Name: hospital_expire_flag, dtype: float64 Mortality per ICU - Totals first_careunit hospital_expire_flag CCU 0 13 1 6 CSRU 0 5 1 1 MICU 0 52 1 25 SICU 0 16 1 7 TSICU 0 4 1 7 Name: hospital_expire_flag, dtype: int64 Mortality per ICU - Percentages first_careunit hospital_expire_flag CCU 0 68.421053 1 31.578947 CSRU 0 83.333333 1 16.666667 MICU 0 67.532468 1 32.467532 SICU 0 69.565217 1 30.434783 TSICU 0 36.363636 1 63.636364 Name: hospital_expire_flag, dtype: float64 Mortality accross all ICUs . Extract length of stay numbers . 1. Length of stay on the ICU across care units . We would like to know how many days each patient has to stay on the ICU. We want to know the median, lower quantile and upper quantile for the length of stay, and also these values for each different ICU. As for all queries, we only select adult patients (age &gt;= 16). . The length of stay on the ICU can be found in the Icustays table. We also need the corresponding care unit and time of admission of each ICU admission. To get the date of birth for each patient (and hence the age, computed from the admission time and date of birth), we combine the Icustays and Patients tables. . . # Compose SQL query query = &quot;&quot;&quot; SELECT i.first_careunit, i.los, round((EXTRACT(EPOCH FROM (i.intime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) as age FROM public.Icustays i INNER JOIN public.patients p ON i.subject_id = p.subject_id WHERE round((EXTRACT(EPOCH FROM (i.intime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) &gt;= 16 &quot;&quot;&quot; # Run query query_output = pd.read_sql_query(query,con) query_output.head() . first_careunit los age . 0 MICU | 1.6325 | 70.6378 | . 1 MICU | 13.8507 | 36.1923 | . 2 MICU | 2.6499 | 87.0874 | . 3 CCU | 2.1436 | 73.6875 | . 4 MICU | 1.2938 | 48.9015 | . # Define function for descriptive stats 5 number summary for a field per icu def icu_descriptive_stats(field, df, boxplot_title): # Get list of ICUs icu_list = df[&#39;first_careunit&#39;].unique() # Plot descriptive stats for each ICU for icu in icu_list: print(&#39; &#39;) print(&#39;Descriptive statistics for &#39; + str(icu) + &#39; by &#39; + field) icu_df = df[df[&#39;first_careunit&#39;] == icu] print(icu_df[field].describe()) # Plot box plot of ICU by field plt.figure(figsize=(20,10)) sns.boxplot(data=df, x=&#39;first_careunit&#39;, y=field) plt.xlabel(&#39;ICU&#39;) plt.title(boxplot_title) # 5 number summary all ICUs for los (length of stay within icu) icu_descriptive_stats(&#39;los&#39;, query_output, &#39;ICU by Length of stay in days within ICU (los)&#39;) . Descriptive statistics for MICU by los count 77.000000 mean 3.955345 std 5.193230 min 0.190400 25% 1.135800 50% 1.925200 75% 4.101400 max 31.123500 Name: los, dtype: float64 Descriptive statistics for CCU by los count 19.000000 mean 5.753900 std 7.024671 min 0.879900 25% 1.862600 50% 2.883300 75% 4.242450 max 24.996800 Name: los, dtype: float64 Descriptive statistics for SICU by los count 23.000000 mean 5.668461 std 8.751901 min 0.743700 25% 1.910350 50% 2.405600 75% 5.022700 max 35.406500 Name: los, dtype: float64 Descriptive statistics for CSRU by los count 6.000000 mean 3.631350 std 3.199466 min 0.901700 25% 1.464500 50% 2.084000 75% 6.010175 max 8.141500 Name: los, dtype: float64 Descriptive statistics for TSICU by los count 11.000000 mean 3.589609 std 6.422052 min 0.105900 25% 0.647600 50% 1.276200 75% 3.110500 max 22.389500 Name: los, dtype: float64 . 2. Length of stay in hospital across care units . This query is almost similar to the previous one, but now we are interested in the length of stay (in days) of adult patients in hospital instead of on the ICUs. We are also interested to know the distribution of those values across ICUs. . We combine the Patients and Icustays tables based on the subject identifier, and the Admissions table based on the unique hospital admission identifier. This time, we use date of birth and time of admission to the hospital to compute age, and filter on it using age &gt;= 16. There is no variable for length of stay in hospital, but we can compute it by subtracting the discharge time from the admission time. Moreover, we will need the ICU the patient is admitted to. The unique hospital admission identifier is used to make sure each hospital admission corresponds to only one ICU (we use the hospital admission identifier to remove duplicates). . . # Compose SQL query query = &quot;&quot;&quot; SELECT i.first_careunit, a.hadm_id, a.dischtime, a.admittime, round((EXTRACT(EPOCH FROM (a.admittime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) as age, round((EXTRACT(EPOCH FROM (a.dischtime-a.admittime))/60/60/24) :: NUMERIC, 4) as hospital_los FROM public.Icustays i INNER JOIN public.patients p ON i.subject_id = p.subject_id INNER JOIN public.Admissions a ON i.hadm_id = a.hadm_id WHERE round((EXTRACT(EPOCH FROM (a.admittime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) &gt;= 16 &quot;&quot;&quot; # Run query query_output = pd.read_sql_query(query,con) # Drop duplicates based on unique hospital admission id query_output = query_output.drop_duplicates([&#39;hadm_id&#39;]) # Remove outliers query_output = query_output[query_output[&#39;hospital_los&#39;] &lt; 100] query_output.head() . first_careunit hadm_id dischtime admittime age hospital_los . 0 MICU | 142345 | 2164-11-01 17:15:00 | 2164-10-23 21:09:00 | 70.6378 | 8.8375 | . 1 MICU | 105331 | 2126-08-28 18:59:00 | 2126-08-14 22:32:00 | 36.1923 | 13.8521 | . 2 MICU | 165520 | 2125-10-07 15:13:00 | 2125-10-04 23:36:00 | 87.0874 | 2.6507 | . 3 CCU | 199207 | 2149-06-03 18:42:00 | 2149-05-26 17:19:00 | 73.6792 | 8.0576 | . 4 MICU | 177759 | 2163-05-15 12:00:00 | 2163-05-14 20:43:00 | 48.9014 | 0.6368 | . # 5 number summary all ICUs for los (length of stay in hospital) icu_descriptive_stats(&#39;hospital_los&#39;, query_output, &#39;ICU by length of stay in days in Hospital accross care units (hospital_los)&#39;) . Descriptive statistics for MICU by hospital_los count 73.000000 mean 8.073463 std 6.847850 min 0.144400 25% 3.903500 50% 5.988200 75% 9.797900 max 36.011800 Name: hospital_los, dtype: float64 Descriptive statistics for CCU by hospital_los count 16.000000 mean 8.274125 std 6.723373 min 0.959000 25% 2.949825 50% 6.618750 75% 11.067150 max 24.997900 Name: hospital_los, dtype: float64 Descriptive statistics for SICU by hospital_los count 22.000000 mean 11.616982 std 10.893085 min 2.107600 25% 3.697600 50% 8.138900 75% 13.385575 max 39.697200 Name: hospital_los, dtype: float64 Descriptive statistics for CSRU by hospital_los count 6.000000 mean 7.588333 std 4.976149 min 0.770800 25% 4.284925 50% 8.046200 75% 12.027300 max 12.281300 Name: hospital_los, dtype: float64 Descriptive statistics for TSICU by hospital_los count 11.000000 mean 5.184909 std 6.637594 min 0.038200 25% 0.760750 50% 2.320100 75% 7.488200 max 22.390300 Name: hospital_los, dtype: float64 . Extracting Vital Signs for a single patient from MIMIC-III . It is useful to be able to extract vital signs and medication of a single patient that was admitted to an intensive care unit, for example we might need to extract clinical variables across patients such as lab exams, sign and wave forms, as well as doctor reports and prescriptions. For thsi example we will pick randomly an ICU stay identifier and the corresponding subject identifier. We will then look the data related to this patient and this ICU stay admission. . 1. Hospital admission of a single patient . First of all we would like to get the hospital admission of a single patient during a single ICU stay. We want to have table with the patient’s unique subject identifier, hospital identifier, the admission type, the diagnosis, the ICU stay identifier, the first and last care unit that he/she was admitted to, and the time of admission to the ICU. We also want to choose for our example a patient who died in-hospital. . We will need the Admissions, Patients, and Icustays tables to collect the information that we need. We join the three tables, and find only the patients who have died i.e. where hospital_expire_flag is 1. . # Load admissions query = &quot;&quot;&quot; SELECT ad.subject_id, ad.hadm_id, ad.admission_type, ad.diagnosis, ic.icustay_id, ic.first_careunit, ic.last_careunit, ic.intime as icu_intime, ad.hospital_expire_flag, pa.expire_flag FROM admissions ad INNER JOIN icustays ic ON ad.subject_id = ic.subject_id INNER JOIN patients pa ON ad.subject_id = pa.subject_id WHERE ad.hospital_expire_flag = 1 ORDER BY ic.intime &quot;&quot;&quot; admissions = pd.read_sql_query(query,con) # Show the (first few) rows of admissions: admissions.head() . subject_id hadm_id admission_type diagnosis icustay_id first_careunit last_careunit icu_intime hospital_expire_flag expire_flag . 0 10102 | 164869 | EMERGENCY | CHRONIC MYELOGENOUS LEUKEMIA;TRANSFUSION REACTION | 223870 | MICU | MICU | 2105-06-08 20:06:06 | 1 | 1 | . 1 10076 | 198503 | EMERGENCY | LUNG CANCER;SHORTNESS OF BREATH | 201006 | MICU | MICU | 2107-03-24 04:06:14 | 1 | 1 | . 2 43746 | 167181 | EMERGENCY | METASTIC MELANOMA;ANEMIA | 289236 | SICU | SICU | 2111-01-07 16:36:48 | 1 | 1 | . 3 43746 | 167181 | EMERGENCY | METASTIC MELANOMA;ANEMIA | 224458 | SICU | SICU | 2111-01-12 15:26:49 | 1 | 1 | . 4 42066 | 171628 | EMERGENCY | TRACHEAL STENOSIS | 244243 | TSICU | TSICU | 2112-02-04 14:49:33 | 1 | 1 | . icustay_id = admissions[&#39;icustay_id&#39;].iloc[2] icustay_id . 289236 . 2. All charted events of a single patient . Charted events contain information such as heart rate and respiratory rate of a patient. We would like all charted events of a single patient, along with the time of the charted events, the time between admission to the ICU and the charted event, the label corresponding to the event, and the value and measurement unit of the event. . We need the Chartevents and Icustays tables to get the charted events for a single patient on the ICU. We also join the D_items table to get the label of a charted event. Moreover, we filter on a specific ICU stay ID to get the data for a single patient and single ICU admission. . # Load chart events query = &quot;&quot;&quot; SELECT ic.icustay_id, ce.charttime, ce.charttime - ic.intime AS icutime, di.label, ce.value, ce.valuenum, ce.valueuom FROM Chartevents ce INNER JOIN D_items as di ON ce.itemid = di.itemid INNER JOIN icustays ic ON ce.icustay_id = ic.icustay_id WHERE ic.icustay_id = &quot;&quot;&quot; + str(icustay_id) + &quot;&quot;&quot; ORDER BY ce.charttime &quot;&quot;&quot; chartevents = pd.read_sql_query(query,con) # Show the (first few) rows of admissions: chartevents.head() . icustay_id charttime icutime label value valuenum valueuom . 0 289236 | 2111-01-07 15:00:00 | -1 days +22:23:12 | Head of Bed | 30 Degrees | NaN | None | . 1 289236 | 2111-01-07 15:00:00 | -1 days +22:23:12 | Activity | Bedrest | NaN | None | . 2 289236 | 2111-01-07 15:00:00 | -1 days +22:23:12 | Turn | Side to Side | NaN | None | . 3 289236 | 2111-01-07 15:00:00 | -1 days +22:23:12 | Assistance Device | 2 Person Assist | NaN | None | . 4 289236 | 2111-01-07 15:00:00 | -1 days +22:23:12 | Position | Left Side | NaN | None | . 3. All outputs recorded during an ICU stay of a single patient . Output events are also recorded during an ICU stay. We would like to collect the time of the output event, the time since admission to the ICU, the label of the event, the value and the corresponding measurement event. . The query is similar to the previous query, however, this time we will need the Outputevents table and combine it with the Icustays and D_items tables. We again filter on a specific ICU stay ID to get the data for a single patient and a single ICU stay. The charted time and time of ICU admission are combined to get the time of the output event since the time of ICU admission. . # Load Output events query = &quot;&quot;&quot; SELECT ic.icustay_id, oe.charttime, oe.charttime - ic.intime AS icutime, di.label, oe.value, oe.valueuom FROM Outputevents oe INNER JOIN D_items as di ON oe.itemid = di.itemid INNER JOIN icustays ic ON oe.icustay_id = ic.icustay_id WHERE ic.icustay_id = &quot;&quot;&quot; + str(icustay_id) + &quot;&quot;&quot; ORDER BY oe.charttime &quot;&quot;&quot; outputevents = pd.read_sql_query(query,con) # Show the (first few) rows of admissions: outputevents.head() . icustay_id charttime icutime label value valueuom . 0 289236 | 2111-01-07 17:00:00 | 0 days 00:23:12 | Pre-Admission | 194.0 | mL | . 1 289236 | 2111-01-07 17:30:00 | 0 days 00:53:12 | Foley | 45.0 | mL | . 2 289236 | 2111-01-07 18:28:00 | 0 days 01:51:12 | Pre-Admission | 194.0 | mL | . 3 289236 | 2111-01-07 18:35:00 | 0 days 01:58:12 | Foley | 20.0 | mL | . 4 289236 | 2111-01-07 19:00:00 | 0 days 02:23:12 | Foley | 30.0 | mL | . 4. All inputs recorded during an ICU stay of a single patient . Input events could be, for example, the use of medication. We would like to collect all input events for a single patient and ICU stay. We are also interested in the corresponding start and end times of the events, those times relative to ICU admission time, the label of the input event, the amount and measurement unit, and how often the input event (medication) is administered. . The input events can be found in the Inputevents_mv table. We join this table with the Icustays and D_items tables to get the time of admission to the ICU and label corresponding to the input event. Again, we filter on a specific ICU stay ID to get the data for a single patient and a single ICU stay. We also filter out rewritten input events. . # Load Input events query = &quot;&quot;&quot; SELECT ic.icustay_id, ie.starttime, ie.endtime, ie.starttime - ic.intime as icustarttime, ie.endtime - ic.intime as icuendtime, di.label, ie.amount, ie.amountuom, ie.rate, ie.rateuom, ie.statusdescription FROM inputevents_mv ie INNER JOIN icustays ic ON ie.icustay_id = ic.icustay_id INNER JOIN D_items as di ON ie.itemid = di.itemid WHERE ic.icustay_id = &quot;&quot;&quot; + str(icustay_id) + &quot;&quot;&quot; AND lower(ie.statusdescription) != &#39;rewritten&#39; ORDER BY ie.starttime &quot;&quot;&quot; inputevents = pd.read_sql_query(query,con) # Show the (first few) rows of admissions: inputevents.head() . icustay_id starttime endtime icustarttime icuendtime label amount amountuom rate rateuom statusdescription . 0 289236 | 2111-01-07 16:45:00 | 2111-01-07 16:46:00 | 0 days 00:08:12 | 0 days 00:09:12 | Pre-Admission Intake | 3400.000000 | ml | NaN | None | FinishedRunning | . 1 289236 | 2111-01-07 17:00:00 | 2111-01-08 00:56:00 | 0 days 00:23:12 | 0 days 08:19:12 | D5 1/2NS | 991.699971 | ml | 125.004198 | mL/hour | FinishedRunning | . 2 289236 | 2111-01-07 18:00:00 | 2111-01-07 18:01:00 | 0 days 01:23:12 | 0 days 01:24:12 | NaCl 0.9% | 500.000000 | ml | NaN | None | FinishedRunning | . 3 289236 | 2111-01-07 18:30:00 | 2111-01-07 18:31:00 | 0 days 01:53:12 | 0 days 01:54:12 | Morphine Sulfate | 2.000000 | mg | NaN | None | FinishedRunning | . 4 289236 | 2111-01-07 20:13:00 | 2111-01-07 20:43:00 | 0 days 03:36:12 | 0 days 04:06:12 | Albumin 25% | 49.999998 | ml | 99.999996 | mL/hour | FinishedRunning | . 5. All lab events recorded during an ICU stay of a single patient . Examples of lab events could be the number of red blood cells in the body, or magnesium levels. We want to get all lab events for a single patient and a single ICU stay. We also are interested to see the time of these lab events, time since ICU admission, label, value, and measurement unit. . Lab events are in the Labevents table. We join the Icustays and D_labitems tables, and filter out any lab events that were recorded before or after the patient was at the ICU. . # Load lab events query = &quot;&quot;&quot; SELECT ic.subject_id, ic.icustay_id, le.charttime, le.charttime - ic.intime as icutime, di.label, le.value, le.valuenum, le.valueuom FROM labevents le INNER JOIN icustays ic ON le.subject_id = ic.subject_id AND le.charttime &gt;= ic.intime AND le.charttime &lt;= ic.outtime INNER JOIN D_labitems as di ON le.itemid = di.itemid WHERE ic.icustay_id = &quot;&quot;&quot; + str(icustay_id) + &quot;&quot;&quot; ORDER BY le.charttime &quot;&quot;&quot; labevents = pd.read_sql_query(query,con) # Show the (first few) rows of admissions: labevents.head() . subject_id icustay_id charttime icutime label value valuenum valueuom . 0 43746 | 289236 | 2111-01-07 17:23:00 | 0 days 00:46:12 | Specific Gravity | 1.024 | 1.024 | | . 1 43746 | 289236 | 2111-01-07 17:23:00 | 0 days 00:46:12 | GR HOLD | HOLD | NaN | None | . 2 43746 | 289236 | 2111-01-07 17:23:00 | 0 days 00:46:12 | Creatinine | 1.9 | 1.900 | mg/dL | . 3 43746 | 289236 | 2111-01-07 17:23:00 | 0 days 00:46:12 | Glucose | NEG | NaN | mg/dL | . 4 43746 | 289236 | 2111-01-07 17:23:00 | 0 days 00:46:12 | Glucose | 139 | 139.000 | mg/dL | .",
            "url": "https://www.livingdatalab.com/health/electronic-health-records/2022/03/15/mimic-clinicial-outcomes-patient-data.html",
            "relUrl": "/health/electronic-health-records/2022/03/15/mimic-clinicial-outcomes-patient-data.html",
            "date": " • Mar 15, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "MIMIC-III (EHR) for Descriptive Health Analytics",
            "content": "Introduction . In an earlier article we looked at how the MIMIC-III EHR database come into being. In this article, we&#39;re going to overview the main architecture of the MIMIC-III Electronic Health Record (EHR) database and how it links information between ICU units in the hospital records. We&#39;re also going to highlight that a key processing step to develop the database was to remove any sensitive fields. When dealing with sensitive health data, we need to particularly pay attention to dates. With MIMIC to protect anonymity, all dates have been shifted relatively to protect the privacy of the subjects. In particular, we will learn about the design of this relational database, and what tools are available to query, extract and visualise descriptive analytics. . MIMIC-III use cases . Electronic health records are complicated. This is for several reasons. Some of this information can be medical images, lab tests, natural language diagnosis from doctors, medications, and hospitalization events. During hospitalization there is a number of tests a patient undergoes, blood test, and vital signs checked. It could be medical images and so on. A single patients data are spread over multiple electronic health record with diverse representation. Another important issue is the meaning of measurements. As simple temperature measure may vary depending on whether it is taking from the mouth or the armpit. Putting all this together, we see that electronic health records are irregularly sampled. Their nature is varied and dynamic. So how we can design the schema of a database to encode this information? . This database should be accessible simultaneously from doctors and other health care providers frequently and in a unified way. Interoperability is a key requirement. This involve enhanced quality, efficiency, and effectiveness of the health care system. Information should be provided in the appropriate format whenever is needed. We should eliminate unnecessary duplications. Database selection and it&#39;s matching schema architecture usually influences that effective management of medical data flexibility, scalability, query performance, and interoperability. Non-proprietary standardized models are necessary to build electronic health record systems which comply the requirement of interoperability. . MIMIC-III is a good example towards this direction. It is the only freely accessible critical care database of its kind. The dataset spans more than a decade, which detailed information about individual patient care. Databases such as MIMIC-III play a key role in accelerating research in machine learning models and end enabling reproducibility studies. MIMIC-III database links the identified information across five intensive units at the hospital of Medical Center in Boston with the hospital electronic health record databases. . . During ICU stay, there are several signals that are monitored and these are the vital signs, there are waveforms. We have alarms, but there are also fluids and medications as well as progression reports noted from the doctors. On the other hand, data recording from the hospital will include billing details and it includes also International Classification of Disease codes which relates to the pathology and the symptoms of the patient during admission. It will include demographics of the patient, and it will also include other nodes, with relation to medical images, discharge summaries, and so on. All the fields related to patient data identification has been removed. This includes his patient name, telephone number, and addresses. In particular dates, we&#39;re shifted into the future by a random offset for each individual patient in a consistent manner. Preserving interval is important. Therefore, dates cannot be completely removed or randomly changed. . . MIMIC-III as a Relational Database . MIMIC-III database consists of 26 tables and they&#39;re all listed below. The schema of the database reflects the inherent hospital sources of information. Since MIMIC-III links data from a hospital, the overall structure represents closely this row data. As we see here, MIMIC-III tables can be categorized in four groups. . One of the group is the patient tracking. These tables are used to define and track patient stay. The tables under ICU data include all the data recorded during icu stays. On the other hand, the data recorded within the table under the hospital category includes all the data recorded in the hospital. Recall that the database links data between the ICU unit and the hospital but these are two different entities. Finally, the last category includes dictionary tables and they all have a prefix of d. . . Here, we&#39;re going to look at the basic tables of MIMIC-III, which are the patients table, the admission table, and the icustays table. Several key summary statistics can be extracted based only on these tables. . The patient table has a subject Id identifier that can link it to the admission table, as well as the ICU table. The patient table includes the date of birth. We should pay attention here because the date of birth has been shifted for patients older than 89 years old. We should also note that the table records three different versions of date of death. These are the date of death according to the hospital. The date of death from Social Security database, and a date of death which matches the two dates and gives priority to the date of death at hospital. The patient&#39;s table also includes an expired flag, which is a binary flag that records whether the patient has died according to either database. . The admissions table has an additional identifier. The hospital admission identify that links the information with the icustays table. The admissions table records every unique hospitalization for each patient in the database. It includes information about admission time, discharge time, death time, type of admission, hospital expiry flag, diagnosis, and whether the patient has chart events data associated with his record. . The icustays table records every unique ICU stay in the database. The icustay identifier is a generated identifier that is not based on any row data identifier. We should point out that the hospital and the ICU database are not mainly linked, they used to be two separate databases. Therefore they don&#39;t have any concept of an ICU and counter identifier. Taking all this together, subject Id refers to a unique patient identifier, hospital admission Id refers to a unique admission to the hospital and icustay identification refers to a unique admission to an intensive care unit. . Information in the icustays table include the first care unit and the last care unit, which are also information defined in the transfers table. It also include the first ward and the last ward, which refers to the physical locations within the hospital. It includes in time and out time of when the patient was transferred in and out of the ICU. It also includes length of stay for the patient. We should point out that the icustays table have been in fact linked with the transfers table. Specifically it groups a transfers stable based on icustay ID and excludes rows where there is no icustay ID. The transfers table, includes additional information of patient movement from bed to bed within the hospital, including ICU admission and discharge. . Finally, the callout table includes information regarding when a patient was cleared from ICU discharge and when the patient was actually discharged. A key table that includes data from the ICU unit is the chart events table. Here we can find all chart event observations for patients. The outputevents stable, on the other hand, contains all measurements related to output for a given patient. When we work with ICU data and in particular with chart events data, we should consider also the dictionary tables. This table provide definitions for identifiers. For example, every row of chart events is associated with a single item ID, which represents the concept measure. In this way, by joining the chart events table with a dictionary items table it is possible to identify the concept represented by a given item ID. The rest of the dictionary&#39;s table, they&#39;re also used for cross-referencing codes against their respective definitions. . Now we highlight some of the tables and the hospital data that are used very often, in particular, the lab events table containing laboratory test results for a patient. There is some duplication between chart events and lab events. In cases where there is a disagreement between measurements, lab events should be taken as the ground truth. In some cases, it would have been possible to merge tables. For example, we can merge the dictionary of ICT procedures with that CPT events table because both contain details relating to procedures and they could be combined. However, since the data sources are significantly different, they have been kept separately. . Researchers are advised to develop database views and transform them as appropriately rather than combining the tables within the mimic data model. We should also point out that the patients&#39; procedures recording in the procedures ICT table are coded using the International Statistical Classification of Diseases. Similarly, the diagnosis ICT table, are hospital assigned diagnosis coded using again, the International Statistical Classification of Diseases System. The corresponding dictionary tables, they hold the relative information with relation to the ICD-9 codes. . Summarizing, the mimic database holds more than 53,000 distinct hospital admissions for patients age 60 years and above, and they were admitted to critical care between 2001 and 2012. To correctly extract information from an electronic health record database, we need to understand the schema of the database, but also the source of the data. In almost every query, we will see that we can use SQL queries to join information between the basic tables which hold data for the patients&#39; admissions in the hospital and ICU stays. In the next sectionss, we will see how to extract information about the patient characteristics such as age, gender, ICU units, as well as outcomes such as mortality and stay of length. We are also going to discuss the coding system used in mimic, which is based on the International Classification of Diseases, ICD-9 system. . This is a good article summerising the characteristics of the MIMIC-III database and its tables. . Calculating Descriptive Statistics for MIMIC-III . Descriptive statistics are powerful. They can be used in retrospective studies to overview historic data and explain trends. Extracting patients can result in different estimations depending on which table identifier we use. Therefore, we really need to understand the schema of the database and how it encodes the data. Normally, descriptive statistics look into patient characteristics, intensive care unit utilization, and patient outcomes such as mortality. A number of factors should be considered while we extract this data, for example, when we are looking into estimating the number of patients, we will realize that there is more than one way leading to similar but not identical results. We can look into the number of distinct patients across care units. We can also look into unique hospital admissions. Some patients have been admitted more than once. Therefore, we would expect that the number of unique patient admissions is less than the number of unique hospital admissions, since a patient can be hospitalized more than once. . We can also consider unique admissions to ICUs and this number will be, again, different than the number of distinct patient across intensive care units because some patients have been admitted to more than one intensive care unit. In particular, for MIMIC-III, it is useful to know the age distribution across intensive units. Dßescriptive analytics can provide us a lot of information about historic data. They can be used to explain trends, but they cannot be used to predict future and prevent disease and high rates of mortality. Therefore, they are limited into retrospective studies. . To calculate some example descriptive statistics we will use the following tools: . A reduced demo version of the MIMIC-III dataset | A PostgreSQL Database with all the tables from the demo MIMIC-III imported into it | Python &amp; Pandas | . # Import libraries import numpy as np import pandas as pd import sys import matplotlib.pyplot as plt from matplotlib import cm import matplotlib.colors as mc import seaborn as sns import colorsys import psycopg2 %matplotlib inline # Local Database Configuration: sqluser = &#39;pranath&#39; dbname = &#39;mimic&#39; schema_name = &#39;mimiciii&#39; . # Connect to MIMIC-III database: con = psycopg2.connect(dbname=dbname, user=sqluser, password=&#39;&#39;) cur = con.cursor() . Calculating the Distribution of Heart rates of all adult patients . Say for example we would like to create a histogram of all adult (age &gt;= 16) patients’ heart rates. Heart rates are registered as two separate charted events, under the label ‘Heart rate’. . To get all adult patients’ heart rates, we first combine the Patients and Admissions tables again to get the age of each patient (computed from each patient’s date of birth and hospital admission time). We filter out all patients younger than 16 years old, and select the values from the charted events related to the unique identifiers mentioned above. . . For the implementation, we will need to filter on charted events with item ID 211 or 220045, which both correspond to heart rate. . # Make sure that we are looking at the right item IDs that correspond to heart rate in the d_items dictionary table: query = &quot;&quot;&quot; SELECT d.itemid, d.label FROM public.d_items d WHERE d.label LIKE &#39;%Heart Rate%&#39; &quot;&quot;&quot; query_output = pd.read_sql_query(query,con) query_output . itemid label . 0 211 | Heart Rate | . 1 3494 | Lowest Heart Rate | . 2 220045 | Heart Rate | . 3 220047 | Heart Rate Alarm - Low | . # Compose and execute SQL queries to get all adult heart rates # First query: Join patients and admissions table to get the age which is admittime - dob, and filter all ages over 16 only, return table with just list of subject id&#39;s # Second query: Filter from chartevents table where subject id&#39;s match those we just returned, and with heart rate item ids 211 or 220045 query = &quot;&quot;&quot; WITH subjects_above16 AS ( SELECT a.subject_id FROM public.admissions a INNER JOIN public.patients p ON a.subject_id = p.subject_id WHERE round((EXTRACT(EPOCH FROM (a.admittime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) &gt;= 16 group by a.subject_id ) , heart_rate_table as ( SELECT width_bucket(ce.valuenum, 0, 300, 301) AS bucket FROM public.chartevents ce INNER JOIN subjects_above16 ON ce.subject_id = subjects_above16.subject_id WHERE ce.itemid in (211, 220045) ) SELECT bucket as heart_rate FROM heart_rate_table ORDER BY bucket; &quot;&quot;&quot; query_output = pd.read_sql_query(query,con) query_output.head() . heart_rate . 0 1.0 | . 1 1.0 | . 2 1.0 | . 3 1.0 | . 4 1.0 | . # Visualize distribution of heart rate: query_output[&#39;heart_rate&#39;].hist(bins=200) . &lt;AxesSubplot:&gt; . # Show 5-Number summary of heart rate: query_output[&#39;heart_rate&#39;].describe() . count 15485.000000 mean 88.766225 std 19.175901 min 1.000000 25% 76.000000 50% 88.000000 75% 101.000000 max 190.000000 Name: heart_rate, dtype: float64 . Extract hospitalisation numbers . We will now look at the basic operations required to extract descriptive statistics from the MIMIC-III database with relation to hospitalisations, age distribution of patients, gender distribution of patients, length of stay in ICUs and mortality. They use the basic tables of MIMIC-III: Patients, Icustays and Admissions. . 1. Number of distinct patients across care units . We would like to know the number of unique adult (age &gt;= 16) patients admitted to an intensive care unit (ICU), as well as the distribution of those patients across the different ICUs. . The diagram below visualizes how to get the information that is needed to get those numbers. The Patients table is combined with the Icustays table to get each patient’s unique identifier and the ICU that they were admitted to. Moreover, we use each patient’s date of birth and the time of admission to compute each patient’s age, and select only adult patients (age &gt;= 16). . . # Compose SQL query query = &quot;&quot;&quot; SELECT i.subject_id, i.first_careunit FROM public.patients p INNER JOIN public.Icustays i ON i.subject_id = p.subject_id WHERE round((EXTRACT(EPOCH FROM (i.intime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) &gt;= 16 &quot;&quot;&quot; # Run query query_output = pd.read_sql_query(query,con) # Filter duplicate patients and group by ICU unit icu_units = query_output.drop_duplicates([&#39;subject_id&#39;]).groupby([&#39;first_careunit&#39;]).count() icu_units = icu_units.reset_index() # Calculate percentage icu_units.columns = [&#39;ICU Unit&#39;, &#39;Total Admissions&#39;] sum_patients = icu_units[&#39;Total Admissions&#39;].sum() icu_units[&#39;Percentage Admissions&#39;] = (icu_units[&#39;Total Admissions&#39;] / sum_patients) * 100 print(&#39;Total Patients: &#39; + str(sum_patients)) icu_units.head(10) . Total Patients: 100 . ICU Unit Total Admissions Percentage Admissions . 0 CCU | 12 | 12.0 | . 1 CSRU | 6 | 6.0 | . 2 MICU | 54 | 54.0 | . 3 SICU | 20 | 20.0 | . 4 TSICU | 8 | 8.0 | . 2. Number of distinct hospital admissions across care units . Some patients might have been admitted to hospital more than once. Apart from the number of unique patients, we would also like to know the number of unique hospital admissions and the corresponding distribution across ICUs. . To get the numbers for hospital admissions, we combine the Patients table with the Icustays table based on each patient’s unique subject identifier. We collect each patient’s identifier and date of birth, and all the unique hospital stay identifiers, along with the corresponding ICU and time of admission. Again, we compute each patient’s age and select only adult patients (age &gt;= 16). . . # Compose SQL query query = &quot;&quot;&quot; SELECT i.first_careunit, i.hadm_id FROM public.patients p INNER JOIN public.Icustays i ON i.subject_id = p.subject_id WHERE round((EXTRACT(EPOCH FROM (i.intime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) &gt;= 16 &quot;&quot;&quot; # Run query query_output = pd.read_sql_query(query,con) # Filter duplicate patients and group by ICU unit icu_units = query_output.drop_duplicates([&#39;hadm_id&#39;]).groupby([&#39;first_careunit&#39;]).count() icu_units = icu_units.reset_index() # Calculate percentage icu_units.columns = [&#39;ICU Unit&#39;, &#39;Total Unique Admissions&#39;] sum_patients = icu_units[&#39;Total Unique Admissions&#39;].sum() icu_units[&#39;Percentage Unique Admissions&#39;] = (icu_units[&#39;Total Unique Admissions&#39;] / sum_patients) * 100 print(&#39;Total Patients: &#39; + str(sum_patients)) icu_units.head(10) . Total Patients: 129 . ICU Unit Total Unique Admissions Percentage Unique Admissions . 0 CCU | 17 | 13.178295 | . 1 CSRU | 6 | 4.651163 | . 2 MICU | 73 | 56.589147 | . 3 SICU | 22 | 17.054264 | . 4 TSICU | 11 | 8.527132 | . 3. Number of distinct ICU stays across care units . Each patient might also have been admitted to multiple ICUs, also within one hospital admission. We would like to know the number of unique admissions to the ICUs and the corresponding distribution of those numbers across the different ICUs. . Again, we combine the Patients and Icustays tables based on each subject’s unique identifier. We collect each patient’s identifier, date of birth, and hospital admission time. The latter two are used to compute age and filter on adult patients only (age &gt;= 16). We also need the unique ICU stay identifier and the corresponding ICU. . . # Compose SQL query query = &quot;&quot;&quot; SELECT i.first_careunit, i.icustay_id FROM public.patients p INNER JOIN public.Icustays i ON i.subject_id = p.subject_id WHERE round((EXTRACT(EPOCH FROM (i.intime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) &gt;= 16 &quot;&quot;&quot; # Run query query_output = pd.read_sql_query(query,con) # Filter duplicate patients and group by ICU unit icu_units = query_output.drop_duplicates([&#39;icustay_id&#39;]).groupby([&#39;first_careunit&#39;]).count() icu_units = icu_units.reset_index() # Calculate percentage icu_units.columns = [&#39;ICU Unit&#39;, &#39;Total Unique ICU Stays&#39;] sum_patients = icu_units[&#39;Total Unique ICU Stays&#39;].sum() icu_units[&#39;Percentage ICU Stays&#39;] = (icu_units[&#39;Total Unique ICU Stays&#39;] / sum_patients) * 100 print(&#39;Total Patients: &#39; + str(sum_patients)) icu_units.head(10) . Total Patients: 136 . ICU Unit Total Unique ICU Stays Percentage ICU Stays . 0 CCU | 19 | 13.970588 | . 1 CSRU | 6 | 4.411765 | . 2 MICU | 77 | 56.617647 | . 3 SICU | 23 | 16.911765 | . 4 TSICU | 11 | 8.088235 | . Extract age across care units . We would like to know the age (in years, with age &gt;= 16) distribution over all care units combined, as well as across the different care units. More specifically, we are interested in the median, lower quartile and upper quartile. It is better not to use the mean here, because, for privacy reasons, age &gt; 89 is set to 300 in the database. . To obtain age, we need to combine the Patients and the Icustays tables. Age can be computed by subtracting the time of admission to the ICU from a patient’s date of birth. Moreover, to get the age distribution across ICUs, we can use the different care units obtained from the Icustays table. . . # Compose SQL query query = &quot;&quot;&quot; SELECT i.first_careunit, round((EXTRACT(EPOCH FROM (i.intime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) as age FROM public.patients p INNER JOIN public.Icustays i ON i.subject_id = p.subject_id WHERE round((EXTRACT(EPOCH FROM (i.intime-p.dob))/60/60/24/365.242) :: NUMERIC, 4) &gt;= 16 &quot;&quot;&quot; # Run query query_output = pd.read_sql_query(query,con) query_output.head() . first_careunit age . 0 MICU | 70.6378 | . 1 MICU | 36.1923 | . 2 MICU | 87.0874 | . 3 CCU | 73.6875 | . 4 MICU | 48.9015 | . # Visualize distribution of age: query_output[&#39;age&#39;].hist(bins=200) . &lt;AxesSubplot:&gt; . We know that ages bigger than 89 have been set to 300 for privacy protection. . # Define function for descriptive stats 5 number summary for a field per icu def icu_descriptive_stats(field, df, boxplot_title): # Get list of ICUs icu_list = df[&#39;first_careunit&#39;].unique() # Plot descriptive stats for each ICU for icu in icu_list: print(&#39; &#39;) print(&#39;Descriptive statistics for &#39; + str(icu) + &#39; by &#39; + field) icu_df = df[df[&#39;first_careunit&#39;] == icu] print(icu_df[field].describe()) # Plot box plot of ICU by field plt.figure(figsize=(20,10)) sns.boxplot(data=df, x=&#39;first_careunit&#39;, y=field) plt.xlabel(&#39;ICU&#39;) plt.title(boxplot_title) # 5 number summary all ICUs for age (remove all ages of 300) df = query_output[query_output[&#39;age&#39;] &lt; 300] icu_descriptive_stats(&#39;age&#39;, df, &#39;ICU by Age&#39;) . Descriptive statistics for MICU by age count 71.000000 mean 69.824277 std 14.777606 min 27.016700 25% 64.061050 50% 70.155800 75% 82.498550 max 88.036400 Name: age, dtype: float64 Descriptive statistics for CCU by age count 18.000000 mean 68.994761 std 14.572275 min 40.606400 25% 57.192850 50% 72.843600 75% 79.406575 max 88.642100 Name: age, dtype: float64 Descriptive statistics for CSRU by age count 6.000000 mean 78.496400 std 6.423162 min 70.754500 25% 73.265675 50% 79.354650 75% 82.011550 max 87.381400 Name: age, dtype: float64 Descriptive statistics for SICU by age count 22.000000 mean 73.492768 std 12.891770 min 44.106900 25% 63.048450 50% 77.671000 75% 81.437725 max 88.738100 Name: age, dtype: float64 Descriptive statistics for TSICU by age count 10.000000 mean 53.558550 std 25.190762 min 17.192000 25% 34.653600 50% 54.343500 75% 68.308100 max 88.063500 Name: age, dtype: float64 .",
            "url": "https://www.livingdatalab.com/health/electronic-health-records/2022/03/14/using-mimic3-ehr-database.html",
            "relUrl": "/health/electronic-health-records/2022/03/14/using-mimic3-ehr-database.html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "The MIMIC-III Electronic Health Record (EHR) database",
            "content": "Introduction . In this article we will look at MIMIC-III, which is the largest publicly Electronic Health Record (EHR) database available to benchmark machine learning algorithms. In particular, we will learn about the design of this relational database, what tools are available to query, extract and visualise descriptive analytics. . The schema and International Classification of Diseases coding is important to understand how to map research questions to data and how to extract key clinical outcomes in order to develop clinically useful machine learning algorithms. . Data and EHR (Electronic Health Records) in Healthcare . Enabling a digital system of Electronic Health Records provide unique opportunities in advancing clinical decision making systems. However, it also poses key challenges. In this article, we are going to talk about the main dimensions data in health care, including volume, variety, time resolution and quality. Then we are going to discuss how clinical decision making depends on a pathway of descriptive analytics to predictive analytics and finally too prescriptive analytics. . Currently, traditional healthcare models rely on disconnected systems, multiple sources of information. The new digital healthcare model will transition towards an inherent capability to ensure seamless information exchange across system. This enable data mining and machine learning approaches to successfully applied and advance our knowledge with relation to clinical decision making systems. Electronic Health Records are massively heterogeneous. They include medical images, lab tests, natural language diagnosis from doctors, medications events and hospitalizations. Often these records are unstructured and they require linkage between different sources. Health care records have a longitudinal nature. In other words, a single patient data are spread over multiple Electronic Health Records with diverse representation over time. . A fundamental principle in medical systems is that clinical data cannot be overwritten. This is an important principle when we design database to retrieve information. When any of this data are modified during further treatment or subsequent hospitalization, we need to a new extract with new data and store those again. A connection should be created to link this new information with the rest of the information available for the patient. In secondary research use of healthcare data, it is common to look for health care, quality evaluation, clinical and epidemiological studies as well as service management. In several cases the research is focused on a particular group of patients who satisfy distinct searching criteria. To understand how to extract value from big data and healthcare we need to understand their dimensions. The main characteristics of big data are volume, velocity, variety, veracity and value. Big healthcare is really big. In 2013 it was estimated that the healthcare data produced globally was 153 billion gigabytes. This is equal to 153 exabytes. This number projected to 2020 results to 2314 exabytes. Considering that data has doubled every year The velocity shows how quickly the data being created, saved, or moved. . The value of the data reflects on whether we can use them to form and test useful hypothesis. It is also important on whether the data can allow us to predict future events, and in this way, we intervene early. Viability is also a dimension that relates to value, and it reflects whether the data are relevant to the use case. Do they include all the information needed to investigate specific questions? Metadata is data about data. Sometimes it might be the file’s origin, date, time, and format. It may also include notes or comments. In healthcare, metadata is important to verify the veracity and effectively the value of the data. . We can conceptualize healthcare information retrieval processes as a pathway from descriptive analytics to diagnostic analytics, predictive analytics, and prescriptive analytics. Descriptive analytics use techniques such as data aggregation, data mining, and intuitive visualizations to provide understanding of historic data. They’re retrieving information. Common examples of descriptive analytics are reports that provide the answers to questions such as, how many patients were admitted to a hospital last year? How many patients died within 30 days? Or how many patients caught an infection? In other words, descriptive analytics offer intuitive ways to summarize the data via histograms and graphs and show the data distribution properties. In most cases, to achieve substantial insight and understanding for health delivery optimization and cost savings, dataset linking is required. In other word, it is desirable to link different sources of data. In its simplest form, this requires to link information related to a patient across all different departments in a hospital. Limitations of descriptive analytics are that it keeps limited ability to guide decision because it is based on a snapshot of the past. Although this is useful, it is not always indicative of the future. Diagnostic analytics is a form of analytics that examines data to answer the question of why something happened. . Diagnostic analytics could comprise of correlation techniques that discovers links between clinical variables, treatments, and drugs. Predictive analytics allow us to predict the outcome and likelihood of an event. We may like, for example, to predict the mortality risk of a patient, the length of hospitalization, or the risk for infection. Predictive analytics exploit historic values of the data with the aim to be able to provide useful information about critical events in the future. Predictive analytics are in demand because health care providers would like evidence based ways to predict and avoid adverse events. . In this way, they can reduce costs as well as avoid failure to control harmonic diseases. Importantly predictive analytics enable early intervention which can save patient lives and improve their quality of life. Prescriptive analytics aim to make decisions for optimal outcomes. In other words, they use all the available information to come to an optimal decision with relation to what action should be taken. Predictive analytics help us to understand the impact of an intervention in clinical care. And confirm whether the system is useful. Prescriptive analytics predicts not only what will happen but also why it will happen. In other words, prescriptive analytics is important to transition a prediction model to a decision making model. . The availability of big data provides several opportunities but it also poses important challenges. And the 1st one is interoperability. With such a diverse health care system that included the hetero continuous data sources and users like healthcare providers, clinicians, government organizations wearable technologies and so on. It is particularly challenging to maintain the high level of interoperability necessary for timely information sharing when needed. . The problem becomes even worse because of the lack of standards in the healthcare industry. Interoperability designs should also take into consideration patient safety and privacy. Lack of interoperability for example could potentially resulted to medical errors and endanger patient safety. In terms of patient safety it is also important to be able to access information quickly. The conflicting needs to share patient information in real time upon appropriate request while also making sure private patient information is kept secure. Makes management of healthcare industry especially complex. Another challenge of big data in health care is the fact that they change quickly. . Therefore it is important to know for how long the data relevant and which historic values to include in the analysis. Vulnerability refers to the fact that we need to keep the data secure and this can involve both IT infrastructure but also regular training procedures. Last but not least, the data growth and the lack of expert ties are difficult to ignore. Some are rising big data in health care presents unique opportunities and challenges. Healthcare data is a valuable asset and is defined based on the volume, variety, velocity veracity and value of the data set. Clinical decision support system exploit information in this data via a pathway from descriptive to predictive and prescriptive analytics. . EHR System in the UK and USA . The US and the UK health care systems are known to be run very differently. UK has the largest public sector system and invest much less on its healthcare system. On the other hand, USA has the largest private-sector system and one of the largest health care expenditure in the world. It is interesting to compare the electronic health record system adaptations in these two countries in order to understand the challenges. . Both USA and UK has succeeded in the adaptation of electronic health records in their systems. UK followed a top-down approach. The difficulty was that clinicians are not used to have technology dictated decisions to them. On the other hand, USA followed up bottom-up approach. This approach was successfully adapted by individual office-based physicians, but it was more difficult to ensure interoperability between larger facilities and hospitals. Overall, we shouldn’t underestimate the complexity of the health care system. In order to fully explore the potential of electronic health records, we need to sustain the interoperability, security, and privacy of patients information. We also need to take into account the possible usage and value of information. . The MIMIC Critical Care Dataset . The MIMIC-III database links data from a hospital with data from patients from the intensive care unit. The database is well maintained and it includes lab tests, medical diagnosis, vital signs, and medication. Researchers at the laboratory of computational physiology at MIT recognized the need to generate new knowledge from existing data. Big data was captured daily during care delivery in the intensive care unit. But none of this was used for further exploration. The motivation was to provide a freely accessible deidentified critical care dataset under a data user agreement. This dataset is available both for academic as well as industrial research in higher education. The health care dataset is not only large, but it also spans over a period of a decade. . This hospital data reflects one of the best examples in systematic gathering of clinical information. It is a valuable, high-quality dataset that highlights the opportunities in machine learning. It’s realistic settings also reveal the challenges in processing electronic health records. Back in 1992, there was an effort to collect multi-parameter recordings of intensive care unit patients. This created the MIMIC project, which is a collection of clinical data. MIMIC-II was the largest multi-parameter intelligent monitoring in intensive care database containing physiological signals and vital sign time series captured from patient monitored. Along with this data, there were also clinical data obtained from the hospital medical information system. Data were collected from intensive care units between 2001 and 2008. This included the medical, surgical, coronary care, and neonatal care unit. With more data updates and also adding a new monitoring system, the MIMIC-II evolved to MIMIC-III and it was published in 2016. . The MIMIC project continues to have huge success. This is obvious from the number of citations that has received over the time. Starting from 2002 with the first release of MIMIC-II and subsequently in 2009 with update version and finally with MIMIC-III in 2016, we see an exponential growth of citations. MIMIC-III had impact in several disciplines beyond medicine. We see here the number of citations that it has attracted across science. The availability of more than 40,000 patient data had an impact in computer science and machine learning. We can also measure the influence of the database in other fields such as mathematics, engineering and physics. A large amount of attention has also received in medical research and there are several articles within critical care medicine, cardiology, gerontology, pathology, neuroscience, and infectious diseases. . Not only MIMIC is impactful, but also the papers that use MIMIC are impactful. MIMIC allowed research in deep learning models that wasn’t possible before. Sophisticated models can be developed, trained, and validated with MIMIC. Furthermore, it enables research in clinical decision support systems. The database also shaped the research in big data analytics in health care. The MIMIC project is also a model that can be used in other clinical databases in order to deidentified free-text as well as other clinical information. Summarizing, MIMIC-III is a big dataset of healthcare data that includes both hospital data as well as intensive care unit data. The data has been carefully deidentified and they can be used to facilitate the reproducibility of clinical studies to develop new algorithms and new technologies. MIMIC-III is the first of its kind that is publicly available. .",
            "url": "https://www.livingdatalab.com/health/electronic-health-records/2022/03/14/data-ehr-healthcare.html",
            "relUrl": "/health/electronic-health-records/2022/03/14/data-ehr-healthcare.html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "Validity and Bias in Epidemiology",
            "content": "Introduction . Epidemiological studies can provide valuable insights about the frequency of a disease, its potential causes and the effectiveness of available treatments. Selecting an appropriate study design can take you a long way when trying to answer such a question. However, this is by no means enough. A study can yield biased results for many different reasons. This article explores some of these factors and provides guidance on how to deal with bias in epidemiological research. We will learn about the main types of bias and what effect they might have on your study findings. We will then look at the concept of confounding and will explore various methods to identify and control for confounding in different study designs. In the last section we will discuss the phenomenon of effect modification, which is key to understanding and interpreting study results. We will finish with a broader discussion of causality in epidemiology and we will highlight how you can decide whether findings indicate a true association and if this can be considered causal. . Validity and Bias . When critiquing epidemiological studies, you will often hear or =read about concepts such as validity and bias which determine whether the results of a study are relevant and should be trusted or not. . When critiquing a particular study, there are some key questions that you would consider. One of these is whether any inferences arising from it are valid for the source population of this study. For example, a study may report an association between a new drug and improved survival among male cancer patients in a university hospital. There are many reasons why this could not reflect the truth such as flaws in the design or the execution of the study. But if we believe that this association truly exists among this group of patients, then we say that this is a study with internal validity. . Another equally important question is whether these inferences are applicable to individuals outside the source population. Internal validity is a prerequisite for this. If we don’t think the results reflect the truth in the source population, discussing if they can be generalized to other groups of people is pointless. But let’s assume that taking this new drug is in fact associated with improved survival among male cancer patients in the university hospital where the study was conducted, and the researchers have done an excellent job showing this. We would say that the study has external validity if we believe that this finding can be applicable to other groups of cancer patients, female patients in the same hospital or patients treated in different settings and countries. . External validity sometimes referred to as generalisability and largely determines the real life impact of a certain finding beyond the specific setting where the research was conducted. Closely linked to validity is the concept of bias. Simply put, an inference is valid when there is no bias. According to one popular definition, bias is any trend in the collection, analysis, interpretation, publication, or review of data that can lead to conclusions that are systematically different from the truth. The key word here is systematically. A systematic error in the design and conduct of the study can result in bias which means that the observed results may be different from the truth. . In conclusion, systematic error can introduce bias in the study which in turn hurts its validity. Bias can take many forms, and scientists have identified many types of bias and their variations over the years. To make things more difficult, there are myriad different classifications and names for bias observed in epidemiological studies. We will consider three broad categories of bias: . Selection bias | Information bias | Confounding | . Selection bias . One of the main categories of bias in epidemiological studies is selection bias. In practice when doing research, it is almost impossible to examine the entire study population. This is why we select the sample. Despite our efforts to select a sample that is representative of the study population, it may happen that an individual’s chance of being included in the study sample is related to both the exposure and the outcome. When this happens, we get a biased estimate of the association between the exposure and the outcome and we say that there is selection bias. . Let’s consider a case control study where the exposure is diet rich in red meat and the outcome is colon cancer. As we’ve discussed, our sample will include only a fraction of the study population. In one scenario, people with colon cancer have a 60 percent probability to be included in the study sample while people without colon cancer have a 40 percent probability to be included. Clearly, the disease status is associated with inclusion in the sample but within its disease category, individuals are equally likely to be selected regardless of whether they eat a lot of red meat or not. In this case, there is no selection bias. . In another possible scenario, the disease could be irrelevant with regard to being included in the sample. However, individuals eating a lot of red meat could be less likely to be included in the study compared to those not eating red meat. For example, because part of the recruitment strategy was to place posters in shops for vegetarians. In this case, the probability of being included in the sample is associated with the exposure eating red meat but not with the outcome which is colon cancer. Therefore, there is no selection bias in the study. . So, when do we have selection bias? Consider the same case control study. This time, 60 percent of people with colon cancer accept to participate regardless of their diet. Among people without colon cancer, 50 percent of those who eat red meat and 40 percent of those who don’t eat red meat decide to participate. In this scenario, participation in the study sample is associated with both the exposure and the outcome. Therefore, it is a typical case of selection bias and our estimate will be biased. . It is not a coincidence that we have used case-control studies in this example, Case-control studies are particularly susceptible to selection bias but there are ways to minimize selection bias, we will mention three of them. First, researchers try to select controls which are representative of the study population in terms of exposure to the factors under study. Also, in all study designs, it is important to keep non-response to a minimum. When many people decline to participate, it becomes more likely that some bias could be introduced. Finally, it is always good practice to compare those included in the sample with those who declined to respond and explore whether there are any systematic differences. Selection bias can seriously undermine the validity of the study, it is therefore really important that you take this into account when designing or critiquing epidemiological research. Of course, there can be other sources of bias as well. . Information bias . Much like selection bias, information bias has many different names and subcategories, but includes misclassification of the exposure or the disease status or both. Let’s consider an example of a case-control study which aims to look at a potential association between smoking and lung cancer. Regarding exposure, we would obviously need to assess whether participants were smokers or not and how much they smoked. We would also need to classify people as having lung cancer or not, as this is the outcome of interest. Both exposure and outcome could be misclassified. For instance, some heavy smokers may be erroneously classified as light smokers or some lung cancer patients may not receive the correct diagnosis. Usually this happens either because the study variables are not properly defined or due to flaws in data collection. . Let’s examine some of these flaws more closely. One common flaw in data collection occurs when interviewers ask individuals about their exposure status. In our example, interviewers would ask individuals with and without lung cancer, if they have been smoking. But the interviewers might be more thorough in assessing past smoking when interviewing people who have been diagnosed with lung cancer, exactly because they expect that lung cancer patients are likely to have been smokers. This would lead to misclassification of exposure status and eventually to a biased odds ratio. This type of information bias is called Interviewer bias. Luckily, this can be prevented if the interviewer does not know the disease status of the individual or if the collection process has been carefully standardised, so that interviewers follow a strictly defined protocol when they collect data from participants. However, interviewers are not the only potential source of information bias. . When patients with lung cancer are asked to report whether they have smoked in the past, they might be more likely to recall a brief period of smoking along time ago compared to those who don’t have lung cancer. This is not unexpected. Our memory is not perfect and we often forget things that have happened in the past. But when we get sick, we try hard to remember any details that could be linked to our disease. Details that we would otherwise erase from our memory. This phenomenon is called Recall bias and is a common type of information bias. We can prevent it by using objective ways to assess exposure such as medical records or biomarkers. We should highlight that Recall bias specifically refers to the differentially inaccurate recall of past exposure between cases and controls. . When all the participants have trouble remembering their exposure status, but this has nothing to do with their disease, there’s no recall bias. This is a principle that can be generalised, when exposure status is misclassified but equally so among cases and controls, we speak of non-differential misclassification. The same term applies when there are errors in determining the outcome, but they occur equally among exposed and non-exposed individuals. When non-differential misclassification occurs, the odds ratio we obtain is biased always towards the null. In contrast, misclassification is differential when errors in determining an individual’s exposure status occur unevenly among cases and controls or when there are errors in the diagnosis of the disease which occur unevenly among the exposed and non-exposed individuals. . Differential misclassification also leads to a biased estimate, but we cannot predict if it is biased towards or away from the null. As we can see, on all these occasions, there is information bias that could lead to a biased estimate. We have seen how these can influence the results of your study and with ways to prevent this. Together with confounding, which we will explain later, the broad categories of selection and information bias can explain essentially all the issues that could undermine the validity of a study. . Association and Confounding . What is Confounding ? . Correlation does not imply causation, one of the reasons we say this is confounding. Consider you are studying Down syndrome, and you come across a graph, which clearly shows that there is an association between birth order and Down syndrome. It seems there’s a higher risk of being born with Down syndrome among children with higher birth order. Now the question is, is it the birth order that increases this risk? You suspect that there maybe another variable correlated with birth order, which is responsible for the observed association. . Then, you find another graph, which shows that the risk of Down syndrome increases with maternal age. There’s no doubt that maternal age is also associated with birth order. Mothers will give birth to their fourth or fifth child are on average older than those who have their first baby. When you look at the risk of down syndrome within each age group, birth order doesn’t seem to matter at all. In summary, maternal age is entirely responsible for the association between two other variables, birth order and Down syndrome. This effect of an extraneous variable that wholly or partially accounts for the apparent effect of the study exposure or that masks in the underlying true association is called confounding. If you hadn’t looked further, you might have thought that birth order might cause Down syndrome, which is clearly not true. Confounding can be a real headache for researchers, and if not properly controlled for, it can produce misleading results. . How to detect Confounding . Confounding can lead to biased estimates and produce misleading results. Therefore, it is something that we should know about when designing, conducting, or critiquing a study. But how can we know if this confounding? There’s no statistical test for confounding, that are of course statistical methods that can help us make an informed decision. But it depends largely on our judgement. We will look at four commonly used ways to identify potential confounding factors in an epidemiological study. . Let’s consider an example of a study which aims to investigate the association between dog ownership and mortality among the elderly. Some previous studies have found that owning a dog can be associated with higher life expectancy. One straightforward way to identify factors that could confound this association, is to explore the literature. Knowledge of the subject matter can heavily influence our decisions regarding confounding. For example, if other studies have shown evidence that the size of the city where people reside is a confounder in the association between dog ownership and mortality, we have every reason to consider it as a confounder in our study. Knowledge of plausible biological pathways can similarly help us identify confounders. . However, this is not always possible, especially when we explore novel associations for which prior research is scarce. In such cases, we can examine whether the variable of interest satisfies the following three conditions. It is associated with the exposure in the source population, it is associated with the outcome in the absence of the exposure, and it is not a consequence of the exposure. In other words, it is not in the causal path between the exposure and the outcome. If we stick to the same example of dog ownership, our exposure, and mortality, our outcome, and we would like to explore whether age may be a confounder, we would need to answer the following questions. Is age associated with dog ownership among the elderly? Is age associated with mortality among those who do not own a dog? Is aging in the causal path between dog ownership and mortality? We can only respond to the first two questions when we analyze data from the study. But let’s assume that age is associated with both the exposure and the outcome. The answer to the last question is obvious here, owning a dog cannot change your age. So, age is not in the causal path. Age satisfies all three conditions. Therefore, we identify it as a confounder in this study. . A different way to think about this is to stratify data by the variable of interest, which is age in our example, and compare the stratum specific estimates with the estimate that we get when we analyze the entire set of data from the study. In our study, we will need to split our sample by age, below 80 and 80 and above for example, and calculate the odds ratio in each subgroup. We might find that owning a dog reduces mortality by 40 percent among those below 80 years old and 38 percent among those at least 80 years old. But when we analyze the entire sample together, we could find that owning a dog only reduces mortality by five percent, which, of course, doesn’t make sense when you consider the stratum specific numbers. When the pooled estimate is considerably different from what you would expect based on stratum specific estimates, it is very reasonable to think that there is confounding. . Lastly, the fourth way to detect confounding is the following. Let’s say we use a simple logistic regression model to estimate the crude odds ratio that expresses the strength of the association between dog ownership and mortality in our study. When we include age in the regression model, we estimate the adjusted odds ratio, adjusted for age in this case. If the adjusted odds ratio differs from the crude odds ratio by 15 percent or more, this may indicate confounding by age. This number is arbitrary and may not always reflect true confounding. It could be that we introduce confounding by adjusting for an additional variable. This is not the optimal method to identify confounding but can sometimes flag occasions where further investigation is required. . People often assume that they need to use all these methods, however you only need one of the above methods to identify confounding. If you can make a decision based on your knowledge of the subject matter, you don’t need to stratify or explore whether the three conditions are satisfied. In conclusion, there are multiple ways to think about confounding. But at least to some extent, we need to use our judgement to decide which factors may cause confounding. This is a critical decision because it will inform the design and data analysis of our study. . In summary we can detect confounding in the following ways: . Subject matter knowledge. Factors identified in existing literature or plausible biological pathways can inform your decisions. | Three criteria for confounding. You need to examine if the suspected extraneous variable satisfies three conditions. – It is associated with the study exposure in the control group (source population) – It is associated with the study outcome in the absence of study exposure – It is not a consequence of exposure, i.e. it is not in the causal path between the exposure and the disease. | Stratification. Stratify data by the extraneous variable to examine if the estimates within strata of the extraneous variable are similar in magnitude and appreciably different from the crude (pooled) estimate. | Statistical adjustment. Controlling for the extraneous variable, e.g. by logistic regression, appreciably (&gt;15%) alters the estimate of the association between the exposure and the outcome. | . Dealing with Confounding . Confounding can be addressed either at the design stage, before data is collected, or at the analysis stage. We will also briefly look at Directed Acyclic Graphs, which is a novel way to detect bias and confounding and control for them. . Design stage . Confounding can lead to biased estimates which essentially defeats the purpose of research. What is the use of a study if we cannot trust its results? To overcome this problem, we always try to control for confounding. We will look at three methods which you can use to control for confounding at the design stage of a study: randomisation, restriction, and matching. . The first and admittedly the best available method to control for confounding is randomisation. When we split our sample into exposed and non-exposed at random, we ensure that the distribution of all factors and characteristics that may influence the outcome is similar between the two groups. With a large enough sample, this neutralizes the impact of any potential confounding factors. The beauty of randomisation is that it controls, not only for known confounders, but also for those that we are not even aware of. Unfortunately, randomisation only applies to trials. For example, we cannot randomise exposure such as smoking or air pollution due to ethical and practical reasons. Therefore, there are certain questions that cannot be answered by conducting a randomised trial. In such cases, we must rely on other methods to control for confounding. . Restriction is such a method. The idea behind restriction is very simple. We restrict the eligibility criteria for subjects to be included in the sample so that we only study subjects within one category of the confounding variable. For instance, if we think that sex may be a confounder, we can decide to restrict our study to women. This solves the problem of confounding in a simple, efficient, and inexpensive way. On the other hand, it might make recruitment of participants more difficult, and in any case, it undermines the generalizability of the study. Finding that the drug is effective among women does not necessarily mean that it would be equally effective among men. . The third method to control for confounding, which is quite popular for case-control studies, is matching. In matching, we pair one or more controls to each case based on their similarity with regard to selected variables which will consider potential confounders. For instance, we suspect that sex and age maybe confounders in our study. We’ll recruit a case who is a woman aged 54 years. If we conduct a match case-control study, we need to find one or more controls that are 50-year old women. This can increase statistical power in our study, but it requires analytical methods that consider the match design. Also, there’s a limit to the number of confounders that we can control for with matching. If we try to match on too many variables, recruitment of controls becomes impractical. We’re also unable to study the variable we use for matching. Importantly, matching cannot be undone, and matching on a variable that is not a confounder actually harms statistical efficiency. So, a decision to match should be well thought out. . It is not always possible to anticipate and control for confounding at the design stage. Luckily, there are additional methods that can be applied during data analysis. . Analysis stage . Ideally, when designing a study, you would like to know all the potential confounding factors and plan how to control for them in advance, but some other confounding factors may only be identified as such when data is analyzed. We will look at the two main strategies to control for confounding at the data analysis stage: Stratification and Multi-variable regression models. . So, let’s say we have conducted a study where the exposure is smoking, and the outcome is chronic lung disease. We suspect that age is a confounder in this association. What can we do at the Data Analysis stage? One option would be stratification. The first step is to stratify our data by age group, and obtain an estimate for the association between smoking and chronic lung disease in each stratum. This means that we calculate an odds ratio, for example, for people 20-29 years old, and now the odds ratio for those 30-39 years old, and so on. In the second step of the process, we calculate a weighted average of the stratum-specific odds ratios. This weighted estimate is called Mantel-Haenszel adjusted odds ratio, and this is essentially the results of our study after controlling for confounding by age. This method allows us to get a sense of what is happening within the different strata, but it becomes really burdensome if you try to control for multiple confounders, and it doesn’t really work for confounding variables which are continuous. . A second option, which is what the majority of researchers do nowadays, is statistical adjustment using regression models. In our example, we can estimate the association between smoking and chronic lung disease by fitting a logistic regression model, where the exposure is the independent variable, and the outcome is the dependent variable. If smoking is the only independent variable we include in the model, we will calculate an unadjusted odds ratio. If we wish to control for confounding by age, we simply need to add it as an additional independent variable in the regression model, and we can easily calculate an odds ratio that is adjusted for age. The great advantage of multivariable regression is that we can control for multiple confounding factors at the same time, although including too many variables can sometimes cause problems. . Directed Acyclic Graphs (DAGs) . There are many other strategies that epidemiologists employ to control for confounding, some more popular than others which include directed acyclic graphs, or simply DAGs, which have become quite popular among researchers in recent years. The directed acyclic graphs, are, as you would expect, graphs. They are essentially a way to present, in a graph, causal associations between variables. If we consider the association between high blood pressure and stroke, High blood pressure is the exposure and stroke is the outcome. . If we think that high blood pressure causes stroke, we will draw an arrow from high blood pressure to stroke. This is a simple way to illustrate what we are talking about. But things are rarely that simple. Let’s introduce one additional factor, age. Old age may affect blood pressure, but it can also affect the probability of having a stroke. To illustrate this, we would add two more arrows in the graph, one going from age to blood pressure and the other also starting from age and going to stroke. And here it is, this is a DAG. Depending on the context of the study, we could add more variables and arrows. Although it becomes quite complicated when you have multiple factors and complex relationships among them. . Using DAGs can help us think about the variables that are relevant to our study and the associations between them. It is also a great tool to communicate this information to others. There’s more to it. Epidemiologists have developed a set of rules called D-separation rules which allow them to identify confounding and other types of bias just by looking at the DAG. One of the benefits of using DAGs is that it is very practical. Applying the D-separation rules, you can identify the minimum set of variables that you need to control for in order to address any sources of bias in your study without having to name or characterize the type of bias that you observed. This is why the focus in DAGs is on confounding and not on confounders. . Effect modification . When we analyze data from an epidemiological study, we usually build a statistical model with the aim to describe what has happened in our study. To do so, we make assumptions and often, intentionally ignore differences between individuals or subgroups, so that we can estimate an average association between the exposure and the outcome that applies to the entire population. But sometimes, after controlling for confounding and bias, there is still a third variable, the impact of which on the association between exposure and outcome is so important that cannot and should not be ignored. This is called effect modification. . Imagine you are conducting a randomised clinical trial which aims to test the effectiveness of a new antibiotic against pneumonia. Some of the patients received this new antibiotic, and the rest are given the older drug that is widely used. You follow all the patients up and there are two potential outcomes, a patient can either recover or die. When you analyze data from the entire sample, you find that the odds ratio of recovery of those exposed to the new drug compared to those exposed to the old drug is 1.5, which means those taking the new antibiotic are 50 percent more likely to recover compared to the controls. This is an important result for the trial and if you have conducted your RCT properly, you don’t need to worry about confounding. But before you publish your results, one of your colleagues decides to stratify the data by sex, and notices that the odds ratio is 1.1 for men and 1.9 for women. Men and women do not differ in terms of age, comorbidities, or other confounding factors. After careful consideration, your team decides that the bias cannot explain this difference. So, what’s happening? . Well, sometimes a drug can be more effective in women compared to men, or vice versa. In other words, sex modifies the association between the drug, your exposure, and recovery, your outcome. This is a phenomenon that we call effect modification. Making the definition more general, we say that effect modification exists when the strength of the association varies over different levels of a third variable. In such cases, reporting the overall estimate would not be helpful at all because it would not reflect what actually happened in either sex. . Should you then find a way to control for effect modification and avoid this problem? Definitely not. Unlike confounding, effect modification is a naturally occurring phenomenon. It’s not a problem of your study. You should have no intention to control for it, but the way you report your results should take it into account. In the case of the trial with the new antibiotic, you simply need to present results stratified by sex. You might need one more table in your paper, but this will allow you to accurately report your findings for both men and women. In general, when effect modification is detected, you must conduct stratified analysis. In the example above, we ignored uncertainty. You probably noticed that we gave the estimates without their confidence intervals. . In real life uncertainty cannot be ignored, and this raises one key question, how can we be certain that the stratum-specific estimates are truly different between them? There are statistical methods that can help us identify effect modification such as the Breslow-Day test, the Q test, and including interaction terms in regression models. Regression models are very frequently used, and the term interaction is often considered equivalent to effect modification. The term synergism means that the effect modifier potentiates the effect of the exposure, and antagonism means that the effect modifier diminishes the effect of the exposure. Effect modification is an important concept in epidemiology because it is relevant to many associations in nature but also one that confuses a lot of people. Perhaps it’s because we’re so used to trying to eliminate bias and confounding, that we find it hard to accept that this is a natural phenomenon that we simply need to describe. . Confounding vs Effect modifcation . We have discussed how to identify confounding, and separately, how to identify effect modification. But things seem to get a bit confusing when you have to assess both confounding and effect modification in the same study. In reality, there’s absolutely no reason to get confused. In a typical study, we have an exposure and an outcome. Let’s also consider a third extraneous variable. I call it extraneous because it is neither the exposure nor the outcome. It could be something like sex or race, for example. You would like to explore whether the extraneous variable is a source of confounding or effect modification or maybe both. . The first thing to do would be to stratify the data by the extraneous variable, and estimate the association between the exposure and the outcome in each stratum. In practical terms, this means that you obtain an odds ratio for men and one for women, if sex is the extraneous variable of interest, of course. If the odds ratio for men is similar to the odds ratio for women, then based on the definition, there’s obviously no effect modification by sex, while the question whether there is confounding by sex is still open. Using the stratum-specific odds ratios, you can estimate an adjusted odds ratio, adjusted for sex. If the adjusted odds ratio is similar to the crude or unadjusted odds ratio, there is probably no confounding by sex, and you don’t need to take any further action. . But if the adjusted odds ratio differs considerably from the unadjusted estimate, this may be an indication of confounding, and you should control for it by presenting the adjusted estimate. What happens if the stratum-specific estimates are different? In our example, what should you do if the odds ratio for men is statistically different from the odds ratio for women? The answer is straightforward. This is a textbook case of effect modification. Therefore, you will just report the stratum-specific odds ratios separately. Again, the question whether sex is also a confounder has not been answered at this stage. However, if you’re presenting separate estimates for men and women, which you do, because there is effect modification, you don’t really care if sex can cause confounding. In practice, you have already controlled for confounding by presenting stratum-specific odds ratios. This strategy should allow you to identify confounding and effect modification in a study. If you think about it, you already know a few methods to assess confounding and you described one of them, while also repeating the method to identify effect modification. In some cases, you might find that a certain variable is both a confounder and an effect modifier, which is possible. In summary, confounding is a problem of our study, and therefore, we try to control for it. Whereas, effect modification is a natural phenomenon, which requires the presentation of stratum-specific estimates. . Causation . The distinction between association and causation is fundamental in epidemiology. Whenever you observe an association between an exposure and an outcome, you have to go through a few alternative explanations before you even start thinking about causality. The first possible explanation is chance which is surprisingly often ignored. There is an entire field of science, statistics, which deals with the uncertainty surrounding research findings. You should always consider the level of uncertainty and how chance may have affected your results. . Let’s assume that you have done all the appropriate statistical tests and you are confident that it is unlikely chance was responsible for your findings. The next step is to think about potential sources of bias and confounding. You’re familiar with the main types of bias and with methods to identify and control for confounding. These must be applied rigorously to exclude any external influences or systematic errors that might have affected your study. Once you’ve concluded that there was no bias or confounding, would you be ready to declare that the association you have detected is causal? . Not exactly. Unless you’re a real pioneer in science, your study is probably not the only one that has investigated this research question. Looking at the bigger picture allows you to make an informed judgement within the context of current scientific knowledge. British epidemiologist, Sir Austin Bradford-Hill, came up with nine criteria that can be helpful in this process. It’s been more than 50 years since he published the list, but I think that there’s still value in considering them. The first thing to consider is the strength of the association. A small effect size doesn’t mean that it’s not causal, but if the association is strong, causality may be more likely. The second criteria in the list is consistency. Consistent findings observed by different people in different settings with different samples can also be an indication of causality. . Causation is also likely in the case of a very specific population at a specific site and disease with no other likely explanation. The more specific and association between a factor and an effect is, the bigger the probability of a causal relationship. Another consideration is temporality. This is an obvious one, the effect has to occur after the cause. It is also helpful if you find that there is a biological gradient in the association. Greater exposure often leads to greater incidence of the disease. Although this is not always the case. Additionally, you can draw evidence from other fields of research. If there is coherence between your epidemiological results and findings from laboratory research and if there is plausible biological explanation of the association, causality becomes more likely. The same is true when you have experimental evidence available and when you consider the effects of similar factors. The Bradford Hill criteria is not a checklist that you need to follow, but they highlight the challenges you might face when thinking about causal inference. Of course, these become relevant only after you have work hard to eliminate factors such as chance, bias and confounding. .",
            "url": "https://www.livingdatalab.com/health/epidemiology/2022/03/06/validity-bias-epidemiology.html",
            "relUrl": "/health/epidemiology/2022/03/06/validity-bias-epidemiology.html",
            "date": " • Mar 6, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "Study Designs in Epidemiology",
            "content": "Introduction . Choosing an appropriate study design in Epidemiology is a critical decision that can largely determine whether a study will successfully answer your research question. A quick look at the contents page of a biomedical journal or even at the health news section of a news website is enough to tell you that there are many different ways to conduct epidemiological research. . In this article, we will learn about the main epidemiological study designs, including cross-sectional and ecological studies, case-control and cohort studies, as well as the more complex nested case-control and case-cohort designs. Finally we will look at randomised controlled trials, which is often considered the optimal study design, especially in clinical research. You will also develop the skills to identify strengths and limitations of the various study designs. . Epidemiological Study Designs . Not all study designs are born equal. It is widely accepted that results from certain types of studies, are more likely to reflect the truth than others. This is often called Hierarchy of Evidence and considers systematic reviews, meta-analysis, and randomized controlled trials, as the best sources of evidence. . While this is mostly true, it does not account for the quality of studies. Many would argue that a well conducted case-control study, can be more informative than a trial with methodological problems. . Websites that publish epidemiological studies include Google Scholar and PubMed. . Descriptive Study Designs . Descriptive Study Designs include case reports, case series cross-sectional studies and ecologic studies. As implied by the name, descriptive studies are used to describe patterns in a population. These patterns can be related to prevalence or incidence or trends. A descriptive study could be about a single individual, this is known as a case report. An example would be an unusual set of symptoms or clinical features, such as a child with visual disturbances accompanied by abdominal pain or it can be about separate individuals with unusual symptoms. This would be known as case series. Descriptive studies can also be based on populations, as is the case with cross-sectional studies. These studies look at a snapshot in a given moment in time. . Using the findings of these descriptive studies, epidemiologists can then develop hypotheses about the causes of disease patterns and about the factors that affect disease risk. To further examine these hypotheses epidemiologists must turn to analytic epidemiology. Where descriptive studies describe the occurrence of disease or its determinants within a population, analytic studies are concerned with how the determinant may influence the occurrence of disease among individuals. . Analytic Study Designs . An analytic study aims to quantify the effect of an intervention or an exposure on an outcome. To quantify the effect you need to know the rate of occurrence in a comparison group as well as in the exposed group. There are 2 types of analytic study: observational and experimental. In an observational study you simply observe the exposure and the disease status of each participant. You don’t try and change the exposure in any way. The 2 most common types of observational studies are case control studies and cohort studies. In a case control study, you would identify your cases when you initiate the study and then you find controls to compare them to. In this type of study you assess the exposure in the disease cases and compare them to the controls. A cohort study is different in that you identify a population first, for example nurses in England and then you would assess exposure, for example physical activity. . The second type of analytic study designs are referred to as experimental studies and you can think of these as analogous to treating people like lab rats. In this case the investigator is able to assign the exposure to individuals from a particular population after which the outcome is measured in exposed and then unexposed groups. Ideally the assignment of the exposure should be random. These types of experiments are called randomized controlled trials and they usually considered the gold standard in analytic epidemiology. An example of a randomized control trial would be assigning some people to receive a particular vaccine, and then other people no vaccine and then examining whether the vaccine works in reducing the occurrence of a given condition. . To summarise, we search for the determinants of disease first by relying on descriptive epidemiology to generate hypotheses about associations between exposures and outcomes and then analytic studies are undertaken to test specific hypotheses. . Ecological Studies . In many epidemiological studies data is collected from individuals who are compared to each other in terms of exposure and outcome. But individual data is not always available and can be difficult to collect. Alternatively, we can conduct an ecological study which does not require data from individuals. . The core principle of ecological studies is that it focuses on the comparison of groups rather than individuals. In other words, the unit of observation is the group. This implies that you analyze only aggregate level data which usually cannot be linked to a specific person. The size of the group can vary. You could use a school or a work site as a unit of analysis, but it could also be something much larger, such as a geographic region or an entire country. Sometimes, the unit of analysis is not geographically defined. It could be an occupation or even a time interval. The idea is the same though. You aggregate data on exposure and outcome at the group level and subsequently, you take a number of groups and use the aggregate data in your analysis. . If we did an ecological study and found an association between a group and a exposure does that imply the exposure caused the outcome? Not necessarily. There can be many alternative explanations for this association. From chance to bias and confounding that apply to all study designs. Association does not always imply causation. But there is also something specific to this study design that we should never forget. Assuming that associations between groups hold for individuals is called ecological fallacy or aggregation bias. . So why bother with ecological studies? Well, usually, an ecological study is the first step in exploring a research question and can generate hypothesis about disease etiology. Ecological studies typically use secondary data sources that are already available. So, they’re relatively inexpensive and quick to complete. Sometimes, the level of inference that you’re interested in is at the population level anyway. For example, when looking at the impact of tax increases on cigarette consumption, in which case, conducting an ecological study is absolutely fine. Ecological studies are also suitable when the variability of exposure within each group is limited. If there is little variation in individual chocolate consumption within each country, you can be more confident about the association shown in the graph. On the other hand, any ecological study is subject to the ecological fallacy and relies on secondary data collected for different purposes which may not always be comparable between countries or time periods. It might also be unclear if the exposure preceded the outcome. . Ecological studies can be a valuable tool in epidemiology especially when we have limited time and resources. However, we should not assume that group level associations are necessarily applicable to individuals. . Primary and Secondary Data . Data collection is crucial for epidemiological research. Whilst there are various methods to collect data, all information which is gathered can be categorised into two different types: primary and secondary. . Primary data is data that has been collected for the first time by an investigator. Primary data can be collected via questionnaires, interviews or tests. The advantage of primary data is that collection methods can be adapted to the objectives of the study. However, collecting primary data can be costly and time intensive, which may mean that it is not always feasible to obtain. . Secondary data, also known as existing data, is data which has already been collected for other purposes. Some examples of secondary data include census data, medical records, employment records and mortality registers. Secondary data is readily available and therefore cheaper to obtain. Moreover, secondary data often has large sample sizes and is collected both comprehensively as well as on a routine basis. This can be advantageous to researchers who want to compare data over time to detect population-level changes. On the other hand, the format of secondary data may not be suitable for the researcher. Similarly, data coverage could be insufficient or the type of data collected may not be tailored to the research objectives of the researcher. . Primary and secondary data have strengths and limitations. The type of data which a researcher chooses to obtain or use can depend on a variety of factors such as the research question at hand, the time and resources available for the project, as well as the skills of the researcher. Several studies make use of both primary and secondary data to fulfil different requirements of the research. . Some COVID-19 examples . The rapid developments during the first few months of the COVID-19 pandemic created an urgent need for data and analyses that would provide much needed information about this new disease. . Examples of primary data used for such analyses include (a) results of PCR tests among travellers leaving Wuhan early in the epidemic (e.g. all passengers in a repatriation flight) to assess the prevalence of infection among them; (b) data from seroprevalence studies in which a representative sample of the population is tested to measure antibodies against the SARS-CoV-2 virus; (c) data collected during clinical trials testing the effectiveness of potential treatments of COVID-19. . Examples of secondary data used for such analyses incude (a) data on the number of confirmed cases or/and deaths by country or region used to conduct ecological analyses; (b) data from the electronic health records of patients hospitalised for COVID-19 to investigate potential risk factors for worse COVID-19 outcomes. . Cross-sectional Studies . Cross-sectional studies, are usually described as snapshots of the population of interest, at a specific point in time. We use the word snapshot, because we assess both the exposure and the outcome at the same moment in time. . The same moment in time, may last for days or weeks, if you’re collecting data from large numbers of people. The point here, is that each individual is only assessed once, and there is no follow up. As a result, you can assess the prevalence of a disease or condition with a cross-sectional study, but not the incidence rate or risk, both of which require follow-up period. This is as you can imagine, the main limitation of cross-sectional studies. No information regarding the temporal relationship between exposure and outcome, can be collected and therefore, you’re unable to determine if the exposure preceded the outcome. This is why surveys are most frequently used for descriptive purposes. If you want to investigate causal associations, you would probably choose a different study design. . The fact that there is no follow-up, makes cross-sectional studies relatively cheap and easy to conduct. On the other hand, the lack of follow-up means that you only assess cases of the disease that are present at the time of the survey. Those who have been cured or have died of the disease, are not in the sample anymore, which limits our ability to measure the true extent of the disease. While the most frequent method of data collection in cross-sectional studies is through questionnaires, you could collect blood samples, use diagnostic tests or do physical measurements. As long as participants are only assessed once, it will still be a cross-sectional study. . Overall cross-sectional studies despite all their limitations, still play a key role in epidemiology and public health, and provide valuable data for both researchers and policy makers. . Case-control Studies . A case control study involves comparing individuals with a particular condition or disease, known as the cases, to a group of individuals with the same general characteristics but without the condition or disease of interest known as controls. Information on past exposure to possible risk factors is obtained for both the cases and the controls, and the frequency and intensity of exposure in the cases is then compared with that in the controls. The starting point of most case control studies is the identification of cases, however prior to selecting cases clear eligibility criteria should be defined, based on the objectives of your study. This is referred to as the case definition, for example, you may only be concerned with a population within a certain age bracket or a specific gender. Cases can be sourced from a variety of places such as hospitals, clinics or the community setting, however, you must be aware of capturing all representative cases, for example not just those that are more advanced that make it to surgery. These cases should be representative of everyone with the disease under investigation. Usually it is not too difficult to obtain a suitable source of cases but selecting controls tends to be more problematic. . Assessing exposure in cases and controls has to be carefully considered. Self reported recall of usual behavior may not be comparable in cases and controls, for example if you have a chronic illness such as cancer, you may be more motivated to find out why you got the disease and thus think about your past differently and more likely to report it differently compared to if you did not have cancer or were a control participant. This is called recall bias. Another important factor is how many cases and controls are required. The number of cases that can be studied is often limited by the rarity of the disease being studied. If this is the case statistical confidence can be increased by having more than one control per case. As a result studies often allocate 2 or more controls per case. . The advantages of case control studies are: they good for studying rare diseases because you can identify all of the existing cases that have already accrued over many years; they are relatively inexpensive to conduct; they can be quick to obtain data because you can assess exposure and outcome all at the same time. However they have disadvantages, and these include: there can be bias associated with exposure assessment, that is, the presence of disease may affect how an individual reports past exposure. There’s often difficulty in selecting a good control group, and they are limited to assessing just one chosen outcome. They also can’t tell you any information about the temporal relationship between exposure and the disease. . The main principle of case-control studies is that we select a group of individuals with the outcome of interest (cases) and a group of individuals without the outcome (controls), and we explore whether they have been exposed to the exposure under study. . . The measure of association that can be estimated in a case-control study is the odds ratio (OR). . . Cohort Studies . In relation to the hierarchy of evidence, we’re climbing up the ladder. And with regards to observational study designs, cohort studies are considered the most robust than case-control studies. . The cohort study typically involves a group of people without disease who are observed over a period of time to see what happens to them. This is also known as a longitudinal study. As a result, the first step in conducting a cohort study is to select your target population and assess their exposure status. Next you will follow these people to check up if they develop the disease of outcome or outcome of interest. So the defining characteristic of a cohort study is that you track people forward in time, you always assess exposure prior to disease. . The key principal of a cohort study is that a number of individuals without the disease or outcome of interest are selected and followed up for a period of time. Some of them are exposed to the exposure under study, while the rest are unexposed. By the end of the study period, some individuals will have developed the disease/outcome of interest both in the exposed and in the unexposed group. . . Depending on the data you have collected during the follow-up period, you can calculate the risk and/or the incidence rate of the disease in the exposed and the unexposed groups. Hence, you are able to calculate the Relative Risk or Risk Ratio (RR), the Risk Difference or Attributable Risk (AR) and the Incidence Rate Ratio (IRR). . . . . . Strengths and Weaknesses of Cohort and Case-control Studies Compared . In epidemiology, studies can be either observational or experimental. Observational studies are studies in which the investigator only observes populations or individuals, and does not interfere or manipulate the exposure. We will look at the strengths and limitations of two most commonly used observational study designs: cohort studies and case-control studies. . Cohort studies . In cohort studies, a group of individuals without the disease are followed-up over a period of time to observe what happens to them. Cohort studies try to find associations between previously defined characteristics of a cohort and the development of disease. . Advantages of cohort studies include: . They enable researchers to investigate multiple outcomes simultaneously. | The temporal relationship between exposure and disease can be explored. In other words, we can be certain that the exposure preceded the disease. | Cohort studies can allow researchers to calculate incidence rates as well as risks (and the respective ratios). | Cohort studies suffer from fewer ethical concerns as researchers are not assigning exposures or intervening with participants. | . On the other hand, there are also limitations of cohort studies which should be acknowledged. . One weakness of cohort studies is that they usually have a long duration which also implies larger costs. | Cohort studies are not useful for studying rare diseases. | Loss to follow-up which is likely to occur when running cohort studies can introduce bias. | In occupational cohorts, the healthy worker effect may introduce bias. The healthy worker effect refers to the low mortality or disease incidence in healthy populations or industrial cohorts compared to the general population. | . Cohort studies are warranted when the time between exposure and disease is relatively short, the occurrence of the disease is not rare, and when adequate funding is available. . Case-control studies . Case-control studies are another type of observational study where the investigator does not interfere or manipulate the exposure. In case-control studies, individuals with a particular disease are compared with individuals without the disease with regard to their exposure status. . Advantages of case-control studies include: . One of the major strengths of a case-control study is that it is good for studying rare diseases. | Compared to cohort studies, it is also relatively inexpensive and has a shorter duration, reducing the time required to acquire results. | . On the other hand, like all study designs, case-control studies have limitations. . Case-control studies are prone to selection bias. Selection bias can occur as a result of how the participants are recruited into the study; this bias can be related to the case-control status of the participant or the exposure status. | Case-control studies do not allow the investigation of multiple outcomes. | . Nested Studies . Cohort studies are often extremely large national or international studies, and subsequently there are very rich data sources. As a result it’s important that epidemiologists utilize this data effectively. One way to do so is to conduct new studies within these cohorts. One such study is a nested case control study. . A nested case control study is a case control study embedded within a prospective cohort study. The prospective cohort study generates cases, and potential controls, for the nested case control study. As a result, the cohort study provides a well defined source population of both cases and controls. One of the main differences between a traditional case control study, and a nested case control study, is that the cases are diagnosed after exposure assessment during the follow up period. . In case cohort studies the aim is to achieve the same goal as cohort studies but more efficiently using a sample of the denominators of the exposed and unexposed cohorts and if conducted properly case cohort studies provide information that should replicate findings from a cohort study case cohort studies are very similar to nested case control studies. The main difference is the way in which the controls is selected. In the case cohort study cases are defined as those participants of the cohort who develop the disease of interest but the control group selected from all cohort participants at baseline before the cases develop. This means that controls are randomly selected from all cohort disciplines regardless of whether they go on to develop the disease of interest or not. . Case cohort studies share the same advantages of nested case control studies including the efficiency, flexibility and the reduction of information and selection bias however they also have some additional benefits. These include the ability to examine multiple outcomes; the ability to include person time in the analyses and they are good when excluding cases from the control group is logistically difficult. For example in diseases with a high proportion of subclinical phases such as prostate cancer to exclude all prostate cancers you would have to screen detect them however case cohort studies are not always feasible in particular they’re not suitable when exposures change over time; for example if exposure is measured at the beginning of a follow up period and differs from the overall exposure during the entire study period. To summarize - the case cohort study is an efficient alternative to analyzing the full cohort. When carefully planned and analyzed it is a strong choice for follow up studies with multiple outcomes of interest. . Nested case-control and case-cohort studies are studies nested within cohort studies. . One of the major strengths of nested case-control and case-cohort studies is that the data or biospecimen is collected prior to the disease, ensuring that the exposure preceded the disease. This also means there is less chance of bias when assessing the exposure. Finally, nested studies also reduce selection bias. | When dealing with valuable biological samples it may be too costly to analyse all biological samples or researchers may want to use samples for investigating multiple research questions. In that case, it is more advantageous to use nested case-control or case-cohort studies than full cohort analyses. Similarly, costs to data entry can be high and it may be more cost-effective to only analyse data from those who become cases and a sub cohort of non-cases. | Overall, they allow for the most efficient use of resources. | Nested studies are useful for studying rare outcomes. | Specific to case-cohort studies, one of its strengths is that it allows for the estimation of risk factor distributions and prevalence rates as well as unbiased assessment of correlations among variables, and can also include person-time in the analyses. | Nested case-control and case-cohort studies have limitations as well. For example, nested case-control studies can suffer from reduced precision and power as a result of the sampling of controls. | . Randomised Controlled Trials . Whether practicing clinical medicine, or working on a research project, all you’re ever trying to look for are associations. In medicine, this could be an association between a clinical symptom, like a cough, or a potential cause, like smoking, with a diagnosis, say heart failure or lung cancer. There are two basic approaches for assessing whether an exposure is associated with a particular outcome: using experimental or observational studies. However, the strength of an association is judged by the robustness of the evidence. We’ve already learnt about observational study designs where, as the name suggests, you simply observe the study sample. . A major problem with observational studies is that the observed groups may differ in many other characteristics in addition to the one being investigated. As a result, clinical medicine puts most emphasis on robust evidence from experimental studies or clinical trials, which are considered gold standards in terms of evidence. The best sort of trials are randomised controlled trials. Randomised controlled trials are experimental studies which compare and assess the effectiveness of two or more treatments, to see if one treatment is better than another. The treatment being tested could be a drug or some method of care, but there must always be a comparator group which acts as the control. . Treatments being tested could be compared with no treatment, ideally using a placebo as the control. For example, if you were testing a new drug, the placebo would be a tablet which looked identical, ideally, to the active drug in every way, but does not contain any active ingredient. Trials using this method are referred to as placebo controlled trials. Alternatively, once you have a treatment that is effective and safe, you may test a new treatment against the existing standard treatment, to check if it is more effective or to examine what the side effects are, and how common they are. . Information from the follow up of the control group allows the researchers to see whether the new treatment, or treatments, that they’re testing are any more or less effective than the existing treatment or placebo. To maximise the value of the clinical trial, the choice of controls is clearly critical. There’s no point in showing you a new drug or intervention is better than one that no one uses, or than the wrong dose of a drug that people do use. Randomised trials are characterised by the fact that the study subjects are allocated by the investigator to the different study groups through the use of randomisation, and the investigators then intervene differentially on participants. It’s an experiment. While randomised controlled trials are recognised as the gold standard study design for evaluating the impact of an intervention on an outcome, the process of randomisation alone does not wholly protect against bias. Incorrect analyses of the data can introduce bias, even where randomisation has been correctly implemented. It’s important to preserve the advantages of randomisation during the conduct of the study, and in analysis. If you don’t investigators may reach an incorrect and biased assessment of results. . For example, by not evaluating patients according to the group which they were originally assigned. This concept of analysing patients according to which group they were originally assigned is called ‘intention to treat’. Imagine you have 200 patients who had an acute myocardial infarction, a heart attack. You randomised them so that 100 go to the coronary care units, and 100 go mountain climbing. In the coronary care unit, 18 died and 82 went home, so the survival rate is 82%. On the other hand, with the mountain climbers, 1 died because he was daft enough to go up the mountain, but 9 others who went up the mountain lived. The other 90 were lost, or if they were wise, they went home - we don’t know whether they went home or died on the mountain. So indeed, they might have died at the mountain somewhere - you don’t know. But if you just analyse the data for the 10 participants that you do have outcome information on, mountain climbing gives you a survival rate of 90% - one died out of the 10 you found. So, mountain climbing appears to be better than the coronary care unit? . This story also emphasises that you have to try very hard not to lose patients. What happened to the 90 last mountaineers is critical to interpreting your trial, but equally importantly, you must include them in your analysis. If patients withdraw from the trial, you try to find out whether they are alive at the end, and what happened to them, and you include them in your original groups, because they were randomised to do that, even if they didn’t take the drugs or carry out the instructions they were supposed to. This is the basic idea of why a trial should be randomised and controlled, and of the importance of selecting control interventions. Remember, importantly, you must account for missing trial participants, and include all participants in your analysis and in their original groups, regardless of whether or not they followed their allocation intervention. . Strengths and Weaknesses of Randomised Controlled Trials . Randomised Controlled Trials (RCTs) is often considered the optimal study design for a number of reasons. . Randomisation substantially reduces the risk of bias in the study. | RCTs are also relevant to actual interventions in populations and settings of interest. | They can provide precise measures of efficacy which we can use to evaluate interventions. | . However, RCTs are also subject to certain limitations, including: . The results may not be generalisable to populations that are different than the sample used in the study. | They can be quite complex and costly to conduct. | Due to cost and practical considerations, they often rely on surrogate endpoints. For example, a biomarker is measured instead of a health outcome which might require a long time to develop. | They are experimental studies, which raises ethical issues. Some exposures (e.g. smoking or radiation) cannot be studied with RCTs because it is unethical to intentionally expose people to them. | . Conclusion . We have looked at the main types of Epidemiological study designs. There are many classifications of study designs which may slightly differ from each other, depending on the criteria they use to characterise studies. We looked at two main categories of studies; analytic vs. descriptive, but one could also start with the contrast between experimental and observational studies. . . Note that a cross-sectional study can also be considered descriptive when, for example, its main purpose is to describe the prevalence of a disease. Experimental studies are, by definition, analytic. Study designs such as nested case-control and case-cohort also belong to the analytic studies. .",
            "url": "https://www.livingdatalab.com/health/epidemiology/2022/03/04/study-designs-in-epidemiology.html",
            "relUrl": "/health/epidemiology/2022/03/04/study-designs-in-epidemiology.html",
            "date": " • Mar 4, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "Measuring Disease in Epidemiology",
            "content": "Introduction . Since the Covid pandemic which began in 2019, Epidemiology (the study of disease) has become far more mainstream in public discourse and the media. However, this growing interest also comes from the great advances that have been made in the treatment of disease more generally in recent years. While the consequences of Epidemilogical studies have become more and more apparent to the public at large, an understanding of the basic tools and methodology of this discipline are not well understood by the public. In this article we will look at the basic tools of Epidemiology with respect to measuring disease. . Measures of disease frequency . One of the main objectives of Epidemiology is to describe the frequency of disease in a population. If the variables of interest are continous such as height, then we can use descriptive statistics to describe these such as mean. median, the five number summary, etc. Often the variables of interest are discrete/categorical, e.g. does someone have a disease or not. In these cases, we need measures that can summarise these categoricals. In this section we look at ways to calculate different measures for these categorical type variables such as the prevalence, odds, cumulative incidence and incidence rate. . The appropriate measure can depend on many things such as the context, and what kind of question we are trying to answer. . Odds . Odds are the ratio of the probabilty of an event to its compliment. . . So for example if the adult population of a village in Tanzania was 6,000 in January 2013. On 1st January, all inhabitants were screened by an infectious disease research team. 800 of the inhabitants were found to be infected with HIV. On 1st April, an additional 300 people were diagnosed with HIV. What are the odds of being HIV-positive on April 1st? . To calculate the odds of being HIV positive, you need to divide the total number of HIV positive individuals by the number of undiagnosed non-HIV-positive individuals at the specified point of time. By April 1st, a total of 1,100 individuals have been found to be HIV-positive. The remaining 4,900 are not HIV-positive. Therefore, the odds of being HIV-positive on April 1st is 1,100/4,900=0.22. . However this is not the most widley used measure of disease frequency that is used in practice. . Prevalence . Prevalence is a proportion of individuals in a population who have the disease or attribute of interest at a specific time point. We can think of this measure as a snapshot of the current situation. To calculate prevalence, we divide the number of people with the disease by the total number of individuals in the population. . . Like Odds, the prevalence is a ratio so can be expressed as a proportion or percentage. However prevalence also requires a specific time point. Because of this, prevalence expresses both the occurence and duration of a disease. . So for example, say a study on diabetes begins with one thousand 40 to 45-year-old men of which 60 are already diabetic. The remaining 940 men are followed for 5 years during which time 75 men develop diabetes. What is the prevalence of diabetes at the start of the study? . To calculate the prevalence we note there were 60 cases of diabetes at the start of the study and the total population was 1,000. Therefore the prevalence is 60/1,000 which is 6%. . Prevalence and odds can be used to assess the health of a population to plan health services and allocate healthcare resources but also to monitor trends of diseases over time. So, especially prevalence is very useful in epidemiology. However this is not such a helpful to measure diseases of short duration and does’nt help us understand the potential causes of a disease. . Cumulative Incidence . Sometimes we are not so interested in how many people have a disease at a specific time, but rather how many new cases of a disease we have during a specific time period. Cumulative incidence is a good measure of this, which is the proportion of the population with a new event during a given time period. . . This measure refers to new cases, which means that individuals that already have the disease are not included in the numerator (number on top of the calculation). . Cumulative incidence, similar to prevalence, has no units and can take values from 0 to 1, or 0 to 100%, if expressed as a percentage. A cumulative incidence value of 0 means that there were no new cases of the disease during the study period. Whereas, a cumulative incidence value of 1 means that every single individual of the study population developed the disease during the time period of interest. . Cumulative incidence is widely used in epidemiology. It also comes with many names, such as incidence proportion and risk. . For example, say a study on diabetes begins with 1000, 40 to 45-year-old men of which 60 are already diabetic. The remaining 940 men are followed for 5 years during which time 75 men develop diabetes. What is the 5-year risk of having diabetes? . The 5-year risk (or cumulative incidence) of having diabetes we can calculate based on there were 75 new cases among 940 men who didn’t have the disease when the study started, therefore the risk is 75/940=7.98% over 5 years of follow-up. . We can only calculate cumulative incidence, if there is follow-up of the participants in our study. It is not possible to do so from a survey, which has no follow-up period. Importantly, this follow-up period must be the same for all participants, and no new participants can enter the group during the follow-up. . This is not always possible. There may be loss to follow-up for some subjects or new subjects entering or leaving the study population. There might also be competing risks. For example, in a study where the outcome is cancer diagnosis, someone could get killed in an accident before the end of the follow-up period. This individual would obviously no longer be at risk of cancer. But we don’t really know if they would have developed cancer had they not been killed in the accident. . In such cases, cumulative incidence is not well defined. These limitations are important and should be considered when trying to calculate cumulative incidence. . Incidence rate . Sometimes in real life studies, subjects are lost to follow up or new participants enter or leave the study population at any time, in which cases we cannot use cumulative incidence, however we can use a different measure in these cases called incidence rate. . Incidence rate uses a concept called person time which is a measure of the time spent in a study by participants. Each individual contributes person-time to the study during the time they could have developed an event that would have been counted as a case. This means that they contribute person-time from the moment they enter the study until they are diagnosed with the disease of interest, die or are lost to follow-up. Person-time can be expressed in different units. Person-years, person-days, person-hours, etc. . Incidence rate can take values from zero to infinity and it is always expressed per unit of person-time. . . For example, say a population of 100 healthy women was followed up for the development of breast cancer. 20 women were followed‐up for 1 year and were then lost to follow‐up. After being followed-up for 5 years, 10 women developed breast cancer. The remaining women who never developed breast cancer were followed for 10 years. What is the incidence rate of breast cancer in this population? . First, you need to calculate the total number of person-years, which equals to 20x1 for the women followed up for 1 year and then lost to follow-up, plus 10x5 for the 10 women who developed breast cancer after being followed for 5 years, and finally plus 70x10 for the remaining who were followed for 10 years. This results in 20 + 50 + 700= 770 person-years. The number of cases over the follow-up period was 10. Therefore, 10/770=0.013 = 13 cases of breast cancer per 1,000 person years during the follow-up period. . The word rate is surprisingly often used inappropriately to describe measures that are clearly risks i.e. ratios. So, be aware when you come across this term. Incidence rate is extremely useful. It accounts for the time of follow-up and for the time when the new event occurred. It is also suitable for studies where participants enter or leave the study at different times and it can deal with loss to follow-up and competing risks. Therefore, it can be used even when cumulative incidence is problematic or cannot be properly defined and is a powerful tool to describe the occurrence of a disease in the population. . Measures of association . While measuring the occurance of disease in a population is valuable, some of the greatest contributions Epidemiology has made is to understanding the causes of disease. In epidemiological research, we typically compare two populations with each other, with regard to exposure and outcome. We call exposure, any potential causal characteristic such as behaviors, environmental factors, treatments, occupation, genetic factors, and so on. The outcome is most often a disease. . The only way to be certain of a causal relationship is to use exactly the same population to both expose a patricular factor that might cause a disease and also not expose them to this factor, yet this is difficult to do in practice. So we have to compare different populations that are exposed and not exposed to a factor, but this means we need to be cautious about what conclusions we can draw. If the two populations are very similar, then we can have more confidence in attributing causality to a particular factor. . The statistical analysis of data generated by epidemiological studies can only provide you with evidence that, an association between the exposure and the outcome exists. It is up to you then, to decide whether it is reasonable to take the extra mental step and declare with little or much confidence that the exposure is what causes the outcome. To sum it up in a sentence, you should always keep in mind that association does not necessarily imply causation. Epidemiological knowledge is essential to decide when association implies causation. . We use measures of assocation for causal inferences and assocations between variables. They can be divided in two broad categories, relative and absolute measures. . The 2x2 Table . Epidemiological studies typically examine the association between an exposure and an outcome. There are, of course, many variations of this and, depending on the context and the study design, the research question might look completely different. . When you are faced with such a study, you can split the participants in the study into two groups, based on the exposure. Some of them are or have been exposed to the exposure of interest (‘exposed’ group), while the rest are not or have never been exposed to it (‘unexposed’ or ‘non-exposed’ group). . Similarly, the same participants can be split into two groups using the outcome, which is frequently a disease, as a criterion. Some of them have the disease, while the rest do not have the disease. . An easy way to represent these groups is using a 2x2 (two-by-two) table, which you will come across very often in Epidemiological studies. A 2x2 table provides a clear format to present the data and makes calculation of measures of frequency and association much simpler. In general, a 2x2 table would look like this: . . For example, consider a study where 500 people, 200 smokers and 300 non-smokers, were followed up for 10 years. The primary outcome of the study is chronic lung disease. Among smokers, 50 developed chronic lung disease. Among non-smokers, 60 developed chronic lung disease. How would you present your data in a 2x2 table? . Using the templates above, you can populate the cells like this: . . Relative measures of assocation . Relative measures of assocation are basically all ratios, Relative measures include the risk ratio, the incidence rate ratio, and the odds ratio. . Risk ratio . The risk ratio is a relative measure of assocation. For the risk ratio or cumulative incidence ratio, the numerator is the risk in the exposed group, and the denominator is the risk in the unexposed group, and you divide one by the other. You always need to mention the time period you are referring to when quoting a risk ratio. . . The question is, how do you interpret the risk ratio? The key value of a risk ratio, of any ratio really, is one. A risk ratio of one means that the risk of disease among the exposed is equal to the risk among the unexposed. Which makes perfect sense, we get a value of one when the numerator and the denominator are equal. If the risk ratio is higher than one, it means that the risk of disease among the exposed is greater than the risk among the unexposed. Finally, a risk ratio lower than one means that the risk of disease among the exposed is smaller than the risk among the unexposed. . For example. say of 600 people aged &gt;50 years who had high blood pressure, 35 experienced a stroke within 10 years of follow-up. Among 3,250 people who had low blood pressure, 40 experienced a stroke within the same follow-up period. What is the risk ratio of having a stroke among people with high blood pressure compared to those with low blood pressure? . Risk ratio is calculated by dividing the risk of an event in the exposed group by the risk of an event in the unexposed group. RR = (35/600) / (40/3250) = 4.74 . If we wish to express this in terms of association, a risk ratio of one means that the exposure is not associated with a disease. A risk ratio higher than one means that the exposure is associated with an increased risk of the disease. And a risk ratio lower than one means that the exposure is associated with a decreased risk of the disease. . Incidence rate ratio . The Incidence rate ratio is a relative measure of assocation. The incidence rate ratio is calculated by dividing the incidence rate among the exposed by the incidence rate among the unexposed. . . The interpretation is similar to the risk ratio e.g. an Incidence rate ratio of 1 indicates no assocation. . For example, A cohort study is conducted to determine whether hormone replacement therapy is associated with an increased risk of coronary artery disease in adults over the age of 40. The study found that the frequency of coronary artery disease amongst those using hormone replacement therapy was 27 per 1,000 person-years. The study also found that the frequency of coronary artery disease amongst those not using hormone replacement therapy was 3 per 1,000 person-years. What is the incidence rate ratio? . The incidence rate ratio is calculated by dividing the incidence rate among the exposed by the incidence rate among the unexposed so 27/1000 divided by 3/1000, so 0.027 / 0.003 = 9. . Odds ratio . The Odds ratio is a relative measure of assocation. To get the odds ratio you need to divide the odds of having the disease among the exposed by the odds of having the disease among the unexposed. . . The interpretation is similar to the risk ratio e.g. an Odds ratio of 1 indicates no assocation. . Absolute measures of assocation . Absolute measures of association quantify the actual absolute differences between the groups. This can be very informative when considering the impact of a factor at the population level. These measures include risk difference and the incidence rate difference. . Risk difference . The risk difference is simply the numerical difference of the risks in the two groups. In other words, the risk among the exposed minus the risk among the unexposed. . . The key value of the risk difference and of the incidence rate difference is zero. When the risk of the disease among the exposed is equal to the risk among the unexposed, the risk difference is zero. Compare these with the ratios where the value indicating no difference between the two groups is one. If the risk difference is higher than zero, it means that the risk of disease among the exposed is greater than the risk among the unexposed. In contrast, when the risk of disease among the exposed is smaller than the risk among the unexposed, the risk difference is a negative number. . Focusing on the concept of association, we would say that the risk difference of zero means that the exposure is not associated with disease. A positive risk difference means that the exposure is associated with an increased risk of the disease, and the negative risk difference that the exposure is associated with a decreased risk of the disease. . For example, In a cohort study, of 1,000 women who took oral contraceptives as a method of birth control, 50 developed ovarian cancer. A comparison group consisted of 1,700 women who did not take oral contraceptives. During the follow-up period, 25 women developed ovarian cancer in the comparison group. What is the Risk Difference for ovarian cancer among women who took oral contraceptives compared to women who did not? . Risk difference is the risk among the exposed minus the risk among the unexposed. RD = (50/1,000) - (25/1,700) = 0.035 over the study period. . Incidence rate difference . The Incidence rate difference is obtained by subtracting the incidence rate among the unexposed from the incidence rate among the exposed over a certain period. . . The interpretation is the same for the risk difference i.e. an Incidence rate difference of zero means the risk of the disease among the exposed is equal to the risk among the unexposed for the given period. . For example, In a study investigating obesity and myocardial infarction (MI), the following results were obtained. Amongst participants with obesity, a total of 80 MI occurred. Amongst normal weight participants, a total of 40 MI occurred. The group with obesity accumulated 90,000 person-years and the normal weight group 175,000 person-years during the study period. What is the incidence rate difference? (per 100,000 person-years) . To calculate the incidence rate difference, you need to subtract the incidence rate among the unexposed from the incidence rate among the exposed. (80/90,000) - (40/175,000) = 0.00066032 x 100,000 = 66.03 per 100,000 person-years. . When to use Relative vs Absolute measures of assocation . Risks and Rates . Let’s begin by considering the differences between risks and rates. Risk is based on a proportion of persons with disease or outcome of interest, as expressed as a percentage. It is also known as cumulative incidence because it refers to the occurs of disease in a group studied over time. Therefore, it is calculated by taking the total number of new cases and dividing it by the population at risk at the beginning of the observation period. . But there are difficulties to calculate this in practice, as highlighted earlier e.g. we would need to catch everyone in the follow up and often people are lost due to leaving the area, or dying. If someone dies for example, how can we know if they might have got the disease or not if they lived? . Also, many diseases can occur more than once and we have to decide how to handle reocurrences. If you include them, the incidence proportion could exceed one. While if you accept only first diagnosis, you may underestimate the true burden of disease. . An alternative is to express incidents as a rate, which is the number of new cases divided by the total person time at risk for the population. As we have seen, person time is calculated by the sum total of time all individuals remain in the study without developing the outcome of interest. . Like cumulative instance or risk, incidence rates also measure the frequency of new cases of disease in a population. But take into account the sum of the time that each participant remained under observation and at risk of developing the outcome under investigation. . You can only calculate an incidence rate if you have periodic follow-up information on each subject. Including not only if they develop the outcome, but also when they developed it. . Cumulative incidence and incidence rates also differ on the range of values they can take. Risk must be a proportion. Therefore, it must be between 0 and 1 and reported as a percentage. Rates, on the other hand, are not restricted between 0 and 1. To sum up, cumulative incidence is useful when you want to describe the incidence of disease in a country, but do not have detailed information on each and every member of the population. . For example, In a study investigating obesity and myocardial infarction (MI), the following results were obtained. Amongst participants with obesity, a total of 85 MIs occurred in 99,573 person-years. Amongst participants with normal weight, a total of 41 MIs occurred in 177,356 person-years. What is the incidence rate difference per 100,000 person-years? . You can calculate the incidence rate difference by subtracting the incidence rate among the unexposed from the incidence rate among the exposed. Incidence rate difference is: (85/99,573) - (41/177,356) = 0.000622 multiplied by 100,000 = 62.2 per 100,000 person-years. . Ratios and differences . Ratios are also known as relative risk comparisons. Relative risk comparisons and risk differences, essentially provide two different perspectives on the same information. Ratios provide a measure of the strength of the association between a factor, and a disease or outcome. They are calculated by dividing the cumulative incidence, or incidence rate in the exposed group, by the cumulative incidence or incidence rate, in the unexposed group. . On the other hand, risk or rate differences, provide a measure of the public health impact of the risk factor. and focus on the number of cases that could potentially be prevented, by eliminating the risk factor. . For example, If the risk difference in a cohort study on smoking and lung cancer was 70 per 1,000 individuals over a 10-year period, how would you interpret this result? . If smoking is a cause of lung cancer, then smoking caused 70 excess cases of lung cancer in a 10-year period in a population of 1,000 smokers. Risk difference calculates the excess risk of the outcome among the exposed compared to the unexposed. . Attributable risk . Published studies often report the magnitude of the association they investigate, which is clearly important when trying to identify causal links. Sometimes though, what we are interested in is the impact of a factor or of a disease on the population as a whole. This is when the concepts of attributable risk and of population attributable risk can be very useful. These measures quantify the population impact of a health-related factor and therefore are particularly useful for health policy. . Attributable risk is a measure of the public health impact of an exposure on the exposed group. In other words, it quantifies the answer to the question, “If we remove the exposure, how much would the disease burden be reduced?” This information will be critical in prioritizing public health interventions. . Attributable risk is essentially the risk or incidence rate difference. When we speak of attributable risk though, instead of risk difference or incidence rate difference, we imply that there is a causal relationship between the exposure and the outcome. We also assume that there are no other sources of bias and the distribution of all other known and unknown factors that influence risk is the same in the exposed and the unexposed. . . . Another concept linked to the attributable risk is the number needed to treat, which is the inverse of the attributable risk. The number needed to treat is very relevant when testing the effectiveness of health interventions and treatments. . Attributable risk and attributable risk percent are quite easy to calculate. They can be really helpful when you need to consider the effect of an exposure among the exposed group, which is something that happens all the time in public health. . For example, A prospective cohort study of smoking during pregnancy and low birth weight of new-born infants showed an attributable risk of 42%; mothers who did not smoke during pregnancy were used as the reference category. Assuming the relationship between smoking during pregnancy and low birth weight is causal this suggests that? . This suggests that 42% of low birth weight babies from mothers who smoked during pregnancy could have been avoided if they had not smoked during pregnancy. Attributable risk assesses how much the burden of disease can be avoided if the exposure was removed. . Population attributable risk . Attributable risk is a great tool for public health. But as we know, it exclusively refers to the exposed group of people. Sometimes we’re interested in quantifying the effect of an exposure on the entire population, and not only on those exposed to it. This is when population attributable risk can be useful. . Population attributable risk is the excess risk of disease in the total study population that is attributable to the exposure. The total study population includes both exposed and unexposed individuals. Once you know the numerical value of the attributable risk, you only need to multiply it by the prevalence of the exposure in the population, and you can easily calculate the population attributable risk. . . . . In contrast to the attributable risk which focuses on the exposed group, this provides an insight into the entire population which is frequently what we’re interested in. The population attributable risk percent depends on both the prevalence of the exposure and the strength of the association. . Attributable risk and population attributable risk provide valuable information about the magnitude of the impact of an exposure which cannot be captured by relative measures of association. This is the kind of information you would need if you wanted to prioritize public health interventions and maximize the benefit for the population. . For example, Which measure would be the most appropriate to provide insight on the impact of an exposure on a population? . Population Attributable Risk is an expression of the impact of the exposure in the entire population. . Summary of Measures in Epidemiology . We can divide measures in Epidemiology in three broad categories: measures of frequency; measures of association; and measures of impact. There is some overlap between measures of association and measures of impact, as attributable risk is essentially the risk (or incidence rate) difference. . . Strategies for prevention . The classic article in epidemiology ‘Sick individuals and sick populations’ by Geoffrey Rose highlights important considerations between different approaches for prevention. . High-risk strategies target individuals or groups that have been identified as having the highest risk of disease and would benefit the most from prevention. On the other hand, population strategies target the entire population, regardless of whether individuals are exposed to risk factors or are “at-risk”. . In Rose’s paper he highlights the prevention paradox which is, a large number of people exposed to low risk generate more cases than a small number of people exposed to high risk, which is counter-intuitive. This observation has substantial implications for policy, because a measure that brings large benefits to the community offers little to each participating individual, which is one of the main reasons why public health is such a complex endeavor. . Disease detection . The measures of frequency and association that we have seen so far are typically calculated under the assumption that we accurately measure the exposure and the outcome. This is not always true. . The measurement tools and diagnostic methods we have at our disposal may lead to erroneous judgments regarding the status of individuals; for example, whether they are sick or healthy. Here we will consider methods available to quantify the inaccuracy of the diagnostic tests and how these can inform clinical and policy decisions, including considerations about screening programs. . Sensitivity and Specificity . Given tests are imperfect, we can quantify the degree of error of a test using these 2 measures. . Consider the confusion matrix for a test, which describes all the possible outcomes of a test. The test results in either a positive or negative indication for a disease, and that test result is either accurate or true or not accurate and false. This leads to 4 possible outcomes of a test which is our confusion matrix. . True Positives (TP) Correctly diagnosed by the test as having the disease | True Negatives (TN) Correctly diagnosed by the test as not having the disease | False Positives (FP) Incorrectly diagnosed by the test as having the disease | False Negatives (FN) Incorrectly diagnosed by the test as not having the disease | . . The sensitivity or true positive rate is calculated by TP/(TP+FN) This could be seen as a measure of ‘How much confidence can we have when your test says you have a disease that you really have it’. . The specificity or true negative rate is calculated by TN/(TN+FP) This could be seen as a measure of ‘How much confidence can we have when your test says you don’t have a disease that you really don’t have it’. . For example, A biomarker is used to detect subclinical cases of prostate cancer. Which metrics are NOT influenced by the prevalence of the disease in the population that is being tested? . This would include Sensitivity and Specificity because there are about the accuaracy of the test itself. . These measures can be very useful for evaluating a diagnostic test. These two measures describe characteristics of the test itself. . These are the same concepts we would use in Data Science and machine learning, with a classification model for example. . Predictive Values . Another pair of metrics valuable in evaluating the performance of diagnostic tests, consists of the positive predictive value and the negative predictive value. . When comparing the true disease status with the result of the diagnostic test, we can have four possible combinations. True Positive, False Positive, False Negative and True Negative. We have already seen how to calculate sensitivity and specificity using the true disease status as the denominator. What if we use the test results as a denominator? The proportion of positive tests that correctly identified diseased individuals, is called Positive Predictive Value. In other words, positive predictive value is the proportion of True Positives among all positive results. The proportion of negative results that correctly identify non-diseased individuals is called Negative Predictive Value. . . Positive Predictive Value (PPV) is calculated by TP/TP+FP . For example, A new blood test is proposed for early diagnosis of prostate cancer. Results of the new test are compared with the method that is considered the gold standard to diagnose this type of cancer. 1,000 men were tested with both methods. Among those who had prostate cancer (according to the gold standard method), 200 tested positive in the new test and 180 tested negative. Among men who did not have prostate cancer (according to the gold standard method), 570 tested negative and 50 tested positive. What is the positive predictive value of the new test? . Positive predictive value is the proportion of true positives among all positive results. PPV=TP/(TP+FP) where TP is true positives and FP is false positives. Therefore, PPV= 200/(200+50)=0.80=80%. . Negative Predictive Value (NPV) is calculated by TN/TN+FN . These definitions reflect a population-based view of the diagnostic test results. . We can also consider it from the perspective of someone who takes the test: if the test result is positive then the PPV is the probability that they has the disease if the test result is positive. If the test result is negative, then the NPV is the probability that they do not have the disease. . For example, Your aunt is 50 years old and, following her physician’s advice, she had a mammography screening done, which was negative. The physician told your aunt that this test is used for early detection of breast cancer and since the test was negative, she shouldn’t worry about breast cancer at the moment. Your aunt knows you have studied epidemiology and she asks you how likely it is that she does not have breast cancer. Searching the literature, you find that mammographies have a sensitivity of 70% and specificity 80%, and 3 in 10 women aged 50 years old have breast cancer. What’s your answer to your aunt? . To give an individual answer for your aunt about her negative result we can use the Negative Predictive Value (NPV) of the test in this population, as well as a confusion matrix to establish the figures we need. If we say 1,000 50-year old women had mammography screening, we can then calculate figures for each of the 4 possibilities of the test (TP, FP, TN, FN) using the fact we know 3/10 women have cancer, and 7/10 don’t have cancer, and complete the confusion matrix that would lead to the values of sensitivity and specificity we have been given. We can then use these derived confusion matrix values to calculate the NPV. . Our assumed population is 1,000, and 3/10 women have cancer so we have 300 with cancer in total and 700 without cancer in total. . We know our test sensitivity (our true positive rate of our test) is 70% so when our test is applied to those 300 with cancer we will get the following: . TP = 300 x 0.7 = 210 FN = 300 - TP = 90 . We know our test specificity (our true negative rate of our test) is 80% so when our test is applied to those 700 without cancer we will get the following: . TN = 700 x 0.8 = 560 FP = 700 - TN = 140 . We now have all 4 values of the confusion matrix. . Thus, NPV = TN/(TN+FN) = 560/(560+90) = 0.86 i.e. the probability that your aunt does not have cancer is 86%. . Sensitivity and Specificity describe characteristics of the test itself and do not vary with the prevelence of the disease in the population, wheras PPV and NPV heavily depend on the prevelance of the disease in the population. . This means there can be big differences between the accuracy of the test itself, and the outcome of the test for an individual taking the test. Consider the following: . . So even an excellent diagnostic test, with 100 percent sensitivity, and 99.9 percent specificity, can yield a positive predictive value as low as 50 percent, when the disease is very rare with a prevalence of 0.1 percent. A positive predictive value of 50 percent means that half of the positive tests are wrong, which is a pretty terrible outcome. The same test when applied to a population where the disease prevalence is 10 percent, yields a positive predictive value of 99 percent. . PPV and NPV give information about the effectiveness of a test within a specific context. They also help you interpret the result from the perspective of the individual who took the test, which is critical for clinical work as well as for policy decisions. . Receiver Operative Characteristic (ROC) Curve . Many biological variables, such as cholesterol or blood pressure are measured in a continuum, and there is no clear threshold below or above which someone should be definitely considered healthy or sick. However, we tend to set such thresholds for practical reasons, especially in clinical practice. Similarly, many diagnostic tests provide measurements the numerical values of which cannot clearly distinguish between healthy and sick individuals. . The Receiver Operative Characteristic (ROC) curve is a tool which helps determine how well a diagnostic test is able to discriminate sick from healthy individuals. ROC curves are also used to decide on the optimal threshold for diagnosis. . To do this, we must plot the sensitivity against the false positive rate (i.e. 1 minus the specificity) for every possible threshold for a test or a combination of tests. This curve allows us to understand the trade-off between sensitivity and specificity depending on the threshold for diagnosis. Ideally, you want to pick a threshold which has the optimal combination of high sensitivity and low false positive rate. . In most circumstances, the closer the ROC to the top-left hand corner of your graph, the more accurate the test is overall. The area under the curve can also be used to calculate the accuracy and usefulness of a test. In other words, the larger the area under the curve, the better the test. The ROC curve is a helpful tool used to evaluate diagnostic tests, although, as you already know non-statistical considerations should also be taken into account. . . Screening . According to the World Health Organization: . Screening is the presumptive identification of unrecognized disease in an apparently healthy, asymptomatic population by means of tests, examinations, or other procedures that can be applied rapidly and easily to the target population. . Firstly, people who participate in screening are classified as either unlikely, or possibly having the disease of interest. This is something that most participants in screening programs do not realize. The definition also refers to unrecognized disease and apparently healthy asymptomatic people. This is very important to understand. We speak of screening only when people without symptoms are targeted. This is a very reasonable conduct screening to detect disease that has not yet shown any symptoms. . Screening is applied to populations in the sense that it targets entire subgroups of the population, and not a small number of individuals that may visit the doctor for some reason. The final part of the World Health Organization’s definition refers to tests or examinations that can be applied rapidly and easily. This is a key element of any screening program. Exactly because it targets many and mostly healthy individuals, the diagnostic test must be easy, quick, and not really costly. Screening has a huge potential to save lives and technological advances have made it a feasible option for an increasing number of diseases. . Screening seems to be a great idea. The objective is to reduce mortality and morbidity by early detection and treatment of a disease. However, even when the outcome of interest is as straightforward as mortality, evaluating a screening program can prove really difficult. . These challenges, methodological, financial, practical, and ethical, is why there is so much debate even around screening for breast and prostate cancer, which have been running for decades in many countries. . Conclusion . In this article we have looked at measuring disease frequency and association and on using these measures to inform decisions about screening and prevention. We have learned that there are different measures of disease frequency and association. . A rate provides more information than a risk, but requires more detailed follow-up. . A relative measure of association is great when exploring causality. But an absolute measure can better describe the impact on the population. . The correct interpretation of such measures is key to understanding research and potential implications for public health, for example when we looked at the high risk and population approaches to prevention. We have also learned about inaccuracies in disease detection, and how to quantify misclassification and how it can affect individual diagnosis and screening programs. . We also made the distinction between association and causation, and this helps us engage critically with the literature and consider the strengths and limitations of research studies. . For further reading on Epidemiology you can refer to the free online book ‘Basic Epidemiology 2nd edition’ by the World Health Organisation. .",
            "url": "https://www.livingdatalab.com/health/epidemiology/2022/02/22/measuring-disease-in-epidemiology.html",
            "relUrl": "/health/epidemiology/2022/02/22/measuring-disease-in-epidemiology.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "Predicting Alzheimers disease using 3D MRI medical images",
            "content": "Introduction . In this project I develop a deep learning model to predict Alzheimer’s disease using 3D MRI medical images. Alzheimer’s disease (AD) is a progressive neurodegenerative disorder that results in impaired neuronal (brain cell) function and eventually, cell death. For patients exhibiting early symptoms, quantifying disease progression over time can help direct therapy and disease management. . The code for this project is available at this github repository. . . A radiological study via MRI exam is currently one of the most advanced methods to quantify the disease. In particular, the measurement of hippocampal volume has proven useful to diagnose and track progression in several brain disorders, most notably in AD. Studies have shown a reduced volume of the hippocampus in patients with AD. . But with fewer and fewer trained Radiologists available, and increasing demands for medical imaging services - this presents a huge challenge for medical services. . . In this project I build an end-to-end deep learning/AI system which features a machine learning algorithm that integrates into a clinical-grade viewer and automatically measures hippocampal volumes of new patients from their MRI images, as their studies are committed to the clinical imaging archive. . . I used a dataset that contains the segmentations of the right hippocampus and will use the U-Net deep learning architecture to build a segmentation model. . . Left: Cropped Hippocampus area from MRI image | Right: Predicted by model Hippocampus anterior (front) volume | . After the model was built, I proceeded to integrate the model into a working clinical PACS such that it runs on every incoming study and produces a report with volume measurements. . The Dataset . I used the “Hippocampus” dataset from the Medical Decathlon competition. This dataset is stored as a collection of NIFTI files, with one file per volume, and one file per corresponding segmentation mask. The original images here are T2 MRI scans of the full brain. . Key files . Key files from the project located in the github repo are the following: . Exploratory Data Analysis of Hippocampus 3D brain MRI images | Building &amp; Training Model for Hippocampus volume prediction | Using model for inference | . Results . The final model achieved a mean dice score of 1.47 and a mean jaccard score of 0.81 in terms of accuracy for correctly classifying the anterior and posterior volumes of the Hippocampus. The model was then integrated into a clinical viewer to generate automated reports and predictions for Hippocampus volumes submitted via brain MRI scans in a PACS environment. This model can then be used by a clinician to assist with diagnosis &amp; prognosis. . Model deployment in automated report with predictions integrated into medical viewer. . .",
            "url": "https://www.livingdatalab.com/health/deep-learning/2022/02/06/predict-alzheimers-3d-medical-imaging.html",
            "relUrl": "/health/deep-learning/2022/02/06/predict-alzheimers-3d-medical-imaging.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post23": {
            "title": "Patient Selection for Diabetes Drug Testing",
            "content": "Introduction . EHR data is becoming a key source of real-world evidence (RWE) for the pharmaceutical industry and regulators to make decisions on clinical trials. . For this project, we have a groundbreaking diabetes drug that is ready for clinical trial testing. It is a very unique and sensitive drug that requires administering the drug over at least 5-7 days of time in the hospital with frequent monitoring/testing and patient medication adherence training with a mobile application. We have been provided a patient dataset from a client partner and are tasked with building a predictive model that can identify which type of patients the company should focus their efforts testing this drug on. Target patients are people that are likely to be in the hospital for this duration of time and will not incur significant additional costs for administering this drug to the patient and monitoring. . In order to achieve our goal we must build a regression model that can predict the estimated hospitalization time for a patient and use this to select/filter patients for this study. . Approach . Utilizing a synthetic dataset (denormalized at the line level augmentation) built off of the UCI Diabetes readmission dataset, we will build a regression model that predicts the expected days of hospitalization time and then convert this to a binary prediction of whether to include or exclude that patient from the clinical trial. . This project will demonstrate the importance of building the right data representation at the encounter level, with appropriate filtering and preprocessing/feature engineering of key medical code sets. We will also analyze and interpret the model for biases across key demographic groups. . Dataset . Due to healthcare PHI regulations (HIPAA, HITECH), there are limited number of publicly available datasets and some datasets require training and approval. So, for the purpose of this study, we are using a dataset from UC Irvine that has been modified. . Dataset Loading and Schema Review . dataset_path = &quot;./data/final_project_dataset.csv&quot; df = pd.read_csv(dataset_path) . # Show first few rows df.head() . encounter_id patient_nbr race gender age weight admission_type_id discharge_disposition_id admission_source_id time_in_hospital payer_code medical_specialty primary_diagnosis_code other_diagnosis_codes number_outpatient number_inpatient number_emergency num_lab_procedures number_diagnoses num_medications num_procedures ndc_code max_glu_serum A1Cresult change readmitted . 0 2278392 | 8222157 | Caucasian | Female | [0-10) | ? | 6 | 25 | 1 | 1 | ? | Pediatrics-Endocrinology | 250.83 | ?|? | 0 | 0 | 0 | 41 | 1 | 1 | 0 | NaN | None | None | No | NO | . 1 149190 | 55629189 | Caucasian | Female | [10-20) | ? | 1 | 1 | 7 | 3 | ? | ? | 276 | 250.01|255 | 0 | 0 | 0 | 59 | 9 | 18 | 0 | 68071-1701 | None | None | Ch | &gt;30 | . 2 64410 | 86047875 | AfricanAmerican | Female | [20-30) | ? | 1 | 1 | 7 | 2 | ? | ? | 648 | 250|V27 | 2 | 1 | 0 | 11 | 6 | 13 | 5 | 0378-1110 | None | None | No | NO | . 3 500364 | 82442376 | Caucasian | Male | [30-40) | ? | 1 | 1 | 7 | 2 | ? | ? | 8 | 250.43|403 | 0 | 0 | 0 | 44 | 7 | 16 | 1 | 68071-1701 | None | None | Ch | NO | . 4 16680 | 42519267 | Caucasian | Male | [40-50) | ? | 1 | 1 | 7 | 1 | ? | ? | 197 | 157|250 | 0 | 0 | 0 | 51 | 5 | 8 | 0 | 0049-4110 | None | None | Ch | NO | . Determine Level of Dataset (Line or Encounter) . Given there are only 101766 unique encounter_id’s yet there are 143424 rows that are not nulls, this looks like the dataset is at the line level. . We would also want to aggregate on the primary_diagnosis_code as there is also only one of these per encounter. By aggregating on these 3 columns, we can create a encounter level dataset. . Analyze Dataset . # Look at range of values &amp; key stats for numerical columns numerical_feature_list = [&#39;time_in_hospital&#39;, &#39;number_outpatient&#39;, &#39;number_inpatient&#39;, &#39;number_emergency&#39;, &#39;num_lab_procedures&#39;, &#39;number_diagnoses&#39;, &#39;num_medications&#39;, &#39;num_procedures&#39; ] df[numerical_feature_list].describe() . time_in_hospital number_outpatient number_inpatient number_emergency num_lab_procedures number_diagnoses num_medications num_procedures . count 143424.000000 | 143424.000000 | 143424.000000 | 143424.000000 | 143424.000000 | 143424.000000 | 143424.000000 | 143424.000000 | . mean 4.490190 | 0.362429 | 0.600855 | 0.195086 | 43.255745 | 7.424434 | 16.776035 | 1.349021 | . std 2.999667 | 1.249295 | 1.207934 | 0.920410 | 19.657319 | 1.924872 | 8.397130 | 1.719104 | . min 1.000000 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | 1.000000 | 1.000000 | 0.000000 | . 25% 2.000000 | 0.000000 | 0.000000 | 0.000000 | 32.000000 | 6.000000 | 11.000000 | 0.000000 | . 50% 4.000000 | 0.000000 | 0.000000 | 0.000000 | 44.000000 | 8.000000 | 15.000000 | 1.000000 | . 75% 6.000000 | 0.000000 | 1.000000 | 0.000000 | 57.000000 | 9.000000 | 21.000000 | 2.000000 | . max 14.000000 | 42.000000 | 21.000000 | 76.000000 | 132.000000 | 16.000000 | 81.000000 | 6.000000 | . # Define utility functions def create_cardinality_feature(df): num_rows = len(df) random_code_list = np.arange(100, 1000, 1) return np.random.choice(random_code_list, num_rows) def count_unique_values(df, cat_col_list): cat_df = df[cat_col_list] cat_df[&#39;principal_diagnosis_code&#39;] = create_cardinality_feature(cat_df) #add feature with high cardinality val_df = pd.DataFrame({&#39;columns&#39;: cat_df.columns, &#39;cardinality&#39;: cat_df.nunique() } ) return val_df categorical_feature_list = [ &#39;race&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;weight&#39;, &#39;payer_code&#39;, &#39;medical_specialty&#39;, &#39;primary_diagnosis_code&#39;, &#39;other_diagnosis_codes&#39;,&#39;ndc_code&#39;, &#39;max_glu_serum&#39;, &#39;A1Cresult&#39;, &#39;change&#39;, &#39;readmitted&#39;] categorical_df = count_unique_values(df, categorical_feature_list) categorical_df . columns cardinality . race race | 6 | . gender gender | 3 | . age age | 10 | . weight weight | 10 | . payer_code payer_code | 18 | . medical_specialty medical_specialty | 73 | . primary_diagnosis_code primary_diagnosis_code | 717 | . other_diagnosis_codes other_diagnosis_codes | 19374 | . ndc_code ndc_code | 251 | . max_glu_serum max_glu_serum | 4 | . A1Cresult A1Cresult | 4 | . change change | 2 | . readmitted readmitted | 3 | . principal_diagnosis_code principal_diagnosis_code | 900 | . Analysis key findings . The ndc_code field has a high amount of missing values (23460) | num_lab_procedures and num_medications seem to have a roughly normal distribution | Fields that have a high cardinality are - medical_specialty, primary_diagnosis_code, other_diagnosis_codes, ndc_code, and principal_diagnosis_code. This is because there are many thousands of these codes that correspond to the many disease and diagnosis sub-classes that exist in the medical field. | The distribution for the age field is approximately normal, which we would expect. The distribution for the gender field is roughly uniform &amp; equal. In this case we discount the very small number of Unknown/valid cases. Again this is not surprising, as the distribution of genders in the general population is also roughly equal so this seems to be a representitive sample from the general population. | . Reduce Dimensionality of the NDC Code Feature . NDC codes are a common format to represent the wide variety of drugs that are prescribed for patient care in the United States. The challenge is that there are many codes that map to the same or similar drug. We are provided with the ndc drug lookup file https://github.com/udacity/nd320-c1-emr-data-starter/blob/master/project/data_schema_references/ndc_lookup_table.csv derived from the National Drug Codes List site(https://ndclist.com/). . We can use this file to come up with a way to reduce the dimensionality of this field and create a new field in the dataset called “generic_drug_name” in the output dataframe. . #NDC code lookup file ndc_code_path = &quot;./medication_lookup_tables/final_ndc_lookup_table&quot; ndc_code_df = pd.read_csv(ndc_code_path) . # Check first new rows ndc_code_df.head() . NDC_Code Proprietary Name Non-proprietary Name Dosage Form Route Name Company Name Product Type . 0 0087-6060 | Glucophage | Metformin Hydrochloride | Tablet, Film Coated | Oral | Bristol-myers Squibb Company | Human Prescription Drug | . 1 0087-6063 | Glucophage XR | Metformin Hydrochloride | Tablet, Extended Release | Oral | Bristol-myers Squibb Company | Human Prescription Drug | . 2 0087-6064 | Glucophage XR | Metformin Hydrochloride | Tablet, Extended Release | Oral | Bristol-myers Squibb Company | Human Prescription Drug | . 3 0087-6070 | Glucophage | Metformin Hydrochloride | Tablet, Film Coated | Oral | Bristol-myers Squibb Company | Human Prescription Drug | . 4 0087-6071 | Glucophage | Metformin Hydrochloride | Tablet, Film Coated | Oral | Bristol-myers Squibb Company | Human Prescription Drug | . # Check for duplicate NDC_Code&#39;s ndc_code_df[ndc_code_df.duplicated(subset=[&#39;NDC_Code&#39;])] . NDC_Code Proprietary Name Non-proprietary Name Dosage Form Route Name Company Name Product Type . 263 0781-5634 | Pioglitazone Hydrochloride And Glimepiride | Pioglitazone Hydrochloride And Glimepiride | Tablet | Oral | Sandoz Inc | Human Prescription Drug | . 264 0781-5635 | Pioglitazone Hydrochloride And Glimepiride | Pioglitazone Hydrochloride And Glimepiride | Tablet | Oral | Sandoz Inc | Human Prescription Drug | . # Remove duplicates ndc_code_df = ndc_code_df.drop(ndc_code_df.index[[263,264]]) ndc_code_df[ndc_code_df.duplicated(subset=[&#39;NDC_Code&#39;])] . NDC_Code Proprietary Name Non-proprietary Name Dosage Form Route Name Company Name Product Type . Select First Encounter for each Patient . In order to simplify the aggregation of data for the model, we will only select the first encounter for each patient in the dataset. This is to reduce the risk of data leakage of future patient encounters and to reduce complexity of the data transformation and modeling steps. We will assume that sorting in numerical order on the encounter_id provides the time horizon for determining which encounters come before and after another. . from student_utils import select_first_encounter first_encounter_df = select_first_encounter(reduce_dim_df) . # unique patients in transformed dataset unique_patients = first_encounter_df[&#39;patient_nbr&#39;].nunique() print(&quot;Number of unique patients:{}&quot;.format(unique_patients)) # unique encounters in transformed dataset unique_encounters = first_encounter_df[&#39;encounter_id&#39;].nunique() print(&quot;Number of unique encounters:{}&quot;.format(unique_encounters)) original_unique_patient_number = reduce_dim_df[&#39;patient_nbr&#39;].nunique() # number of unique patients should be equal to the number of unique encounters and patients in the final dataset assert original_unique_patient_number == unique_patients assert original_unique_patient_number == unique_encounters . Number of unique patients:71518 | Number of unique encounters:71518 | . Aggregate Dataset to Right Level for Modelling . To make it simpler, we are creating dummy columns for each unique generic drug name and adding those are input features to the model. . exclusion_list = [&#39;generic_drug_name&#39;] grouping_field_list = [c for c in first_encounter_df.columns if c not in exclusion_list] agg_drug_df, ndc_col_list = aggregate_dataset(first_encounter_df, grouping_field_list, &#39;generic_drug_name&#39;) . assert len(agg_drug_df) == agg_drug_df[&#39;patient_nbr&#39;].nunique() == agg_drug_df[&#39;encounter_id&#39;].nunique() . Prepare Fields and Cast Dataset . Feature Selection . # Look at counts for payer_code categories ax = sns.countplot(x=&quot;payer_code&quot;, data=agg_drug_df) . . # Look at counts for weight categories ax = sns.countplot(x=&quot;weight&quot;, data=agg_drug_df) . . From the category counts above, we can see that for payer_code while there are many unknown values i.e. ‘?’, there are still many values for other payer codes, these may prove useful predictors for our target variable. For weight, there are so few unknown ‘?’ codes, that this feature is likely to be not very helpful for predicting our target variable. . # Selected features required_demo_col_list = [&#39;race&#39;, &#39;gender&#39;, &#39;age&#39;] student_categorical_col_list = [ &quot;change&quot;, &quot;readmitted&quot;, &quot;payer_code&quot;, &quot;medical_specialty&quot;, &quot;primary_diagnosis_code&quot;, &quot;other_diagnosis_codes&quot;, &quot;max_glu_serum&quot;, &quot;A1Cresult&quot;, &quot;admission_type_id&quot;, &quot;discharge_disposition_id&quot;, &quot;admission_source_id&quot;] + required_demo_col_list + ndc_col_list student_numerical_col_list = [&quot;number_outpatient&quot;, &quot;number_inpatient&quot;, &quot;number_emergency&quot;, &quot;num_lab_procedures&quot;, &quot;number_diagnoses&quot;, &quot;num_medications&quot;, &quot;num_procedures&quot;] PREDICTOR_FIELD = &#39;time_in_hospital&#39; . def select_model_features(df, categorical_col_list, numerical_col_list, PREDICTOR_FIELD, grouping_key=&#39;patient_nbr&#39;): selected_col_list = [grouping_key] + [PREDICTOR_FIELD] + categorical_col_list + numerical_col_list return agg_drug_df[selected_col_list] . selected_features_df = select_model_features(agg_drug_df, student_categorical_col_list, student_numerical_col_list, PREDICTOR_FIELD) . Preprocess Dataset - Casting and Imputing . We will cast and impute the dataset before splitting so that we do not have to repeat these steps across the splits in the next step. For imputing, there can be deeper analysis into which features to impute and how to impute but for the sake of time, we are taking a general strategy of imputing zero for only numerical features. . processed_df = preprocess_df(selected_features_df, student_categorical_col_list, student_numerical_col_list, PREDICTOR_FIELD, categorical_impute_value=&#39;nan&#39;, numerical_impute_value=0) . Split Dataset into Train, Validation, and Test Partitions . In order to prepare the data for being trained and evaluated by a deep learning model, we will split the dataset into three partitions, with the validation partition used for optimizing the model hyperparameters during training. One of the key parts is that we need to be sure that the data does not accidently leak across partitions. . We will split the input dataset into three partitions(train, validation, test) with the following requirements: . Approximately 60%/20%/20% train/validation/test split | Randomly sample different patients into each data partition | We need to take care that a patient’s data is not in more than one partition, so that we can avoid possible data leakage. | We need to take care the total number of unique patients across the splits is equal to the total number of unique patients in the original dataset | Total number of rows in original dataset = sum of rows across all three dataset partitions | . from student_utils import patient_dataset_splitter d_train, d_val, d_test = patient_dataset_splitter(processed_df, &#39;patient_nbr&#39;) . Total number of unique patients in train = 32563 | Total number of unique patients in validation = 10854 | Total number of unique patients in test = 10854 | Training partition has a shape = (32563, 43) | Validation partition has a shape = (10854, 43) | Test partition has a shape = (10854, 43) | . Demographic Representation Analysis of Split . After the split, we should check to see the distribution of key features/groups and make sure that there is representative samples across the partitions. . Label Distribution Across Partitions . Are the histogram distribution shapes similar across partitions? . show_group_stats_viz(processed_df, PREDICTOR_FIELD) . . show_group_stats_viz(d_train, PREDICTOR_FIELD) . . show_group_stats_viz(d_test, PREDICTOR_FIELD) . . Demographic Group Analysis . We should check that our partitions/splits of the dataset are similar in terms of their demographic profiles. . # Full dataset before splitting patient_demo_features = [&#39;race&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;patient_nbr&#39;] patient_group_analysis_df = processed_df[patient_demo_features].groupby(&#39;patient_nbr&#39;).head(1).reset_index(drop=True) show_group_stats_viz(patient_group_analysis_df, &#39;gender&#39;) . . # Training partition show_group_stats_viz(d_train, &#39;gender&#39;) . . # Test partition show_group_stats_viz(d_test, &#39;gender&#39;) . . Convert Dataset Splits to TF Dataset . # Convert dataset from Pandas dataframes to TF dataset batch_size = 128 diabetes_train_ds = df_to_dataset(d_train, PREDICTOR_FIELD, batch_size=batch_size) diabetes_val_ds = df_to_dataset(d_val, PREDICTOR_FIELD, batch_size=batch_size) diabetes_test_ds = df_to_dataset(d_test, PREDICTOR_FIELD, batch_size=batch_size) . # We use this sample of the dataset to show transformations later diabetes_batch = next(iter(diabetes_train_ds))[0] def demo(feature_column, example_batch): feature_layer = layers.DenseFeatures(feature_column) print(feature_layer(example_batch)) . Create Features . Create Categorical Features with TF Feature Columns . Before we can create the TF categorical features, we must first create the vocab files with the unique values for a given field that are from the training dataset. . # Build Vocabulary for Categorical Features vocab_file_list = build_vocab_files(d_train, student_categorical_col_list) . Create Categorical Features with Tensorflow Feature Column API . from student_utils import create_tf_categorical_feature_cols tf_cat_col_list = create_tf_categorical_feature_cols(student_categorical_col_list) . test_cat_var1 = tf_cat_col_list[0] print(&quot;Example categorical field: n{}&quot;.format(test_cat_var1)) demo(test_cat_var1, diabetes_batch) . Create Numerical Features with TF Feature Columns . from student_utils import create_tf_numeric_feature . def calculate_stats_from_train_data(df, col): mean = df[col].describe()[&#39;mean&#39;] std = df[col].describe()[&#39;std&#39;] return mean, std def create_tf_numerical_feature_cols(numerical_col_list, train_df): tf_numeric_col_list = [] for c in numerical_col_list: mean, std = calculate_stats_from_train_data(train_df, c) tf_numeric_feature = create_tf_numeric_feature(c, mean, std) tf_numeric_col_list.append(tf_numeric_feature) return tf_numeric_col_list . tf_cont_col_list = create_tf_numerical_feature_cols(student_numerical_col_list, d_train) . test_cont_var1 = tf_cont_col_list[0] print(&quot;Example continuous field: n{} n&quot;.format(test_cont_var1)) demo(test_cont_var1, diabetes_batch) . Build Deep Learning Regression Model with Sequential API and TF Probability Layers . Use DenseFeatures to combine features for model . Now that we have prepared categorical and numerical features using Tensorflow’s Feature Column API, we can combine them into a dense vector representation for the model. Below we will create this new input layer, which we will call ‘claim_feature_layer’. . claim_feature_columns = tf_cat_col_list + tf_cont_col_list claim_feature_layer = tf.keras.layers.DenseFeatures(claim_feature_columns) . Build Sequential API Model from DenseFeatures and TF Probability Layers . def build_sequential_model(feature_layer): model = tf.keras.Sequential([ feature_layer, tf.keras.layers.Dense(150, activation=&#39;relu&#39;), tf.keras.layers.Dense(200, activation=&#39;relu&#39;),# New tf.keras.layers.Dense(75, activation=&#39;relu&#39;), tfp.layers.DenseVariational(1+1, posterior_mean_field, prior_trainable), tfp.layers.DistributionLambda( lambda t:tfp.distributions.Normal(loc=t[..., :1], scale=1e-3 + tf.math.softplus(0.01 * t[...,1:]) ) ), ]) return model def build_diabetes_model(train_ds, val_ds, feature_layer, epochs=5, loss_metric=&#39;mse&#39;): model = build_sequential_model(feature_layer) opt = tf.keras.optimizers.Adam(learning_rate=0.01) model.compile(optimizer=opt, loss=loss_metric, metrics=[loss_metric]) #model.compile(optimizer=&#39;rmsprop&#39;, loss=loss_metric, metrics=[loss_metric]) #early_stop = tf.keras.callbacks.EarlyStopping(monitor=loss_metric, patience=3) history = model.fit(train_ds, validation_data=val_ds, #callbacks=[early_stop], epochs=epochs) return model, history . diabetes_model, history = build_diabetes_model(diabetes_train_ds, diabetes_val_ds, claim_feature_layer, epochs=10) . Show Model Uncertainty Range with TF Probability . Now that we have trained a model with TF Probability layers, we can extract the mean and standard deviation for each prediction. . feature_list = student_categorical_col_list + student_numerical_col_list diabetes_x_tst = dict(d_test[feature_list]) diabetes_yhat = diabetes_model(diabetes_x_tst) preds = diabetes_model.predict(diabetes_test_ds) . from student_utils import get_mean_std_from_preds m, s = get_mean_std_from_preds(diabetes_yhat) . Show Prediction Output . prob_outputs = { &quot;pred&quot;: preds.flatten(), &quot;actual_value&quot;: d_test[&#39;time_in_hospital&#39;].values, &quot;pred_mean&quot;: m.numpy().flatten(), &quot;pred_std&quot;: s.numpy().flatten() } prob_output_df = pd.DataFrame(prob_outputs) . prob_output_df.head() . pred actual_value pred_mean pred_std . 0 3.587955 | 3.0 | 4.673843 | 0.693749 | . 1 5.007016 | 2.0 | 4.673843 | 0.693749 | . 2 4.809363 | 9.0 | 4.673843 | 0.693749 | . 3 5.003417 | 2.0 | 4.673843 | 0.693749 | . 4 5.346958 | 8.0 | 4.673843 | 0.693749 | . prob_output_df.describe() . pred actual_value pred_mean pred_std . count 10854.000000 | 10854.000000 | 10854.000000 | 10854.000000 | . mean 4.376980 | 4.429888 | 4.673843 | 0.693749 | . std 0.908507 | 3.002044 | 0.000000 | 0.000000 | . min 0.976290 | 1.000000 | 4.673843 | 0.693749 | . 25% 3.755292 | 2.000000 | 4.673843 | 0.693749 | . 50% 4.382993 | 4.000000 | 4.673843 | 0.693749 | . 75% 5.002859 | 6.000000 | 4.673843 | 0.693749 | . max 7.529900 | 14.000000 | 4.673843 | 0.693749 | . Convert Regression Output to Classification Output for Patient Selection . from student_utils import get_student_binary_prediction student_binary_prediction = get_student_binary_prediction(prob_output_df, &#39;pred&#39;) . student_binary_prediction.value_counts() . 0:8137 | 1:2717 | . Add Binary Prediction to Test Dataframe . Using the student_binary_prediction output that is a numpy array with binary labels, we can use this to add to a dataframe to better visualize and also to prepare the data for the Aequitas toolkit. The Aequitas toolkit requires that the predictions be mapped to a binary label for the predictions (called ‘score’ field) and the actual value (called ‘label_value’). . def add_pred_to_test(test_df, pred_np, demo_col_list): for c in demo_col_list: test_df[c] = test_df[c].astype(str) test_df[&#39;score&#39;] = pred_np test_df[&#39;label_value&#39;] = test_df[&#39;time_in_hospital&#39;].apply(lambda x: 1 if x &gt;=5 else 0) return test_df pred_test_df = add_pred_to_test(d_test, student_binary_prediction, [&#39;race&#39;, &#39;gender&#39;]) . pred_test_df[[&#39;patient_nbr&#39;, &#39;gender&#39;, &#39;race&#39;, &#39;time_in_hospital&#39;, &#39;score&#39;, &#39;label_value&#39;]].head() . patient_nbr gender race time_in_hospital score label_value . 0 122896787 | Male | Caucasian | 3.0 | 0 | 0 | . 1 102598929 | Male | Caucasian | 2.0 | 1 | 0 | . 2 80367957 | Male | Caucasian | 9.0 | 0 | 1 | . 3 6721533 | Male | Caucasian | 2.0 | 1 | 0 | . 4 104346288 | Female | Caucasian | 8.0 | 1 | 1 | . Model Evaluation Metrics . Now it is time to use the newly created binary labels in the ‘pred_test_df’ dataframe to evaluate the model with some common classification metrics. We will create a report summary of the performance of the model and give the ROC AUC, F1 score(weighted), class precision and recall scores. . # AUC, F1, precision and recall # Summary y_true = pred_test_df[&#39;label_value&#39;].values y_pred = pred_test_df[&#39;score&#39;].values . accuracy_score(y_true, y_pred) . 0.5627418463239359 | . roc_auc_score(y_true, y_pred) . 0.5032089104088319 | . Precision-recall tradeoff - The model has been optimised to identify those patients correct for the trial with the fewest mistakes, while also trying to ensure we identify as many of them as possible. . Areas of imporovement - we could look to engineer new features that might help us better predict our target patients. . Evaluating Potential Model Biases with Aequitas Toolkit . Prepare Data For Aequitas Bias Toolkit . Using the gender and race fields, we will prepare the data for the Aequitas Toolkit. . # Aequitas from aequitas.preprocessing import preprocess_input_df from aequitas.group import Group from aequitas.plotting import Plot from aequitas.bias import Bias from aequitas.fairness import Fairness ae_subset_df = pred_test_df[[&#39;race&#39;, &#39;gender&#39;, &#39;score&#39;, &#39;label_value&#39;]] ae_df, _ = preprocess_input_df(ae_subset_df) g = Group() xtab, _ = g.get_crosstabs(ae_df) absolute_metrics = g.list_absolute_metrics(xtab) clean_xtab = xtab.fillna(-1) aqp = Plot() b = Bias() . model_id, score_thresholds 1 {‘rank_abs’: [2717]} | . absolute_metrics = g.list_absolute_metrics(xtab) xtab[[col for col in xtab.columns if col not in absolute_metrics]] . model_id score_threshold k attribute_name attribute_value pp pn fp fn tn tp group_label_pos group_label_neg group_size total_entities . 0 1 | binary 0/1 | 2717 | race | ? | 86 | 240 | 56 | 85 | 155 | 30 | 115 | 211 | 326 | 10854 | . 1 1 | binary 0/1 | 2717 | race | AfricanAmerican | 491 | 1530 | 291 | 592 | 938 | 200 | 792 | 1229 | 2021 | 10854 | . 2 1 | binary 0/1 | 2717 | race | Asian | 15 | 60 | 10 | 16 | 44 | 5 | 21 | 54 | 75 | 10854 | . 3 1 | binary 0/1 | 2717 | race | Caucasian | 2030 | 6038 | 1249 | 2298 | 3740 | 781 | 3079 | 4989 | 8068 | 10854 | . 4 1 | binary 0/1 | 2717 | race | Hispanic | 52 | 141 | 35 | 48 | 93 | 17 | 65 | 128 | 193 | 10854 | . 5 1 | binary 0/1 | 2717 | race | Other | 43 | 128 | 26 | 40 | 88 | 17 | 57 | 114 | 171 | 10854 | . 6 1 | binary 0/1 | 2717 | gender | Female | 1413 | 4306 | 820 | 1675 | 2631 | 593 | 2268 | 3451 | 5719 | 10854 | . 7 1 | binary 0/1 | 2717 | gender | Male | 1304 | 3831 | 847 | 1404 | 2427 | 457 | 1861 | 3274 | 5135 | 10854 | . Reference Group Selection . # Test reference group with Caucasian Male bdf = b.get_disparity_predefined_groups(clean_xtab, original_df=ae_df, ref_groups_dict={&#39;race&#39;:&#39;Caucasian&#39;, &#39;gender&#39;:&#39;Male&#39; }, alpha=0.05, check_significance=False) f = Fairness() fdf = f.get_group_value_fairness(bdf) . Race and Gender Bias Analysis for Patient Selection . # Plot two metrics # Is there significant bias in your model for either race or gender? fpr_disparity1 = aqp.plot_disparity(bdf, group_metric=&#39;fpr_disparity&#39;, attribute_name=&#39;race&#39;) . . We notice that while with most races, there is no significant indication of bias, there is an indication that Asians are less likely to be itentified by the model, based on the 0.4 disparity in relation to the Caucasian reference group. . fpr_disparity2 = aqp.plot_disparity(bdf, group_metric=&#39;fpr_disparity&#39;, attribute_name=&#39;gender&#39;) . . With gender, there does not seem to be any significant indication of bias. . Fairness Analysis Example - Relative to a Reference Group . # Reference group fairness plot fpr_fairness = aqp.plot_fairness_group(fdf, group_metric=&#39;fpr&#39;, title=True) . . Here again we can see that there appears to be signficant disparity with the Asian race being under-represented with a magnitude of 0.19. .",
            "url": "https://www.livingdatalab.com/health/deep-learning/electronic-health-records/2022/02/06/patient-select-diabetes.html",
            "relUrl": "/health/deep-learning/electronic-health-records/2022/02/06/patient-select-diabetes.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post24": {
            "title": "Pneumonia Detection From Chest X-Rays",
            "content": "Introduction . In this project, I will analyze data from the NIH Chest X-ray Dataset and train a CNN to classify a given chest x-ray for the presence or absence of pneumonia. This project will culminate in a model that aims to predict the presence of pneumonia with human radiologist-level accuracy that can be prepared for submission to the United States FDA (Food and Drug Administration) for 510(k) clearance as software as a medical device. As part of the submission preparation, I will formally describe my model, the data that it was trained on, and a validation plan that meets FDA criteria. . The project will use a dataset of 112,000 chest x-rays with disease labels acquired from 30,000 patients. . The full project code and details are available at this github repo. . Project Highlights: . Used imaging modalities for common clinical applications of 2D medical imaging | Performed exploratory data analysis (EDA) on medical imaging data to inform model training and explain model performance | Established the appropriate ‘ground truth’ methodologies for training algorithms to label medical images | Extracted images from a DICOM medical format dataset | Trained common CNN deep learning architectures to classify 2D medical images | Translated outputs of medical imaging models for use by a clinician | Planned necessary validations to prepare a medical imaging model for regulatory approval | . Load, view and clean dataset . ## Read full image filepaths into a dataframe for easier manipulation ## Load the NIH data to all_xray_df all_xray_df = pd.read_csv(&#39;/data/Data_Entry_2017.csv&#39;) all_image_paths = {os.path.basename(x): x for x in glob(os.path.join(&#39;/data&#39;,&#39;images*&#39;, &#39;*&#39;, &#39;*.png&#39;))} print(&#39;Scans found:&#39;, len(all_image_paths), &#39;, Total Headers&#39;, all_xray_df.shape[0]) all_xray_df[&#39;path&#39;] = all_xray_df[&#39;Image Index&#39;].map(all_image_paths.get) all_xray_df.sample(3) . Scans found: 112120 , Total Headers 112120 . Image Index Finding Labels Follow-up # Patient ID Patient Age Patient Gender View Position OriginalImage[Width Height] OriginalImagePixelSpacing[x y] Unnamed: 11 path . 54694 00013684_000.png | No Finding | 0 | 13684 | 64 | M | PA | 2992 | 2991 | 0.143 | 0.143 | NaN | /data/images_006/images/00013684_000.png | . 61239 00015102_000.png | No Finding | 0 | 15102 | 48 | F | PA | 2446 | 2991 | 0.143 | 0.143 | NaN | /data/images_007/images/00015102_000.png | . 102440 00027295_002.png | No Finding | 2 | 27295 | 47 | M | PA | 2992 | 2991 | 0.143 | 0.143 | NaN | /data/images_011/images/00027295_002.png | . # Drop any unreasonable ages! all_xray_df = all_xray_df[all_xray_df[&#39;Patient Age&#39;] &lt; 120] all_xray_df.describe() . Follow-up # Patient ID Patient Age OriginalImage[Width Height] OriginalImagePixelSpacing[x y] Unnamed: 11 . count 112104.000000 | 112104.000000 | 112104.000000 | 112104.000000 | 112104.000000 | 112104.000000 | 112104.000000 | 0.0 | . mean 8.574172 | 14345.720724 | 46.872574 | 2646.035253 | 2486.393153 | 0.155651 | 0.155651 | NaN | . std 15.406734 | 8403.980520 | 16.598152 | 341.243771 | 401.270806 | 0.016174 | 0.016174 | NaN | . min 0.000000 | 1.000000 | 1.000000 | 1143.000000 | 966.000000 | 0.115000 | 0.115000 | NaN | . 25% 0.000000 | 7308.000000 | 35.000000 | 2500.000000 | 2048.000000 | 0.143000 | 0.143000 | NaN | . 50% 3.000000 | 13993.000000 | 49.000000 | 2518.000000 | 2544.000000 | 0.143000 | 0.143000 | NaN | . 75% 10.000000 | 20673.000000 | 59.000000 | 2992.000000 | 2991.000000 | 0.168000 | 0.168000 | NaN | . max 183.000000 | 30805.000000 | 95.000000 | 3827.000000 | 4715.000000 | 0.198800 | 0.198800 | NaN | . ## Create some extra columns in the table with binary indicators of certain diseases ## rather than working directly with the &#39;Finding Labels&#39; column # Re-format multi-label column into separate columns for each label binary encoded all_labels = np.unique(list(chain(*all_xray_df[&#39;Finding Labels&#39;].map(lambda x: x.split(&#39;|&#39;)).tolist()))) all_labels = [x for x in all_labels if len(x)&gt;0] print(&#39;All Labels ({}): {}&#39;.format(len(all_labels), all_labels)) for c_label in all_labels: if len(c_label)&gt;1: # ignore empty labels all_xray_df[c_label] = all_xray_df[&#39;Finding Labels&#39;].map(lambda finding: 1 if c_label in finding else 0) all_xray_df.sample(3) . All Labels (15): [&#39;Atelectasis&#39;, &#39;Cardiomegaly&#39;, &#39;Consolidation&#39;, &#39;Edema&#39;, &#39;Effusion&#39;, &#39;Emphysema&#39;, &#39;Fibrosis&#39;, &#39;Hernia&#39;, &#39;Infiltration&#39;, &#39;Mass&#39;, &#39;No Finding&#39;, &#39;Nodule&#39;, &#39;Pleural_Thickening&#39;, &#39;Pneumonia&#39;, &#39;Pneumothorax&#39;] . Image Index Finding Labels Follow-up # Patient ID Patient Age Patient Gender View Position OriginalImage[Width Height] OriginalImagePixelSpacing[x ... Emphysema Fibrosis Hernia Infiltration Mass No Finding Nodule Pleural_Thickening Pneumonia Pneumothorax . 43672 00011246_000.png | No Finding | 0 | 11246 | 40 | M | PA | 2992 | 2991 | 0.143 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 60931 00015040_001.png | Atelectasis|Effusion | 1 | 15040 | 48 | F | AP | 2500 | 2048 | 0.168 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 25642 00006741_001.png | No Finding | 1 | 6741 | 54 | F | PA | 2992 | 2991 | 0.143 | ... | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 3 rows × 28 columns . ## Here we can create a new column called &#39;pneumonia_class&#39; that will allow us to look at ## images with or without pneumonia for binary classification all_xray_df[&#39;pneumonia_class&#39;] = np.where(all_xray_df[&#39;Pneumonia&#39;]==1, &#39;Pneumonia&#39;, &#39;No Pneumonia&#39;) all_xray_df.head() . Image Index Finding Labels Follow-up # Patient ID Patient Age Patient Gender View Position OriginalImage[Width Height] OriginalImagePixelSpacing[x ... Fibrosis Hernia Infiltration Mass No Finding Nodule Pleural_Thickening Pneumonia Pneumothorax pneumonia_class . 0 00000001_000.png | Cardiomegaly | 0 | 1 | 58 | M | PA | 2682 | 2749 | 0.143 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | No Pneumonia | . 1 00000001_001.png | Cardiomegaly|Emphysema | 1 | 1 | 58 | M | PA | 2894 | 2729 | 0.143 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | No Pneumonia | . 2 00000001_002.png | Cardiomegaly|Effusion | 2 | 1 | 58 | M | PA | 2500 | 2048 | 0.168 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | No Pneumonia | . 3 00000002_000.png | No Finding | 0 | 2 | 81 | M | PA | 2500 | 2048 | 0.171 | ... | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | No Pneumonia | . 4 00000003_000.png | Hernia | 0 | 3 | 81 | F | PA | 2582 | 2991 | 0.143 | ... | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | No Pneumonia | . 5 rows × 29 columns . Split data into training and testing sets . # Total Pneumonia cases all_xray_df[&#39;Pneumonia&#39;].sum() . 1430 . So in our dataset we have: . Pneumonia cases: 1,430 or 1.2% | Non-Pneumonia cases: 110,674 or 98.8% | . Given that we want: . Our training set to be balanced between Pneumonia and Non-Pneumonia cases i.e. equal | Our test set to reflect the real world proportions i.e. Pneumonia 1.2% and Non-Pneumonia 98.8% | To split our data between training and test sets in a 80% to 20% proportion | . This leads to the following training &amp; test sets: . Training set: 1,144 (50%) Pneumonia cases, 1,144 (50%) Non-Pneumonia cases - Total 2,288 | Test set: 286 (1.2%) Pneumonia cases, 23,547 (98.8%) Non-Pneumonia cases - Total 23,833 | . def create_splits(vargs): ## It&#39;s important to consider here how balanced or imbalanced we want each of those sets to be ## for the presence of pneumonia # Select rows with Pneumonia cases pneumonia_df = all_xray_df[all_xray_df[&#39;Pneumonia&#39;] == 1] # Select rows with No-Pneumonia cases no_pneumonia_df = all_xray_df[all_xray_df[&#39;Pneumonia&#39;] == 0] # Split Pneumonia cases 80% - 20% between train and validation train_data, val_data = skl.train_test_split(pneumonia_df, test_size = 0.2) # Split No-Pneumonia cases into two separate groups equal size train_no_pneumonia_data, val_no_pneumonia_data = skl.train_test_split(no_pneumonia_df, test_size = 0.5) # Sample from No-Pneumonia train set to be same size as Pneumonia train set train_no_pneumonia_data = train_no_pneumonia_data.sample(train_data.shape[0]) # Merge No-Pneumonia train set into train set train_data = pd.concat([train_data, train_no_pneumonia_data]) # Calculate proportion required of No-Pneumonia cases for test set at 98.8% no_pneumonia_test_count = int((val_data.shape[0] / 1.2) * 98.8) # Sample from No-Pneumonia test set to be 98.8% of test set val_no_pneumonia_data = val_no_pneumonia_data.sample(no_pneumonia_test_count) # Merge No-Pneumonia test set into test set val_data = pd.concat([val_data, val_no_pneumonia_data]) return train_data, val_data # Create train and validation splits train_df, valid_df = create_splits(all_xray_df) . # View Pneumonia vs No-Pneumonia counts for training train_df[&#39;Pneumonia&#39;].value_counts() . 1 1144 0 1144 Name: Pneumonia, dtype: int64 . # View Pneumonia vs No-Pneumonia counts for validation valid_df[&#39;Pneumonia&#39;].value_counts() . 0 23547 1 286 Name: Pneumonia, dtype: int64 . Model building &amp; training . Image Augmentation . Deep learning modls need large amount of training data to achieve good performance. To build a powerful image classifier using limited raining data, image augmentation is usually required to boost the performance of deep networks. Image augmentation artificially creates training images through different ways of processing or combination of multiple processing, such as random rotation, shifts, shear and flips, etc. . Lets now define some Image Augmentation to create more data. . # Define image size IMG_SIZE = (224, 224) . def my_image_augmentation(train=True): # Create image generator if train: # Training augmentations + normalisation idg = ImageDataGenerator(rescale=1. / 255.0, horizontal_flip = True, vertical_flip = False, height_shift_range= 0.1, width_shift_range=0.1, rotation_range=10, shear_range = 0.1, zoom_range=0.1) else: # Otherwise test set - no augmentation! just normalisation idg = ImageDataGenerator(rescale=1. / 255.0) return idg def make_train_gen(df): # Create image generator idg = my_image_augmentation() # Apply image generator to generate more images train_gen = idg.flow_from_dataframe(dataframe=df, directory=None, x_col = &#39;path&#39;, y_col = &#39;pneumonia_class&#39;, class_mode = &#39;binary&#39;, target_size = IMG_SIZE, batch_size = 16) return train_gen def make_val_gen(df): # Create image generator idg = my_image_augmentation(train=False) # Apply image generator to generate more images - large batch 10% of total validation to get enough Pneumonia val_gen = idg.flow_from_dataframe(dataframe=df, directory=None, x_col = &#39;path&#39;, y_col = &#39;pneumonia_class&#39;, class_mode = &#39;binary&#39;, target_size = IMG_SIZE, batch_size = 2000) return val_gen . # Create training image generator train_gen = make_train_gen(train_df) # Create validation image generator val_gen = make_val_gen(valid_df) . Found 2288 validated image filenames belonging to 2 classes. Found 23833 validated image filenames belonging to 2 classes. . Let us check the distribution of key demographic values within the training &amp; validation sets. . # Compare age distributions of training vs validation data fig, axes = plt.subplots(1, 2) train_df[&#39;Patient Age&#39;].hist(ax=axes[0],figsize=(20,5)) valid_df[&#39;Patient Age&#39;].hist(ax=axes[1],figsize=(20,5)) axes[0].set_title(&#39;Distribution of ages for training data&#39;) axes[0].set_xlabel(&quot;Age&quot;) axes[0].set_ylabel(&quot;Number of x-ray observations&quot;) axes[1].set_title(&#39;Distribution of ages for validation data&#39;) axes[1].set_xlabel(&quot;Age&quot;) axes[1].set_ylabel(&quot;Number of x-ray observations&quot;) . Text(0, 0.5, &#39;Number of x-ray observations&#39;) . # Compare gender between training vs validation data fig, axes = plt.subplots(1, 2) train_df[&#39;Patient Gender&#39;].value_counts().plot(ax=axes[0],kind=&#39;bar&#39;,figsize=(20,5)) valid_df[&#39;Patient Gender&#39;].value_counts().plot(ax=axes[1],kind=&#39;bar&#39;,figsize=(20,5)) axes[0].set_title(&#39;Gender count for training data&#39;) axes[0].set_xlabel(&quot;Gender&quot;) axes[0].set_ylabel(&quot;Number of x-ray observations&quot;) axes[1].set_title(&#39;Gender count for validation data&#39;) axes[1].set_xlabel(&quot;Gender&quot;) axes[1].set_ylabel(&quot;Number of x-ray observations&quot;) . Text(0, 0.5, &#39;Number of x-ray observations&#39;) . # Compare view position training vs validation data fig, axes = plt.subplots(1, 2) train_df[&#39;View Position&#39;].value_counts().plot(ax=axes[0],kind=&#39;bar&#39;,figsize=(20,5)) valid_df[&#39;View Position&#39;].value_counts().plot(ax=axes[1],kind=&#39;bar&#39;,figsize=(20,5)) axes[0].set_title(&#39;View position count training data&#39;) axes[0].set_xlabel(&quot;View position&quot;) axes[0].set_ylabel(&quot;Number of x-ray observations&quot;) axes[1].set_title(&#39;View position count for validation data&#39;) axes[1].set_xlabel(&quot;View position&quot;) axes[1].set_ylabel(&quot;Number of x-ray observations&quot;) . Text(0, 0.5, &#39;Number of x-ray observations&#39;) . # Compare Pneumonia vs No Pneumonia cases between training vs validation data fig, axes = plt.subplots(1, 2) train_df[&#39;Pneumonia&#39;].value_counts().plot(ax=axes[0],kind=&#39;bar&#39;,figsize=(20,5)) valid_df[&#39;Pneumonia&#39;].value_counts().plot(ax=axes[1],kind=&#39;bar&#39;,figsize=(20,5)) axes[0].set_title(&#39;Pneumonia vs No Pneumonia for training data&#39;) axes[0].set_xlabel(&quot;Gender&quot;) axes[0].set_ylabel(&quot;Number of x-ray observations&quot;) axes[1].set_title(&#39;Pneumonia vs No Pneumonia for validation data&#39;) axes[1].set_xlabel(&quot;Gender&quot;) axes[1].set_ylabel(&quot;Number of x-ray observations&quot;) . Text(0, 0.5, &#39;Number of x-ray observations&#39;) . So these proportions of key features are as we wished and expected. The distributions of ages and proportions of gender in the training and validation are roughly the same. For the Pneumonia vs No Pneumonia cases, in our training set we have equal amounts of each case to give the model the best chance for training, while in the validation data we have a much smaller proportion of Pneumonia cases that matches the real world disease prevelance that we observed earlier here and in the EDA study. . Lets now look over more the training and validation data. . ## May want to pull a single large batch of random validation data for testing after each epoch: valX, valY = val_gen.next() . # Get a batch of training data t_x, t_y = next(train_gen) # Print mean and std dev of training batch print(&#39;Train mean &amp; std dev&#39;, t_x.mean(), t_x.std()) . Train mean &amp; std dev 0.54569376 0.23733293 . ## May want to look at some examples of our augmented training data. ## This is helpful for understanding the extent to which data is being manipulated prior to training, ## and can be compared with how the raw data look prior to augmentation fig, m_axs = plt.subplots(4, 4, figsize = (16, 16)) for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()): c_ax.imshow(c_x[:,:,0], cmap = &#39;bone&#39;) if c_y == 1: c_ax.set_title(&#39;Pneumonia&#39;) else: c_ax.set_title(&#39;No Pneumonia&#39;) c_ax.axis(&#39;off&#39;) . So these image augmentations seem reasonable. . Build model . Using a pre-trained network downloaded from Keras for fine-tuning . def load_pretrained_model(): # Load pre-trained resnet50 model with imagenet trained weights model = ResNet50(include_top=True, weights=&#39;imagenet&#39;) return model . def build_my_model(): # Load the pre-trained model model = load_pretrained_model() model.layers.pop() predictions = Dense(1, activation=&#39;sigmoid&#39;)(model.layers[-1].output) my_model = Model(inputs=model.input, outputs=predictions) my_model.compile(optimizer = Adam(lr=0.0001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;binary_accuracy&#39;]) # Print model structure my_model.summary() return my_model # Build model my_model = build_my_model() . Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5 102973440/102967424 [==============================] - 1s 0us/step Model: &#34;model_1&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) (None, 224, 224, 3) 0 __________________________________________________________________________________________________ conv1_pad (ZeroPadding2D) (None, 230, 230, 3) 0 input_1[0][0] __________________________________________________________________________________________________ conv1_conv (Conv2D) (None, 112, 112, 64) 9472 conv1_pad[0][0] __________________________________________________________________________________________________ conv1_bn (BatchNormalization) (None, 112, 112, 64) 256 conv1_conv[0][0] __________________________________________________________________________________________________ conv1_relu (Activation) (None, 112, 112, 64) 0 conv1_bn[0][0] __________________________________________________________________________________________________ pool1_pad (ZeroPadding2D) (None, 114, 114, 64) 0 conv1_relu[0][0] __________________________________________________________________________________________________ pool1_pool (MaxPooling2D) (None, 56, 56, 64) 0 pool1_pad[0][0] __________________________________________________________________________________________________ conv2_block1_1_conv (Conv2D) (None, 56, 56, 64) 4160 pool1_pool[0][0] __________________________________________________________________________________________________ conv2_block1_1_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block1_1_conv[0][0] __________________________________________________________________________________________________ conv2_block1_1_relu (Activation (None, 56, 56, 64) 0 conv2_block1_1_bn[0][0] __________________________________________________________________________________________________ conv2_block1_2_conv (Conv2D) (None, 56, 56, 64) 36928 conv2_block1_1_relu[0][0] __________________________________________________________________________________________________ conv2_block1_2_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block1_2_conv[0][0] __________________________________________________________________________________________________ conv2_block1_2_relu (Activation (None, 56, 56, 64) 0 conv2_block1_2_bn[0][0] __________________________________________________________________________________________________ conv2_block1_0_conv (Conv2D) (None, 56, 56, 256) 16640 pool1_pool[0][0] __________________________________________________________________________________________________ conv2_block1_3_conv (Conv2D) (None, 56, 56, 256) 16640 conv2_block1_2_relu[0][0] __________________________________________________________________________________________________ conv2_block1_0_bn (BatchNormali (None, 56, 56, 256) 1024 conv2_block1_0_conv[0][0] __________________________________________________________________________________________________ conv2_block1_3_bn (BatchNormali (None, 56, 56, 256) 1024 conv2_block1_3_conv[0][0] __________________________________________________________________________________________________ conv2_block1_add (Add) (None, 56, 56, 256) 0 conv2_block1_0_bn[0][0] conv2_block1_3_bn[0][0] __________________________________________________________________________________________________ conv2_block1_out (Activation) (None, 56, 56, 256) 0 conv2_block1_add[0][0] __________________________________________________________________________________________________ conv2_block2_1_conv (Conv2D) (None, 56, 56, 64) 16448 conv2_block1_out[0][0] __________________________________________________________________________________________________ conv2_block2_1_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block2_1_conv[0][0] __________________________________________________________________________________________________ conv2_block2_1_relu (Activation (None, 56, 56, 64) 0 conv2_block2_1_bn[0][0] __________________________________________________________________________________________________ conv2_block2_2_conv (Conv2D) (None, 56, 56, 64) 36928 conv2_block2_1_relu[0][0] __________________________________________________________________________________________________ conv2_block2_2_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block2_2_conv[0][0] __________________________________________________________________________________________________ conv2_block2_2_relu (Activation (None, 56, 56, 64) 0 conv2_block2_2_bn[0][0] __________________________________________________________________________________________________ conv2_block2_3_conv (Conv2D) (None, 56, 56, 256) 16640 conv2_block2_2_relu[0][0] __________________________________________________________________________________________________ conv2_block2_3_bn (BatchNormali (None, 56, 56, 256) 1024 conv2_block2_3_conv[0][0] __________________________________________________________________________________________________ conv2_block2_add (Add) (None, 56, 56, 256) 0 conv2_block1_out[0][0] conv2_block2_3_bn[0][0] __________________________________________________________________________________________________ conv2_block2_out (Activation) (None, 56, 56, 256) 0 conv2_block2_add[0][0] __________________________________________________________________________________________________ conv2_block3_1_conv (Conv2D) (None, 56, 56, 64) 16448 conv2_block2_out[0][0] __________________________________________________________________________________________________ conv2_block3_1_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block3_1_conv[0][0] __________________________________________________________________________________________________ conv2_block3_1_relu (Activation (None, 56, 56, 64) 0 conv2_block3_1_bn[0][0] __________________________________________________________________________________________________ conv2_block3_2_conv (Conv2D) (None, 56, 56, 64) 36928 conv2_block3_1_relu[0][0] __________________________________________________________________________________________________ conv2_block3_2_bn (BatchNormali (None, 56, 56, 64) 256 conv2_block3_2_conv[0][0] __________________________________________________________________________________________________ conv2_block3_2_relu (Activation (None, 56, 56, 64) 0 conv2_block3_2_bn[0][0] __________________________________________________________________________________________________ conv2_block3_3_conv (Conv2D) (None, 56, 56, 256) 16640 conv2_block3_2_relu[0][0] __________________________________________________________________________________________________ conv2_block3_3_bn (BatchNormali (None, 56, 56, 256) 1024 conv2_block3_3_conv[0][0] __________________________________________________________________________________________________ conv2_block3_add (Add) (None, 56, 56, 256) 0 conv2_block2_out[0][0] conv2_block3_3_bn[0][0] __________________________________________________________________________________________________ conv2_block3_out (Activation) (None, 56, 56, 256) 0 conv2_block3_add[0][0] __________________________________________________________________________________________________ conv3_block1_1_conv (Conv2D) (None, 28, 28, 128) 32896 conv2_block3_out[0][0] __________________________________________________________________________________________________ conv3_block1_1_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block1_1_conv[0][0] __________________________________________________________________________________________________ conv3_block1_1_relu (Activation (None, 28, 28, 128) 0 conv3_block1_1_bn[0][0] __________________________________________________________________________________________________ conv3_block1_2_conv (Conv2D) (None, 28, 28, 128) 147584 conv3_block1_1_relu[0][0] __________________________________________________________________________________________________ conv3_block1_2_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block1_2_conv[0][0] __________________________________________________________________________________________________ conv3_block1_2_relu (Activation (None, 28, 28, 128) 0 conv3_block1_2_bn[0][0] __________________________________________________________________________________________________ conv3_block1_0_conv (Conv2D) (None, 28, 28, 512) 131584 conv2_block3_out[0][0] __________________________________________________________________________________________________ conv3_block1_3_conv (Conv2D) (None, 28, 28, 512) 66048 conv3_block1_2_relu[0][0] __________________________________________________________________________________________________ conv3_block1_0_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block1_0_conv[0][0] __________________________________________________________________________________________________ conv3_block1_3_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block1_3_conv[0][0] __________________________________________________________________________________________________ conv3_block1_add (Add) (None, 28, 28, 512) 0 conv3_block1_0_bn[0][0] conv3_block1_3_bn[0][0] __________________________________________________________________________________________________ conv3_block1_out (Activation) (None, 28, 28, 512) 0 conv3_block1_add[0][0] __________________________________________________________________________________________________ conv3_block2_1_conv (Conv2D) (None, 28, 28, 128) 65664 conv3_block1_out[0][0] __________________________________________________________________________________________________ conv3_block2_1_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block2_1_conv[0][0] __________________________________________________________________________________________________ conv3_block2_1_relu (Activation (None, 28, 28, 128) 0 conv3_block2_1_bn[0][0] __________________________________________________________________________________________________ conv3_block2_2_conv (Conv2D) (None, 28, 28, 128) 147584 conv3_block2_1_relu[0][0] __________________________________________________________________________________________________ conv3_block2_2_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block2_2_conv[0][0] __________________________________________________________________________________________________ conv3_block2_2_relu (Activation (None, 28, 28, 128) 0 conv3_block2_2_bn[0][0] __________________________________________________________________________________________________ conv3_block2_3_conv (Conv2D) (None, 28, 28, 512) 66048 conv3_block2_2_relu[0][0] __________________________________________________________________________________________________ conv3_block2_3_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block2_3_conv[0][0] __________________________________________________________________________________________________ conv3_block2_add (Add) (None, 28, 28, 512) 0 conv3_block1_out[0][0] conv3_block2_3_bn[0][0] __________________________________________________________________________________________________ conv3_block2_out (Activation) (None, 28, 28, 512) 0 conv3_block2_add[0][0] __________________________________________________________________________________________________ conv3_block3_1_conv (Conv2D) (None, 28, 28, 128) 65664 conv3_block2_out[0][0] __________________________________________________________________________________________________ conv3_block3_1_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block3_1_conv[0][0] __________________________________________________________________________________________________ conv3_block3_1_relu (Activation (None, 28, 28, 128) 0 conv3_block3_1_bn[0][0] __________________________________________________________________________________________________ conv3_block3_2_conv (Conv2D) (None, 28, 28, 128) 147584 conv3_block3_1_relu[0][0] __________________________________________________________________________________________________ conv3_block3_2_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block3_2_conv[0][0] __________________________________________________________________________________________________ conv3_block3_2_relu (Activation (None, 28, 28, 128) 0 conv3_block3_2_bn[0][0] __________________________________________________________________________________________________ conv3_block3_3_conv (Conv2D) (None, 28, 28, 512) 66048 conv3_block3_2_relu[0][0] __________________________________________________________________________________________________ conv3_block3_3_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block3_3_conv[0][0] __________________________________________________________________________________________________ conv3_block3_add (Add) (None, 28, 28, 512) 0 conv3_block2_out[0][0] conv3_block3_3_bn[0][0] __________________________________________________________________________________________________ conv3_block3_out (Activation) (None, 28, 28, 512) 0 conv3_block3_add[0][0] __________________________________________________________________________________________________ conv3_block4_1_conv (Conv2D) (None, 28, 28, 128) 65664 conv3_block3_out[0][0] __________________________________________________________________________________________________ conv3_block4_1_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block4_1_conv[0][0] __________________________________________________________________________________________________ conv3_block4_1_relu (Activation (None, 28, 28, 128) 0 conv3_block4_1_bn[0][0] __________________________________________________________________________________________________ conv3_block4_2_conv (Conv2D) (None, 28, 28, 128) 147584 conv3_block4_1_relu[0][0] __________________________________________________________________________________________________ conv3_block4_2_bn (BatchNormali (None, 28, 28, 128) 512 conv3_block4_2_conv[0][0] __________________________________________________________________________________________________ conv3_block4_2_relu (Activation (None, 28, 28, 128) 0 conv3_block4_2_bn[0][0] __________________________________________________________________________________________________ conv3_block4_3_conv (Conv2D) (None, 28, 28, 512) 66048 conv3_block4_2_relu[0][0] __________________________________________________________________________________________________ conv3_block4_3_bn (BatchNormali (None, 28, 28, 512) 2048 conv3_block4_3_conv[0][0] __________________________________________________________________________________________________ conv3_block4_add (Add) (None, 28, 28, 512) 0 conv3_block3_out[0][0] conv3_block4_3_bn[0][0] __________________________________________________________________________________________________ conv3_block4_out (Activation) (None, 28, 28, 512) 0 conv3_block4_add[0][0] __________________________________________________________________________________________________ conv4_block1_1_conv (Conv2D) (None, 14, 14, 256) 131328 conv3_block4_out[0][0] __________________________________________________________________________________________________ conv4_block1_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block1_1_conv[0][0] __________________________________________________________________________________________________ conv4_block1_1_relu (Activation (None, 14, 14, 256) 0 conv4_block1_1_bn[0][0] __________________________________________________________________________________________________ conv4_block1_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block1_1_relu[0][0] __________________________________________________________________________________________________ conv4_block1_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block1_2_conv[0][0] __________________________________________________________________________________________________ conv4_block1_2_relu (Activation (None, 14, 14, 256) 0 conv4_block1_2_bn[0][0] __________________________________________________________________________________________________ conv4_block1_0_conv (Conv2D) (None, 14, 14, 1024) 525312 conv3_block4_out[0][0] __________________________________________________________________________________________________ conv4_block1_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block1_2_relu[0][0] __________________________________________________________________________________________________ conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block1_0_conv[0][0] __________________________________________________________________________________________________ conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block1_3_conv[0][0] __________________________________________________________________________________________________ conv4_block1_add (Add) (None, 14, 14, 1024) 0 conv4_block1_0_bn[0][0] conv4_block1_3_bn[0][0] __________________________________________________________________________________________________ conv4_block1_out (Activation) (None, 14, 14, 1024) 0 conv4_block1_add[0][0] __________________________________________________________________________________________________ conv4_block2_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block1_out[0][0] __________________________________________________________________________________________________ conv4_block2_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block2_1_conv[0][0] __________________________________________________________________________________________________ conv4_block2_1_relu (Activation (None, 14, 14, 256) 0 conv4_block2_1_bn[0][0] __________________________________________________________________________________________________ conv4_block2_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block2_1_relu[0][0] __________________________________________________________________________________________________ conv4_block2_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block2_2_conv[0][0] __________________________________________________________________________________________________ conv4_block2_2_relu (Activation (None, 14, 14, 256) 0 conv4_block2_2_bn[0][0] __________________________________________________________________________________________________ conv4_block2_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block2_2_relu[0][0] __________________________________________________________________________________________________ conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block2_3_conv[0][0] __________________________________________________________________________________________________ conv4_block2_add (Add) (None, 14, 14, 1024) 0 conv4_block1_out[0][0] conv4_block2_3_bn[0][0] __________________________________________________________________________________________________ conv4_block2_out (Activation) (None, 14, 14, 1024) 0 conv4_block2_add[0][0] __________________________________________________________________________________________________ conv4_block3_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block2_out[0][0] __________________________________________________________________________________________________ conv4_block3_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block3_1_conv[0][0] __________________________________________________________________________________________________ conv4_block3_1_relu (Activation (None, 14, 14, 256) 0 conv4_block3_1_bn[0][0] __________________________________________________________________________________________________ conv4_block3_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block3_1_relu[0][0] __________________________________________________________________________________________________ conv4_block3_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block3_2_conv[0][0] __________________________________________________________________________________________________ conv4_block3_2_relu (Activation (None, 14, 14, 256) 0 conv4_block3_2_bn[0][0] __________________________________________________________________________________________________ conv4_block3_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block3_2_relu[0][0] __________________________________________________________________________________________________ conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block3_3_conv[0][0] __________________________________________________________________________________________________ conv4_block3_add (Add) (None, 14, 14, 1024) 0 conv4_block2_out[0][0] conv4_block3_3_bn[0][0] __________________________________________________________________________________________________ conv4_block3_out (Activation) (None, 14, 14, 1024) 0 conv4_block3_add[0][0] __________________________________________________________________________________________________ conv4_block4_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block3_out[0][0] __________________________________________________________________________________________________ conv4_block4_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block4_1_conv[0][0] __________________________________________________________________________________________________ conv4_block4_1_relu (Activation (None, 14, 14, 256) 0 conv4_block4_1_bn[0][0] __________________________________________________________________________________________________ conv4_block4_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block4_1_relu[0][0] __________________________________________________________________________________________________ conv4_block4_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block4_2_conv[0][0] __________________________________________________________________________________________________ conv4_block4_2_relu (Activation (None, 14, 14, 256) 0 conv4_block4_2_bn[0][0] __________________________________________________________________________________________________ conv4_block4_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block4_2_relu[0][0] __________________________________________________________________________________________________ conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block4_3_conv[0][0] __________________________________________________________________________________________________ conv4_block4_add (Add) (None, 14, 14, 1024) 0 conv4_block3_out[0][0] conv4_block4_3_bn[0][0] __________________________________________________________________________________________________ conv4_block4_out (Activation) (None, 14, 14, 1024) 0 conv4_block4_add[0][0] __________________________________________________________________________________________________ conv4_block5_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block4_out[0][0] __________________________________________________________________________________________________ conv4_block5_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block5_1_conv[0][0] __________________________________________________________________________________________________ conv4_block5_1_relu (Activation (None, 14, 14, 256) 0 conv4_block5_1_bn[0][0] __________________________________________________________________________________________________ conv4_block5_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block5_1_relu[0][0] __________________________________________________________________________________________________ conv4_block5_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block5_2_conv[0][0] __________________________________________________________________________________________________ conv4_block5_2_relu (Activation (None, 14, 14, 256) 0 conv4_block5_2_bn[0][0] __________________________________________________________________________________________________ conv4_block5_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block5_2_relu[0][0] __________________________________________________________________________________________________ conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block5_3_conv[0][0] __________________________________________________________________________________________________ conv4_block5_add (Add) (None, 14, 14, 1024) 0 conv4_block4_out[0][0] conv4_block5_3_bn[0][0] __________________________________________________________________________________________________ conv4_block5_out (Activation) (None, 14, 14, 1024) 0 conv4_block5_add[0][0] __________________________________________________________________________________________________ conv4_block6_1_conv (Conv2D) (None, 14, 14, 256) 262400 conv4_block5_out[0][0] __________________________________________________________________________________________________ conv4_block6_1_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block6_1_conv[0][0] __________________________________________________________________________________________________ conv4_block6_1_relu (Activation (None, 14, 14, 256) 0 conv4_block6_1_bn[0][0] __________________________________________________________________________________________________ conv4_block6_2_conv (Conv2D) (None, 14, 14, 256) 590080 conv4_block6_1_relu[0][0] __________________________________________________________________________________________________ conv4_block6_2_bn (BatchNormali (None, 14, 14, 256) 1024 conv4_block6_2_conv[0][0] __________________________________________________________________________________________________ conv4_block6_2_relu (Activation (None, 14, 14, 256) 0 conv4_block6_2_bn[0][0] __________________________________________________________________________________________________ conv4_block6_3_conv (Conv2D) (None, 14, 14, 1024) 263168 conv4_block6_2_relu[0][0] __________________________________________________________________________________________________ conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096 conv4_block6_3_conv[0][0] __________________________________________________________________________________________________ conv4_block6_add (Add) (None, 14, 14, 1024) 0 conv4_block5_out[0][0] conv4_block6_3_bn[0][0] __________________________________________________________________________________________________ conv4_block6_out (Activation) (None, 14, 14, 1024) 0 conv4_block6_add[0][0] __________________________________________________________________________________________________ conv5_block1_1_conv (Conv2D) (None, 7, 7, 512) 524800 conv4_block6_out[0][0] __________________________________________________________________________________________________ conv5_block1_1_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block1_1_conv[0][0] __________________________________________________________________________________________________ conv5_block1_1_relu (Activation (None, 7, 7, 512) 0 conv5_block1_1_bn[0][0] __________________________________________________________________________________________________ conv5_block1_2_conv (Conv2D) (None, 7, 7, 512) 2359808 conv5_block1_1_relu[0][0] __________________________________________________________________________________________________ conv5_block1_2_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block1_2_conv[0][0] __________________________________________________________________________________________________ conv5_block1_2_relu (Activation (None, 7, 7, 512) 0 conv5_block1_2_bn[0][0] __________________________________________________________________________________________________ conv5_block1_0_conv (Conv2D) (None, 7, 7, 2048) 2099200 conv4_block6_out[0][0] __________________________________________________________________________________________________ conv5_block1_3_conv (Conv2D) (None, 7, 7, 2048) 1050624 conv5_block1_2_relu[0][0] __________________________________________________________________________________________________ conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048) 8192 conv5_block1_0_conv[0][0] __________________________________________________________________________________________________ conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048) 8192 conv5_block1_3_conv[0][0] __________________________________________________________________________________________________ conv5_block1_add (Add) (None, 7, 7, 2048) 0 conv5_block1_0_bn[0][0] conv5_block1_3_bn[0][0] __________________________________________________________________________________________________ conv5_block1_out (Activation) (None, 7, 7, 2048) 0 conv5_block1_add[0][0] __________________________________________________________________________________________________ conv5_block2_1_conv (Conv2D) (None, 7, 7, 512) 1049088 conv5_block1_out[0][0] __________________________________________________________________________________________________ conv5_block2_1_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block2_1_conv[0][0] __________________________________________________________________________________________________ conv5_block2_1_relu (Activation (None, 7, 7, 512) 0 conv5_block2_1_bn[0][0] __________________________________________________________________________________________________ conv5_block2_2_conv (Conv2D) (None, 7, 7, 512) 2359808 conv5_block2_1_relu[0][0] __________________________________________________________________________________________________ conv5_block2_2_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block2_2_conv[0][0] __________________________________________________________________________________________________ conv5_block2_2_relu (Activation (None, 7, 7, 512) 0 conv5_block2_2_bn[0][0] __________________________________________________________________________________________________ conv5_block2_3_conv (Conv2D) (None, 7, 7, 2048) 1050624 conv5_block2_2_relu[0][0] __________________________________________________________________________________________________ conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048) 8192 conv5_block2_3_conv[0][0] __________________________________________________________________________________________________ conv5_block2_add (Add) (None, 7, 7, 2048) 0 conv5_block1_out[0][0] conv5_block2_3_bn[0][0] __________________________________________________________________________________________________ conv5_block2_out (Activation) (None, 7, 7, 2048) 0 conv5_block2_add[0][0] __________________________________________________________________________________________________ conv5_block3_1_conv (Conv2D) (None, 7, 7, 512) 1049088 conv5_block2_out[0][0] __________________________________________________________________________________________________ conv5_block3_1_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block3_1_conv[0][0] __________________________________________________________________________________________________ conv5_block3_1_relu (Activation (None, 7, 7, 512) 0 conv5_block3_1_bn[0][0] __________________________________________________________________________________________________ conv5_block3_2_conv (Conv2D) (None, 7, 7, 512) 2359808 conv5_block3_1_relu[0][0] __________________________________________________________________________________________________ conv5_block3_2_bn (BatchNormali (None, 7, 7, 512) 2048 conv5_block3_2_conv[0][0] __________________________________________________________________________________________________ conv5_block3_2_relu (Activation (None, 7, 7, 512) 0 conv5_block3_2_bn[0][0] __________________________________________________________________________________________________ conv5_block3_3_conv (Conv2D) (None, 7, 7, 2048) 1050624 conv5_block3_2_relu[0][0] __________________________________________________________________________________________________ conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048) 8192 conv5_block3_3_conv[0][0] __________________________________________________________________________________________________ conv5_block3_add (Add) (None, 7, 7, 2048) 0 conv5_block2_out[0][0] conv5_block3_3_bn[0][0] __________________________________________________________________________________________________ conv5_block3_out (Activation) (None, 7, 7, 2048) 0 conv5_block3_add[0][0] __________________________________________________________________________________________________ avg_pool (GlobalAveragePooling2 (None, 2048) 0 conv5_block3_out[0][0] __________________________________________________________________________________________________ dense_1 (Dense) (None, 1) 2049 avg_pool[0][0] ================================================================================================== Total params: 23,589,761 Trainable params: 23,536,641 Non-trainable params: 53,120 __________________________________________________________________________________________________ . ## Add checkpoints to model to save the &#39;best&#39; version of your model by comparing it to previous epochs of training weight_path=&quot;{}_my_model.best.hdf5&quot;.format(&#39;xray_class&#39;) checkpoint = ModelCheckpoint(weight_path, monitor=&#39;val_loss&#39;, verbose=1, save_best_only=True, mode=&#39;min&#39;, save_weights_only = True) early = EarlyStopping(monitor=&#39;val_loss&#39;, mode=&#39;min&#39;, patience=10) callbacks_list = [checkpoint, early] . Train model . ## train model history = my_model.fit_generator(train_gen, validation_data = (valX, valY), epochs = 20, callbacks = callbacks_list) . Epoch 1/20 143/143 [==============================] - 177s 1s/step - loss: 0.7218 - binary_accuracy: 0.5800 - val_loss: 1.7579 - val_binary_accuracy: 0.0090 Epoch 00001: val_loss improved from inf to 1.75792, saving model to xray_class_my_model.best.hdf5 Epoch 2/20 143/143 [==============================] - 136s 953ms/step - loss: 0.6603 - binary_accuracy: 0.6329 - val_loss: 0.8520 - val_binary_accuracy: 0.0120 Epoch 00002: val_loss improved from 1.75792 to 0.85198, saving model to xray_class_my_model.best.hdf5 Epoch 3/20 143/143 [==============================] - 133s 927ms/step - loss: 0.6492 - binary_accuracy: 0.6482 - val_loss: 0.6309 - val_binary_accuracy: 0.8430 Epoch 00003: val_loss improved from 0.85198 to 0.63085, saving model to xray_class_my_model.best.hdf5 Epoch 4/20 143/143 [==============================] - 133s 931ms/step - loss: 0.6207 - binary_accuracy: 0.6726 - val_loss: 0.3729 - val_binary_accuracy: 0.9910 Epoch 00004: val_loss improved from 0.63085 to 0.37292, saving model to xray_class_my_model.best.hdf5 Epoch 5/20 143/143 [==============================] - 133s 932ms/step - loss: 0.5719 - binary_accuracy: 0.7050 - val_loss: 0.7439 - val_binary_accuracy: 0.4025 Epoch 00005: val_loss did not improve from 0.37292 Epoch 6/20 143/143 [==============================] - 133s 931ms/step - loss: 0.5624 - binary_accuracy: 0.7185 - val_loss: 0.8474 - val_binary_accuracy: 0.3575 Epoch 00006: val_loss did not improve from 0.37292 Epoch 7/20 143/143 [==============================] - 133s 929ms/step - loss: 0.5092 - binary_accuracy: 0.7535 - val_loss: 0.8491 - val_binary_accuracy: 0.5005 Epoch 00007: val_loss did not improve from 0.37292 Epoch 8/20 143/143 [==============================] - 133s 932ms/step - loss: 0.5162 - binary_accuracy: 0.7513 - val_loss: 1.6125 - val_binary_accuracy: 0.0600 Epoch 00008: val_loss did not improve from 0.37292 Epoch 9/20 143/143 [==============================] - 133s 933ms/step - loss: 0.4646 - binary_accuracy: 0.7898 - val_loss: 0.3865 - val_binary_accuracy: 0.8650 Epoch 00009: val_loss did not improve from 0.37292 Epoch 10/20 143/143 [==============================] - 133s 933ms/step - loss: 0.4275 - binary_accuracy: 0.8046 - val_loss: 1.2120 - val_binary_accuracy: 0.3795 Epoch 00010: val_loss did not improve from 0.37292 Epoch 11/20 143/143 [==============================] - 134s 934ms/step - loss: 0.4122 - binary_accuracy: 0.8094 - val_loss: 0.8254 - val_binary_accuracy: 0.5480 Epoch 00011: val_loss did not improve from 0.37292 Epoch 12/20 143/143 [==============================] - 133s 932ms/step - loss: 0.3847 - binary_accuracy: 0.8291 - val_loss: 0.2506 - val_binary_accuracy: 0.9140 Epoch 00012: val_loss improved from 0.37292 to 0.25058, saving model to xray_class_my_model.best.hdf5 Epoch 13/20 143/143 [==============================] - 134s 935ms/step - loss: 0.3418 - binary_accuracy: 0.8575 - val_loss: 0.7763 - val_binary_accuracy: 0.6620 Epoch 00013: val_loss did not improve from 0.25058 Epoch 14/20 143/143 [==============================] - 133s 932ms/step - loss: 0.3291 - binary_accuracy: 0.8558 - val_loss: 0.4665 - val_binary_accuracy: 0.8085 Epoch 00014: val_loss did not improve from 0.25058 Epoch 15/20 143/143 [==============================] - 133s 933ms/step - loss: 0.3020 - binary_accuracy: 0.8728 - val_loss: 0.1557 - val_binary_accuracy: 0.9455 Epoch 00015: val_loss improved from 0.25058 to 0.15575, saving model to xray_class_my_model.best.hdf5 Epoch 16/20 143/143 [==============================] - 133s 933ms/step - loss: 0.2802 - binary_accuracy: 0.8824 - val_loss: 1.2408 - val_binary_accuracy: 0.5035 Epoch 00016: val_loss did not improve from 0.15575 Epoch 17/20 143/143 [==============================] - 133s 931ms/step - loss: 0.2636 - binary_accuracy: 0.8872 - val_loss: 0.3008 - val_binary_accuracy: 0.9000 Epoch 00017: val_loss did not improve from 0.15575 Epoch 18/20 143/143 [==============================] - 133s 932ms/step - loss: 0.2531 - binary_accuracy: 0.9003 - val_loss: 0.7074 - val_binary_accuracy: 0.7445 Epoch 00018: val_loss did not improve from 0.15575 Epoch 19/20 143/143 [==============================] - 133s 932ms/step - loss: 0.2507 - binary_accuracy: 0.9078 - val_loss: 0.4600 - val_binary_accuracy: 0.8335 Epoch 00019: val_loss did not improve from 0.15575 Epoch 20/20 143/143 [==============================] - 133s 933ms/step - loss: 0.2182 - binary_accuracy: 0.9161 - val_loss: 0.9295 - val_binary_accuracy: 0.6160 Epoch 00020: val_loss did not improve from 0.15575 . After training for some time, look at the performance of your model by plotting some performance statistics: . Note, these figures will come in handy for your FDA documentation later in the project . ## After training, make some predictions to assess model&#39;s overall performance my_model.load_weights(weight_path) pred_Y = my_model.predict(valX, batch_size = 32, verbose = True) . 2000/2000 [==============================] - 24s 12ms/step . # Plotting the history of model training: def plot_history(history): N = len(history.history[&quot;loss&quot;]) plt.style.use(&quot;ggplot&quot;) plt.figure() plt.plot(np.arange(0, N), history.history[&quot;loss&quot;], label=&quot;train_loss&quot;) plt.plot(np.arange(0, N), history.history[&quot;val_loss&quot;], label=&quot;val_loss&quot;) plt.title(&quot;Training vs Validation Loss&quot;) plt.xlabel(&quot;Epoch #&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend(loc=&quot;lower left&quot;) plt.figure() plt.plot(np.arange(0, N), history.history[&quot;binary_accuracy&quot;], label=&quot;train_acc&quot;) plt.plot(np.arange(0, N), history.history[&quot;val_binary_accuracy&quot;], label=&quot;val_acc&quot;) plt.title(&quot;Training vs Validation Accuracy&quot;) plt.xlabel(&quot;Epoch #&quot;) plt.ylabel(&quot;Accuracy&quot;) plt.legend(loc=&quot;lower left&quot;) plot_history(history) . So after trying a few different model variations I have settled for this simpler model, given the limited time for this project. This simplier model made more progress training in a shorter time, due to having fewer trainable parameters. . Dispite this, the training is still relatively unstable even after 20 epochs, as can be seen from the highly volatile validation accuracy and loss we can see in the charts above. . Rather than let the model make fixed predictions on its own assumptions, we can get the best results from our model if we look at the raw probabilities - and then determine what the best threshold value might be to decide between the classes i.e. in our case to decide between Pneumonia and No Pneumonia cases. . With this in mind, let us first look at a histogram of the distribution of predictions for our validation data. . # Look at the distribution of the prediction probabilities plt.hist(pred_Y, bins=20) . (array([1217., 235., 97., 88., 80., 60., 43., 36., 18., 16., 24., 14., 18., 13., 10., 10., 6., 5., 4., 6.]), array([1.4908544e-06, 4.7190338e-02, 9.4379187e-02, 1.4156803e-01, 1.8875688e-01, 2.3594573e-01, 2.8313458e-01, 3.3032343e-01, 3.7751228e-01, 4.2470112e-01, 4.7188997e-01, 5.1907885e-01, 5.6626767e-01, 6.1345655e-01, 6.6064537e-01, 7.0783424e-01, 7.5502306e-01, 8.0221194e-01, 8.4940076e-01, 8.9658964e-01, 9.4377846e-01], dtype=float32), &lt;a list of 20 Patch objects&gt;) . So we can see from this right-skewed distribution that most of the predicted values are between 0.0 and 0.2. This is to be expected of course because: . The majority of the samples are for the prediction 0.0 i.e. &#39;No Pneumonia&#39; | From what we saw in our exploratory data analysis, the intensity profile of the Pneumonia examples can be very difficult to distinguish from other diseases i.e. from No Pneumonia cases | . We might therefore estimate our optimum threshold value might be somewhere between 0.0-0.2. . We will now also look at some further metrics to help determine the optimial threshold value. . The project suggests the use of the roc-auc metric. However this is not a very good metric to use when we have very imbalanced classes, such as our use-case. See This article and this paper for reasons why. . Instead I believe better metric for this would be the precison-recall curve. We will however plot both of these and compare as well as an f1-threshold plot. . # Get ROC curve FPR and TPR from true labels vs score values fpr, tpr, _ = roc_curve(valY, pred_Y) # Calculate ROC Area Under the Curve (AUC) from FPR and TPR data points roc_auc = auc(fpr, tpr) # Calculate precision and recall from true labels vs score values precision, recall, thresholds = precision_recall_curve(valY, pred_Y) # Calculate f1 vs threshold scores f1_scores = [] for i in thresholds: f1 = f1_score(valY.astype(int), binarize(pred_Y,i)) f1_scores.append(f1) # Plot charts fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,5)) lw = 2 ax1.plot(fpr, tpr, color=&#39;darkorange&#39;, lw=lw, label=&#39;ROC curve (area = %0.4f)&#39; % roc_auc) ax1.plot([0, 1], [0, 1], color=&#39;navy&#39;, lw=lw, linestyle=&#39;--&#39;) ax1.set_xlabel(&#39;False Positive Rate&#39;) ax1.set_ylabel(&#39;True Positive Rate&#39;) ax1.title.set_text(&#39;ROC Curve&#39;) ax1.legend(loc=&quot;upper left&quot;) ax1.grid(True) ax2.step(recall, precision, color=&#39;orange&#39;, where=&#39;post&#39;) ax2.set_xlabel(&#39;Recall&#39;) ax2.set_ylabel(&#39;Precision&#39;) ax2.title.set_text(&#39;Precision-Recall Curve&#39;) ax2.grid(True) ax3.plot(thresholds, f1_scores, label = &#39;F1 Score&#39;) ax3.set_xlabel(&#39;Threshold&#39;) ax3.set_ylabel(&#39;F1 Score&#39;) ax3.title.set_text(&#39;F1 Score vs Threshold&#39;) ax3.legend(loc=&quot;upper left&quot;) ax3.grid(True) plt.show() . So lets us first state what in any trade off we prefer between false negatives and false positives. I would argue we would prefer to minimise false negatives over false positives - why? we are better to avoid missing any actual Pneumonia cases, even if that means we flag up more people as having Pneumonia. A healthy person being incorrectly diagnosed could get a second test or diagnosis to confirm, this is more an inconvenience. But if we fail to flag a person that actually has Pneumonia, this is far more serious. So these will be our priorities and how we define our type 1 vs type 2 errors. . Looking at the ROC curve we can see the model seems to have some skill (area above diagonal) but I am skeptical for this interpretation given the unbalanced classes and note the articles I referred to area. So Instead I would look more to the Precison-Recall curve, given we have few examples of a positive event i.e. Pneumonia, and we are less interested in the many true negatives. Here we see the curve is very low, and not far off the &#39;no skill&#39; line of our imbalanced dataset which would be around the proportion of one class to another which in our validation sample of 2000 cases was 21/1971 which is around 0.01. . So we will now explore threshold values between 0.05 to 0.2 and for each of these, observe the confusion matrix, and the precison, recall and f1 scores. Given we want to prioritise minimising false negatives, we will want to find a threshold that gives a higher value for Recall for the postive class 1.0. . for threshold in [0.05, 0.1, 0.15, 0.2]: # test 3 score thresholds which are used to determine if a class is predicted to be 0 or 1 print(&quot;threshold:&quot;, threshold) print(&quot;-&quot;) y_pred = [0 if y &lt; threshold else 1 for y in pred_Y] # from sklearn.metrics import confusion_matrix cm = confusion_matrix(valY, y_pred) # Pandas &#39;crosstab&#39; displays a better formated confusion matrix than the one in sklearn cm = pd.crosstab(pd.Series(valY), pd.Series(y_pred), rownames=[&#39;Reality&#39;], colnames=[&#39;Predicted&#39;], margins=True) print(cm) print() print(&quot;Classification report:&quot;) print(classification_report(valY, y_pred)) print() ## Minimise false negatives so highest recall . threshold: 0.05 - Predicted 0 1 All Reality 0.0 1231 751 1982 1.0 8 10 18 All 1239 761 2000 Classification report: precision recall f1-score support 0.0 0.99 0.62 0.76 1982 1.0 0.01 0.56 0.03 18 accuracy 0.62 2000 macro avg 0.50 0.59 0.40 2000 weighted avg 0.98 0.62 0.76 2000 threshold: 0.1 - Predicted 0 1 All Reality 0.0 1456 526 1982 1.0 9 9 18 All 1465 535 2000 Classification report: precision recall f1-score support 0.0 0.99 0.73 0.84 1982 1.0 0.02 0.50 0.03 18 accuracy 0.73 2000 macro avg 0.51 0.62 0.44 2000 weighted avg 0.99 0.73 0.84 2000 threshold: 0.15 - Predicted 0 1 All Reality 0.0 1553 429 1982 1.0 11 7 18 All 1564 436 2000 Classification report: precision recall f1-score support 0.0 0.99 0.78 0.88 1982 1.0 0.02 0.39 0.03 18 accuracy 0.78 2000 macro avg 0.50 0.59 0.45 2000 weighted avg 0.98 0.78 0.87 2000 threshold: 0.2 - Predicted 0 1 All Reality 0.0 1646 336 1982 1.0 13 5 18 All 1659 341 2000 Classification report: precision recall f1-score support 0.0 0.99 0.83 0.90 1982 1.0 0.01 0.28 0.03 18 accuracy 0.83 2000 macro avg 0.50 0.55 0.47 2000 weighted avg 0.98 0.83 0.90 2000 . Generally we can see that regardless of threshold value, the model struggles to do a good job classifying positive Pneumonia cases - with roughly half getting mis-classified in all cases. . We can see from the above metrics that this is a difficult threshold value to balance. While the threshold value of 0.05 gives us the highest Recall value of 0.56 for the 1.0 Pneumonia cases - and the lowest false negatives, we can see this comes at a great cost of creating 751 false positives (as seen in the confusion matrix). While we want to priortise reducing false negatives, we still care about false positives. . If we look at the next threshold value of 0.1, while it has a slightly lower recall value of 0.50 and just one more false negative, this drastically reduces the false postives from 751 down to 526 false postives. So on balance, for this model I would suggest the best threshold value, would be 0.1. . At this threshhold of 0.1, we should expect a false positive rate of 526/(526+1456) = 0.27 = 27%. . At this threshhold of 0.1, we should expect a false negative rate of 9/(9+9) = 0.5 = 50%. . Conclusion . I have looked at how for the model I have made, how we might get the best performance from it for classifying Pneuomonia cases by finding the optimal threshold value to decide between positive and negative cases. In my judgment I have suggested that for this model, a threshold value of 0.1 gives us the best balance of results for the classifier. . ## Save model architecture to a .json: model_json = my_model.to_json() with open(&quot;my_model.json&quot;, &quot;w&quot;) as json_file: json_file.write(model_json) .",
            "url": "https://www.livingdatalab.com/health/deep-learning/2022/02/06/detect-pneumonia-chest-xrays.html",
            "relUrl": "/health/deep-learning/2022/02/06/detect-pneumonia-chest-xrays.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post25": {
            "title": "Python Power Tools for Data Science - Pycaret Anomaly Detection",
            "content": "Python Power Tools for Data Science . In this series of articles Python Power Tools for Data Science I will be looking at a series of python tools that can make a significant improvement on common Data Science tasks. In particular, Python Power Tools are python tools that can significantly automate or simplify common tasks a Data Scientist would need to perform. . Automation and simplifcation of common tasks can bring many benefits such as: . Less time needed to complete tasks | Reduction of mistakes due to less complex code | Improved readability and understanding of code | Increased consistancy of approach to different problems | Easier reproducability, verification, and comparison of results | . Pycaret Anomaly Detection Module . Pycaret is a low code python library that aims to automate many tasks required for machine learning. Tasks that would usually take hundreds of lines of code can often be replaced with just a couple of lines. It was inspired by the Caret library in R. . In comparison with the other open-source machine learning libraries, PyCaret is an alternate low-code library that can be used to replace hundreds of lines of code with few words only. This makes experiments exponentially fast and efficient. PyCaret is essentially a Python wrapper around several machine learning libraries and frameworks such as scikit-learn, XGBoost, LightGBM, CatBoost, spaCy, Optuna, Hyperopt, Ray, and many more. (Pycaret Documentation) . Pycaret has different modules specialised for different machine learning use-cases these include:- Classification- Regression . Clustering | Anomaly Detection | Natural Language Processing | Assocation Rule Mining | Time Series | . See further articles about these other Pycaret modules and what they can offer. . In this article we will use the Anomaly Detection Module of Pycaret which is an unsupervised machine learning module that is used for identifying rare items, events, or observations. It has over 13 algorithms and plots to analyze the results, plus many other features. . Dataset - New York Taxi Passengers . The NYC Taxi &amp; Limousine Commission (TLC) has released public datasets that contain data for taxi trips in NYC, including timestamps, pickup &amp; drop-off locations, number of passengers, type of payment, and fare amount. . We will specifically use the data that contains the number of taxi passengers from July 2014 to January 2015 at half-hourly intervals, so this is a time series dataset. . # Download tax passenger data data = pd.read_csv(&#39;https://raw.githubusercontent.com/numenta/NAB/master/data/realKnownCause/nyc_taxi.csv&#39;) data[&#39;timestamp&#39;] = pd.to_datetime(data[&#39;timestamp&#39;]) # Show first few rows data.head() . timestamp value . 0 2014-07-01 00:00:00 | 10844 | . 1 2014-07-01 00:30:00 | 8127 | . 2 2014-07-01 01:00:00 | 6210 | . 3 2014-07-01 01:30:00 | 4656 | . 4 2014-07-01 02:00:00 | 3820 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; # Show last few rows data.tail() . timestamp value . 10315 2015-01-31 21:30:00 | 24670 | . 10316 2015-01-31 22:00:00 | 25721 | . 10317 2015-01-31 22:30:00 | 27309 | . 10318 2015-01-31 23:00:00 | 26591 | . 10319 2015-01-31 23:30:00 | 26288 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; # Plot dataset plt.figure(figsize=(20,10)) sns.lineplot(x = &quot;timestamp&quot;, y = &quot;value&quot;, data=data) plt.title(&#39;Number of NYC Taxi passengers by date July 2014 - January 2015&#39;) plt.show() . So we can&#39;t directly use timestamp data for anomaly detection models, we need to convert this data into other features such as day, year, hour etc before we can use it - so lets do this. . # Set timestamp to index data.set_index(&#39;timestamp&#39;, drop=True, inplace=True) # Resample timeseries to hourly data = data.resample(&#39;H&#39;).sum() # Create more features from date data[&#39;day&#39;] = [i.day for i in data.index] data[&#39;day_name&#39;] = [i.day_name() for i in data.index] data[&#39;day_of_year&#39;] = [i.dayofyear for i in data.index] data[&#39;week_of_year&#39;] = [i.weekofyear for i in data.index] data[&#39;hour&#39;] = [i.hour for i in data.index] data[&#39;is_weekday&#39;] = [i.isoweekday() for i in data.index] data.head() . value day day_name day_of_year week_of_year hour is_weekday . timestamp . 2014-07-01 00:00:00 18971 | 1 | Tuesday | 182 | 27 | 0 | 2 | . 2014-07-01 01:00:00 10866 | 1 | Tuesday | 182 | 27 | 1 | 2 | . 2014-07-01 02:00:00 6693 | 1 | Tuesday | 182 | 27 | 2 | 2 | . 2014-07-01 03:00:00 4433 | 1 | Tuesday | 182 | 27 | 3 | 2 | . 2014-07-01 04:00:00 4379 | 1 | Tuesday | 182 | 27 | 4 | 2 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Pycaret workflow . Setup . The Pycaret setup() is the first part of the workflow that always needs to be performed, and is a function that takes our data in the form of a pandas dataframe and performs a number of tasks to get reading for the machine learning pipeline. . # Setup from pycaret.anomaly import * s = setup(data, session_id = 123) . Description Value . 0 session_id | 123 | . 1 Original Data | (5160, 7) | . 2 Missing Values | False | . 3 Numeric Features | 5 | . 4 Categorical Features | 2 | . 5 Ordinal Features | False | . 6 High Cardinality Features | False | . 7 High Cardinality Method | None | . 8 Transformed Data | (5160, 19) | . 9 CPU Jobs | -1 | . 10 Use GPU | False | . 11 Log Experiment | False | . 12 Experiment Name | anomaly-default-name | . 13 USI | 5a80 | . 14 Imputation Type | simple | . 15 Iterative Imputation Iteration | None | . 16 Numeric Imputer | mean | . 17 Iterative Imputation Numeric Model | None | . 18 Categorical Imputer | mode | . 19 Iterative Imputation Categorical Model | None | . 20 Unknown Categoricals Handling | least_frequent | . 21 Normalize | False | . 22 Normalize Method | None | . 23 Transformation | False | . 24 Transformation Method | None | . 25 PCA | False | . 26 PCA Method | None | . 27 PCA Components | None | . 28 Ignore Low Variance | False | . 29 Combine Rare Levels | False | . 30 Rare Level Threshold | None | . 31 Numeric Binning | False | . 32 Remove Outliers | False | . 33 Outliers Threshold | None | . 34 Remove Multicollinearity | False | . 35 Multicollinearity Threshold | None | . 36 Remove Perfect Collinearity | False | . 37 Clustering | False | . 38 Clustering Iteration | None | . 39 Polynomial Features | False | . 40 Polynomial Degree | None | . 41 Trignometry Features | False | . 42 Polynomial Threshold | None | . 43 Group Features | False | . 44 Feature Selection | False | . 45 Feature Selection Method | classic | . 46 Features Selection Threshold | None | . 47 Feature Interaction | False | . 48 Feature Ratio | False | . 49 Interaction Threshold | None | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Calling the setup() function with one line of code does the following in the background: . Data types will be inferred for each column | A table of key information about the dataset and configuration settings is generated | Based on the types inferred and configuration chosen, the dataset will be transformed to be ready for the machine learning algorithms | . Various configuration settings are available, but defaults are selected so none are required. . Some key configuration settings available include: . Missing numeric values are imputed (default: mean) iterative option uses lightgbm model to estimate values | Missing categorical values are imputed (default: constant dummy value, alteratives include mode and iterative) | Encode categorical values as ordinal e.g. ‘low’, ‘medium’, ‘high’ | High cardinality (default: false) options to compress to fewer levels or replace with frequency or k-means clustering derived class. | Define date fields explictly | Normalise numeric fields (default: false) options include zscore, minmax, maxabs, robust | Power transforms (default: false) will transform to make data more gaussian options include yeo-johnson, quantile | PCA: Principal components analysis (default: false) reduce the dimensionality of the data down to a specified number of components | . Selecting and training a model . At time of writing this article, there are 12 different anomaly detection models available within Pycaret, which we can display with the models() function. . # Check list of available models models() . Name Reference . ID . abod Angle-base Outlier Detection | pyod.models.abod.ABOD | . cluster Clustering-Based Local Outlier | pyod.models.cblof.CBLOF | . cof Connectivity-Based Local Outlier | pyod.models.cof.COF | . iforest Isolation Forest | pyod.models.iforest.IForest | . histogram Histogram-based Outlier Detection | pyod.models.hbos.HBOS | . knn K-Nearest Neighbors Detector | pyod.models.knn.KNN | . lof Local Outlier Factor | pyod.models.lof.LOF | . svm One-class SVM detector | pyod.models.ocsvm.OCSVM | . pca Principal Component Analysis | pyod.models.pca.PCA | . mcd Minimum Covariance Determinant | pyod.models.mcd.MCD | . sod Subspace Outlier Detection | pyod.models.sod.SOD | . sos Stochastic Outlier Selection | pyod.models.sos.SOS | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; We will choose to use the Isolation Forrest model. Isolation Forrest is similar to Random Forrest in that it&#39;s an algorithm based on multiple descison trees, however rather than aiming to model normal data points - Isolation Forrest explictly tries to identify anomalous data points. . There are many configuration hyperparameters for this model, which can be seen when we create and print the model details as we see below. . # Create model and print configuration hyper-parameters iforest = create_model(&#39;iforest&#39;) print(iforest) . IForest(behaviour=&#39;new&#39;, bootstrap=False, contamination=0.05, max_features=1.0, max_samples=&#39;auto&#39;, n_estimators=100, n_jobs=-1, random_state=123, verbose=0) . One of the key configuration options is contamination which is the proportion of outliers we are saying is in the data set. This is used when fitting the model to define the threshold on the scores of the samples. This is set by default to be 5% i.e. 0.05. . We will now train and assign the model to the dataset. . # Train and assign model to dataset iforest_results = assign_model(iforest) iforest_results.head() . value day day_name day_of_year week_of_year hour is_weekday Anomaly Anomaly_Score . timestamp . 2014-07-01 00:00:00 18971 | 1 | Tuesday | 182 | 27 | 0 | 2 | 0 | -0.015450 | . 2014-07-01 01:00:00 10866 | 1 | Tuesday | 182 | 27 | 1 | 2 | 0 | -0.006367 | . 2014-07-01 02:00:00 6693 | 1 | Tuesday | 182 | 27 | 2 | 2 | 0 | -0.010988 | . 2014-07-01 03:00:00 4433 | 1 | Tuesday | 182 | 27 | 3 | 2 | 0 | -0.017091 | . 2014-07-01 04:00:00 4379 | 1 | Tuesday | 182 | 27 | 4 | 2 | 0 | -0.017006 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; This adds 2 new columns to the dataset, an Anomaly column which gives a binary value if a datapoint is considered an anomaly or not, and a Anomaly_Score column which has a float value as a measure of how anomalous a datapoint is. . Model Evaluation . So lets now evaluate our model by examining the datapoints the model has labelled as anomalies. . # Show dates for first few anomalies iforest_results[iforest_results[&#39;Anomaly&#39;] == 1].head() . value day day_name day_of_year week_of_year hour is_weekday Anomaly Anomaly_Score . timestamp . 2014-07-13 50825 | 13 | Sunday | 194 | 28 | 0 | 7 | 1 | 0.002663 | . 2014-07-27 50407 | 27 | Sunday | 208 | 30 | 0 | 7 | 1 | 0.009264 | . 2014-08-03 48081 | 3 | Sunday | 215 | 31 | 0 | 7 | 1 | 0.003045 | . 2014-09-28 53589 | 28 | Sunday | 271 | 39 | 0 | 7 | 1 | 0.004440 | . 2014-10-05 48472 | 5 | Sunday | 278 | 40 | 0 | 7 | 1 | 0.000325 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; # Plot data with anomalies highlighted in red fig, ax = plt.subplots(figsize=(20,10)) # Create list of outlier_dates outliers = iforest_results[iforest_results[&#39;Anomaly&#39;] == 1] p1 = sns.scatterplot(data=outliers, x = outliers.index, y = &quot;value&quot;, ax=ax, color=&#39;r&#39;) p2 = sns.lineplot(x = iforest_results.index, y = &quot;value&quot;, data=iforest_results, color=&#39;b&#39;, ax=ax) plt.title(&#39;Number of NYC Taxi passengers by date July 2014 - January 2015: Anomalies highlighted&#39;) plt.show() . So we can see the model has labelled a few isolated points as anomalies between 2014-7 and the end of 2014. However near the end of 2014 and the start of 2015, we can see a huge number of anomalies, in particular for all of January 2015. . Let&#39;s focus in on the period from January 2015. . # Plot data with anomalies highlighted in red fig, ax = plt.subplots(figsize=(20,10)) # Focus on dates after Jan 2015 focus = iforest_results[iforest_results.index &gt; &#39;2015-01-01&#39;] # Create list of outlier_dates outliers = focus[focus[&#39;Anomaly&#39;] == 1] p1 = sns.scatterplot(data=outliers, x = outliers.index, y = &quot;value&quot;, ax=ax, color=&#39;r&#39;) p2 = sns.lineplot(x = focus.index, y = &quot;value&quot;, data=focus, color=&#39;b&#39;, ax=ax) plt.title(&#39;Number of NYC Taxi passengers by date January - Feburary 2015: Anomalies highlighted&#39;) plt.show() . So the model seems to be indicating that for all of Janurary 2015 we had a large number of highly unusual passenger number patterns. What might have been going on here? . Researching the date January 2015 in New York brings up many articles about the North American Blizzard of January 2015 : . The January 2015 North American blizzard was a powerful and severe blizzard that dumped up to 3 feet (910 mm) of snowfall in parts of New England. Originating from a disturbance just off the coast of the Northwestern United States on January 23, it initially produced a light swath of snow as it traveled southeastwards into the Midwest as an Alberta clipper on January 24–25. It gradually weakened as it moved eastwards towards the Atlantic Ocean, however, a new dominant low formed off the East Coast of the United States late on January 26, and rapidly deepened as it moved northeastwards towards southeastern New England, producing pronounced blizzard conditions. . Time lapsed satellite images from the period reveals the severe weather patterns that occured. . . Some photos from the New York area at the time of the Blizzard. . . So our model seems to have been able to detect very well this highly unusual pattern in taxi passenger behaviour caused by this Blizzard event. . Conclusion . In this article we have looked at the Pycaret Anomaly detection module as a potential Python Power Tool for Data Science. . With very little code, this module has helped us detect a well documented anomaly event even just using the default configuration. . Some key advantages of using this are: . Quick and easy to use with little code, default parameters can work well | The model library is kept up to date with the latest anomaly detection models, which can help make it easier to consider a range of different models quickly | Despite being simple and easy to use, the library has many configuration options, as well as extra funcationality such as data pre-processing, data visualisation tools, and the ability to load and save models together with the data pipleine easily | . Certrainly from this example, we can see that the Pycaret Anomaly detection module seems a great candidate as a Python Power Tool for Data Science. .",
            "url": "https://www.livingdatalab.com/python-power-tools/pycaret/2022/01/02/python-power-tools-pycaret-anomaly.html",
            "relUrl": "/python-power-tools/pycaret/2022/01/02/python-power-tools-pycaret-anomaly.html",
            "date": " • Jan 2, 2022"
        }
        
    
  
    
        ,"post26": {
            "title": "Topic Modelling using Non-negative Matrix Factorization (NMF)",
            "content": "Introduction . Non-negative Matrix Factorization (NMF) is a method from Linear Algebra that is used in a wide range of applications in science and engineering, similar to Singular Value Decomopistion (SVD) which I covered in an earlier article. It can be used for tasks such as missing data imputation, audio signal processing and bioinformatics. . Topic modeling is an unsupervised machine learning technique used in Natural Language Processing (NLP) that’s capable of scanning a set of texts, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of documents. . In this article we will will use NMF to perform topic modelling. . This article is based in large part on the material from the fastai linear algebra course. . Dataset . We will use the 20 Newsgroups dataset which consists of 20,000 messages taken from 20 different newsgroups from the Usenet bulletin board service, which pre-dates the world-wide-web and websites. We will look at a subset of 4 of these newsgroup categories: . rec.motorcycles | talk.politics.mideast | sci.med | sci.crypt | . We will now get this data. . categories = [&#39;rec.motorcycles&#39;, &#39;talk.politics.mideast&#39;, &#39;sci.med&#39;, &#39;sci.crypt&#39;] remove = (&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;) newsgroups_train = fetch_20newsgroups(subset=&#39;train&#39;, categories=categories, remove=remove) newsgroups_test = fetch_20newsgroups(subset=&#39;test&#39;, categories=categories, remove=remove) . Let&#39;s check how many posts this gives us in total . newsgroups_train.filenames.shape, newsgroups_train.target.shape . ((2351,), (2351,)) . Let&#39;s print the first few lines of 3 of the posts to see what the text looks like . print(&quot; n&quot;.join(newsgroups_train.data[0].split(&quot; n&quot;)[:3])) . I am not an expert in the cryptography science, but some basic things seem evident to me, things which this Clinton Clipper do not address. . print(&quot; n&quot;.join(newsgroups_train.data[2].split(&quot; n&quot;)[:3])) . Does the Bates method work? I first heard about it in this newsgroup several years ago, and I have just got hold of a book, &#34;How to improve your sight - simple daily drills in relaxation&#34;, by Margaret D. Corbett, . print(&quot; n&quot;.join(newsgroups_train.data[5].split(&quot; n&quot;)[:3])) . Suggest McQuires #1 plastic polish. It will help somewhat but nothing will remove deep scratches without making it worse than it already is. . We can also get the newsgroup category for each from the &#39;target_names&#39; attribute . np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]] . array([&#39;sci.crypt&#39;, &#39;sci.med&#39;, &#39;sci.med&#39;], dtype=&#39;&lt;U21&#39;) . To use this text dataset for topic modelling we will need to convert this into a document-term matrix. This is a matrix where the rows will correspond to to each of the newsgroup posts (a &#39;document&#39; conceptually) and the columns will be for each of the words that exists in all posts (a &#39;term&#39; conceptually). The values of the matrix will be the count of the number of words that exists for a particular post for each post/word combination in the matrix. . . This method of converting text into a count of the words in the text matrix, without regard for anything else (such as order, context etc) is called a bag of words model. We can create this matrix using a CountVectoriser() function. . vectorizer = CountVectorizer(stop_words=&#39;english&#39;) vectors = vectorizer.fit_transform(newsgroups_train.data).todense() # (documents, vocab) vectors.shape . (2351, 32291) . We can see this matrix has the same number of rows as we have posts (2351) and we must have 32,291 unique words accross all posts which is the number of columns we have. . print(len(newsgroups_train.data), vectors.shape) . 2351 (2351, 32291) . If we print the matrix, its just an array of counts for each of the words in each post . vectors . matrix([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 2, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]]) . This matrix does not actually contain the names of the words, so it will be helpful for us to extract these as well to create a vocabulary of terms used in the matrix. We can extract these using get_feature_names() . vocab = np.array(vectorizer.get_feature_names()) vocab.shape . (32291,) . vocab[:32000] . array([&#39;00&#39;, &#39;000&#39;, &#39;0000&#39;, ..., &#39;yarn&#39;, &#39;yarvin&#39;, &#39;yashir&#39;], dtype=&#39;&lt;U79&#39;) . While we have the newsgroup categories here, we will not actually use them for our topic modelling exercise, where we want to create topics independantly based on the posts alone, but we would hope these will correspond to the newsgroup categories in some way, indeed this would be a good check that the topic modelling is working. . Now we have our Document-Term matrix and the vocabulary, we are now ready to use Singular Value Decompostion. . Non-negative Matrix Factorization (NMF) . NMF is a method of matrix decomposition, so for a given matrix A we can convert it into 2 other matrices: W and H. Also A most have non-negative values, and as such W and H will also have non-negative values. . . K is a value we choose in advance, in the case of our intention here K will repesent the number of topics we want to create for our topic model of the newsgroup posts. . So if we assume in the original matrix A for our exercise, N are the documents/posts and M are the words in our Document-Term matrix, each of these matricies represents the following: . W: Feature Matrix this has M rows for words and K columns for the topics, and indicates which words characterise which topics. | H: Coefficient Matrix this has K rows for topics, and N columns for documents/posts, and indicates which topics best describe which documents/posts. | . So one reason NMF can be more popular to use, is due to that fact that the factors it produces are always positive and so are more easily interpretable. Consider for example with SVD we could produce factors that indicated negative values for topics - what would that mean to say a text has &#39;negative indications for the topic of bikes&#39; ? . Another difference with SVD is that NMF is not an exact decompostion - which means if we multiply W and H matrices we won&#39;t get back our original matrix A exactly. . So we can peform NMF on our Document-Term matrix using the sklearn decomposition module. . # Define constants and functions m,n=vectors.shape d=10 # num topics num_top_words=8 def show_topics(a): top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]] topic_words = ([top_words(t) for t in a]) return [&#39; &#39;.join(t) for t in topic_words] . # Calculate NMF %time clf = decomposition.NMF(n_components=d, random_state=1) . CPU times: user 29 µs, sys: 0 ns, total: 29 µs Wall time: 34.3 µs . We can notice here this has run extremely fast taking just 19.6 microseconds. If we recall in an earlier article for the same dataset when we performed one of the fastest versions of SVD Randomised/Trucated SVD this took 20 seconds. . # Extract W and H matrices W1 = clf.fit_transform(vectors) H1 = clf.components_ # Show topics from H matrix print(&#39;Top 10 topics, described by top words in each topic&#39;) show_topics(H1) . Top 10 topics, described by top words in each topic . [&#39;db mov bh si cs byte al bl&#39;, &#39;people said didn know don went just like&#39;, &#39;privacy internet pub eff email information computer electronic&#39;, &#39;health 1993 use hiv medical 10 20 number&#39;, &#39;turkish jews turkey armenian jewish nazis ottoman war&#39;, &#39;anonymous anonymity posting internet anon service people users&#39;, &#39;key encryption des chip ripem use keys used&#39;, &#39;edu com cs david ca uk org john&#39;, &#39;dod rec denizens motorcycle motorcycles doom like terrible&#39;, &#39;version machines contact type edu pc comments ftp&#39;] . So if you recall our original news group categories were: . rec.motorcycles | talk.politics.mideast | sci.med | sci.crypt | . We can see that the topics discovered correspond fairly well to these, bar a few anomalies. . # Show dimensions of matrices print(W1.shape, H1.shape) . (2351, 10) (10, 32291) . The shapes of the matrices also make sense. Given our original matrix A was 2351 rows for posts and 32291 columns for words, and we requested 10 topics this NMF has returned: . Matrix W with 2351 rows for posts and 10 columns for topics | Matrix H with 10 rows for topics and 32291 columns for words | . NMF using Gradient Descent . So in the method just used, we performed NMF using a built in library function from Sklearn. One of the obvious benefits of using this is that it runs extremely fast. However, in order to create this function it took many years of research and expertise in this area. Using this function also means we are limited, if we want to do something slightly different, we can&#39;t really change it. . Alternatively, we can use a very different method to calculate the NMF matrices using Gradient Descent. . The basic process of Gradient Descent is as follows: . Randomly choose some weights to start | Loop: Use weights to calculate a prediction | Calculate the loss (loss is a measure of the difference between the prediction and what we want) | Calculate the derivative of the loss | Update the weights using this derivative to tell us how much to change them | . | Repeat step 2 lots of times. Eventually we end up with some decent weights | In our case, the weights would be the values of the matrices we want to calculate for NMF which are the values of W and H. . In Stocastic Gradient Decent (SGD) we evaluate our loss function on just a sample of our data (sometimes called a mini-batch). We would get different loss values on different samples of the data, so this is why it is stochastic. It turns out that this is still an effective way to optimize, and it&#39;s much more efficient. . SGD is also a key technique used in Deep Learning which I have covered in an earlier article. . Applying SGD to NMF . The Frobenius norm is a way to measure how different two matrices are. We can use this to calculate the loss by multipling W and H together to create a matrix, and then calculating the Frobenius norm between this matrix and our original matrix A to give us our loss value. . Goal: Decompose $A ;(m times n)$ into $$A approx WH$$ where $W ;(m times k)$ and $H ;(k times n)$, $W, ;H ;&gt;= ;0$, and we&#39;ve minimized the Frobenius norm of $A-WH$. . Approach: We will pick random positive $W$ &amp; $H$, and then use SGD to optimize. . We will also make use of the Pytorch library for these calculations for 2 key reasons: . It facilitates calculations on the GPU which enables matrix calculations to be run in parallel and therefore much faster | Pytorch has the autograd functionality which will automatically calculate the derivatives of functions for us and thereby give us the gradients that we need for the process in a convenient way | . # Define constants and functions required lam=1e6 lr = 0.05 # Create W and H matrices pW = Variable(tc.FloatTensor(m,d), requires_grad=True) pH = Variable(tc.FloatTensor(d,n), requires_grad=True) pW.data.normal_(std=0.01).abs_() pH.data.normal_(std=0.01).abs_() # Define report def report(): W,H = pW.data, pH.data print((A-pW.mm(pH)).norm(2).item(), W.min(), H.min(), (W&lt;0).sum(), (H&lt;0).sum()) # Define penalty - encourage positive and low loss values def penalty(P): return torch.pow((P&lt;0).type(tc.FloatTensor)*torch.clamp(P, max=0.), 2) # Define penalise - for both W and H matrices we want to improve def penalize(): return penalty(pW).mean() + penalty(pH).mean() # Define loss - Calculate the Frobenius norm between Matrix A and Matrices W x H def loss(): return (A-pW.mm(pH)).norm(2) + penalize()*lam # Define optimiser to update weights using gradients opt = torch.optim.Adam([pW,pH], lr=1e-3, betas=(0.9,0.9)) # Load our original matrix A onto the GPU t_vectors = torch.Tensor(v.astype(np.float32)).cuda() A = Variable(t_vectors).cuda() . Create and run the Stocastic Gradient Descent process . # For 1000 cycles for i in range(1000): # Clear the previous gradients opt.zero_grad() # Calculate the loss i.e. the Frobenius norm between Matrix A and Matrices W x H l = loss() # Calculate the gradients l.backward() # Update the values of Matrices W x H using the gradients opt.step() # Every 100 cycles print a report of progress if i % 100 == 99: report() lr *= 0.9 # learning rate annealling . 47.2258186340332 tensor(-0.0010, device=&#39;cuda:0&#39;) tensor(-0.0023, device=&#39;cuda:0&#39;) tensor(1013, device=&#39;cuda:0&#39;) tensor(42676, device=&#39;cuda:0&#39;) 46.8864631652832 tensor(-0.0008, device=&#39;cuda:0&#39;) tensor(-0.0027, device=&#39;cuda:0&#39;) tensor(1424, device=&#39;cuda:0&#39;) tensor(53463, device=&#39;cuda:0&#39;) 46.73139572143555 tensor(-0.0004, device=&#39;cuda:0&#39;) tensor(-0.0031, device=&#39;cuda:0&#39;) tensor(929, device=&#39;cuda:0&#39;) tensor(53453, device=&#39;cuda:0&#39;) 46.66544723510742 tensor(-0.0004, device=&#39;cuda:0&#39;) tensor(-0.0020, device=&#39;cuda:0&#39;) tensor(736, device=&#39;cuda:0&#39;) tensor(54012, device=&#39;cuda:0&#39;) 46.620338439941406 tensor(-0.0006, device=&#39;cuda:0&#39;) tensor(-0.0018, device=&#39;cuda:0&#39;) tensor(631, device=&#39;cuda:0&#39;) tensor(56201, device=&#39;cuda:0&#39;) 46.586158752441406 tensor(-0.0003, device=&#39;cuda:0&#39;) tensor(-0.0018, device=&#39;cuda:0&#39;) tensor(595, device=&#39;cuda:0&#39;) tensor(56632, device=&#39;cuda:0&#39;) 46.576072692871094 tensor(-0.0003, device=&#39;cuda:0&#39;) tensor(-0.0019, device=&#39;cuda:0&#39;) tensor(585, device=&#39;cuda:0&#39;) tensor(54036, device=&#39;cuda:0&#39;) 46.573974609375 tensor(-0.0003, device=&#39;cuda:0&#39;) tensor(-0.0018, device=&#39;cuda:0&#39;) tensor(578, device=&#39;cuda:0&#39;) tensor(53401, device=&#39;cuda:0&#39;) 46.573814392089844 tensor(-0.0003, device=&#39;cuda:0&#39;) tensor(-0.0017, device=&#39;cuda:0&#39;) tensor(667, device=&#39;cuda:0&#39;) tensor(52781, device=&#39;cuda:0&#39;) 46.573760986328125 tensor(-0.0003, device=&#39;cuda:0&#39;) tensor(-0.0019, device=&#39;cuda:0&#39;) tensor(662, device=&#39;cuda:0&#39;) tensor(52658, device=&#39;cuda:0&#39;) . # Show topics discovered h = pH.data.cpu().numpy() show_topics(h) . [&#39;msg don people know just food think like&#39;, &#39;clipper chip phone crypto phones government nsa secure&#39;, &#39;armenian armenians turkish genocide armenia turks turkey people&#39;, &#39;jews adam jewish land shostack das harvard arabs&#39;, &#39;com edu pgp mail faq rsa list ripem&#39;, &#39;israel israeli lebanese arab lebanon peace israelis arabs&#39;, &#39;key keys bit chip serial bits 80 number&#39;, &#39;encryption government technology law privacy enforcement administration use&#39;, &#39;geb dsl cadre chastity n3jxp pitt intellect shameful&#39;, &#39;bike bikes ride motorcycle riding dod dog good&#39;] . So if you recall our original news group categories were: . rec.motorcycles | talk.politics.mideast | sci.med | sci.crypt | . We can see that the topics discovered using SGD correspond fairly well to these, bar a few anomalies. . Comparing Approaches . If we compare our two approaches to calculating NMF. . Scikit-Learn&#39;s NMF . Fast | No parameter tuning | Relies on decades of academic research, took experts a long time to implement | Can&#39;t be customised | Method can only be applied to calculating NMF | . Using PyTorch and SGD . Took an hour to implement, didn&#39;t have to be NMF experts | Parameters were fiddly | Not as fast | Easily customised | Method can be applied to a vast range of problems | . Conclusion . In this article we introduced Non-negative Matrix Factorization (NMF) and saw how it could be applied to the task of topic modelling in NLP. We also compared two approaches to calculating NMF using Scikit-Learn&#39;s library function as well as Stocastic Gradient Descent (SGD) and highlighted various pros and cons of each approach. .",
            "url": "https://www.livingdatalab.com/mathematics/linear-algebra/natural-language-processing/2021/12/28/topic-modelling-nmf.html",
            "relUrl": "/mathematics/linear-algebra/natural-language-processing/2021/12/28/topic-modelling-nmf.html",
            "date": " • Dec 28, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "Topic Modelling using Singular Value Decomposition (SVD)",
            "content": "Introduction . Singular Value Decomposition (SVD) is a method from Linear Algebra that is used in a wide range of applications in science and engineering. It can be used for tasks such as dimensionality reduction, image compression, and even understanding entanglement in quantum theory. . Topic modeling is an unsupervised machine learning technique used in Natural Language Processing (NLP) that’s capable of scanning a set of texts, detecting word and phrase patterns within them, and automatically clustering word groups and similar expressions that best characterize a set of documents. . In this article we will will use SVD to perform topic modelling. . This article is based in large part on the material from the fastai linear algebra course. . Dataset . We will use the 20 Newsgroups dataset which consists of 20,000 messages taken from 20 different newsgroups from the Usenet bulletin board service, which pre-dates the world-wide-web and websites. We will look at a subset of 4 of these newsgroup categories: . rec.motorcycles | talk.politics.mideast | sci.med | sci.crypt | . We will now get this data. . categories = [&#39;rec.motorcycles&#39;, &#39;talk.politics.mideast&#39;, &#39;sci.med&#39;, &#39;sci.crypt&#39;] remove = (&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;) newsgroups_train = fetch_20newsgroups(subset=&#39;train&#39;, categories=categories, remove=remove) newsgroups_test = fetch_20newsgroups(subset=&#39;test&#39;, categories=categories, remove=remove) . Let&#39;s check how many posts this gives us in total . newsgroups_train.filenames.shape, newsgroups_train.target.shape . ((2351,), (2351,)) . Let&#39;s print the first few lines of 3 of the posts to see what the text looks like . print(&quot; n&quot;.join(newsgroups_train.data[0].split(&quot; n&quot;)[:3])) . I am not an expert in the cryptography science, but some basic things seem evident to me, things which this Clinton Clipper do not address. . print(&quot; n&quot;.join(newsgroups_train.data[2].split(&quot; n&quot;)[:3])) . Does the Bates method work? I first heard about it in this newsgroup several years ago, and I have just got hold of a book, &#34;How to improve your sight - simple daily drills in relaxation&#34;, by Margaret D. Corbett, . print(&quot; n&quot;.join(newsgroups_train.data[5].split(&quot; n&quot;)[:3])) . Suggest McQuires #1 plastic polish. It will help somewhat but nothing will remove deep scratches without making it worse than it already is. . We can also get the newsgroup category for each from the &#39;target_names&#39; attribute . np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]] . array([&#39;sci.crypt&#39;, &#39;sci.med&#39;, &#39;sci.med&#39;], dtype=&#39;&lt;U21&#39;) . To use this text dataset for topic modelling we will need to convert this into a document-term matrix. This is a matrix where the rows will correspond to to each of the newsgroup posts (a &#39;document&#39; conceptually) and the columns will be for each of the words that exists in all posts (a &#39;term&#39; conceptually). The values of the matrix will be the count of the number of words that exists for a particular post for each post/word combination in the matrix. . . This method of converting text into a count of the words in the text matrix, without regard for anything else (such as order, context etc) is called a bag of words model. We can create this matrix using a CountVectoriser() function. . vectorizer = CountVectorizer(stop_words=&#39;english&#39;) vectors = vectorizer.fit_transform(newsgroups_train.data).todense() # (documents, vocab) vectors.shape . (2351, 32291) . We can see this matrix has the same number of rows as we have posts (2351) and we must have 32,291 unique words accross all posts which is the number of columns we have. . print(len(newsgroups_train.data), vectors.shape) . 2351 (2351, 32291) . If we print the matrix, its just an array of counts for each of the words in each post . vectors . matrix([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 2, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]]) . This matrix does not actually contain the names of the words, so it will be helpful for us to extract these as well to create a vocabulary of terms used in the matrix. We can extract these using get_feature_names() . vocab = np.array(vectorizer.get_feature_names()) vocab.shape . (32291,) . vocab[:32000] . array([&#39;00&#39;, &#39;000&#39;, &#39;0000&#39;, ..., &#39;yarn&#39;, &#39;yarvin&#39;, &#39;yashir&#39;], dtype=&#39;&lt;U79&#39;) . While we have the newsgroup categories here, we will not actually use them for our topic modelling exercise, where we want to create topics independantly based on the posts alone, but we would hope these will correspond to the newsgroup categories in some way, indeed this would be a good check that the topic modelling is working. . Now we have our Document-Term matrix and the vocabulary, we are now ready to use Singular Value Decompostion. . Singular Value Decomposition (SVD) . SVD is a method of matrix decomposition, so for a given matrix A we can convert it into 3 other matrices: U, $ sum_{}$, and $V^{T}$ . . R is a value we choose in advance, in the case of our intention here R will repesent the number of topics we want to create for our topic model of the newsgroup posts. . Each of these matricies represents the following . U: Left singular vectors this has the same number of rows as our original matrix A (m rows/posts) and a column for each of our chosen number of topics (r columns). This matrix has orthogonal (or orthonormal) columns i.e. vectors along the r topics column axis. | $ sum_{}$: Singular values has r rows by r columns, in our case this means topics by topics. This represents the ranked relative importance of each topic so the most important topic is topic 1 which is in row 1, column 1 - and the value at this index will be a measure of the importance, and so on for topic 2 etc. This is a matrix of diagonal singular values (all other values off the diagonal are zero). | $V^{T}$: Right singular vectors this has the same number of columns as our original matrix A (n columns) and a row for each of our chosen number of topics (r rows) | . If we were to choose a R value equal to N this would be an exact decompostion of the matrix A, which would mean if we were to multiply U, $ sum_{}$, and $V^{T}$ we would get back exactly the same matrix A. . However there are many reasons why in practice we may not want to do a full decompostion, including in the case of large matricies this can be extermely time consuming, and often we may not require all potential topics, just the most important. So in practice we are likely to choose a value for R that is far smaller than N. . Latent Semantic Analysis (LSA) or Latent Semantic Index (LSI) is a common name given to applying SVD to topic modelling in NLP in this way i.e. using a Document-Term matrix. . Another way to think about SVD more generally is that whatever is represented by a matrix A by columns M and N, is mapped into a &#39;latent space&#39; defined by the R dimension. Futhermore, this mapping is done in such a way that co-occuring values of N are projected into the same R dimensions with higher values, and conversley non-couccuring values on N are projected into different R dimensions. . In other words, the latent space R dimensions allow us to show which M are similar or different based on their values of N. . So we can peform full SVD on our Document-Term matrix using the scipy linalg module. . %time U, s, Vh = linalg.svd(vectors, full_matrices=False) . CPU times: user 1min 55s, sys: 5.34 s, total: 2min Wall time: 1min 2s . print(U.shape, s.shape, Vh.shape) . (2351, 2351) (2351,) (2351, 32291) . This has performed a full SVD, and took around 2 mins. . We can test that this is a decomposition by multipling these matrices and checking if they are close to equal to the original matrix using the allclose() function from numpy. . # Confirm that U, s, Vh is a decomposition of the var Vectors # Multiply matrices reconstructed_vectors = U @ np.diag(s) @ Vh # Calculate the Frobenius norm between the original matrix A and this reconstructed one - which is a measure of the distance/differences between these matrices np.linalg.norm(reconstructed_vectors - vectors) . 4.063801905115974e-12 . # Check if two matrices are approximately equal within a small difference np.allclose(reconstructed_vectors, vectors) . True . We can also check that U and Vh are orthonormal matrices. If we multiply these by their transpose this should be close to equal to the identity matrix for each of these (by definition).. . # Confirm that U, Vh are orthonormal np.allclose(U.T @ U, np.eye(U.shape[0])) np.allclose(Vh @ Vh.T, np.eye(Vh.shape[0])) . True . If we look at the singular values matrix, we can get an idea of the relative importance of each of the topics (topics on x axis) . plt.plot(s) plt.xlabel(&#39;Topic number&#39;) plt.ylabel(&#39;Importance&#39;) . Text(0, 0.5, &#39;Importance&#39;) . Let&#39;s have a look at the topics discovered by SVD, we will do this by looking at the top 8 words that score most highly for each topic. This will be orderded by most important topic first. . num_top_words=8 def show_topics(a): top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]] topic_words = ([top_words(t) for t in a]) return [&#39; &#39;.join(t) for t in topic_words] print(&#39;Top 10 topics, described by top words in each topic&#39;) show_topics(Vh[:10]) . Top 10 topics, described by top words in each topic . [&#39;melittin wimp disgruntled rebelling toxin sorta bikeless litte&#39;, &#39;db mov bh si bl di maxbyte cx&#39;, &#39;said didn people know don went apartment came&#39;, &#39;health 1993 hiv medical use 10 number 20&#39;, &#39;edu com anonymous health posting anon service cs&#39;, &#39;key privacy eff pub encryption use law health&#39;, &#39;internet email privacy anonymous anonymity health eff hiv&#39;, &#39;anonymous posting anonymity use anon key users postings&#39;, &#39;com edu encryption privacy government said chip technology&#39;, &#39;version machines contact type pc comments ftp keyboard&#39;] . So if you recall our original news group categories were: . rec.motorcycles | talk.politics.mideast | sci.med | sci.crypt | . We can see that the topics discovered correspond fairly well to these, bar a few anomalies. . Truncated SVD . So we saw from our attempt at full SVD was quite slow to calculate (approx 2 mins) we can imagine this is likely to get far worse with bigger matrices. We also know that perhaps we don&#39;t need to calculate a full set of topics, especially given for most practical applications we are most likely interested in using the strongest topics that distinguish posts, rather than topics that are not very useful. The approaches to calculate full SVD use particular algorithms to create the decomposition, and Halko et al highlighted some of the key disadvantages of this approach: . Matrices are &quot;stupendously big&quot; | Data are often missing or inaccurate. Why spend extra computational resources when imprecision of input limits precision of the output? | Data transfer now plays a major role in time of algorithms. Techniques the require fewer passes over the data may be substantially faster, even if they require more flops (flops = floating point operations). | Important to take advantage of GPUs. | . In the same paper, Halko et al argued for the advantages of using randomised approaches which include: . They are inherently stable | Performance guarantees do not depend on subtle spectral properties | Needed matrix-vector products can be done in parallel i.e. on a GPU | . So Truncated SVD using a randomised approach, allows us to calculate just the largest singular values and the corresponding matrices, which should be much quicker to calculate. . We can use sklearn&#39;s decomposition module to calculated randomised SVD, we will specify the top 10 topics only. . %time u, s, v = decomposition.randomized_svd(vectors, 10) . CPU times: user 18.7 s, sys: 2.09 s, total: 20.8 s Wall time: 15.2 s . Lets see the top 10 topics its discovered. . show_topics(v) . [&#39;db mov bh si cs byte al bl&#39;, &#39;people said know don didn anonymous privacy internet&#39;, &#39;privacy internet anonymous information pub email eff use&#39;, &#39;health 1993 hiv medical use 10 number 20&#39;, &#39;turkish jews turkey key privacy government armenian eff&#39;, &#39;turkish edu jews com turkey anonymous jewish nazis&#39;, &#39;key edu encryption des com ripem chip keys&#39;, &#39;com edu pub eff ftp electronic org computer&#39;, &#39;dod rec denizens motorcycle motorcycles doom ftp terrible&#39;, &#39;version machines contact type pc comments ftp keyboard&#39;] . So this is much faster taking a total of 20 seconds for randomised SVD compared to the full SVD of 2 minutes. . Facebook Research implemented a version of Randomised SVD based on the Halko paper. . Conclusion . In this article we introduced Singular Value Decomposition (SVD) and saw how it could be applied to the task of topic modelling in NLP. We also saw how this could be optimised for speed when only concerned with the most important topics, using truncated SVD implemented using a randomised approach. .",
            "url": "https://www.livingdatalab.com/mathematics/linear-algebra/natural-language-processing/2021/12/27/topic-modelling-svd.html",
            "relUrl": "/mathematics/linear-algebra/natural-language-processing/2021/12/27/topic-modelling-svd.html",
            "date": " • Dec 27, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "Python Power Tools for Data Science - Pycaret",
            "content": "Python Power Tools for Data Science . In this series of articles Python Power Tools for Data Science I will be looking at a series of python tools that can make a significant improvement on common Data Science tasks. In particular, Python Power Tools are python tools that can significantly automate or simplify common tasks a Data Scientist would need to perform. . Automation and simplifcation of common tasks can bring many benefits such as: . Less time needed to complete tasks | Reduction of mistakes due to less complex code | Improved readability and understanding of code | Increased consistancy of approach to different problems | Easier reproducability, verification, and comparison of results | . Pycaret . Pycaret is a low code python library that aims to automate many tasks required for machine learning. Tasks that would usually take hundreds of lines of code can often be replaced with just a couple of lines. It was inspired by the Caret library in R. . In comparison with the other open-source machine learning libraries, PyCaret is an alternate low-code library that can be used to replace hundreds of lines of code with few words only. This makes experiments exponentially fast and efficient. PyCaret is essentially a Python wrapper around several machine learning libraries and frameworks such as scikit-learn, XGBoost, LightGBM, CatBoost, spaCy, Optuna, Hyperopt, Ray, and many more. (Pycaret Documentation) . Pycaret has different modules specialised for different machine learning use-cases these include:- Classification- Regression . Clustering | Anomaly Detection | Natural Language Processing | Assocation Rule Mining | Time Series | . See further articles about these other Pycaret modules and what they can offer. . In this article to demonstrate the caperbilities of Pycaret we will use the classification module which has over 18 algorithms and 14 plots to analyze the results, plus many other features. . Dataset - Palmer Penguins . We will use Pycaret on the Palmer Penguins Dataset which contains size and other measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica. We will use the Pycaret classification module to train a model to predict the penguin species category. Given there are 3 species of Penguin, this would be considered a Multiclass classification problem . . # Load penguins dataset and show first few rows penguins_df = load_penguins() penguins_df.head() . species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year . 0 Adelie | Torgersen | 39.1 | 18.7 | 181.0 | 3750.0 | male | 2007 | . 1 Adelie | Torgersen | 39.5 | 17.4 | 186.0 | 3800.0 | female | 2007 | . 2 Adelie | Torgersen | 40.3 | 18.0 | 195.0 | 3250.0 | female | 2007 | . 3 Adelie | Torgersen | NaN | NaN | NaN | NaN | NaN | 2007 | . 4 Adelie | Torgersen | 36.7 | 19.3 | 193.0 | 3450.0 | female | 2007 | . # Some more info on the data penguins_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 344 entries, 0 to 343 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 species 344 non-null object 1 island 344 non-null object 2 bill_length_mm 342 non-null float64 3 bill_depth_mm 342 non-null float64 4 flipper_length_mm 342 non-null float64 5 body_mass_g 342 non-null float64 6 sex 333 non-null object 7 year 344 non-null int64 dtypes: float64(4), int64(1), object(3) memory usage: 21.6+ KB . # Percentage of penguins of each species in dataset penguins_df[&#39;species&#39;].value_counts(normalize=True) . Adelie 0.441860 Gentoo 0.360465 Chinstrap 0.197674 Name: species, dtype: float64 . . We can see that the dataset has different proportions of each penguin species. . The data consists of a mixture of numeric and categorical data, which should help us test the caperbilities of Pycaret with regards to the machine learning workflow. . Data Preparation . We will split our data into a training and test subset of our data to validate our final trained classification model on, this needs to be done without the use of Pycaret. We will ensure that our training and testing subsets have the same proportion for each penguin species as the original dataset. . # Split data into train/test and stratified on target class X = penguins_df.iloc[:,1:] Y = penguins_df[&#39;species&#39;] X_train, X_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.1) train_df = X_train train_df[&#39;species&#39;] = y_train test_df = X_test test_df[&#39;species&#39;] = y_test # Verify datasets have same proportion of each penguin species as the original print(train_df.shape) print(test_df.shape) print(train_df[&#39;species&#39;].value_counts(normalize=True)) print(test_df[&#39;species&#39;].value_counts(normalize=True)) . (309, 8) (35, 8) Adelie 0.443366 Gentoo 0.359223 Chinstrap 0.197411 Name: species, dtype: float64 Adelie 0.428571 Gentoo 0.371429 Chinstrap 0.200000 Name: species, dtype: float64 . Pycaret workflow . Setup . The Pycaret setup() is the first part of the workflow that always needs to be performed, and is a function that takes our data in the form of a pandas dataframe as well as the name of the target class to predict, and performs a number of tasks to get reading for the machine learning pipeline. . # Prepare data for further processing predict_penguin_species_experiment = setup(data = train_df, target = &#39;species&#39;, session_id=123) . Description Value . 0 session_id | 123 | . 1 Target | species | . 2 Target Type | Multiclass | . 3 Label Encoded | Adelie: 0, Chinstrap: 1, Gentoo: 2 | . 4 Original Data | (309, 8) | . 5 Missing Values | True | . 6 Numeric Features | 4 | . 7 Categorical Features | 3 | . 8 Ordinal Features | False | . 9 High Cardinality Features | False | . 10 High Cardinality Method | None | . 11 Transformed Train Set | (216, 13) | . 12 Transformed Test Set | (93, 13) | . 13 Shuffle Train-Test | True | . 14 Stratify Train-Test | False | . 15 Fold Generator | StratifiedKFold | . 16 Fold Number | 10 | . 17 CPU Jobs | -1 | . 18 Use GPU | False | . 19 Log Experiment | False | . 20 Experiment Name | clf-default-name | . 21 USI | ee22 | . 22 Imputation Type | simple | . 23 Iterative Imputation Iteration | None | . 24 Numeric Imputer | mean | . 25 Iterative Imputation Numeric Model | None | . 26 Categorical Imputer | constant | . 27 Iterative Imputation Categorical Model | None | . 28 Unknown Categoricals Handling | least_frequent | . 29 Normalize | False | . 30 Normalize Method | None | . 31 Transformation | False | . 32 Transformation Method | None | . 33 PCA | False | . 34 PCA Method | None | . 35 PCA Components | None | . 36 Ignore Low Variance | False | . 37 Combine Rare Levels | False | . 38 Rare Level Threshold | None | . 39 Numeric Binning | False | . 40 Remove Outliers | False | . 41 Outliers Threshold | None | . 42 Remove Multicollinearity | False | . 43 Multicollinearity Threshold | None | . 44 Remove Perfect Collinearity | True | . 45 Clustering | False | . 46 Clustering Iteration | None | . 47 Polynomial Features | False | . 48 Polynomial Degree | None | . 49 Trignometry Features | False | . 50 Polynomial Threshold | None | . 51 Group Features | False | . 52 Feature Selection | False | . 53 Feature Selection Method | classic | . 54 Features Selection Threshold | None | . 55 Feature Interaction | False | . 56 Feature Ratio | False | . 57 Interaction Threshold | None | . 58 Fix Imbalance | False | . 59 Fix Imbalance Method | SMOTE | . Calling the setup() function with one line of code does the following in the background: . Data types will be inferred for each column | A table of key information about the dataset and configuration settings is generated | Included in this table are the names of the target categories and the numbers they will be encoded as | Based on the types inferred and configuration chosen, the dataset will be transformed to be ready for the machine learning algorithms | Split the data into training and validation (test) sets | . Various configuration settings are available, but defaults are selected so none are required. . Some key configuration settings available include: . Missing numeric values are imputed (default: mean) iterative option uses lightgbm model to estimate values | Missing categorical values are imputed (default: constant dummy value, alteratives include mode and iterative) | Encode categorical values as ordinal e.g. ‘low’, ‘medium’, ‘high’ | High cardinality (default: false) options to compress to fewer levels or replace with frequency or k-means clustering derived class. | Define date fields explictly | Ignore fields for training models | Normalise numeric fields (default: false) options include zscore, minmax, maxabs, robust | Power transforms (default: false) will transform to make data more gaussian options include yeo-johnson, quantile | PCA: Principal components analysis (default: false) reduce the dimensionality of the data down to a specified number of components | Remove outliers from training data (using SVD) | Remove features with high correlations with each other | Create cluster category based on data | Automatic feature selection (using ensemble models to identify best features) | Fix target class imbalance using SMOTE synthentic data generation or resampling | Stratify train-test split of datasetby target variable | Various cross-validation strategies for splitting data for model training | . Comparing All Models . In Pycaret we can use a single line command compare_models() to train 14 different classification models on our data with default parameters to find the best model. Each model is trained using cross-fold validation accross multiple folds (default 10) and the average metric scores for multiple classification metrics are shown, including Accuracy, F1, etc. . The results are shown in a grid, ranked by highest scoring on Accuracy by default. . # Train all classification models on data with default parameters using cross-fold validation best_model = compare_models() . Model Accuracy AUC Recall Prec. F1 Kappa MCC TT (Sec) . ridge Ridge Classifier | 0.9955 | 0.0000 | 0.9917 | 0.9959 | 0.9952 | 0.9927 | 0.9930 | 0.013 | . lda Linear Discriminant Analysis | 0.9955 | 1.0000 | 0.9917 | 0.9959 | 0.9952 | 0.9927 | 0.9930 | 0.016 | . lr Logistic Regression | 0.9907 | 1.0000 | 0.9875 | 0.9916 | 0.9905 | 0.9852 | 0.9858 | 0.423 | . rf Random Forest Classifier | 0.9814 | 0.9988 | 0.9755 | 0.9832 | 0.9811 | 0.9706 | 0.9716 | 0.464 | . et Extra Trees Classifier | 0.9814 | 0.9987 | 0.9717 | 0.9840 | 0.9810 | 0.9701 | 0.9715 | 0.460 | . lightgbm Light Gradient Boosting Machine | 0.9766 | 0.9996 | 0.9721 | 0.9797 | 0.9765 | 0.9630 | 0.9643 | 0.090 | . gbc Gradient Boosting Classifier | 0.9721 | 0.9974 | 0.9630 | 0.9761 | 0.9708 | 0.9556 | 0.9580 | 0.250 | . dt Decision Tree Classifier | 0.9580 | 0.9685 | 0.9565 | 0.9638 | 0.9584 | 0.9353 | 0.9375 | 0.015 | . ada Ada Boost Classifier | 0.9494 | 0.9772 | 0.9356 | 0.9574 | 0.9486 | 0.9202 | 0.9241 | 0.093 | . nb Naive Bayes | 0.8333 | 0.9958 | 0.8726 | 0.9139 | 0.8388 | 0.7522 | 0.7853 | 0.016 | . knn K Neighbors Classifier | 0.7636 | 0.8905 | 0.6803 | 0.7660 | 0.7498 | 0.6143 | 0.6264 | 0.116 | . dummy Dummy Classifier | 0.4355 | 0.5000 | 0.3333 | 0.1904 | 0.2647 | 0.0000 | 0.0000 | 0.016 | . svm SVM - Linear Kernel | 0.4310 | 0.0000 | 0.3810 | 0.3575 | 0.3068 | 0.0860 | 0.1481 | 0.062 | . qda Quadratic Discriminant Analysis | 0.1758 | 0.0000 | 0.3333 | 0.0312 | 0.0529 | 0.0000 | 0.0000 | 0.018 | . We can see that the Extra Trees Classifier is the best performing model, which we would normally choose. For this example we will select a model that performs less well so has some mistakes, which will be useful later - so we will choose to use the Randon Forrest (rf) classifier. . Selecting and Fine Tuning the Model . So we will create a Random Forrest Model. When we do this, it will train the model on the training data, using cross-fold validation (default 10 folds) and show the metrics for each fold iteration. This will train our model with default parameters, so should give us the same result as we observed in the compare models process. . # Create and train the random forrest model on our data rf = create_model(&#39;rf&#39;) . Accuracy AUC Recall Prec. F1 Kappa MCC . 0 0.9545 | 1.0000 | 0.9167 | 0.9591 | 0.9525 | 0.9269 | 0.9302 | . 1 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 2 0.9545 | 0.9880 | 0.9167 | 0.9591 | 0.9525 | 0.9269 | 0.9302 | . 3 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 4 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 5 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 6 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 7 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 8 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 9 0.9048 | 1.0000 | 0.9213 | 0.9143 | 0.9058 | 0.8521 | 0.8552 | . Mean 0.9814 | 0.9988 | 0.9755 | 0.9832 | 0.9811 | 0.9706 | 0.9716 | . SD 0.0312 | 0.0036 | 0.0375 | 0.0281 | 0.0313 | 0.0489 | 0.0476 | . We can also print some details about our trained model. . # Print model summary print(rf) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=False, random_state=123, verbose=0, warm_start=False) . We can now fine tune our model to optimise parameters to get our best model using tune_model. This process uses Random Grid Search to find the best combination of parameters that produces the highest score. This will output the results of the cross-fold validation from our best model. . # Fine tune our model using Random Grid Search on parameters tuned_rf = tune_model(rf) . Accuracy AUC Recall Prec. F1 Kappa MCC . 0 0.9545 | 1.0000 | 0.9167 | 0.9591 | 0.9525 | 0.9269 | 0.9302 | . 1 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 2 0.9545 | 0.9819 | 0.9167 | 0.9591 | 0.9525 | 0.9269 | 0.9302 | . 3 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 4 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 5 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 6 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 7 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 8 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | 1.0000 | . 9 0.9524 | 0.9893 | 0.9630 | 0.9619 | 0.9536 | 0.9263 | 0.9297 | . Mean 0.9861 | 0.9971 | 0.9796 | 0.9880 | 0.9859 | 0.9780 | 0.9790 | . SD 0.0212 | 0.0060 | 0.0333 | 0.0183 | 0.0216 | 0.0336 | 0.0321 | . We can observe that the grid search has improved our model Accuracy. . Model Evaluation . Once we have our best model, it&#39;s normal practice to look at the details of how its performing, what classification errors it makes, and what it gets correct.e can do this through a series of plots. The plot_model() function in Pycaret allows us to easily display a range of these plots to help with this. . A confusion matrix is a very common plot to show the details of classification predicted vs actual results which we can plot with one line. . # Plot confusion matrix plot_model(tuned_rf, plot = &#39;confusion_matrix&#39;) . We can see that our fine-tuned model only makes one mistake, predicting a penguin of class 0 as a class 2 penguin. Referring to our table from the setup() function we can see that the penguin species target class has the following number encodings: . Adelie: 0 | Chinstrap: 1 | Gentoo: 2 | . So it has predicted a Adelie penguin as a Gentoo penguin! . We can also plot a decision boundry for the model to see how it divides the parameter space to be able to classify the penguins. . # Plot model descision boundary plot_model(tuned_rf, plot=&#39;boundary&#39;) . We can see that for class 2 (Gentoo) penguins, there is a well defined decision boundry. However the decision boundry between the Adelie and Chinstrap penguins is more messy, implying its harder to distinguish between these two types of penguins. We will make a note of this for later. . We can also total up the errors in a error bar plot in Pycaret. . # Plot class prediction error bar plot plot_model(tuned_rf, plot = &#39;error&#39;) . Here we can see our one case of an Adelie penguin (blue/0) predicted as a Gentoo penguin (red/2) again. . Another common plot when trying to understand how our model works is a feature importance plot. This plot will show us the most important features for the model to be able to predict the penguin species class. . Again we can create this plot with one line of Pycaret. . # Plot feature importance plot_model(tuned_rf, plot = &#39;feature&#39;) . So it seems like bill length and flipper length are two of the most important features to help predict penguin species. . The interpret_model() function is available to use a Game Theory approach on the model predictions on training data to explain the output of the model. This is mostly based upon the python SHAP package. However this can only be used with tree-based models, which is why we deliberately chose the Random Forrest classifier earlier to be able to demonstrate this feature. . # Plot shapley values for model interpretation interpret_model(tuned_rf) . Prepare Model for Use . Once we are happy with our final model, we can prepare it for us with a range of functions. We can create our final model for deployment using the finalise_model() function, which will train the model on the entire training dataset. . # Train final model on all training data final_rf = finalize_model(tuned_rf) print(final_rf) . RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=&#39;balanced_subsample&#39;, criterion=&#39;entropy&#39;, max_depth=4, max_features=&#39;log2&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0002, min_impurity_split=None, min_samples_leaf=5, min_samples_split=9, min_weight_fraction_leaf=0.0, n_estimators=130, n_jobs=-1, oob_score=False, random_state=123, verbose=0, warm_start=False) . We can now test our final model on the holdout dataset we kept at the start, to get further confirmation of its performance. We can use the predict_model() function using our final model and the holdout test dataset to generate a set if predictions. . This will also automatically apply any data transformations we configured in our setup() function to this new test dataset before the data is passed to the model. . # Use holdout test dataset to generate predictions for final model new_predictions = predict_model(final_rf, data=test_df) new_predictions.head() . island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year species Label Score . 263 Biscoe | 49.8 | 15.9 | 229.0 | 5950.0 | male | 2009 | Gentoo | Gentoo | 0.9857 | . 216 Biscoe | 45.8 | 14.2 | 219.0 | 4700.0 | female | 2008 | Gentoo | Gentoo | 0.9802 | . 68 Torgersen | 35.9 | 16.6 | 190.0 | 3050.0 | female | 2008 | Adelie | Adelie | 0.9280 | . 55 Biscoe | 41.4 | 18.6 | 191.0 | 3700.0 | male | 2008 | Adelie | Adelie | 0.9251 | . 206 Biscoe | 46.5 | 14.4 | 217.0 | 4900.0 | female | 2008 | Gentoo | Gentoo | 0.9851 | . Note the predicted penguin class is in the newly created Label column. The actual penguin species is still in the original species column. We can use Pycaret&#39;s utility check_metric() function to apply a metric to our predictions, in this case we will calculate the F1 classification metric. . # Evaluate final model on test dataset predictions check_metric(new_predictions[&#39;species&#39;], new_predictions[&#39;Label&#39;], metric = &#39;F1&#39;) . 1.0 . So we can see our final model has performed exteremely well on our holdout test data, getting a perfect score of 1.0. . We can now save our final model using the save_model() function. . # Save final model (and data transformation pipeline process) save_model(final_rf,&#39;Final Penguin Model&#39;) . Transformation Pipeline and Model Successfully Saved . (Pipeline(memory=None, steps=[(&#39;dtypes&#39;, DataTypes_Auto_infer(categorical_features=[], display_types=True, features_todrop=[], id_columns=[], ml_usecase=&#39;classification&#39;, numerical_features=[], target=&#39;species&#39;, time_features=[])), (&#39;imputer&#39;, Simple_Imputer(categorical_strategy=&#39;not_available&#39;, fill_value_categorical=None, fill_value_numerical=None, numeric_stra... RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=&#39;balanced_subsample&#39;, criterion=&#39;entropy&#39;, max_depth=4, max_features=&#39;log2&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0002, min_impurity_split=None, min_samples_leaf=5, min_samples_split=9, min_weight_fraction_leaf=0.0, n_estimators=130, n_jobs=-1, oob_score=False, random_state=123, verbose=0, warm_start=False)]], verbose=False), &#39;Final Penguin Model.pkl&#39;) . Our saved model is easily re-loaded for use using the load_model() function. Note this also loads any data transformation configured as well specifed in our original setup() function. . # Load final model (and data transformation pipeline process) saved_final_rf = load_model(&#39;Final Penguin Model&#39;) . Transformation Pipeline and Model Successfully Loaded . Review . Overall, Pycaret is an incredibly useful and powerful library for speeding up and automating the machine learning pipeline and process. Lets highlight some key pros and cons. . Pros . Less code: The library really lives up to its motto of being a &#39;low code&#39; library, often one of code will replace what would normally have been an entire manually coded process of many lines of code. Accross a whole project, as we have seen in this example project, hundreds of lines of code can be replace by just a few lines. Note how most of this article length is more due to describing what the code does, than the code itself! . Easy to use: Pycaret library functions are well named, intiutive and easy to use, and easy to customise and configure. . A more consistant approach: Another benefit of being a low code library where most processes have been automated is that this ensures a more consistant approach when using Pycaret accross different projects. This is important not only for scientific reproducability, but for reducing the possibility of errors that are more likely when more custom and manual code is required to be written for a process. . Good practice: Each step of the machine learning pipeline that Pycaret simplifies and automates for you, does so in such a way to bake in best practice in Data Science. For example, when testing models cross-fold validation is done by default on all models. When evaluating models, multiple and relevant metrics are used to evaluate performance. . Performs all key tasks and more: Pycaret automates every key task in machine learning process, from wrangling to preparing your data, for selecting a model, for optimising and evaluating a final model, then testing and saving a model ready for deployment and use. In addition, Pycaret offers easy access to extra functions while not always required, can be useful for particular projects - for example the ability to calculate Shapley values as we have seen for model interpretability. . Educational: Using this library helps all kinds of users, from amateurs to professional Data Scientists, keep up to date with the latest methods and techniques. For example, Pycaret maintains a list of the most widely used models which are included automatically when selecting a potential model. For model understanding and interpretation, a wide range of plots and analyses are available. I was not fully aware for example about Shapley values, and how they can help interpret models from a very different perspective. These are some of the many advantages of having an open source library like Pycaret that&#39;s intended to automate the Data Science process, everyone&#39;s collaberative effort to use and update the library helps keep highlighting and providing some of the latest and best techniques to all who use it. . Excellent data wrangling and transformation: As we saw with the setup() function there are many useful features available to perform many common tasks that would normally require many lines of code. For example, the inclusion of the SMOTE and resampling techniques often used to correct for imbalances in the target variable in a dataset. Sensible automatic imputation methods by default to deal with missing values, and normalisation methods to scale and prepare numeric data - are key common tasks that need to be performed, expertly automated by the Pycaret library. . Quick consideration of a wide range of models: Pycaret&#39;s compare_models(), create_model() and tune_model() functions allow you to quickly compare a wide range of the best models available (currently 18), then select and optimise the best model - in just 3 lines of code. . Creating a pipeline not just a model: The machine learning process is not just about producing a good model, you also need a process to transform the data into a format required for that model. This is often consider a separate bit of extra work, often referred to as an ETL process. (Extract, Transform &amp; Load). Pycaret blends these two essential things together for you, another benefit of the automation it provides, so when you save your model, you also save this data transformation process, all together. And when you load it ready for use, you load the data transformation and the model together - ready for immediate use - a huge saving of time and effort. . These are just some of the key pros of the Pycaret library, in my opinion there are many many more. To illustrate what a huge advance and benefit the Pycaret library is in the pros highlighted, compare this to a previous machine learning project of mine to classify breast cancer data, where I used the common and more manual process of many more lines of code for each part of the machine learning pipeline. . Cons . Not good for beginners: Despite being pitched for begginners, this library may not be ideal for beginners in my opinion. While the functions are easy for a beginner to use, and indeed as highlighted you can run the entire machine learning process very easily, I would say this can be a bit deceptive and misleading. Simply running the process with little understanding what is going on underneath, is not a substitute for understanding the basics. For example when, why &amp; how should we transform data? (e.g. normalisation of numeric values) which is the most appropriate metric to interpret results? (e.g. balanced vs imbalanced target variable). . No ability to customose plots: This is perhaps a minor issue, but it would be nice to be able to customise plots at least a little for example to adjust the size of plots. . Can&#39;t easily see what is going on under the hood: In a way, this is I feel both a Pro and a Con. If you know what is going on with these automated functions underneath, then to some extent it can be nice to not be overloaded with lots of detail about it. On the other hand, for both experienced Data Scientist&#39;s and begginners it can be helpful to actually understand more of what each automated function is doing. Many functions do give some insight as to what they are doing, but many things are hidden - and can only be discovered by reading the documentation, which I would suggest is a good idea for anyone using this library, experienced or not. But again I feel this is a relatively minor con, as its a difficult balance to achieve in the trade off between simplifying and automating the process vs making every part of the process transparent. . Conclusion . In this article we have looked at Pycaret as a potential Python Power Tool for Data Science. . While it does have some minor drawbacks in my view, overall I would say Pycaret is an incredibly useful and powerful tool that helps simplify the machine learning process. I will be using Pycaret from now on in my day to day Data Science work by default - I&#39;m hugely impressed by this library and its ongoing development. . In my honest opinion, I have no doubt in declaring the Pycaret is indeed a Python Power Tool for Data Science. .",
            "url": "https://www.livingdatalab.com/python-power-tools/pycaret/2021/12/04/python-power-tools-pycaret.html",
            "relUrl": "/python-power-tools/pycaret/2021/12/04/python-power-tools-pycaret.html",
            "date": " • Dec 4, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "Python Power Tools for Data Science - Pandas Profilling",
            "content": "Python Power Tools for Data Science . In this series of articles Python Power Tools for Data Science I will be looking at a series of python tools that can make a significant improvement on common Data Science tasks. In particular, Python Power Tools are python tools that can significantly automate or simplify common tasks a Data Scientist would need to perform. . Automation and simplifcation of common tasks can bring many benefits such as: . Less time needed to complete tasks | Reduction of mistakes due to less complex code | Improved readability and understanding of code | Increased consistancy of approach to different problems | Easier reproducability, verification, and comparison of results | . Pandas Profilling . Pandas Profilling is a python library that takes a Pandas dataframe and can perform Exploratory Data Analysis (EDA) automatically on it with just one line of code. In this article we will apply this library to a test dataset and then evaluate how good it is to helping with the task of EDA. . Dataset - Palmer Penguins . We will use Pandas Profilling on the Palmer Penguins Dataset which contains size measurements for three penguin species observed on three islands in the Palmer Archipelago, Antarctica. . . # Load penguins dataset and show first few rows penguins_df = load_penguins() penguins_df.head() . species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year . 0 Adelie | Torgersen | 39.1 | 18.7 | 181.0 | 3750.0 | male | 2007 | . 1 Adelie | Torgersen | 39.5 | 17.4 | 186.0 | 3800.0 | female | 2007 | . 2 Adelie | Torgersen | 40.3 | 18.0 | 195.0 | 3250.0 | female | 2007 | . 3 Adelie | Torgersen | NaN | NaN | NaN | NaN | NaN | 2007 | . 4 Adelie | Torgersen | 36.7 | 19.3 | 193.0 | 3450.0 | female | 2007 | . . The data consists of a mixture of numeric and categorical data, which should help us test the caperbilities of Pandas Profilling with regards to EDA. . Pandas Profilling renders the EDA as an HTML report, which has been included in this article. Note due to article formatting, the width is restricted which can make the report a bit cramped. See a wider version of this Pandas Profilling report on the Palmer Penguins here. . report = ProfileReport(penguins_df, title=&quot;Pandas Profiling Report - Palmer Penguins&quot;, explorative=True, html={&quot;style&quot;: {&quot;full_width&quot;: True}}) report . . Pandas Profilling Report Overview . For each column the following statistics - if relevant for the column type - are presented in the interactive HTML report: . Type inference: detect the types of columns in a dataframe. | Essentials: type, unique values, missing values | Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range | Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness | Most frequent values | Histograms | Correlations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices | Missing values matrix, count, heatmap and dendrogram of missing values | Duplicate rows Lists the most occurring duplicate rows | Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data | . The report itself is structured into 6 sections, which can be navigated from a top menu: . Overview: Summary stats, Alerts for notable missing values and high correlations, and report run details | Variables: For each variable - distinct values, missing values, bar chart (categorical), histogram/mean/min/max/zeros/negative (numeric) | Interactions: Scatterplots between all numeric variables | Correlations: Heatmap correlation plots between numeric &amp; categorical variables | Missing values: Various plots to highlight missing values | Sample: The first and last 10 rows from the dataset with all columns | . The report is also highly configurable allowing a large range of customisations. . Here are a few more examples of pandas profilling applied to other datasets . Review . Pandas Profilling certainly generates a lot of useful information, and very efficiently from simply one line of code. However, it does have some downsides as well for performing EDA on a dataset. Lets highlight some key pros and cons. . Pros . Simple to use: The report can be generated with one line of code, and uses a pandas dataframe which is the most common format for python data analysis. . Excellent summary stats: The summary stats of the overall dataset, such as variable types, missing values etc are exactly the kind of key information needed to get an overall view of the dataset for EDA. . Excellent univariate analysis: Univariate analysis is a key first step in EDA, and this report does this very well for a range of different variable types. . Some multi-variate analysis: The scatterplots for numeric variables and correlations for all variables provide some useful insights. . Nice format: The report layout is neat, modern, clean and spacious, making it easy to read. . Highly configurable: The report is highly customisable, which means it can be focussed on whats most important from the options available. . Good output formats: There are some great output formats for the report, such as inside a jupyter notebook, or exported as an html file. The report is highly interactive. . Cons . Difficult to use with large datasets: Because of the calculations done by default accross all variables, for large datasets the profilling report can be slow to run. Two possible approaches to deal with this are: . Running the report on a smaller randomly sampled subset of the data | Customising the report to reduce the number of sections &amp; calculations done in the report. | . Inadequate for multi-variate analysis: While for univariate analysis the report does an excellent job, for multi-variate analysis this report and the few plots and calculations provided e.g. scatterplots and correlation matrices, is unlikely to be sufficient for an EDA on most datasets to provide the depth of understanding required. . Lets consider looking at flipper length. Pandas Profilling provides a simple histogram: . . This gives us a sense of a wide distribution, with multiple peaks, but not much else. . In multi-variate analysis, its common to colour plots of numeric variables by a categorical variable to gain more insight. . In the original study, histograms for each penguin type are plotted separately with a unique colour, then overlaid with transparency: . . This customised plot gives us more insight, for example we can see different penguin types have disntinct ranges of values for flipper length. We can see for example that the Adelie and Chinstrap penguins generally have lower distribution of values for flipper lengths than the Gentoo penguins. . Let&#39;s also consider for example one the scatterplots generated in the report for body mass v flipper length: . . We can see there is a linear relationship between the two variables, but we can&#39;t see much else. . In the original study, a custom scatterplot of these variables was created, which also coloured the points by the categorical variable for the penguin type: . . This customised plot again gives us far more insight than we get from pandas profilling. We can see for example not only is there a linear relationship, but we can also see different penguin types have disntinct ranges of values for these variables. We can see for example that the Adelie and Chinstrap penguins generally have both lower body mass and flipper lengths than the Gentoo penguins. . It is perhaps not surprising the report falls short for multi-variate analysis, the number of potential combinations of variables and plot types grows exponentially, and what is important ot focus on often depends on domain and context knowledge specific to a dataset. For example, it would not be possible for Pandas Profilling to know that Penguin Type would be a good categorical variable to colour the scatter plots and histograms by. . Conclusion . In this article we have looked at Pandas Profilling as a potential Python Power Tool for Data Science. . While it does have some drawbacks, for example the limitations of use on large datasets, and that it is unlikely to be sufficient for a complete multi-variate analysis on a dataset - it still automatically performs many of the common tasks a Data Scientist would need to do for EDA on a dataset - and so I would argue Pandas Profilling is an exteremly useful Python Power Tool for Data Science. .",
            "url": "https://www.livingdatalab.com/python-power-tools/2021/11/20/python-power-tools-pandas-profilling.html",
            "relUrl": "/python-power-tools/2021/11/20/python-power-tools-pandas-profilling.html",
            "date": " • Nov 20, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "Network Analysis Fundamentals - An Analysis of Zacharys Karate Club",
            "content": "Introduction . In this study we will introduce network analysis, and apply it to understanding the structure and functioning of a karate club. . What is Network Analysis? . So we will define a network as a group of objects and a set of relationships between them. The mathematical term for this is a Graph. This could represent a range of different things, such as a group of people, electrical circuits, the flight pattens of aeroplanes, a set of bridges or roads in a city, or biological networks. . Network Analysis helps us better understand the structure, relationships and functioning of a network. . This Study - Zachary&#39;s karate club . Zachary&#39;s karate club is a well known benchmark dataset in Network analysis. . The dataset is a network of friendships between the 34 members of a karate club at a US university, as described by Wayne Zachary in 1977. This was first used in the paper W. W. Zachary, An information flow model for conflict and fission in small groups, Journal of Anthropological Research 33, 452-473 (1977) . Network Fundamentials . Nodes and Edges . Before looking at our data lets first define some basic terms used to describe networks. Nodes (also called vertices) are the objects of the network, so in a network of people each node would represent a person. Edges (also called links) are the connections between nodes, so in a network of people each edge would represent a relationship or connection between two people. . . Our dataset is represented as a list of nodes and a list of edges. We will use the NetworkX python library for dealing with networks. . Lets load our Karate dataset and print some basic stats about it. . G = nx.karate_club_graph() # Print summary print(nx.info(G)) . Graph named &#34;Zachary&#39;s Karate Club&#34; with 34 nodes and 78 edges . The 34 nodes represent the members of the karate club, and the edges describes which people know each other i.e. the relationships that exist between different people. . So we have some very basic information here about our Graph already, i.e. the number of nodes and edges. . Attributes . Currently our Network is a set of people and the relationships that exist between them. But we can also add extra infromation about each person i.e. add extra information to each Node, these are called Attributes. . Lets see what attributes the nodes of our Karate network have. . # Print node attributes for all nodes for nodex in G.nodes(data=True): for b in (nodex[1]): print(b, &quot; &quot;) . club club club club club club club club club club club club club club club club club club club club club club club club club club club club club club club club club club . So we see to have just one attribute for all our nodes called &#39;club&#39;. Lets see what the values are for these for all our nodes. . for n in G.nodes(): print(n, G.nodes[n][&#39;club&#39;]) . 0 Mr. Hi 1 Mr. Hi 2 Mr. Hi 3 Mr. Hi 4 Mr. Hi 5 Mr. Hi 6 Mr. Hi 7 Mr. Hi 8 Mr. Hi 9 Officer 10 Mr. Hi 11 Mr. Hi 12 Mr. Hi 13 Mr. Hi 14 Officer 15 Officer 16 Mr. Hi 17 Mr. Hi 18 Officer 19 Mr. Hi 20 Officer 21 Mr. Hi 22 Officer 23 Officer 24 Officer 25 Officer 26 Officer 27 Officer 28 Officer 29 Officer 30 Officer 31 Officer 32 Officer 33 Officer . So we can see for club nodes either have a value of &#39;Officer&#39; or &#39;Mr. Hi&#39;. We will return to what these values mean later. . We can plot a very basic visualisation of the network using the Matplotlib python library. . plt.figure(figsize=(25,10)) ax = plt.gca() ax.set_title(&#39;Zacharys Karate Club - Network Circular Plot&#39;) nx.draw_circular(G,with_labels = True) . We can get a general sense that some nodes seem more connected to each other, for eample some of the nodes on the right have many more connections than most others. . Lets see if we can get more precise measurements of the properties of this network using metrics. . Network Metrics . Metrics allow us to start going beyond just nodes and edges and starting to really understand overall features that start to describe the unique charactersitics of this particular network. . As well as the number of nodes and edges, we also know we have one attribute for our nodes (club). We also assume in this case that these relationships are symmetrical i.e. if person A knows person B, then person B knows person A. This is not always the case, for example in a network of airline flights, just because there is a flight from city A to B, that does not always imply there is a reciprical flight from city B to A. Symmetrical relationship type graphs are known as undirected graphs, and non-symmetrical relationships are known as directed graphs. . These kind of properties such as the number of nodes and edges, available attributes, if the network is directed or not - determine the kind of things you can do with the network, including the types of analyses possible. For example, a network with too few nodes might be difficult to draw conclusions from, or an undirected network requires the appropriate usage of certain measures but not others. . For example, in our Karate dataset you can determine what communities people find themselves in, but you can’t determine the directional routes through which information might flow along the network (you’d need a directed network for that). By using the symmetric, undirected relationships in this case, you’ll be able to find sub-communities and the people who are important to those communities, a process that would be more difficult (though still possible) with a directed network. . NetworkX allows you to perform most analyses you might conceive, but you must understand the affordances of your dataset and realize some NetworkX algorithms are more appropriate than others. . Shape . While we got a sneak peek at the network by plotting that earlier, more complex networks can be difficult to understand by just plotting them out. Shape is a characteristic of a network we can get numerical measures for to help us understand it better in terms of overall structure for example do nodes cluster together, or are they equally spread out? Are there complex structures, or is every node arranged along a straight line? . We can plot again a basic plot of the network, but this time not in a circular layout, and lets increase the size of the nodes so we can identify each node number more clearly. . plt.figure(figsize=(25,15)) ax = plt.gca() ax.set_title(&#39;Zacharys Karate Club - Network Plot&#39;) nx.draw(G,with_labels = True, node_size=3000) . We can see that all nodes are part of one big network. Knowing how many groups or components of a network can help us focus calculations on whats most useful. . We can also observe again some nodes seem more connected than others, e.g. node 0 and node 33. Lets highlight these and plot with a circular style. . plt.figure(figsize=(25,15)) ax = plt.gca() ax.set_title(&#39;Zacharys Karate Club - Network Plot - Highlighted most connected nodes&#39;) # To plot using networkx we first need to get the positions we want for each node. circ_pos = nx.circular_layout(G) # Use the networkx draw function to easily visualise the graph nx.draw(G,circ_pos, with_labels = True, node_size=3000) #let&#39;s highlight two of the most connected nodes 0 and 33 nx.draw_networkx_nodes(G, circ_pos, nodelist=[0], node_color=&#39;g&#39;, alpha=1) nx.draw_networkx_nodes(G, circ_pos, nodelist=[33], node_color=&#39;r&#39;, alpha=1) . &lt;matplotlib.collections.PathCollection at 0x7fbcef8c6f90&gt; . We can now see what seem to be two of the most connected nodes highlighted in red and green. . However as mentioned earlier, a purely visual understanding of a network may not be accurate for large and complex networks, so numerical measures can be more useful and accurate. Quantitative metrics let you differentiate networks, learn about their topologies, and turn a jumble of nodes and edges into something you can learn from. . A good beggining metric is density which is a ratio of the actual edges in a network to all possible edges in a network. Density gives you a quick measure of how closely knit the network is. . density = nx.density(G) print(&quot;Network density:&quot;, density) . Network density: 0.13903743315508021 . The density value is 0.139, so this implies a not very dense network (on a scale from 0-1). . A shortest path measurement is a bit more complex. It calculates the shortest possible series of nodes and edges that stand between any two nodes, something hard to see in large network visualizations. This measure is essentially finding friends-of-friends—if my mother knows someone that I don’t, then mom is the shortest path between me and that person. The Six Degrees of Kevin Bacon game, is basically a game of finding shortest paths (with a path length of six or less) from Kevin Bacon to any other actor. . There are many network metrics derived from shortest path lengths. One such measure is diameter, which is the longest of all shortest paths. After calculating all shortest paths between every possible pair of nodes in the network, diameter is the length of the path between the two nodes that are furthest apart. The measure is designed to give you a sense of the network’s overall size, the distance from one end of the network to another. . Diameter uses a simple command: nx.diameter(G). However, running this command on a graph that is not full connected will give an error. . You can check this by first finding out if your Graph “is connected” (i.e. all one component) and, if not connected, finding the largest component and calculating diameter on that component alone. . print(nx.is_connected(G)) # Calculate diameter diameter = nx.diameter(G) print(&quot;Network diameter:&quot;, diameter) . True Network diameter: 5 . The network diameter is 5: there is a path length of 5 between the two farthest-apart nodes in the network. Unlike density which is scaled from 0 to 1, it is difficult to know from this number alone whether 5 is a large or small diameter. For some global metrics, it can be best to compare it to networks of similar size and shape. . The final structural calculation we will make on this network concerns the concept of triadic closure. Triadic closure supposes that if two people know the same person, they are likely to know each other. If Fox knows both Fell and Whitehead, then Fell and Whitehead may very well know each other, completing a triangle in the visualization of three edges connecting Fox, Fell, and Whitehead. The number of these enclosed triangles in the network can be used to find clusters and communities of individuals that all know each other fairly well. . One way of measuring triadic closure is called clustering coefficient because of this clustering tendency, but the structural network measure you will learn is known as transitivity. Transitivity is the ratio of all triangles over all possible triangles. A possible triangle exists when one person (Fox) knows two people (Fell and Whitehead). . So transitivity, like density, expresses how interconnected a graph is in terms of a ratio of actual over possible connections. Remember, measurements like transitivity and density concern likelihoods rather than certainties. All the outputs of the Python script must be interpreted, like any other object of research. Transitivity allows you a way of thinking about all the relationships in your graph that may exist but currently do not. . triadic_closure = nx.transitivity(G) print(&quot;Triadic closure:&quot;, triadic_closure) . Triadic closure: 0.2556818181818182 . Also like density, transitivity is scaled from 0 to 1, and you can see that the network’s transitivity is about 0.255, somewhat higher than its 0.139 density. Because the graph is not very dense, there are fewer possible triangles to begin with, which may result in slightly higher transitivity. That is, nodes that already have lots of connections are likely to be part of these enclosed triangles. To back this up, you’ll want to know more about nodes with many connections. . Centrality . Now we have some measures of the overall network i.e. measures of the shape of the network, a good next step can be to identify important nodes in the network. In network analysis, measures of the importance of nodes are referred to as centrality measures. Because there are many ways of approaching the question “Which nodes are the most important?” there are many different ways of calculating centrality. . Degree is the simplest and the most common way of finding important nodes. A node’s degree is the sum of its edges. If a node has three lines extending from it to other nodes, its degree is three. Five edges, its degree is five. It’s really that simple. Since each of those edges will always have a node on the other end, you might think of degree as the number of people to which a given person is directly connected. The nodes with the highest degree in a social network are the people who know the most people. These nodes are often referred to as hubs, and calculating degree is the quickest way of identifying hubs. . degree_dict = dict(G.degree(G.nodes())) nx.set_node_attributes(G, degree_dict, &#39;degree&#39;) . sorted_degree = sorted(degree_dict.items(), key=itemgetter(1), reverse=True) print(&quot;Top 20 nodes by degree:&quot;) for d in sorted_degree[:20]: print(d) . Top 20 nodes by degree: (33, 17) (0, 16) (32, 12) (2, 10) (1, 9) (3, 6) (31, 6) (8, 5) (13, 5) (23, 5) (5, 4) (6, 4) (7, 4) (27, 4) (29, 4) (30, 4) (4, 3) (10, 3) (19, 3) (24, 3) . Degree can tell you about the biggest hubs, but it can’t tell you that much about the rest of the nodes. And in many cases, those hubs it’s telling you about. We can see here for example this confirms our earlier intuition that nodes 33 and 0 are two of the most connected people, two of the biggest hubs. . Thankfully there are other centrality measures that can tell you about more than just hubs. Eigenvector centrality is a kind of extension of degree—it looks at a combination of a node’s edges and the edges of that node’s neighbors. Eigenvector centrality cares if you are a hub, but it also cares how many hubs you are connected to. It’s calculated as a value from 0 to 1: the closer to one, the greater the centrality. Eigenvector centrality is useful for understanding which nodes can get information to many other nodes quickly. If you know a lot of well-connected people, you could spread a message very efficiently. If you’ve used Google, then you’re already somewhat familiar with Eigenvector centrality. Their PageRank algorithm uses an extension of this formula to decide which webpages get to the top of its search results. . Betweenness centrality is a bit different from the other two measures in that it doesn’t care about the number of edges any one node or set of nodes has. Betweenness centrality looks at all the shortest paths that pass through a particular node (see above). To do this, it must first calculate every possible shortest path in your network, so keep in mind that betweenness centrality will take longer to calculate than other centrality measures (but it won’t be an issue in a dataset of this size). Betweenness centrality, which is also expressed on a scale of 0 to 1, is fairly good at finding nodes that connect two otherwise disparate parts of a network. If you’re the only thing connecting two clusters, every communication between those clusters has to pass through you. In contrast to a hub, this sort of node is often referred to as a broker. Betweenness centrality is not the only way of finding brokerage (and other methods are more systematic), but it’s a quick way of giving you a sense of which nodes are important not because they have lots of connections themselves but because they stand between groups, giving the network connectivity and cohesion. . These two centrality measures are even simpler to run than degree—they don’t need to be fed a list of nodes, just the graph G. You can run them with these functions: . betweenness_dict = nx.betweenness_centrality(G) # Run betweenness centrality eigenvector_dict = nx.eigenvector_centrality(G) # Run eigenvector centrality # Assign each to an attribute in your network nx.set_node_attributes(G, betweenness_dict, &#39;betweenness&#39;) nx.set_node_attributes(G, eigenvector_dict, &#39;eigenvector&#39;) sorted_betweenness = sorted(betweenness_dict.items(), key=itemgetter(1), reverse=True) print(&quot;Top 20 nodes by betweenness centrality:&quot;) for b in sorted_betweenness[:20]: print(b) . Top 20 nodes by betweenness centrality: (0, 0.43763528138528146) (33, 0.30407497594997596) (32, 0.145247113997114) (2, 0.14365680615680618) (31, 0.13827561327561325) (8, 0.05592682780182781) (1, 0.053936688311688304) (13, 0.04586339586339586) (19, 0.03247504810004811) (5, 0.02998737373737374) (6, 0.029987373737373736) (27, 0.02233345358345358) (23, 0.017613636363636363) (30, 0.014411976911976909) (3, 0.011909271284271283) (25, 0.0038404882154882154) (29, 0.0029220779220779218) (24, 0.0022095959595959595) (28, 0.0017947330447330447) (9, 0.0008477633477633478) . Interestingly, nodes 33 and 0 again come up top for betweeness centrality as well. Lets rank everyone and show betweeness and degree together. . top_betweenness = sorted_betweenness[:20] #Then find and print their degree for tb in top_betweenness: # Loop through top_betweenness degree = degree_dict[tb[0]] # Use degree_dict to access a node&#39;s degree, see footnote 2 print(&quot;Person:&quot;, tb[0], &quot;| Betweenness Centrality:&quot;, tb[1], &quot;| Degree:&quot;, degree) . Person: 0 | Betweenness Centrality: 0.43763528138528146 | Degree: 16 Person: 33 | Betweenness Centrality: 0.30407497594997596 | Degree: 17 Person: 32 | Betweenness Centrality: 0.145247113997114 | Degree: 12 Person: 2 | Betweenness Centrality: 0.14365680615680618 | Degree: 10 Person: 31 | Betweenness Centrality: 0.13827561327561325 | Degree: 6 Person: 8 | Betweenness Centrality: 0.05592682780182781 | Degree: 5 Person: 1 | Betweenness Centrality: 0.053936688311688304 | Degree: 9 Person: 13 | Betweenness Centrality: 0.04586339586339586 | Degree: 5 Person: 19 | Betweenness Centrality: 0.03247504810004811 | Degree: 3 Person: 5 | Betweenness Centrality: 0.02998737373737374 | Degree: 4 Person: 6 | Betweenness Centrality: 0.029987373737373736 | Degree: 4 Person: 27 | Betweenness Centrality: 0.02233345358345358 | Degree: 4 Person: 23 | Betweenness Centrality: 0.017613636363636363 | Degree: 5 Person: 30 | Betweenness Centrality: 0.014411976911976909 | Degree: 4 Person: 3 | Betweenness Centrality: 0.011909271284271283 | Degree: 6 Person: 25 | Betweenness Centrality: 0.0038404882154882154 | Degree: 3 Person: 29 | Betweenness Centrality: 0.0029220779220779218 | Degree: 4 Person: 24 | Betweenness Centrality: 0.0022095959595959595 | Degree: 3 Person: 28 | Betweenness Centrality: 0.0017947330447330447 | Degree: 3 Person: 9 | Betweenness Centrality: 0.0008477633477633478 | Degree: 2 . This seems to confirm the importance of nodes 0 and 33, as both have the highest betweeness centrality and degree. . Community Detection . Another common thing to ask about a network dataset is what the subgroups or communities are within the larger social structure. Is your network one big, happy family where everyone knows everyone else? Or is it a collection of smaller subgroups that are only connected by one or two intermediaries? The field of community detection in networks is designed to answer these questions. There are many ways of calculating communities, cliques, and clusters in your network, but the most popular method currently is modularity. Modularity is a measure of relative density in your network: a community (called a module or modularity class) has high density relative to other nodes within its module but low density with those outside. Modularity gives you an overall score of how fractious your network is, and that score can be used to partition the network and return the individual communities. . Very dense networks are often more difficult to split into sensible partitions. Luckily, as you discovered earlier, this network is not all that dense. There aren’t nearly as many actual connections as possible connections. Its worthwhile partitioning this sparse network with modularity and seeing if the result make analytical sense. . communities = community.greedy_modularity_communities(G) modularity_dict = {} # Create a blank dictionary for i,c in enumerate(communities): # Loop through the list of communities, keeping track of the number for the community for name in c: # Loop through each person in a community modularity_dict[name] = i # Create an entry in the dictionary for the person, where the value is which group they belong to. # Now you can add modularity information like we did the other metrics nx.set_node_attributes(G, modularity_dict, &#39;modularity&#39;) . The method greedy_modularity_communities() tries to determine the number of communities appropriate for the graph, and groups all nodes into subsets based on these communities. Unlike the centrality functions, the above code will not create a dictionary. Instead it creates a list of special “frozenset” objects (similar to lists). There’s one set for each group, and the sets contain the node number of the people in each group. In order to add this information to your network in the now-familiar way, you must first create a dictionary that labels each person with a number value for the group to which they belong. . As always, you can combine these measures with others. For example, here’s how you find the highest eigenvector centrality nodes in modularity class 0 (the first one): . class0 = [n for n in G.nodes() if G.nodes[n][&#39;modularity&#39;] == 0] # Then create a dictionary of the eigenvector centralities of those nodes class0_eigenvector = {n:G.nodes[n][&#39;eigenvector&#39;] for n in class0} # Then sort that dictionary and print the first 5 results class0_sorted_by_eigenvector = sorted(class0_eigenvector.items(), key=itemgetter(1), reverse=True) print(&quot;Modularity Class 0 Sorted by Eigenvector Centrality:&quot;) for node in class0_sorted_by_eigenvector[:5]: print(&quot;Person:&quot;, node[0], &quot;| Eigenvector Centrality:&quot;, node[1]) . Modularity Class 0 Sorted by Eigenvector Centrality: Person: 33 | Eigenvector Centrality: 0.373371213013235 Person: 32 | Eigenvector Centrality: 0.3086510477336959 Person: 8 | Eigenvector Centrality: 0.2274050914716605 Person: 31 | Eigenvector Centrality: 0.19103626979791702 Person: 30 | Eigenvector Centrality: 0.17476027834493085 . Using eigenvector centrality as a ranking can give you a sense of the important people within this modularity class, so for example in this class we can see person 33 again has the highest eigenvector centrality and so this person is likely an important person within this group. . In smaller networks like this one, a common task is to find and list all of the modularity classes and their members. You can do this by looping through the communities list: . for i,c in enumerate(communities): # Loop through the list of communities print(&#39;Class &#39;+str(i)+&#39;:&#39;, list(c)) # Print out the classes and their members . Class 0: [8, 14, 15, 18, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33] Class 1: [1, 2, 3, 7, 9, 12, 13, 17, 21] Class 2: [0, 16, 19, 4, 5, 6, 10, 11] . So we seem to have 3 groups, lets see who the most important people within each of these groups are. . for modularity_class in range(3): # First get a list of just the nodes in that class classN = [n for n in G.nodes() if G.nodes[n][&#39;modularity&#39;] == modularity_class] # Then create a dictionary of the eigenvector centralities of those nodes class_eigenvector = {n:G.nodes[n][&#39;eigenvector&#39;] for n in classN} # Then sort that dictionary and print the first 5 results class_sorted_by_eigenvector = sorted(class_eigenvector.items(), key=itemgetter(1), reverse=True) print(&#39; &#39;) print(&quot;Modularity Class &quot; + str(modularity_class) + &quot; Sorted by Eigenvector Centrality:&quot;) for node in class_sorted_by_eigenvector[:5]: print(&quot;Person:&quot;, node[0], &quot;| Eigenvector Centrality:&quot;, node[1]) . Modularity Class 0 Sorted by Eigenvector Centrality: Person: 33 | Eigenvector Centrality: 0.373371213013235 Person: 32 | Eigenvector Centrality: 0.3086510477336959 Person: 8 | Eigenvector Centrality: 0.2274050914716605 Person: 31 | Eigenvector Centrality: 0.19103626979791702 Person: 30 | Eigenvector Centrality: 0.17476027834493085 Modularity Class 1 Sorted by Eigenvector Centrality: Person: 2 | Eigenvector Centrality: 0.31718938996844476 Person: 1 | Eigenvector Centrality: 0.2659538704545025 Person: 13 | Eigenvector Centrality: 0.22646969838808148 Person: 3 | Eigenvector Centrality: 0.2111740783205706 Person: 7 | Eigenvector Centrality: 0.17095511498035434 Modularity Class 2 Sorted by Eigenvector Centrality: Person: 0 | Eigenvector Centrality: 0.3554834941851943 Person: 19 | Eigenvector Centrality: 0.14791134007618667 Person: 5 | Eigenvector Centrality: 0.07948057788594247 Person: 6 | Eigenvector Centrality: 0.07948057788594247 Person: 4 | Eigenvector Centrality: 0.07596645881657382 . So we seem to have 3 communities, with persons 33, 2 and 0 being the most important members of their communities. . Summary of initial findings . Having processed and reviewed an array of network metrics in Python, we now have evidence from which arguments can be made and conclusions drawn about this network of people in the Karate club. . We know, for example, that the network has relatively low density, suggesting loose associations and/or incomplete original data. We know that the community is organized around several disproportionately large hubs, in particular persons 0 and 33. . Finally we learned that the network is made of 3 distinct communities. . Each of these findings is an invitation to more research rather than an endpoint or proof. Network analysis is a set of tools for asking targeted questions about the structure of relationships within a dataset, and NetworkX provides a relatively simple interface to many of the common techniques and metrics. Networks are a useful way of extending your research into a group by providing information about community structure. . Validation against ground truth . For this network beyond the data, we actually have other information to give us insight into the nature of relations at this karate club from Zachary&#39;s research paper. During the study a conflict arose between the administrator &quot;John A&quot; and instructor &quot;Mr. Hi&quot; (pseudonyms), which led to the split of the club into two. Half of the members formed a new club around Mr. Hi; members from the other part found a new instructor or gave up karate. . In our dataset person 0 is Mr Hi, and person 33 is John A. Also the network node attribute &#39;club&#39; highlighted earlier, corresponds to the final faction each member of the club ended up becoming a member of e.g. Mr Hi is &#39;Mr Hi&#39;, and John A is &#39;Officer&#39;. . So does our network and analysis support this ground truth? Certainly our analysis has correctly identified Mr Hi and John A as key players in this group, indeed central hubs. Lets see how the idenfified 3 communities relate to each faction. . plt.figure(figsize=(25,15)) ax = plt.gca() ax.set_title(&#39;Zacharys Karate Club - Network Plot - Predicted Communities (colour) vs Actual Factions (text)&#39;) # Define communities community_0 = sorted(communities[0]) community_1 = sorted(communities[1]) community_2 = sorted(communities[2]) #Let&#39;s display the labels of which club each member ended up joining club_labels = nx.get_node_attributes(G,&#39;club&#39;) # draw each set of nodes in a seperate colour nx.draw_networkx_nodes(G,circ_pos, nodelist=community_0, node_color=&#39;g&#39;, alpha=0.5) nx.draw_networkx_nodes(G,circ_pos, nodelist=community_1, node_color=&#39;r&#39;, alpha=0.5) nx.draw_networkx_nodes(G,circ_pos, nodelist=community_2, node_color=&#39;b&#39;, alpha=0.5) # now we can add edges to the drawing nx.draw_networkx_edges(G,circ_pos, style=&#39;dashed&#39;,width = 0.2) # finally we can add labels to each node corresponding to the final club each member joined nx.draw_networkx_labels(G,circ_pos,club_labels,font_size=18) plt.show() . So here, the colour represents the predicted community from our network analysis, and the text label represents the ground truth actual faction each person joined that we know. . Firstly we can see a strong relationship between the green community and the Officer (John A) faction, in fact its almost a perfect match bar once exception at the top where one green node ends up in Mr Hi faction. Both blue and red communities seem to match perfectly with Mr Hi&#39;s faction. Lets merge the blue comminity into the red one together to see this more clearly. . plt.figure(figsize=(25,15)) ax = plt.gca() ax.set_title(&#39;Zacharys Karate Club - Network Plot - Predicted + Merged Communities (colour) vs Actual Factions (text)&#39;) combined_community = community_1 + community_2 # draw each set of nodes in a seperate colour nx.draw_networkx_nodes(G,circ_pos, nodelist=community_0, node_color=&#39;g&#39;, alpha=0.5) nx.draw_networkx_nodes(G,circ_pos, nodelist=combined_community, node_color=&#39;r&#39;, alpha=0.5) # now we can add edges to the drawing nx.draw_networkx_edges(G,circ_pos, style=&#39;dashed&#39;,width = 0.2) # finally we can add labels to each node corresponding to the final club each member joined nx.draw_networkx_labels(G,circ_pos,club_labels,font_size=18) plt.show() . So firstly we might conclude that Mr Hi&#39;s faction might consist of 2 sub-communites. Secondly, that our analysis predicts the actual factions very well making only one mistake, so with an accuracy of around 94%, based on the data of assocations within the club alone. . Conclusion . This study demonstrates the potential power of network analysis to understand real life networks and how they function. The idea that we can develop a mathmatical framework that can predict an individuals choices based off of their relationships with others is immensely powerful. We live in an interconnected world and the study of networks allows us to explore those connections. . Each of these findings is an invitation to more research rather than an endpoint or proof. Network analysis is a set of tools for asking targeted questions about the structure of relationships within a dataset, and NetworkX provides a relatively simple interface to many of the common techniques and metrics. Networks are a useful way of extending your research into a group by providing information about community structure. .",
            "url": "https://www.livingdatalab.com/network-analysis/2021/10/31/network-analysis-karate.html",
            "relUrl": "/network-analysis/2021/10/31/network-analysis-karate.html",
            "date": " • Oct 31, 2021"
        }
        
    
  
    
        ,"post31": {
            "title": "Understanding CNN's with a CAM - A Class Activation Map",
            "content": "Introduction . In this article we will look at how Class Acivation Maps (CAM&#39;s) can be used to understand and interpret the decisions that Convolutional Neural Networks (CNN&#39;s) make. . CAM and Pytorch hooks . A Class Activation Map (CAM) and help us understand why Convolutional Neural Networks (CNN&#39;s) make the descisions they do. CAM&#39;s do this by looking at the outputs of the last convolutional layer just before the average pooling layer - combined with the predictions, to give a heatmap visualisation of why the model made that descision. . At each point in our final convolutional layer, we have as many channels as in the last linear layer. We can compute a dot product of those activations with the final weights to get for each location in our feature map, the score of the feature that was used to make that decision. In other words, we can identify the relationships between the parts of the network that are most active in generating the correct choice. . We can access activations inside the network using Pytorch hooks. Wheras fastai callbacks allow you to inject code into the training loop, Pytorch hooks allow you to inject code into the forward and backward calculations themselves.. . Lets see an example looking at a dataset of cats and dogs. . path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=21, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1) . Downloading: &#34;https://download.pytorch.org/models/resnet34-b627a593.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth . . /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1156.) return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode) . epoch train_loss valid_loss error_rate time . 0 | 0.138940 | 0.025390 | 0.008796 | 00:48 | . epoch train_loss valid_loss error_rate time . 0 | 0.047596 | 0.024207 | 0.007442 | 00:52 | . We can get a cat image. For CAM we want to store the activations of the last convolutional layer, lets create a hook function in a class with a state. . img = PILImage.create(image_cat()) x, = first(dls.test_dl([img])) class Hook(): def hook_func(self, m, i, o): self.stored = o.detach().clone() . We can then instantiate a hook and attach it to any layer, in this case the last layer of the CNN body. . hook_output = Hook() hook = learn.model[0].register_forward_hook(hook_output.hook_func) . Then we can grab a batch of images and feed it through our model. . with torch.no_grad(): output = learn.model.eval()(x) . Then we can extract our stored activations . act = hook_output.stored[0] . And check our predictions. . F.softmax(output, dim=-1) . tensor([[1.1078e-08, 1.0000e+00]], device=&#39;cuda:0&#39;) . So 0 means dog, but just to check. . dls.vocab . [False, True] . So the model seems quite confident the image is a cat. . To perform our dot product of the weight matrix with the activations we can use einsum. . x.shape . torch.Size([1, 3, 224, 224]) . cam_map = torch.einsum(&#39;ck,kij-&gt;cij&#39;, learn.model[1][-1].weight, act) cam_map.shape . torch.Size([2, 7, 7]) . So for each image in the batch, we get a 7x7 channel map that tells us which activations were higher or lower, which will allow us to see what parts of the image most influenced the models choice. . x_dec = TensorImage(dls.train.decode((x,))[0][0]) _,ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map[1].detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . The parts in bright yellow correspond to higher activations and purple lower activations. So we can see the paws are the main area that made the model decide it was a cat. Its good to remove a hook once used as it can leak memory. . hook.remove() . We can manage hooks better by using a class, to handle all these things automatically. . class Hook(): def __init__(self, m): self.hook = m.register_forward_hook(self.hook_func) def hook_func(self, m, i, o): self.stored = o.detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() with Hook(learn.model[0]) as hook: with torch.no_grad(): output = learn.model.eval()(x.cuda()) act = hook.stored . This Hook class is provided by fastai. This approach only works for the last layer. . Gradient CAM . The previous approach only works for the last layer, but what if we want to look at activations for earlier layers? Gradient CAM lets us do this. Normally the gradients for weights are not stored after the backward pass, but we can store them, and then pick them up with a hook. . class HookBwd(): def __init__(self, m): self.hook = m.register_backward_hook(self.hook_func) def hook_func(self, m, gi, go): self.stored = go[0].detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() . Let&#39;s try this approach on the last layer, as we did before. However we can use this approach to calculate the gradients for any layer, with respect to the output. . cls = 1 with HookBwd(learn.model[0]) as hookg: with Hook(learn.model[0]) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0,cls].backward() grad = hookg.stored . /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(&#34;Using a non-full backward hook when the forward contains multiple autograd Nodes &#34; . The weights for the Grad-CAM approach are given by the average of our gradients accross the feature/channel map. . w = grad[0].mean(dim=[1,2], keepdim=True) cam_map = (w * act[0]).sum(0) _,ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . Let&#39;s now try this on a different layer, the second to last ResNet group layer. . with HookBwd(learn.model[0][-2]) as hookg: with Hook(learn.model[0][-2]) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0,cls].backward() grad = hookg.stored w = grad[0].mean(dim=[1,2], keepdim=True) cam_map = (w * act[0]).sum(0) _,ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . /usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior. warnings.warn(&#34;Using a non-full backward hook when the forward contains multiple autograd Nodes &#34; . Conclusion . In this article we saw how we can use Class Activation Map&#39;s to understand and interpret the choices a CNN makes. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/06/19/understanding-cnn-with-cam-class-activation-maps.html",
            "relUrl": "/deep-learning-theory/2021/06/19/understanding-cnn-with-cam-class-activation-maps.html",
            "date": " • Jun 19, 2021"
        }
        
    
  
    
        ,"post32": {
            "title": "Building a Neural Network from the Foundations",
            "content": "Introduction . In this article we will cover building a basic neural network from the most basic elements (arrays and Pytorch modules). We will also cover some of the key theory required for this. . This article and it&#39;s content is based on the fastai deep learning course, chapter 17. . Building a Neural Network from basic elements . Creating a neuron . A neuron takes a series of inputs, each of which is multipled by a weight, summing up all those inputs, and adding a bias - this input is then put thorugh an activation function. We could represent these as: . output = sum([x*w for x,w in zip(inputs,weights)]) + bias . def relu(x): return x if x &gt;= 0 else 0 . A deep learning model stacks many of these neurons in layers. So for the output of an entire layer, using matrices we would have: . y = x @ w.t() + b . Matrix multiplication . So we can define a function to manually do a matrix product using loops. . import torch from torch import tensor def matmul(a,b): ar,ac = a.shape # n_rows * n_cols br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): for k in range(ac): c[i,j] += a[i,k] * b[k,j] return c . However this is hugely slower than we can do using Pytorch matrix multiplciation. . Elementwise calculations . We can do element wise operations on tensors - as long as they are the same shape, for example. . a = tensor([10., 6, -4]) b = tensor([2., 8, 7]) a + b . tensor([12., 14., 3.]) . Broadcasting . Broadcasting allows 2 arrays of different sizes to be compatible for arthimetic operations, by repeating the smaller array so it matches the size of the larger one. . For example we can use unsqeeze in Pytorch to add extra dimensions explictly. . c = tensor([10.,20,30]) c.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape . (torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1])) . We can now replace our matrix multiplication with 3 loops with a broadcasting equivilent much shorter. . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): # c[i,j] = (a[i,:] * b[:,j]).sum() # previous c[i] = (a[i ].unsqueeze(-1) * b).sum(dim=0) return c . Forward and Backward passes of a Neural Network . Defining and initialising a layer . So we can define a basic linear layer in the following way. . def lin(x, w, b): return x @ w + b . Let&#39;s create some dummy data, and some simple layers. . x = torch.randn(200, 100) y = torch.randn(200) w1 = torch.randn(100,50) b1 = torch.zeros(50) w2 = torch.randn(50,1) b2 = torch.zeros(1) l1 = lin(x, w1, b1) l1.shape . torch.Size([200, 50]) . But we have a problem to do with how the parameters are initialised consider . l1.mean(), l1.std() . (tensor(-0.2733), tensor(10.1770)) . The std dev is 10, consider how if this is one layer which multiples by 10 how many layers could generate huge numbers that would be unmanagable and be a network hard to train. So we want our std dev to be close to one, and there is an equation for scaling our weights to this is so. . $1/ sqrt{n_{in}}$ . where $n_{in}$ represents the number of inputs. This is known as Xavier initialization (or Glorot initialization). . For example if we have 100 inputs, we should scale our weights by 0.1. . x = torch.randn(200, 100) for i in range(50): x = x @ (torch.randn(100,100) * 0.1) print(x[0:5,0:5]) print(x.std()) . tensor([[-0.6374, -0.3009, 0.4669, -0.7221, 0.1983], [-1.0054, 0.0244, 0.3540, -1.0580, 0.2675], [ 0.0789, 0.6670, 0.2132, 0.2511, -1.3466], [ 0.7786, -0.2874, -1.2391, 0.4132, 1.9071], [ 2.1194, 0.0046, -1.7749, 1.5797, 1.4981]]) tensor(1.1794) . Re-working our model with this in mind . x = torch.randn(200, 100) y = torch.randn(200) from math import sqrt w1 = torch.randn(100,50) / sqrt(100) b1 = torch.zeros(50) w2 = torch.randn(50,1) / sqrt(50) b2 = torch.zeros(1) l1 = lin(x, w1, b1) l1.mean(),l1.std() . (tensor(-0.0135), tensor(1.0176)) . Now we need to define an activation function. . def relu(x): return x.clamp_min(0.) l2 = relu(l1) l2.mean(),l2.std() . (tensor(0.3988), tensor(0.5892)) . So now the mean is no longer zero and our std dev is less like 1. So the Glorot method is not intended to be used with Relu and was invented before. . A newer initialisation by Kaiming He et al workes better with Relu. It&#39;s formula is: . $ sqrt{2 / n_{in}}$ . where $n_{in}$ is the number of inputs of our model. . Applying this. . x = torch.randn(200, 100) y = torch.randn(200) w1 = torch.randn(100,50) * sqrt(2 / 100) b1 = torch.zeros(50) w2 = torch.randn(50,1) * sqrt(2 / 50) b2 = torch.zeros(1) l1 = lin(x, w1, b1) l2 = relu(l1) l2.mean(), l2.std() . (tensor(0.5710), tensor(0.8222)) . Now we can define a whole model. . def model(x): l1 = lin(x, w1, b1) l2 = relu(l1) l3 = lin(l2, w2, b2) return l3 out = model(x) out.shape . torch.Size([200, 1]) . So we don&#39;t want this unit dimension. We can define a loss function and also get rid of this unit dimension. . def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() loss = mse(out, y) . Gradients and the Backwards Pass . So PyTorch computes the gradients for us with loss.backward but behind the scenes is a bit of calculus. Given the whole network is a huge function, with each part a sub-function, lets start with the final part the loss function. . We can calculate the loss with the loss function. If we take the derivative of the loss function with respect to the final weights, we can calculate the loss with respect to these weights. We can then use the chain rule to propagate these values backward, and calculate the loss with respect to every parameter in the model. . Lets define a function to calculate the gradients of the loss function with respect to the final weights. . def mse_grad(inp, targ): # grad of loss with respect to output of previous layer inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0] . Let&#39;s now define functions to calculate the gradients for the activation functions and also the linear layers. . def relu_grad(inp, out): # grad of relu with respect to input activations inp.g = (inp&gt;0).float() * out.g def lin_grad(inp, out, w, b): # grad of matmul with respect to input inp.g = out.g @ w.t() w.g = inp.t() @ out.g b.g = out.g.sum(0) . Model refactoring . Let&#39;s now put together everything: the model, the forward and backward pass methods. . class Relu(): def __call__(self, inp): self.inp = inp self.out = inp.clamp_min(0.) return self.out def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g class Lin(): def __init__(self, w, b): self.w,self.b = w,b def __call__(self, inp): self.inp = inp self.out = inp@self.w + self.b return self.out def backward(self): self.inp.g = self.out.g @ self.w.t() self.w.g = self.inp.t() @ self.out.g self.b.g = self.out.g.sum(0) class Mse(): def __call__(self, inp, targ): self.inp = inp self.targ = targ self.out = (inp.squeeze() - targ).pow(2).mean() return self.out def backward(self): x = (self.inp.squeeze()-self.targ).unsqueeze(-1) self.inp.g = 2.*x/self.targ.shape[0] class Model(): def __init__(self, w1, b1, w2, b2): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() # Create model model = Model(w1, b1, w2, b2) # Forward pass loss = model(x, y) # Backward pass model.backward() loss . tensor(2.7466) . Converting the model to Pytorch . We could build this more simply using Pytorch methods, and in fact fastai methods built on these. . class Model(Module): def __init__(self, n_in, nh, n_out): self.layers = nn.Sequential( nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)) self.loss = mse def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ) . Conclusion . In this article we have build a neural network from the most basic elements. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/06/17/neural-network-from-foundations.html",
            "relUrl": "/deep-learning-theory/2021/06/17/neural-network-from-foundations.html",
            "date": " • Jun 17, 2021"
        }
        
    
  
    
        ,"post33": {
            "title": "Optimisation Methods for Deep Learning",
            "content": "Introduction . In this article we will look at methods to improve gradient decent optimisation for training neural networks beyond SGD. These include momentum, RMSProp and Adam. We will also look at the fastai library system of callbacks which make changes to the training loop easier. . This article is based on content from the fastai deep learning course, chapter 16. . Basic SGD . We will first define a baseline using basic SGD to compare how further enhancements improve results. We will use the fastai curated imagenette dataset here. . dls = get_data(URLs.IMAGENETTE_160, 160, 128) . A new version of this dataset is available, downloading... . File downloaded is broken. Remove /root/.fastai/archive/imagenette2-160.tgz and try again. . We will also create an untrained ResNet-34 architecture for our model which we will train from scratch. . def get_learner(**kwargs): return cnn_learner(dls, resnet34, pretrained=False, metrics=accuracy, **kwargs).to_fp16() learn = get_learner() learn.fit_one_cycle(3, 0.003) . epoch train_loss valid_loss accuracy time . 0 | 2.611032 | 1.885956 | 0.362293 | 00:26 | . 1 | 1.987230 | 1.666735 | 0.449172 | 00:26 | . 2 | 1.615224 | 1.509878 | 0.567134 | 00:26 | . That was with all the default settings used by fastai. Lets explicitly use just basic SGD. . learn = get_learner(opt_func=SGD) learn.lr_find() . SuggestedLRs(lr_min=0.005754399299621582, lr_steep=6.309573450380412e-07) . So we will need to use a higher learning rate than we normally use. We will also need to explictly turn momentum off, as we are here trying to illustrate just using basic SGD. . learn.fit_one_cycle(3, 0.03, moms=(0,0,0)) . epoch train_loss valid_loss accuracy time . 0 | 2.869628 | 2.315048 | 0.284586 | 00:25 | . 1 | 2.269993 | 1.699830 | 0.414522 | 00:25 | . 2 | 1.978710 | 1.616934 | 0.444841 | 00:25 | . Defining a generic optimiser . The fastai library provides a flexible approach to optimisers that makes it easier to add custom changes using optimiser callbacks. A key part of this is the Optimiser class which includes these two methods. . def zero_grad(self): for p,*_ in self.all_params(): p.grad.detach_() p.grad.zero_() def step(self): for p,pg,state,hyper in self.all_params(): for cb in self.cbs: state = _update(state, cb(p, **{**state, **hyper})) self.state[p] = state . zero_grad is handy for clearing all the gradients. Note the step method loops through other potential callbacks which is how different aspects of optimisation old and new are done. Even basic SGD is one of these callbacks. . def sgd_cb(p, lr, **kwargs): p.data.add_(-lr, p.grad.data) . We can add this as a callback like this. . opt_func = partial(Optimizer, cbs=[sgd_cb]) . Let&#39;s now train with this. . learn = get_learner(opt_func=opt_func) learn.fit(3, 0.03) . epoch train_loss valid_loss accuracy time . 0 | 2.663601 | 1.871811 | 0.344968 | 00:25 | . 1 | 2.256670 | 1.914813 | 0.354650 | 00:25 | . 2 | 1.995262 | 1.813828 | 0.442548 | 00:25 | . Momentum . So the idea of Momentum is we want to go faster in the direction we are going with gradient decent to get there sooner. We could for example use a moving average. . weight.avg = beta weight.avg + (1-beta) weight.grad . new_weight = weight - lr * weight.avg . beta helps control how much momentum to use, so if its zero there is no momentum and we have just basic SGD. But if closer to 1 then the main direction is the average of the previous steps. . High beta can help us get over small &#39;bumps&#39; in the loss landscape and keep going faster in the general direction of progress followed so far, but if too high can cause us to overshoot completly. . Beta too high means we really miss important changes in direction. . fit_one_cycle starts with a high beta of 0.95, going down to 0.85 then back up to 0.95. . Let&#39;s add momentum, by keeping track of the moving average gradient, which we can do with another callback. . def average_grad(p, mom, grad_avg=None, **kwargs): if grad_avg is None: grad_avg = torch.zeros_like(p.grad.data) return {&#39;grad_avg&#39;: grad_avg*mom + p.grad.data} def momentum_step(p, lr, grad_avg, **kwargs): p.data.add_(-lr, grad_avg) opt_func = partial(Optimizer, cbs=[average_grad,momentum_step], mom=0.9) . Note Learner will automatically schedule the momentum and learning rate mom and lr, so fit_one_cycle will even work with our custom Optimiser. . learn = get_learner(opt_func=opt_func) learn.fit_one_cycle(3, 0.03) . epoch train_loss valid_loss accuracy time . 0 | 2.744289 | 2.736736 | 0.278471 | 00:25 | . 1 | 2.402794 | 1.715736 | 0.425732 | 00:25 | . 2 | 2.038843 | 1.557327 | 0.485096 | 00:25 | . learn.recorder.plot_sched() . RMSProp . RMSProp uses an adaptive learning rate, each parameter gets its own learning rate controlled by a global learning rate. The individual learning rate can be determined by looking at the gradients, for example if the gradients are close to zero for a while it might need a higher learning rate, and vice versa if the gradients are too high or unstable. . We can use a moving average to get the general direction, specifically a moving average of the gradients squared. . w.square_avg = alpha w.square_avg + (1-alpha) (w.grad ** 2) . new_w = w - lr * w.grad / math.sqrt(w.square_avg + eps) . The eps (epsilon) is added for numerical stability (usually set at 1e-8), and the default value for alpha is usually 0.99. . def average_sqr_grad(p, sqr_mom, sqr_avg=None, **kwargs): if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data) return {&#39;sqr_avg&#39;: sqr_mom*sqr_avg + (1-sqr_mom)*p.grad.data**2} def rms_prop_step(p, lr, sqr_avg, eps, grad_avg=None, **kwargs): denom = sqr_avg.sqrt().add_(eps) p.data.addcdiv_(-lr, p.grad, denom) opt_func = partial(Optimizer, cbs=[average_sqr_grad,rms_prop_step], sqr_mom=0.99, eps=1e-7) learn = get_learner(opt_func=opt_func) learn.fit_one_cycle(3, 0.003) . epoch train_loss valid_loss accuracy time . 0 | 2.810043 | nan | 0.108535 | 00:26 | . 1 | 2.242717 | 1.917789 | 0.354140 | 00:26 | . 2 | 1.790359 | 1.510692 | 0.496815 | 00:26 | . Adam . Adam combines SGD, momentum and RMSProp together. One difference is Adam uses an unbiased moving average. . w.avg = beta w.avg + (1-beta) w.grad . unbias_avg = w.avg / (1 - (beta**(i+1))) . With all the steps combined we have: . w.avg = beta1 w.avg + (1-beta1) w.grad . unbias_avg = w.avg / (1 - (beta1**(i+1))) . w.sqr_avg = beta2 w.sqr_avg + (1-beta2) (w.grad ** 2) . new_w = w - lr * unbias_avg / sqrt(w.sqr_avg + eps) . Adam is the default optimiser in fastai. . Decoupled Weight Decay . When using Adam, we need to use a different kind of weight decay. Recall basic weight decay. . new_weight = weight - lrweight.grad - lrwd*weight . And alternative formulation is: . weight.grad += wd*weight . With SGD these are the same, but not for Adam. So we need to use decoupled weight decay when using Adam. . Fastai Callbacks . Fastai callbacks allow you to add custom behaviour to the training loop at any point. . . This has enabled easier adding of many new custom changes such as the below examples. . . Creating a Callback . Let&#39;s try defining a model reset callback. . class ModelResetter(Callback): def begin_train(self): self.model.reset() def begin_validate(self): self.model.reset() . Here is another example RNN regulariser callback. . class RNNRegularizer(Callback): def __init__(self, alpha=0., beta=0.): self.alpha,self.beta = alpha,beta def after_pred(self): self.raw_out,self.out = self.pred[1],self.pred[2] self.learn.pred = self.pred[0] def after_loss(self): if not self.training: return if self.alpha != 0.: self.learn.loss += self.alpha * self.out[-1].float().pow(2).mean() if self.beta != 0.: h = self.raw_out[-1] if len(h)&gt;1: self.learn.loss += self.beta * (h[:,1:] - h[:,:-1] ).float().pow(2).mean() . Inside the callback you can access global variables and objects such as self.model. . Callback Ordering and Exceptions . Callbacks can also interrupt any part of the training loop by using a system of exceptions, for example to skip a batch or stop training completely. . This callback will stop training any time the loss becomes infinate. . class TerminateOnNaNCallback(Callback): run_before=Recorder def after_batch(self): if torch.isinf(self.loss) or torch.isnan(self.loss): raise CancelFitException . Sometimes callbacks need to be called in a particular order. You can use run_before or run_after in the callback to set the ordering needed. . Conclusion . In this article we looked at standard SGD enhacements for optimisation, as well as looking at the fastai&#39;s library callbacks that help make changes easier. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/06/13/optimisation-methods-for-deep-learning.html",
            "relUrl": "/deep-learning-theory/2021/06/13/optimisation-methods-for-deep-learning.html",
            "date": " • Jun 13, 2021"
        }
        
    
  
    
        ,"post34": {
            "title": "Resnets - The Key to Training Deeper Neural Networks",
            "content": "Introduction . In this article we will build a ResNet type convolutional image networks from scratch using PyTorch. We will see why these type of networks are key to enabling the building much deeper networks that can be easily trained and perform well. . This article and it&#39;s content is based on the fast ai deep learning course, chapter 14. . Improving Convolutional Networks - Average pooling . In an earlier article about convolutional networks in the models we used we ended up with a single vector of activations for each image by using enough stride-2 convolutions to down-sample each layer of activations so that we would end up with a grid size of 1. . If we tried this approach with other, bigger images we would face 2 issues: . We would need many more layers | The model would not be able to work on images of a different size to which it was trained on | . By using this type of architecture, we are in essence hard coding the architecture and making it difficult to reuse. We could for example flatten the final layer regardless of the grid size it was beyond 1x1, which was indeed an earlier approach followed, but this would still not work on images of a different size, and takes a lot of memory. . The problem was better solved by using fully convolutional networks which take the average of activations accross a final grid e.g. over the x and y axis. . def avg_pool(x): return x.mean((2,3)) . This will always convert a grid of activations into a single activation per image. . A full convolutional network then has a number of convolutional layers some of stride 2, at the end of which is an adaptive average pooling layer - to a layer to flatten and remove the unit axis, and a final linear layer. . We can define a fully convoltional network in the following way. . def block(ni, nf): return ConvLayer(ni, nf, stride=2) def get_model(): return nn.Sequential( block(3, 16), block(16, 32), block(32, 64), block(64, 128), block(128, 256), nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(256, dls.c)) . Because of the nature fo average pooling, this may not be suitable for some vision tasks, as you&#39;re loosing certain types of information. For example if you were trying to recognise digits of 6 and 9, the orientation and relational aspect of groups of pixels matters - so fully convoltuonal may not be good here. However for other images like animals, the orientation does&#39;nt really matter - a cat is a cat even if its upside down! So the fully convolutional networks which loose this relational information would be fine here. . When we come out of the convolutional layers, we have activations of dimensions bs x ch x h x w (batch size, a certain number of channels, height, and width). We want to end up with a tensor of bs x ch, so we can take the average over the last two dimensions and flatten the trailing 1×1 dimension like we did in our previous model. . There are other types of pooling we could use for example max pooling. For instance, max pooling layers of size 2, which were very popular in older CNNs, reduce the size of our image by half on each dimension by taking the maximum of each 2×2 window (with a stride of 2). . We are going to use a new dataset Imagenette which is a smaller version of the famous ImageNet dataset, this smaller one being with just 10 classes of image. . Lets get the data and train our new model. . def get_data(url, presize, resize): path = untar_data(url) return DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;val&#39;), get_y=parent_label, item_tfms=Resize(presize), batch_tfms=[*aug_transforms(min_scale=0.5, size=resize), Normalize.from_stats(*imagenet_stats)], ).dataloaders(path, bs=128) dls = get_data(URLs.IMAGENETTE_160, 160, 128) dls.show_batch(max_n=4) . File downloaded is broken. Remove /root/.fastai/archive/imagenette2-160.tgz and try again. . def get_learner(m): return Learner(dls, m, loss_func=nn.CrossEntropyLoss(), metrics=accuracy ).to_fp16() learn = get_learner(get_model()) learn.lr_find() . SuggestedLRs(lr_min=0.002290867641568184, lr_steep=0.007585775572806597) . # 3e-3 often a good learning rate for CNN&#39;s learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.882259 | 1.813273 | 0.404076 | 00:30 | . 1 | 1.522370 | 1.521868 | 0.504459 | 00:30 | . 2 | 1.276501 | 1.225626 | 0.606624 | 00:30 | . 3 | 1.135786 | 1.183137 | 0.623185 | 00:30 | . 4 | 1.042103 | 1.048710 | 0.665733 | 00:30 | . This is quite a good result, considering this is not a pre-trained model trying and to predict 10 image categories from scratch. But to improve this, we will need to do more than just add more layers. . Modern CNN&#39;s - ResNet . Skip connections . The authors of the original ResNet paper noticed when training deeper models, even when using BatchNorm, that a network with more layers often did worse than a network with less layers. . . It seems that a bigger network has a lot of trouble discovering the parameters of even the smaller better network when left by itself to just train this bigger network. . While this had been noticed before, what the authors of the paper did that was new was to realise it should be possible to create a deeper network that should do at least as well as a more shallow network, by essentially turning off the extra layers i.e. using an identity mapping. . An identity mapping is where you are passing through the signal from earlier layers directly, skipping over the current layer. Remember from Batch norm layers we have the transformative factors of gamma and beta - if we set gamma to zero for the extra layers - this would essentially turn off the actions of the extra layers - and allow the signal from the earlier layers to come through unaltered. This is called the skip connection. . . This can allow the model to only change the later layers gradually. The original ResNet paper actually defined the skip connection as jumping over 2 layers, as seen in the diagram above. . Another way to think about ResNet&#39;s and these skip connections is to consider the function here i.e. . Y = X + block(X) . So we are not asking this block layer to directly predict the output Y, we are asking the block to learn to predict the difference between X and Y to minimise the error i.e. block(X) wants to help X get closer to Y. So a ResNet is good at learning about slight differences between doing nothing and adding a little something to the previous signal to make it better. This is how ResNet&#39;s got their name, as they are predicting &#39;residuals&#39; i.e. a residual is the prediction - target. . Also what is key here is the idea of making learning more gradual and easier. Even though the Universal Approximation Theorem states that a sufficiently large network can learn any function, in practice there is a difference between how different architectures can make it easy and difficult to learn. . Let&#39;s define a ResNet block with a skip connection, here norm_type=NormType.BatchZero causes fastai to init the gamma weights of the last batchnorm layer to zero). . class ResBlock(Module): def __init__(self, ni, nf): self.convs = nn.Sequential( ConvLayer(ni,nf), ConvLayer(nf,nf, norm_type=NormType.BatchZero)) def forward(self, x): return x + self.convs(x) . There are 2 problems with this though, it can&#39;t handle strides of more than 1, and it needs ni=nf. If we recall, convolutional operations change the dimensions of the output based on the output channels, as do strides of more than 1. This would prevent us from adding X to conv(X) as they would be of different dimensions. . To remedy this, we need a way to change the dimensions of x to match conv(x). So we can halve the grid size using and average pooling layer with stride 2, and we can change the number of channels using a convolution. We need to make the convolution as simple as possible, and that would be one with a kernal size of 1. . So we can now define a better ResBlock that uses these tricks to handle the changing shape of the skip connection. . def _conv_block(ni,nf,stride): return nn.Sequential( ConvLayer(ni, nf, stride=stride), ConvLayer(nf, nf, act_cls=None, norm_type=NormType.BatchZero)) . class ResBlock(Module): def __init__(self, ni, nf, stride=1): self.convs = _conv_block(ni,nf,stride) self.idconv = noop if ni==nf else ConvLayer(ni, nf, 1, act_cls=None) self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True) def forward(self, x): return F.relu(self.convs(x) + self.idconv(self.pool(x))) . We are using the noop function here which just returns the input unchanged, so idconv does nothing if ni==nf, and pool does nothing if stride=1 - which is what we want in our skip connection. . Also we moved the Relu after both layers, treating as the whole ResNet block like one layer. . Lets try this model. . def block(ni,nf): return ResBlock(ni, nf, stride=2) learn = get_learner(get_model()) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.947870 | 1.877467 | 0.335796 | 00:32 | . 1 | 1.671832 | 1.602260 | 0.456561 | 00:32 | . 2 | 1.379121 | 1.492799 | 0.533503 | 00:32 | . 3 | 1.170203 | 1.069924 | 0.662166 | 00:33 | . 4 | 1.032529 | 1.050656 | 0.672357 | 00:33 | . While this is not spectacularly better, the point is this allows us to now train deeper models more easily. For example we can make a model with twice as many layers in the following way. . def block(ni, nf): return nn.Sequential(ResBlock(ni, nf, stride=2), ResBlock(nf, nf)) learn = get_learner(get_model()) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.945738 | 1.871942 | 0.353631 | 00:36 | . 1 | 1.632775 | 1.519365 | 0.492484 | 00:36 | . 2 | 1.331637 | 1.168114 | 0.622930 | 00:36 | . 3 | 1.081849 | 1.036962 | 0.665733 | 00:35 | . 4 | 0.944774 | 0.946332 | 0.695287 | 00:36 | . This deeper model is now doing better with the same number of epochs. . For the ResNet breakthrough and many others a key note might be that many of these breakthroughs have come through experimental observations of odd things, and then trying to figure out why these occour. So deep learning is a very experimental field where many breakthroughs come through experiments. . Further work exploring ResNet&#39;s showed how the skip connections actually helped smooth the loss landscape making training easier, more gradual, and easier to avoid getting stuck in a local minima. . . State of the art ResNet&#39;s . Current ResNet&#39;s used have a few further tweaks that improve their performance. This include the earlier layers being just convolutional layers followed by a max pooling layer, without a full ResNet block and skip connections. These earlier layers are called the stem of the network. . def _resnet_stem(*sizes): return [ ConvLayer(sizes[i], sizes[i+1], 3, stride = 2 if i==0 else 1) for i in range(len(sizes)-1) ] + [nn.MaxPool2d(kernel_size=3, stride=2, padding=1)] . _resnet_stem(3,32,32,64) . Why this approach? with deep convolutional networks, most of the computation occours in the earlier layers of the network. Therefore it helps to keep the earlier layers as simple as possible.. ResNet blocks take far more computation than a plain convolutional block. . Lets now try this approach with improving out ResNet architecture with these improvements in mind. . class ResNet(nn.Sequential): def __init__(self, n_out, layers, expansion=1): stem = _resnet_stem(3,32,32,64) self.block_szs = [64, 64, 128, 256, 512] for i in range(1,5): self.block_szs[i] *= expansion blocks = [self._make_layer(*o) for o in enumerate(layers)] super().__init__(*stem, *blocks, nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(self.block_szs[-1], n_out)) def _make_layer(self, idx, n_layers): stride = 1 if idx==0 else 2 ch_in,ch_out = self.block_szs[idx:idx+2] return nn.Sequential(*[ ResBlock(ch_in if i==0 else ch_out, ch_out, stride if i==0 else 1) for i in range(n_layers) ]) . The various versions of the models (ResNet-18, -34, -50, etc.) just change the number of blocks in each of those groups. This is the definition of a ResNet-18: . rn = ResNet(dls.c, [2,2,2,2]) . Let&#39;s try training this new Resnet-18 architecture. . learn = get_learner(rn) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.625527 | 2.041075 | 0.396688 | 00:55 | . 1 | 1.329917 | 1.507927 | 0.541147 | 00:54 | . 2 | 1.065707 | 1.900392 | 0.499618 | 00:54 | . 3 | 0.870085 | 0.987169 | 0.682293 | 00:54 | . 4 | 0.765841 | 0.779386 | 0.753631 | 00:54 | . Bottleneck Layers . We can use another method when making even deeper models to try and reduce the amount of memory used to make it faster, this might be fore ResNet&#39;s of depth 50 or more. . Rather than stacking 2 convolutions with a kernal size of 3, we can use 3 different convolutions two 1x1 at the start and end, and one 3x3. This is called a bottleneck layer. . . How does this help? 1x1 convolutions are much faster, so this type of block runs much faster than the ones with only 3x3 kernals. This then allows us to use more channels, 4 times more in fact (we end up with 256 channels out instead of just 64) which reduce then restore the number of channels (ie the name bottleneck). . So we end up using more channels in the same amout of time with this type of block architecture. Lets try improving our model with a bottleneck block and use it to build a bigger model ResNet-50. . def _conv_block(ni,nf,stride): return nn.Sequential( ConvLayer(ni, nf//4, 1), ConvLayer(nf//4, nf//4, stride=stride), ConvLayer(nf//4, nf, 1, act_cls=None, norm_type=NormType.BatchZero)) . To get better results from this bigger model we will need to train it longer and we can use bigger images as well. . dls = get_data(URLs.IMAGENETTE_320, presize=320, resize=224) rn = ResNet(dls.c, [3,4,6,3], 4) . File downloaded is broken. Remove /root/.fastai/archive/imagenette2-320.tgz and try again. . Bear in mind even though we are using bigger images, we don&#39;t need to really change our network due to this because its fully convolutional it works just fine (remember the use of pooling layers). This also allows us to use the fastai technique of progressive resizing. . Conclusion . In this article we have built a ResNet convolutional deep learning image model from scratch, using many iterations and variations - including some of the most recent state of the art techniques. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/06/12/resnets-the-key-to-training-deep-neural-networks.html",
            "relUrl": "/deep-learning-theory/2021/06/12/resnets-the-key-to-training-deep-neural-networks.html",
            "date": " • Jun 12, 2021"
        }
        
    
  
    
        ,"post35": {
            "title": "Fastai Application Architectures",
            "content": "Introduction . The fastai deep learning library (as of 2021) is a layered API that has 4 levels of abstraction. . Application layer | High level API | Mid level API | Low level API | . . In this article we will look at how to build custom applications in the fastai library, by looking at how current fastai image model applications are actually built. . Fastai Image Model Applications . cnn_learner . When using this application, the first parameter we need to give it is an architecture which will be used as the body of the network. Usually this will be a ResNet architecture we pre-trained weights that is automaticially downloaded for you. . Next the final layer of the pre-trained model is cut, in fact all layers after the final pooling layer is also cut as well. Within each model we have a dictionary of information that allows us to identify these different points within the layers called model_meta here for example for ResNet50. . model_meta[resnet50] . {&#39;cut&#39;: -2, &#39;split&#39;: &lt;function fastai.vision.learner._resnet_split&gt;, &#39;stats&#39;: ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])} . Key parts of the network are: . Head - The part of the network specialised for a particular task i.e. with a CNN the part after the adaptive average pooling layer | Body - Everything else not the Head including the Stem | Stem - The first layers of the network | . We we take all the layers before the cut point of -2, we get the body of the model that fastai will keep to use for transfer learning. Then we can add a new head. . create_head(20,2) . With this function we can choose how many extra layers should be added at the end as well as how much dropout and pooling. Fastai by default adds 2 linear layers rather than just one, as fastai have found this helps transfer learning work more quickly and easily than just one extra layer. . unet_learner . This architecture is most often used for image segmentation tasks. . We start of building this in the same way as the cnn_learner, chopping off the old head. For image segmentation, we are going to have to add a very different type of head to end up with a model that actually generates an image for segmentation. . One way we could do this is to add layers that can increase the grid size in a CNN, for example duplicating each of the pixels to make an image twice as big - this is known as nearest neighbour interpolation. Another approach uses strides, in this case a stride of half, which is known as transposed convolution. However neither of these approaches works well in practice. . They key problem here is there is simply not enough information in these downsampled activations alone to be able to recreate something like the oroginal image quality needed for segmentation - its a big ask! And perhaps not realistic. . The solution to this problem here is our friend again skip connections however using them not accross one layer - but reaching these connections far accross to the opposite side of the architecture. . . Here on the left half of the model is a CNN, and the transposed convolutional layers on the right, with the extra skip connections in gray. This helps the Unet do a much better job at generate the type of images we want for segmentation. One challenge with Unet&#39;s is the exact architecture does in this case depend on the image size, however fastai has a DynamicUnet object that automatically generates the correct architecture based on the data and image sizes given. . A Siamese Network . Let&#39;s now try to create a custom model. In an earlier article we looked at creating a Siamese network model. Let&#39;s recap the details of that model. . Let&#39;s now build a custom model for the Siamese task. We will use a pre-trained model, pass 2 images through it, concatinate the results, then send them to a custom head that will return 2 predictions. . In terms of overall architecture and models lets define it like this. . class SiameseModel(Module): def __init__(self, encoder, head): self.encoder,self.head = encoder,head def forward(self, x1, x2): ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim=1) return self.head(ftrs) . We can create a body/encoder by taking a pre-trained model and cutting it, we just need to specify where we want to cut. The cut position for a ResNet is -2. . encoder = create_body(resnet34, cut=-2) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . . We can then create a head. If we look at the encoder/body it will tell us the last layer has 512 features, so this head will take 2*512 - as we will have 2 images. . head = create_head(512*2, 2, ps=0.5) . We can now build our model from our constructed head and body. . model = SiameseModel(encoder, head) . Before we can use a Learner to train the model we need to define 2 more things. Firstly, a loss function. We might use here cross-entropy, but as our targets are boolean we need to convert them to integers or Pytorch will throw and error. . Secondly, we need to define a custom splitter that will tell the fastai library how to split the model into parameter groups, which will help train only the head of the model when we do transfer learning. Here we want 2 parameter groups one for the encoder/body and one for the head. So lets define a splitter as well. . def loss_func(out, targ): return nn.CrossEntropyLoss()(out, targ.long()) def siamese_splitter(model): return [params(model.encoder), params(model.head)] . We can now define a learner using our data, model, loss function, splitter and a metric. As we are defining a learner manually here, we also have to call freeze manually as well, to ensure only the last paramete group i.e. the head is trained. . learn = Learner(dls, model, loss_func=loss_func, splitter=siamese_splitter, metrics=accuracy) learn.freeze() . Let&#39;s now train our model. . learn.fit_one_cycle(4, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.523447 | 0.334643 | 0.861299 | 03:03 | . 1 | 0.373501 | 0.231564 | 0.913396 | 03:02 | . 2 | 0.299143 | 0.209658 | 0.920162 | 03:02 | . 3 | 0.251663 | 0.188553 | 0.928281 | 03:03 | . This has trained only our head. Lets now unfreeze the whole model to make it all trainable, and use discriminative learning rates. This will give a lower learning rate for the body and a higher one for the head. . learn.unfreeze() learn.fit_one_cycle(4, slice(1e-6,1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.235140 | 0.188717 | 0.924222 | 04:15 | . 1 | 0.233328 | 0.179823 | 0.932341 | 04:12 | . 2 | 0.210744 | 0.172465 | 0.928958 | 04:12 | . 3 | 0.224448 | 0.176144 | 0.930311 | 04:14 | . Points to consider with architectures . There are a few points to consider when training models in practice. if you are running out of memory or time - then training a smaller model could be a good approach. If you are not training long enough to actually overfit, then you are probably not taking advantage of the capacity of your model. . So one should first try to get to the point where your model is overfitting. . . Often many people when faced with a model that overfits, start with the wrong thing first i.e. to use a smaller model, or more regularization. Using a smaller model should be one of the last steps one tries, as this reduces the capaity of your model to actually learn what is needed. . A better approach is to actually try to use more data, such as adding more labels to the data, or using data augmentation for example. Mixup can be useful for this. Only once you are using much more data and are still overfitting, one could consider more generalisable architectures - for example adding batch norm could help here. . After this if its still not working, one could use regularisation, such as adding dropout to the last layers, but also throughout the model. Only after these have failed one should consider using a smaller model. . Conclusion . In this article we have looked at how to build custom fastai application architectures, using image model examples. .",
            "url": "https://www.livingdatalab.com/fastai/2021/06/12/fastai-application-architectures.html",
            "relUrl": "/fastai/2021/06/12/fastai-application-architectures.html",
            "date": " • Jun 12, 2021"
        }
        
    
  
    
        ,"post36": {
            "title": "Building a Convolutional Image Model from scratch",
            "content": "Introduction . In this article we are going to look at building a convolutional neural network from scratch, using Pytorch. We are also going to look at other techniques than help train models better, such as one-cycle training and batch normalisation. . This article is based on the content from the fastai deep learning course chapter 13. . Convolutions in PyTorch . Pytorch defines a convolutional layer using the method F.conv2d. This uses two rank 4 tensors. . Input tensor (minibatch, in_channels, iH, iW) | Weight tensor (out_channels, in_channels, kH, kW) | . Where iH, iW, kH, kW are the image and kernal widths and heights respectively. Pytorch expects rank 4 tensors as it will process an entire mini-batch of images in one go, as well as applying multiple kernals in one go - which is more efficient to do on a GPU. . Lets try this out by creating a tensor of multiple kernals. . top_edge = tensor([[-1,-1,-1], [ 0, 0, 0], [ 1, 1, 1]]).float() left_edge = tensor([[-1,1,0], [-1,1,0], [-1,1,0]]).float() diag1_edge = tensor([[ 0,-1, 1], [-1, 1, 0], [ 1, 0, 0]]).float() diag2_edge = tensor([[ 1,-1, 0], [ 0, 1,-1], [ 0, 0, 1]]).float() edge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge]) edge_kernels.shape . torch.Size([4, 3, 3]) . We can also create a data loader, and extract one minibatch to test. . mnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(), get_y=parent_label) dls = mnist.dataloaders(path) xb,yb = first(dls.valid) xb,yb = to_cpu(xb),to_cpu(yb) xb.shape . torch.Size([64, 1, 28, 28]) . So we are not quite there, because currently our composite kernal is a rank 3 tensor, and it needs to be rank 4. So in this case we need to define an axis for the number of input channels which will be one (because our greyscale images have one channel). So we can insert an extra axis of 1 in the right place using the unsqueeze method. . edge_kernels.shape,edge_kernels.unsqueeze(1).shape . (torch.Size([4, 3, 3]), torch.Size([4, 1, 3, 3])) . We can now pass the kernals to the convolutional layer along with the data and process the image. . edge_kernels = edge_kernels.unsqueeze(1) batch_features = F.conv2d(xb, edge_kernels) batch_features.shape . torch.Size([64, 4, 26, 26]) . This gives us a tensor of a batch of 64 images, with 4 kernals and 26x26 image (we lost one pixel from each side by convolutions of 28x28). Lets look at one of the images on one channel to see the applied convolution. . show_image(batch_features[0,0]); . So with pure convolutions we lost parts of the image, which become a bit smaller. We can get around this by using padding. We can also use stride to end up with a smaller sampled image. . Model 1 - A basic Convolutional Neural Network to predict 2 digits . We are going to build a simple model to predict 2 digits a 3 or 7, as a multi-class classification problem so we will expect probabilities for each image for the likelihood of it being either 3 or 7. . So we can use gradient descent to actually learn the best values for each of the kernals. . nn.Conv2d is a better method to use when creating a network as it automatically creates a weight matrix. Lets try a very simple model. . broken_cnn = sequential( nn.Conv2d(1,30, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(30,1, kernel_size=3, padding=1) ) broken_cnn(xb).shape . torch.Size([64, 1, 28, 28]) . Note we didn&#39;t need to specify an input size as we do with normal linear layers, as a convolution is applied to every pixel whatever the size of the image. The weights of a convolutional layer are to do with the number of input and output channels and the kernal size. . Putting our test batch through this, we can see it produces an output of 28x28 activations which is not ideal for classification. We could use a series of layers with strides, to reduce down the output activations. . # Stride 2 convolutional layer which will downsample our image def conv(ni, nf, ks=3, act=True): res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2) if act: res = nn.Sequential(res, nn.ReLU()) return res simple_cnn = sequential( conv(1 ,4), #14x14 conv(4 ,8), #7x7 conv(8 ,16), #4x4 conv(16,32), #2x2 conv(32,2, act=False), #1x1 Flatten(), ) simple_cnn(xb).shape . torch.Size([64, 2]) . Create a learner object with this. . learn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy) learn.summary() . Sequential (Input shape: 64) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 4 x 14 x 14 Conv2d 40 True ReLU ____________________________________________________________________________ 64 x 8 x 7 x 7 Conv2d 296 True ReLU ____________________________________________________________________________ 64 x 16 x 4 x 4 Conv2d 1168 True ReLU ____________________________________________________________________________ 64 x 32 x 2 x 2 Conv2d 4640 True ReLU ____________________________________________________________________________ 64 x 2 x 1 x 1 Conv2d 578 True ____________________________________________________________________________ [] Flatten ____________________________________________________________________________ Total params: 6,722 Total trainable params: 6,722 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7ff1128d58c0&gt; Loss function: &lt;function cross_entropy at 0x7ff13e6b55f0&gt; Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Note that the final conv layer output is 64x2x1x1 and flatten removes these unit axes, which is basically the squeeze function but as a network layer. . Let&#39;s try training this model. . learn.fit_one_cycle(2, 0.01) . epoch train_loss valid_loss accuracy time . 0 | 0.080185 | 0.035895 | 0.988714 | 00:13 | . 1 | 0.024726 | 0.029886 | 0.990186 | 00:13 | . Convolutional arithmetic . So we can see in this example the input size is 64x1x28x28, and these axes are batch, channel, height, width. This is often represented in Pytorch as NCHW (where N is batch size). . When we use a stride-2 convolution, we often increase the number of features because we&#39;re decreasing the number of activations in the activation map by a factor of 4; we don&#39;t want to decrease the capacity of a layer by too much at a time. . We also have one bias weight for each channel. So in this example, our stide 2 convolutions halve the grid size - but we also double the number of filters at each layer. This means we get the same amount of computation done. . A receptive field is the area of an image involved in the calculation of a layer. As we go deeper through the layers, a larger area of the original image layers progressively contribute to smaller areas of later layers that have smaller grid sizes. . Model 2 - A Convolutional Neural Network to predict 10 digits . As our previous model did well on predicting 2 digits, we will now try to build a model that predicts all 10 digits, using the full MNIST dataset. . path = untar_data(URLs.MNIST) . The images are in 2 folders training and testing, so we can use the GrandparentSplitter but need to tell it explictly as by default it expects train and valid. . We will define a function to make it easy to define different dataloaders with different batch sizes. . def get_dls(bs=64): return DataBlock( blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(&#39;training&#39;,&#39;testing&#39;), get_y=parent_label, batch_tfms=Normalize() ).dataloaders(path, bs=bs) dls = get_dls() dls.show_batch(max_n=9, figsize=(4,4)) . So we will try and improve our previous model with one with more activations, and we will probably need more filters to handle more numbers, so we could double them for each layer. . But there is a potential problem, if we add more filters we are producing an image of a similar size to our input, which does not force the network to learn useful features. If we use a larger kernal in the first layer, such as 5x5 instead of 3x3, this will force the network to find more useful features from this more limited information. . from fastai.callback.hook import * def simple_cnn(): return sequential( conv(1 ,8, ks=5), #14x14 conv(8 ,16), #7x7 conv(16,32), #4x4 conv(32,64), #2x2 conv(64,10, act=False), #1x1 Flatten(), ) def fit(epochs=1): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit(epochs, 0.06) return learn learn = fit() . /usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 0.694318 | 0.672285 | 0.793600 | 01:06 | . So we trained for one epoch, but that did&#39;nt do well. We can use callbacks to investigate why right after training. The ActivationStats callback use some useful plots, for example we can plot the mean and std dev of the activations of a layer you give the index for, along with the percentage of activations which are zero. . learn.activation_stats.plot_layer_stats(0) . So ideally we want our model to have a smooth mean and std dev during training. Activations near zero are not helpful, as it gives gradient descent little to work with. If we have little to zero activations in earlier layers, this gets even worse in later layers. . learn.activation_stats.plot_layer_stats(-2) . We could try to make training more stable by increasing the batch size with better info for gradients, but less often updated due to larger batch sizes. . Lets try larger batch size. . dls = get_dls(512) learn = fit() . /usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 2.327641 | 2.302224 | 0.113500 | 00:56 | . learn.activation_stats.plot_layer_stats(-2) . This has&#39;nt helped much with the activations, lets see what else we can do. . One cycle training . We have been training our model at the same learning rate, but it may be more helpful to vary the learning rate at different points - for example to have it higher when we are far in the loss landscape from the minimum, and have it lower when we are in the minimum area. In one cycle training, we start at a lower learning rate, building up gradually to a maximum, then gradually reducing the learning rate again. . def fit(epochs=1, lr=0.06): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit_one_cycle(epochs, lr) return learn learn = fit() . /usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 0.197716 | 0.083136 | 0.975800 | 00:55 | . learn.recorder.plot_sched() . Once cycle training also involves varying the momentum with the opposite pattern to the learning rate. . Looking at our layer stats again, we can see there is some improvement but still a large percentage of zero weights. . We can use the colour_dim module to show how the activations of a layer changes through the training accross time. . learn.activation_stats.color_dim(-2) . Here for example we can see on the far left mostly white is with most of the activations at zero, then as time passes from left to right, we can see an expontential build up of activations, which then collapses into zero activations (white bands). Eventually the bands go and you get more consistant activations for most of the model. . Ideally if we can avoid this crashing of activations this can result in better training. . Batch Normalisation . Batch norm is a method we can use to stablise training to try and avoid extreme rises and crashes in activations. Essentially batch norm normalises the activations of each layer using the mean and std dev of the activations. Batch norm also uses extra 2 learnable parameters per layer gamma and beta which are addative and multiplicative factors that can then scale the activations of a layer. . Batchnorm layer output = (Normalised Activations * gamma) + beta . So each layer has its own normalisation and scaling with batchnorm layers. The normalisation is different during training vs validation: in training we use the mean and std dev of a batch to normalise, in validation we use the running mean and std dev calculated during training. . Lets add batchnorm to our layer definition. . def conv(ni, nf, ks=3, act=True): layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)] if act: layers.append(nn.ReLU()) layers.append(nn.BatchNorm2d(nf)) return nn.Sequential(*layers) learn = fit() . /usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 0.129701 | 0.057382 | 0.985700 | 00:58 | . learn.activation_stats.color_dim(-4) . This has given us more gradual training without the crashes in activations. Now we are using batchnorm in our layers it should make it easier to learn at a higher learning rate. . learn = fit(5, lr=0.1) . /usr/local/lib/python3.7/dist-packages/fastai/callback/core.py:50: UserWarning: You are shadowing an attribute (modules) that exists in the learner. Use `self.learn.modules` to avoid this warn(f&#34;You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this&#34;) . epoch train_loss valid_loss accuracy time . 0 | 0.180499 | 0.142405 | 0.957800 | 00:58 | . 1 | 0.078111 | 0.064472 | 0.979600 | 00:58 | . 2 | 0.051010 | 0.052857 | 0.983600 | 00:58 | . 3 | 0.031543 | 0.030566 | 0.990000 | 00:58 | . 4 | 0.015607 | 0.024703 | 0.991900 | 00:58 | . Conclusion . In this article we look at how we can build a convolutional neural network using Pytorch, as well as useful extra techniques to help with model training such as one-cycle training and batch normalisation. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/06/11/convolutional-image-model-from-scratch.html",
            "relUrl": "/deep-learning-theory/2021/06/11/convolutional-image-model-from-scratch.html",
            "date": " • Jun 11, 2021"
        }
        
    
  
    
        ,"post37": {
            "title": "Building an LSTM Language Model from scratch",
            "content": "Introduction . In this article we will look at how we build an LSTM language model that is able to predict the next word in a sequence of words. As part of this, we will also explore several regularization methods. We will build a range of models using basic python &amp; Pytorch to illustrate the fundamentals of this type of model, while also using aspects of the fastai library. We will end up exploring all the different aspects that make up the AWD-LSTM model architecture. . This work is based on material from the fastai deep learning book, chapter 12. . Dataset . We will use the fastai curated Human Numbers dataset for this exercise. This is a dataset of the first 10,000 numbers written as words in english. . path = untar_data(URLs.HUMAN_NUMBERS) Path.BASE_PATH = path path.ls() . (#2) [Path(&#39;valid.txt&#39;),Path(&#39;train.txt&#39;)] . lines = L() with open(path/&#39;train.txt&#39;) as f: lines += L(*f.readlines()) with open(path/&#39;valid.txt&#39;) as f: lines += L(*f.readlines()) lines . (#9998) [&#39;one n&#39;,&#39;two n&#39;,&#39;three n&#39;,&#39;four n&#39;,&#39;five n&#39;,&#39;six n&#39;,&#39;seven n&#39;,&#39;eight n&#39;,&#39;nine n&#39;,&#39;ten n&#39;...] . text = &#39; . &#39;.join([l.strip() for l in lines]) text[:100] . &#39;one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo&#39; . tokens = text.split(&#39; &#39;) tokens[:10] . [&#39;one&#39;, &#39;.&#39;, &#39;two&#39;, &#39;.&#39;, &#39;three&#39;, &#39;.&#39;, &#39;four&#39;, &#39;.&#39;, &#39;five&#39;, &#39;.&#39;] . vocab = L(*tokens).unique() vocab . (#30) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;five&#39;,&#39;six&#39;,&#39;seven&#39;,&#39;eight&#39;,&#39;nine&#39;...] . word2idx = {w:i for i,w in enumerate(vocab)} nums = L(word2idx[i] for i in tokens) nums . (#63095) [0,1,2,1,3,1,4,1,5,1...] . Language Model 1 - Linear Neural Network . Lets first try a simple linear model that will aim to predict each word based on the previous 3 words. To do this we can create our input variable as every sequence of 3 words, and our output/target variable as the next word after each sequence of 3. . So in python as tokens and pytorch tensors as numeric values seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3)) seqswe could construct these variables in the following way. . L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3)) . (#21031) [([&#39;one&#39;, &#39;.&#39;, &#39;two&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;three&#39;, &#39;.&#39;], &#39;four&#39;),([&#39;four&#39;, &#39;.&#39;, &#39;five&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;six&#39;, &#39;.&#39;], &#39;seven&#39;),([&#39;seven&#39;, &#39;.&#39;, &#39;eight&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;nine&#39;, &#39;.&#39;], &#39;ten&#39;),([&#39;ten&#39;, &#39;.&#39;, &#39;eleven&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;twelve&#39;, &#39;.&#39;], &#39;thirteen&#39;),([&#39;thirteen&#39;, &#39;.&#39;, &#39;fourteen&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;fifteen&#39;, &#39;.&#39;], &#39;sixteen&#39;)...] . seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3)) seqs . (#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1, 14]), 1),(tensor([ 1, 15, 1]), 16)...] . We can group these into batches using the DataLoader class. . bs = 64 cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False) . So we will create a linear neural network with 3 layers, and a couple of specific features. . The first feature is to do with using embeddings. The first layer will take the first word embeddings, the second layer the second word embeddings plus the first layer activations, and the third layer the third word embeddings plus the second layer activations. The key observation here is that each word/layer is interpreted in the context of the previous word/layer. . The second feature is that each of these 3 layers will actually be the same layer, that it will have just one weight matrix. Each layer would run into different words even as separate, so really this layer should be able to be repeatedly used to do the same job for each of the 3 words. In other words, while activation values will change as words move through the network, the layer weights will not change from layer to layer. . This way, a layer doesn&#39;t just learn to handle one position i.e. second word position, its forced to generalise and learn to handle all 3 word positions. . class LMModel1(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) def forward(self, x): h = F.relu(self.h_h(self.i_h(x[:,0]))) h = h + self.i_h(x[:,1]) h = F.relu(self.h_h(h)) h = h + self.i_h(x[:,2]) h = F.relu(self.h_h(h)) return self.h_o(h) . So we have 3 key layers: . An embedding layer | A linear layer to create activations (for next word) | A final layer to predict the target 4th word | . Lets try training a model built with this architecture. . learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.824297 | 1.970941 | 0.467554 | 00:01 | . 1 | 1.386973 | 1.823242 | 0.467554 | 00:01 | . 2 | 1.417556 | 1.654497 | 0.494414 | 00:01 | . 3 | 1.376440 | 1.650849 | 0.494414 | 00:01 | . So how might we establish a baseline to judge these results? What if we defined a naive predictor that simply predicted the most common word. Lets find the most common word, and then calculate an accuracy when predicting always the most common word. . n,counts = 0,torch.zeros(len(vocab)) for x,y in dls.valid: n += y.shape[0] for i in range_of(vocab): counts[i] += (y==i).long().sum() idx = torch.argmax(counts) idx, vocab[idx.item()], counts[idx].item()/n . (tensor(29), &#39;thousand&#39;, 0.15165200855716662) . Language Model 2 - Recurrent Neural Network . So in the forward() method rather than repeating the lines for each layer, we could convert this into a for loop which would not only make our code simplier, but allow us to extend to data that was more than 3 words long and of different lengths. . class LMModel2(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) def forward(self, x): h = 0 for i in range(3): h = h + self.i_h(x[:,i]) h = F.relu(self.h_h(h)) return self.h_o(h) . learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.816274 | 1.964143 | 0.460185 | 00:01 | . 1 | 1.423805 | 1.739964 | 0.473259 | 00:01 | . 2 | 1.430327 | 1.685172 | 0.485382 | 00:01 | . 3 | 1.388390 | 1.657033 | 0.470406 | 00:01 | . Note that each time we go through the loop, the resulting activations are passed along to the next loop using the h variable, which is called the hidden state. A recurrent neural network is simply a network that is defined using a loop like this. . Language Model 3 - A better RNN . So notice in the latest model we initialise the hidden state to zero with each run through i.e. each batch, this means our batch size greatly effects the amount of information carried over. Also is there a way we can have more &#39;signal&#39;? rather than just the 4th word, we could try to predict the others for example. . To not loose our hidden state so frequently and carry over more useful information, we could initialise it outside the forward method. However this now makes our model as deep as the sequence of tokens i.e. 10,000 tokens leads to a 10,000 layer network, which will mean to calculate all the gradients back to the first word/layer could be very time consuming. . So rather than calculate all gradients, we can just keep the last 3 layers. To delete all the gradient history in Pytorch we use the detach() method. . This version of the model now carries over activations between calls to forward(), we could call this kind of model stateful. . class LMModel3(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): for i in range(3): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) out = self.h_o(self.h) self.h = self.h.detach() return out def reset(self): self.h = 0 . To use this model we need to ensure our data is in the correct order, for example here we are going to divide it into 64 equally sized parts, with each text of size 3. . m = len(seqs)//bs m,bs,len(seqs) . (328, 64, 21031) . def group_chunks(ds, bs): m = len(ds) // bs new_ds = L() for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs)) return new_ds cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets( group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) batch = dls.one_batch() batch[0].size() . torch.Size([64, 3]) . learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.708583 | 1.873094 | 0.401202 | 00:01 | . 1 | 1.264271 | 1.781330 | 0.433173 | 00:01 | . 2 | 1.087642 | 1.535732 | 0.521875 | 00:01 | . 3 | 1.007973 | 1.578549 | 0.542308 | 00:01 | . 4 | 0.945740 | 1.660635 | 0.569231 | 00:01 | . 5 | 0.902835 | 1.605541 | 0.551923 | 00:01 | . 6 | 0.878297 | 1.527385 | 0.579087 | 00:01 | . 7 | 0.814197 | 1.451913 | 0.606250 | 00:01 | . 8 | 0.783523 | 1.509463 | 0.604087 | 00:01 | . 9 | 0.763500 | 1.511033 | 0.608413 | 00:01 | . Language Model 4 - Creating more signal . So with the current model we still predict just one word for every 3 words which limits the amount of signal - what if we predicted the next word after every word? . To do this we need to restructure our data, so that the target variable has the 3 next words after the 3 first words, we can make this a variable sl for sequence length in this case to 16. . sl = 16 seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) batch = dls.one_batch() batch[0].size() . torch.Size([64, 16]) . [L(vocab[o] for o in s) for s in seqs[0]] . [(#16) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;...], (#16) [&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;,&#39;six&#39;...]] . Now we can refactor our model to predict the next word after each word rather than after each 3 word sequence. . class LMModel4(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): outs = [] for i in range(sl): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) outs.append(self.h_o(self.h)) self.h = self.h.detach() return torch.stack(outs, dim=1) def reset(self): self.h = 0 # Need to reshape output before passing to loss function def loss_func(inp, targ): return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1)) . learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func, metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.226453 | 3.039626 | 0.200033 | 00:00 | . 1 | 2.295425 | 1.925965 | 0.439697 | 00:00 | . 2 | 1.743091 | 1.818798 | 0.423258 | 00:00 | . 3 | 1.471100 | 1.779967 | 0.467285 | 00:00 | . 4 | 1.267640 | 1.823129 | 0.504883 | 00:00 | . 5 | 1.100705 | 1.991244 | 0.500814 | 00:00 | . 6 | 0.960767 | 2.086404 | 0.545085 | 00:00 | . 7 | 0.857365 | 2.240561 | 0.556803 | 00:00 | . 8 | 0.776844 | 2.004017 | 0.568766 | 00:00 | . 9 | 0.711604 | 1.991193 | 0.588949 | 00:00 | . 10 | 0.659614 | 2.064157 | 0.585775 | 00:00 | . 11 | 0.619464 | 2.033359 | 0.606283 | 00:00 | . 12 | 0.587681 | 2.100323 | 0.614176 | 00:00 | . 13 | 0.565472 | 2.145048 | 0.603760 | 00:00 | . 14 | 0.553879 | 2.149167 | 0.605550 | 00:00 | . Because the task is now harder (predicting after each word) we need to train for longer, but we still do well. Since this is effectively a very deep NN, the results can vary each time because the gradients and vary hugely. . Language Model 5 - Multi-layer RNN . While we already in a sense have a multi-layer NN, our repeated part is just once layer still. A deeper RNN gives us more computational power to do better at each loop. . We can use the RNN class to effectively replace the previous class, and allows us to build a new model with multiple stacked RNN&#39;s rather than just the previous one we had. . class LMModel5(Module): def __init__(self, vocab_sz, n_hidden, n_layers): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = torch.zeros(n_layers, bs, n_hidden) def forward(self, x): res,h = self.rnn(self.i_h(x), self.h) self.h = h.detach() return self.h_o(res) def reset(self): self.h.zero_() . learn = Learner(dls, LMModel5(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.008033 | 2.559917 | 0.449707 | 00:00 | . 1 | 2.113339 | 1.726179 | 0.471273 | 00:00 | . 2 | 1.688941 | 1.823044 | 0.389648 | 00:00 | . 3 | 1.466082 | 1.699160 | 0.462646 | 00:00 | . 4 | 1.319908 | 1.701673 | 0.516764 | 00:00 | . 5 | 1.177464 | 1.837683 | 0.543050 | 00:00 | . 6 | 1.041084 | 2.043768 | 0.554688 | 00:00 | . 7 | 0.923601 | 2.067982 | 0.549886 | 00:00 | . 8 | 0.819859 | 2.061354 | 0.562988 | 00:00 | . 9 | 0.735049 | 2.076721 | 0.568685 | 00:00 | . 10 | 0.664878 | 2.080706 | 0.570231 | 00:00 | . 11 | 0.614425 | 2.117641 | 0.586263 | 00:00 | . 12 | 0.577034 | 2.142265 | 0.588053 | 00:00 | . 13 | 0.554870 | 2.124338 | 0.591227 | 00:00 | . 14 | 0.543019 | 2.121613 | 0.590658 | 00:00 | . So this model actually did worse than our previous - why? Because we have a deeper model now (just by one extra layer) we probably have exploding and vanishing activations. . Generally having a deeper layered model gives us more compute to get better results, however this also makes it more difficult to train because the compunded activations can explode or vanish - think matrix multiplications! . Researchers have developed 2 approaches to try and rectify this: long short-term memory layers (LSTM&#39;s) and gated reccurent units (GRU&#39;s). . Language Model 6 - LSTM&#39;s . LSTM&#39;s were invented by Jürgen Schmidhuber and Sepp Hochreiter in 1997, and they have 2 hidden states. . In our previous RNN we have one hidden state &#39;h&#39; that does 2 things: . Holds signal to help predict the next word | Holds signal of all previous words | . These are potentially very different things to remember together in one value, and in practice RRN&#39;s are not very good at retaining the second long term information. LSTM&#39;s have a second hidden state called a cell state specifically to focus on this second requirement as a kind of long short-term memory. . Lets look at the architecture of a LSTM. . . So the inputs come in from the left which are: . Xt: input | ht-1: previous hidden state | ct-1: previous cell state | . The 4 orange boxes are layers with either sigmoid or tanh activation functions. The green circles are element-wise operations. The outputs on the right are: . ht: new hidden state | ct: new cell state | . Which will be used at the next input. The 4 orange layers are called gates. Note also how little the cell state at the top is changed, this is what allows it to better persist over time. . The 4 Gates of an LSTM . Forget gate | Input gate | Cell gate | Output gate | The first gate the forget gate, is a linear layer followed by a sigmoid, gives the LSTM the ability to forget things about its long term state held in the cell state. For example, when the input is a xxbos token, we might expect the LTSM will learn to trigger this to reset its cell state. . The second and third gates work together to update/add to the cell state. The input gate decides which parts of the cell state to update, and the cell gate decides what those updated values should be. . The output gate decides what information from the cell state is used to generate the output. . We can define this as the following class. . class LSTMCell(Module): def __init__(self, ni, nh): self.forget_gate = nn.Linear(ni + nh, nh) self.input_gate = nn.Linear(ni + nh, nh) self.cell_gate = nn.Linear(ni + nh, nh) self.output_gate = nn.Linear(ni + nh, nh) def forward(self, input, state): h,c = state h = torch.cat([h, input], dim=1) forget = torch.sigmoid(self.forget_gate(h)) c = c * forget inp = torch.sigmoid(self.input_gate(h)) cell = torch.tanh(self.cell_gate(h)) c = c + inp * cell out = torch.sigmoid(self.output_gate(h)) h = out * torch.tanh(c) return h, (h,c) . We can refactor the code to make this more efficient, in particular creating just one big matrix multiplication rather than 4 smaller ones. . class LSTMCell(Module): def __init__(self, ni, nh): self.ih = nn.Linear(ni,4*nh) self.hh = nn.Linear(nh,4*nh) def forward(self, input, state): h,c = state # One big multiplication for all the gates is better than 4 smaller ones gates = (self.ih(input) + self.hh(h)).chunk(4, 1) ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3]) cellgate = gates[3].tanh() c = (forgetgate*c) + (ingate*cellgate) h = outgate * c.tanh() return h, (h,c) . The Pytorch chunk method helps us split our tensor into 4 parts. . t = torch.arange(0,10); t . tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . t.chunk(2) . (tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9])) . Here we will define a 2 layer LSTM which is the same network as model 5. We can actually train this at a higher learning rate for less time and do better, as this network should be more stable and easier to train. . class LMModel6(Module): def __init__(self, vocab_sz, n_hidden, n_layers): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)] def forward(self, x): res,h = self.rnn(self.i_h(x), self.h) self.h = [h_.detach() for h_ in h] return self.h_o(res) def reset(self): for h in self.h: h.zero_() . learn = Learner(dls, LMModel6(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelResetter) learn.fit_one_cycle(15, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 3.007779 | 2.770814 | 0.284017 | 00:01 | . 1 | 2.204949 | 1.782870 | 0.425944 | 00:01 | . 2 | 1.606196 | 1.831585 | 0.462402 | 00:01 | . 3 | 1.296969 | 1.999463 | 0.479411 | 00:01 | . 4 | 1.080299 | 1.889699 | 0.553141 | 00:01 | . 5 | 0.828938 | 1.813550 | 0.593262 | 00:01 | . 6 | 0.623377 | 1.710710 | 0.662516 | 00:01 | . 7 | 0.479048 | 1.723749 | 0.687663 | 00:01 | . 8 | 0.350940 | 1.458227 | 0.718913 | 00:01 | . 9 | 0.260764 | 1.484386 | 0.732096 | 00:01 | . 10 | 0.201649 | 1.384711 | 0.752523 | 00:01 | . 11 | 0.158970 | 1.384149 | 0.753011 | 00:01 | . 12 | 0.132954 | 1.377875 | 0.750244 | 00:01 | . 13 | 0.117867 | 1.367185 | 0.756104 | 00:01 | . 14 | 0.109761 | 1.366078 | 0.756104 | 00:01 | . Language Model 7 - Weight-Tied Regularized LSTM&#39;s . While this new LSTM model did much better, we can see it&#39;s overfitting to the training data i.e. notice how while the training loss is going down, the validation loss does not really improve so the model is not generalising well. Dropout can be a regularization method that we can use here to try to prevent overfitting. And architecture that uses dropout as well as an LSTM is called an AWD-LSTM. . Activation regularization (AR) and temporal activation regularization (TAR) are two regularization methods very similar to weight decay. . To regularize the final activations these need to be stored, then we add the means of the squares of them to the loss (times a factor alpha for control). . loss += alpha * activations.pow(2).mean() . TAR is connected to the sequential nature of text i.e. that that outputs of LSTM&#39;s should make sense when in order. TAR encourages this by penalising large differences between consequtive activations so to encourage them to be as small as possible. . loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean() . AR is usually applied to dropped out activations (to not penalise activations zeroed) while TAR is applied to non-dropped out activations for the opposite reasons. The RNNRegularizer callback will apply both of these. . With Weight tying we make use of a symmeterical aspect of embeddings in this model. At the start of the model the embedding layer converts words to embedding numbers, at the end of the model we map the final layer to words. We might expect these could be very similar mappings if not the same, so we can explictly encourage this by actually making the weights the same for this first and final layers/embeddings. . self.h_o.weight = self.i_h.weight . So we can combine dropout with AR &amp; TAR and weight tying to train our LSTM. . class LMModel7(Module): def __init__(self, vocab_sz, n_hidden, n_layers, p): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.drop = nn.Dropout(p) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h_o.weight = self.i_h.weight self.h = [torch.zeros(n_layers, bs, n_hidden) for _ in range(2)] def forward(self, x): raw,h = self.rnn(self.i_h(x), self.h) out = self.drop(raw) self.h = [h_.detach() for h_ in h] return self.h_o(out),raw,out def reset(self): for h in self.h: h.zero_() # Create regularized learner using RNNRegularizer learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)]) # This is the equivilent as the TextLearner automatically adds these callbacks learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . # Train the model and add extra regularization with weight decay learn.fit_one_cycle(15, 1e-2, wd=0.1) . epoch train_loss valid_loss accuracy time . 0 | 2.513700 | 1.898873 | 0.498942 | 00:01 | . 1 | 1.559825 | 1.421029 | 0.651937 | 00:01 | . 2 | 0.810041 | 1.324630 | 0.703695 | 00:01 | . 3 | 0.406249 | 0.870849 | 0.801514 | 00:01 | . 4 | 0.211201 | 1.012451 | 0.776774 | 00:01 | . 5 | 0.117430 | 0.748297 | 0.827474 | 00:01 | . 6 | 0.072397 | 0.652809 | 0.843587 | 00:01 | . 7 | 0.050372 | 0.740491 | 0.826172 | 00:01 | . 8 | 0.037560 | 0.796995 | 0.831462 | 00:01 | . 9 | 0.028582 | 0.669326 | 0.850830 | 00:01 | . 10 | 0.022323 | 0.614551 | 0.855632 | 00:01 | . 11 | 0.018281 | 0.670560 | 0.858317 | 00:01 | . 12 | 0.014915 | 0.645430 | 0.856771 | 00:01 | . 13 | 0.012732 | 0.656426 | 0.855387 | 00:01 | . 14 | 0.011765 | 0.683027 | 0.853271 | 00:01 | . Conclusion . In this article we have examined how we build an LSTM language model, in particular the AWD-LSTM architecture, which also makes use of several regularization techniques. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/natural-language-processing/2021/05/31/lstm-language-model-from-scratch.html",
            "relUrl": "/deep-learning-theory/natural-language-processing/2021/05/31/lstm-language-model-from-scratch.html",
            "date": " • May 31, 2021"
        }
        
    
  
    
        ,"post38": {
            "title": "The fastai Mid-level API",
            "content": "Introduction . In this article we will introduce and explore the fastai mid-level API, in particular it&#39;s data preparation features. The mid-level api offers more control and customisation than the high-level api. We will apply the mid-level api to the example of predicting Siamese Images. . The fastai library (as of 2021) is a layered API that has 4 levels of abstraction. . Application layer | High level API | Mid level API | Low level API | . . Mid-level API key concepts . Transforms . In a previous article on text classification we saw how tokenisation and numericalisation were used to prepare the text data for the model. . Both of these classes also have a decode() method, that allows us to reverse the process i.e. to convert tokens back into text, though this may not be exactly the same as the default tokeniser currently is not entirely reversable. . decode is also used by show_batch() and show_results(), as well as by other inference methods. . When we create an object of the tokenizer or numeraclize class, a setup method is called (which trains a tokenizer if needed and creates a vocab for the numericalizer) each is then applied to the text stream in turn. These transformation type tasks are common, so fastai has created a base level class to encapsulate them called the Transform class. Both Tokenize and Numericalize are Transforms. . In general, a Transform is an object that behaves like a function and has an optional setup method that will initialize some inner state (like the vocab inside num) and an optional decode that will reverse the function (this reversal may not be perfect, as we saw with tok). . Another aspect about transforms is that they are always used with tuples, as this reflects the common nature of our data in terms of input &amp; output variables. Also when we apply a transform to this tuple e.g. Resize we may want to apply it in a different way to the input and output variables (if at all). . Creating your own transform . So to create your own transform you can do this by writing a function, and then passing it to the Transform class. The Transform class will only apply the given function to one of a matching type, so for example because we have specified the type as int here the transform is not applied when the input is a floating point number. . def f(x:int): return x+1 tfm = Transform(f) tfm(2),tfm(2.0) . (3, 2.0) . Also note here no setup() or decode() methods have been created here. . This approach of passing a function as an argument to another function is called a decorator which is specified by being preceeded by an &#39;@&#39; symbol and putting it before a function definition. So we can do the same as above using this approach. . @Transform def f(x:int): return x+1 f(2),f(2.0) . (3, 2.0) . If we want to specify a setup or decode method we will instead need to subclass Transform and implement the methods that way. . class NormalizeMean(Transform): def setups(self, items): self.mean = sum(items)/len(items) def encodes(self, x): return x-self.mean def decodes(self, x): return x+self.mean . When used, this class will first run the setup method, then apply the encodes method. The decode method will do the reverse when run. . tfm = NormalizeMean() tfm.setup([1,2,3,4,5]) start = 2 y = tfm(start) z = tfm.decode(y) tfm.mean,y,z . (3.0, -1.0, 2.0) . Note the methods implmented and called are different i.e. setups vs setup. The reason for this is for example here setup also does some other things before then calling setup for you. . Pipelines . To join several transforms together we can use the Pipeline class, which is essentially a list of transforms. . tok = Tokenizer.from_folder(path) tok.setup(txts) toks = txts.map(tok) num = Numericalize() num.setup(toks) nums = toks.map(num) tfms = Pipeline([tok, num]) t = tfms(txts[0]); t[:20] . TensorText([ 2, 19, 932, 81, 27, 20, 32, 34, 7, 260, 119, 1256, 143, 62, 64, 11, 8, 415, 1289, 14]) . You can also decode the pipeline, but there is no setup. . tfms.decode(t)[:100] . &#39;xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the f&#39; . TfmdLists . The class we can use to connect our raw data (e.g. files) to a pipeline is the TfmdLists class. This can also run the appropriate setup methods for us. We can do this in a short, one line way for example. . tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize]) . When initialised, TfmdLists will run the setup method of each transform in order, passing the items transformed by the previous transform. We can see the result of the pipeline on any item by indexing into the objected created. . t = tls[0]; t[:20] . TensorText([ 2, 19, 1033, 73, 28, 20, 30, 35, 7, 265, 120, 1061, 176, 56, 70, 10, 8, 457, 1440, 14]) . TfmdLists also can decode. . tls.decode(t)[:100] . &#39;xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the f&#39; . And show. . tls.show(t) . xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the film stood out even when viewing it so many years after it was made . xxmaj the story by the little known c xxmaj virgil xxmaj georghiu is remarkable , almost resembling a xxmaj tolstoy - like story of a man buffeted by a cosmic scheme that he can not comprehend . xxmaj compare this film with better - known contemporary works such as xxmaj xxunk &#39;s &#34; schindler &#39;s xxmaj list &#34; and you begin to realize the trauma of the xxmaj world xxmaj war xxup ii should be seen against the larger canvas of racism beyond the simplistic xxmaj nazi notion of xxmaj aryan vs xxmaj jews . xxmaj this film touches on the xxmaj hungarians dislike for the xxmaj romanians , the xxmaj romanians dislike of the xxmaj russians and so on .. even touching on the xxmaj jews &#39; questionable relationships with their xxmaj christian xxmaj romanian friends , while under stress . xxmaj as i have not read the book , it is difficult to see how much has been changed by the director and screenplay writers . xxmaj for instance , it is interesting to study the xxmaj romanian peasant &#39;s view of emigrating to xxup usa with the view of making money only to return to xxmaj romania and invest his earnings there . xxmaj in my opinion , the character of xxmaj johann xxmaj moritz was probably one of the finest roles played by xxmaj anthony xxmaj quinn ranking alongside his work in &#34; la xxunk the xxmaj greek &#34; and &#34; xxunk &#34; . xxmaj the finest and most memorable sequence in the film is the final one with xxmaj anthony xxmaj quinn and xxmaj virna xxmaj lisi trying to smile . xxmaj the father carrying a daughter born out his wife &#39;s rape by xxmaj russians is a story in itself but the director is able to show the reconciliation by a simple gesture -- the act of carrying the child without slipping into melodramatic footage . xxmaj today after the death of xxmaj princess xxmaj diana we often remark about the insensitive paparazzi . xxmaj the final sequence is an indictment of the paparazzi and the insensitive media ( director xxmaj verneuil also makes a similar comment during the court scene as the cameramen get ready to pounce on xxmaj moritz ) . xxmaj the interaction between xxmaj church and xxmaj state was so beautifully summed up in the orthodox priest &#39;s laconic statement &#34; i pray to xxmaj god that xxmaj he guides those who have power to use them well . &#34; xxmaj some of the brief shots , such as those of a secretary of a minister doodling while listening to a petition -- said so much in so little footage . xxmaj the direction was so impressive that the editing takes a back seat . xxmaj finally what struck me most was the exquisite rich texture of colors provided by the cameraman xxmaj andreas xxmaj winding -- from the brilliant credit sequences to the end . i recalled that he was the cameraman of another favorite xxmaj french film of mine called &#34; ramparts of xxmaj clay &#34; directed by jean - louis xxmaj xxunk . i have not seen such use of colors in a long while save for the xxmaj david xxmaj lean epics . xxmaj there were flaws : i wish xxmaj virna xxmaj lisi &#39;s character was more fleshed out . i could never quite understand the xxmaj serge xxmaj xxunk character -- the only intellectual in the entire film . xxmaj the railroad station scene at the end seems to be lifted out of xxmaj sergio xxmaj leone westerns . xxmaj finally , the film was essentially built around a love story , that unfortunately takes a back seat . xxmaj to sum up this film impressed me in more departments than one . xxmaj the story is relevant today as it was when it was made . . TfmdLists is plural because it can accomodate both training and validation data using a splits parameter, you just need to pass the indicies for each set. . cut = int(len(files)*0.8) splits = [list(range(cut)), list(range(cut,len(files)))] tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize], splits=splits) . You can then access the train and validation parts using the train and valid attributes. . tls.valid[0][:20] . TensorText([ 2, 22, 15452, 12, 9, 8, 16833, 22, 16, 13, 483, 2773, 12, 2472, 596, 46, 13, 955, 24, 4841]) . You can also convert a TfmdLists object directly into a Dataloaders object using the dataloaders() method. . More generally, you will most likely have 2 or more parallel pipelines of transforms: one for processing raw data into inputs and one to process raw data into outputs/targets. . So in this example, to get the target (a label) we can get it from the parent folder. There is a function parent_label() that can do this for us. . lbls = files.map(parent_label) lbls . (#50000) [&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;...] . We then need a transform that can take these targets, and extract the unique class names to build a vocab during the setup() method, and transform these string class names into integers. The Categorize class can do this for us. . cat = Categorize() cat.setup(lbls) cat.vocab, cat(lbls[0]) . ([&#39;neg&#39;, &#39;pos&#39;], TensorCategory(1)) . So putting these together, from our raw files data we can create a TfmdLists object that will take our files reference, and chain these two transforms together so we get our processed target variable. . tls_y = TfmdLists(files, [parent_label, Categorize()]) tls_y[0] . TensorCategory(1) . But this means we have separate TfmdLists objects for our input and output variables. To bind these into one object we need the Datasets class. . Datasets and Dataloaders . The Datasets object allows us to create two or more piplines bound together and output a tuple result. It will do the setup() for us, and if we index into this Datasets object it will return a tuple with the results of each pipeline. . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms]) x,y = dsets[0] x[:20],y . (TensorText([ 2, 19, 1033, 73, 28, 20, 30, 35, 7, 265, 120, 1061, 176, 56, 70, 10, 8, 457, 1440, 14]), TensorCategory(1)) . As before if we pass a splits parameter, this will further split these into separate train and validation sets. . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms], splits=splits) x,y = dsets.valid[0] x[:20],y . (TensorText([ 2, 22, 15452, 12, 9, 8, 16833, 22, 16, 13, 483, 2773, 12, 2472, 596, 46, 13, 955, 24, 4841]), TensorCategory(0)) . We can also reverse the process to get back to our raw data using decode. . t = dsets.valid[0] dsets.decode(t) . (&#39;xxbos &#34; igor and the xxmaj lunatics &#34; is a totally inept and amateurish attempt at a crazy - hippie - cult - killing - spree horror movie . xxmaj apparently even nearly twenty years later , xxmaj charles xxmaj manson was still inspiring overenthusiastic but incompetent trash - filmmakers . xxmaj this is a typical xxmaj troma production , meaning in other words , there &#39;s a lot of boring and totally irrelevant padding footage to accompany the nonsensical plot . xxmaj there &#39;s a bit of random gore and gratuitous nudity on display x96 which is n &#39;t bad x96 but it &#39;s all so very pointless and ugly that it becomes frustrating to look at . &#34; igor and the xxmaj lunatics &#34; is so desperate that it &#39;s even using a lot of the footage twice , like the circle saw killing for example . xxmaj the incoherent plot tries to tell the story of a hippie cult run by the drug - addicted and xxmaj charlie xxmaj manson wannabe xxmaj paul . xxmaj one of xxmaj paul &#39;s lower ranked disciples , named xxmaj igor , becomes a little bit too obsessed with the xxmaj bible stories and drug orgies and gradually causes the entire cult to descent further into criminal insanity . xxmaj just to illustrate through a little example exactly how crazy xxmaj igor is : he tears the heart straight out of the chest of a really sexy black hitch - hiker girl ! xxmaj there &#39;s an annoying synthesizer soundtrack and some truly embarrassingly lame pseudo - artistic camera tricks , like slow - motion footage and lurid dream sequences . xxmaj maybe there &#39;s one sequence that more or less qualifies as worthwhile for trash fanatics and that &#39; is when a poor girl is cut in half with a machete . xxmaj for no particular reason , the camera holds the shot of the blade in the bloodied stomach for fifteen whole seconds .&#39;, &#39;neg&#39;) . Finally before we can use this data to train a model, we need to convert this Datasets object into a Dataloaders object. In this text example, we also need to pass along a special argument to take care of the padding problem with text data, just before we batch the elements which can do using the before_batch argument. . dls = dsets.dataloaders(bs=64, before_batch=pad_input) . dataloaders directly calls DataLoader on each subset of our Datasets. fastai&#39;s DataLoader expands the PyTorch class of the same name and is responsible for collating the items from our datasets into batches. It has a lot of points of customization, but the most important ones that you should know are: . after_item:: Applied on each item after grabbing it inside the dataset. This is the equivalent of item_tfms in DataBlock. | before_batch:: Applied on the list of items before they are collated. This is the ideal place to pad items to the same size. | after_batch:: Applied on the batch as a whole after its construction. This is the equivalent of batch_tfms in DataBlock. | . So putting all these steps together taking our raw data to ending up with a Dataloaders object ready to train a model. . tfms = [[Tokenizer.from_folder(path), Numericalize], [parent_label, Categorize]] files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;]) splits = GrandparentSplitter(valid_name=&#39;test&#39;)(files) dsets = Datasets(files, tfms, splits=splits) dls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input) . Note also the use of GrandparentSplitter and dl_type. This last argument is to tell dataloaders to use the SortedDL class of DataLoader, and not the usual one. SortedDL constructs batches by putting samples of roughly the same lengths into batches. . So the above is equalivilent to what we did with the high-level datablock api, just using the mid-level api which exposes more control, customisation and choices. The mid-level api version of all this was of course this. . path = untar_data(URLs.IMDB) dls = DataBlock( blocks=(TextBlock.from_folder(path),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path) . Applying the Mid-level API: Siamese Pair . So we will apply using the mid-level api to a Siamese pair use case. A Siamese model takes 2 images and has to decide if they are of the same category or not. We will use fastai&#39;s pets dataset for this exercise. . Lets get the data. . from fastai.vision.all import * path = untar_data(URLs.PETS) files = get_image_files(path/&quot;images&quot;) . If we didn&#39;t need to show our input data, we could just create one stream to process the input images. Since we would also like to be able to look at the input images as well, we need to do something different, creating a custom type. When you call the show() method on a TfmdLists or Datasets object, it will decode items till you end up with items of the same type of object that the show method is called upon. . We will create a SiameseImage class that is subclassed from fastuple and will contain 3 things: 2 images, and a boolean that indicates of they are the same class. We will also implement a custom show method, that joins the 2 images with a black line divider. . The most important part of this class are the last 3 lines. . class SiameseImage(fastuple): def show(self, ctx=None, **kwargs): img1,img2,same_breed = self if not isinstance(img1, Tensor): if img2.size != img1.size: img2 = img2.resize(img1.size) t1,t2 = tensor(img1),tensor(img2) t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1) else: t1,t2 = img1,img2 line = t1.new_zeros(t1.shape[0], t1.shape[1], 10) return show_image(torch.cat([t1,line,t2], dim=2), title=same_breed, ctx=ctx) . img = PILImage.create(files[0]) s = SiameseImage(img, img, True) s.show(); . img1 = PILImage.create(files[1]) s1 = SiameseImage(img, img1, False) s1.show(); . The key thing about transforms is that they dispatch over tuples or their subclasses. Thats why we subclassed from fastuple, so we can apply any transform that works on images to our SiameseImage object and it will be applied to each image in the tuple. . For example. . s2 = Resize(224)(s1) s2.show(); . Here the Resize transform is applied to each of the images, but not the boolean target variable. . Lets now build a better SiameseTransform class for training our model. . Lets first define a function that will extract the target classes of our images. . def label_func(fname): return re.match(r&#39;^(.*)_ d+.jpg$&#39;, fname.name).groups()[0] . So this is how we cill create our dataset. We will pick a series of images, and for each image we pick we will with a probability of 0.5 pick an image of the same or different class, and assign a true or false label accordingly. This will be done in the _draw() method. . There is also a difference between the training and validation sets, which is exactly why the transforms need to be initialised with the splits: with the training set we will make that random pick each time we read an image, whereas with the validation set we will make the random pick only once at initialisation - which is a kind of data augmentation that allows us to get more varied samples during training - but ensures a consistant validation set throughout. . class SiameseTransform(Transform): def __init__(self, files, label_func, splits): self.labels = files.map(label_func).unique() self.lbl2files = {l: L(f for f in files if label_func(f) == l) for l in self.labels} self.label_func = label_func self.valid = {f: self._draw(f) for f in files[splits[1]]} def encodes(self, f): f2,t = self.valid.get(f, self._draw(f)) img1,img2 = PILImage.create(f),PILImage.create(f2) return SiameseImage(img1, img2, t) def _draw(self, f): same = random.random() &lt; 0.5 cls = self.label_func(f) if not same: cls = random.choice(L(l for l in self.labels if l != cls)) return random.choice(self.lbl2files[cls]),same . splits = RandomSplitter()(files) tfm = SiameseTransform(files, label_func, splits) tfm(files[0]).show(); . So to recap: in the mid-level API we have 2 classes that can help us apply transforms to our data: TfmdLists and Datasets. One applies a single pipeline of transforms, while the other can apply several pipelines in parallel to build tuples. Given our new transform here already creates tuples, so we can just use TfmdLists in this case. . tls = TfmdLists(files, tfm, splits=splits) show_at(tls.valid, 0); . We are almost there, to create a Dataloader from this we just call the dataloaders method on this object. But we need to be careful, as our new transform class does not take item_tfms and batch_tfms like a DataBlock. . However the fastai DataLoader has several hooks that are named after events; here what we apply on the items after they are grabbed is called after_item, and what we apply on the batch once it&#39;s built is called after_batch. . dls = tls.dataloaders(after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]) . Note also we have to explictly pass more transforms than we would previously, this is because the DataBlock class/API usually adds these automatically, and since we have create a custom transform we need to explictly request these. . ToTensor is the one that converts images to tensors (again, it&#39;s applied on every part of the tuple). | IntToFloatTensor converts the tensor of images containing integers from 0 to 255 to a tensor of floats, and divides by 255 to make the values between 0 and 1. | . We now have the right DataLoaders object ready to train a model to predict on Siamese images. . Conclusion . We have seen how we can use fastai&#39;s mid-level api to do more custom work as needed, with more control than we would have with the high-level data block api. .",
            "url": "https://www.livingdatalab.com/fastai/2021/05/30/fastai-midlevel-api.html",
            "relUrl": "/fastai/2021/05/30/fastai-midlevel-api.html",
            "date": " • May 30, 2021"
        }
        
    
  
    
        ,"post39": {
            "title": "Creating a custom text classifier for movie reviews",
            "content": "Introduction . In this article we are going to train a deep learning text classifier using the fastai library. We will do this for the IMDB movie reviews dataset. In particular, we will look at fastai&#39;s ULMFit approach which involves fine tuning a language model more with specialised text before using this language model as a basis for a classification model. . Text Pre-processing . So how might we proceed with building a language model, that we can then use for clasisifcation? Consider with one of the simplest neural networks, a collaberative filtering model. This uses embedding matrices to encode different items (such as films) and users, combine these using dot products to calculate a value, which we test against known ratings - and use gradient descent to learn the correct embedding matrices to best predict these ratings. . Optionally, we can create instead a deep learning model from this by concatinating the embedding matrices instead of the dot product, then putting the result through an activtion function, and more layers etc. . So we could use a similar approach, where we put a sequence of words through a neural network via encoding them in an embedding martix for words. However a significant difference from the collaberative filtering approach here is the idea of a sequence. . We can proceed with these 5 steps: . Tokenisation: convert words to recognised units | Numericalisation: convert tokens to numbers | Create data loader: Create a data loader to train the language model which creates a target variable offset by one word from the input variable from the text data | Train language model: We need to train a model that can take an amount of text data of variable length, and be able to predict the next word for any word in the sequence. | Train classifier model: Using what the language model has learned about the text as a basis, we can build on top of this to create and train a language model. | This is an approach pioneered by fastai called the Universal Langauage Model Fine-tuining (ULMFit) approach. . . Tokenisation . Lets get the data and tokenise it using the fastai library tools. . path = untar_data(URLs.IMDB) files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) # Show example text data txt = files[0].open().read(); txt[:75] . &#39;I caught up with this movie on TV after 30 years or more. Several aspects o&#39; . Fastai has an english word tokeniser, lets see how it works. . # Test word tokeniser function spacy = WordTokenizer() toks = first(spacy([txt])) print(coll_repr(toks, 30)) . (#626) [&#39;I&#39;,&#39;caught&#39;,&#39;up&#39;,&#39;with&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;on&#39;,&#39;TV&#39;,&#39;after&#39;,&#39;30&#39;,&#39;years&#39;,&#39;or&#39;,&#39;more&#39;,&#39;.&#39;,&#39;Several&#39;,&#39;aspects&#39;,&#39;of&#39;,&#39;the&#39;,&#39;film&#39;,&#39;stood&#39;,&#39;out&#39;,&#39;even&#39;,&#39;when&#39;,&#39;viewing&#39;,&#39;it&#39;,&#39;so&#39;,&#39;many&#39;,&#39;years&#39;,&#39;after&#39;,&#39;it&#39;...] . # Test word tokeniser class tkn = Tokenizer(spacy) print(coll_repr(tkn(txt), 31)) . (#699) [&#39;xxbos&#39;,&#39;i&#39;,&#39;caught&#39;,&#39;up&#39;,&#39;with&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;on&#39;,&#39;xxup&#39;,&#39;tv&#39;,&#39;after&#39;,&#39;30&#39;,&#39;years&#39;,&#39;or&#39;,&#39;more&#39;,&#39;.&#39;,&#39;xxmaj&#39;,&#39;several&#39;,&#39;aspects&#39;,&#39;of&#39;,&#39;the&#39;,&#39;film&#39;,&#39;stood&#39;,&#39;out&#39;,&#39;even&#39;,&#39;when&#39;,&#39;viewing&#39;,&#39;it&#39;,&#39;so&#39;,&#39;many&#39;,&#39;years&#39;...] . The class goes beyond just converting the text to tokens for words, for example it creates tokens like &#39;xxbos&#39; which is a special token to indicate the beginning of a new text sequence i.e. &#39;beggining of stream&#39; standard NLP concept. . The class applies a series fo rules and transformations to the text, here is a list of them. . defaults.text_proc_rules . [&lt;function fastai.text.core.fix_html&gt;, &lt;function fastai.text.core.replace_rep&gt;, &lt;function fastai.text.core.replace_wrep&gt;, &lt;function fastai.text.core.spec_add_spaces&gt;, &lt;function fastai.text.core.rm_useless_spaces&gt;, &lt;function fastai.text.core.replace_all_caps&gt;, &lt;function fastai.text.core.replace_maj&gt;, &lt;function fastai.text.core.lowercase&gt;] . Numericalisation . # Get first 2000 reviews to test txts = L(o.open().read() for o in files[:2000]) # Tokenise toks = tkn(txt) # Select subset of tokenised reviews toks200 = txts[:200].map(tkn) num = Numericalize() # Numericalise tokens - create a vocab num.setup(toks200) # Show first 20 tokens of vocab coll_repr(num.vocab,20) . &#34;(#2096) [&#39;xxunk&#39;,&#39;xxpad&#39;,&#39;xxbos&#39;,&#39;xxeos&#39;,&#39;xxfld&#39;,&#39;xxrep&#39;,&#39;xxwrep&#39;,&#39;xxup&#39;,&#39;xxmaj&#39;,&#39;the&#39;,&#39;.&#39;,&#39;,&#39;,&#39;and&#39;,&#39;a&#39;,&#39;of&#39;,&#39;to&#39;,&#39;is&#39;,&#39;in&#39;,&#39;it&#39;,&#39;i&#39;...]&#34; . # Now we can convert tokens to numbers for example nums = num(toks)[:20]; nums . TensorText([ 2, 19, 726, 79, 29, 21, 32, 31, 7, 314, 112, 1195, 138, 63, 71, 10, 8, 393, 1524, 14]) . Create data loader . So we need to join all the text together, and then divide it into specific sized batches of multiple lines of text of fixed length, which maintain the correct order of the text within each batch. At every epoch the order of the reviews is shuffled, but we then join these all together and construct mini-batches in order, which our model will process and learn from. This is all done automatically by the fastai library tools. . # Get some example numericalised tokens nums200 = toks200.map(num) # Pass to dataloader dl = LMDataLoader(nums200) # Get first batch of data and check sizes x,y = first(dl) x.shape,y.shape . (torch.Size([64, 72]), torch.Size([64, 72])) . # Examine example input variable should be start of a text &#39; &#39;.join(num.vocab[o] for o in x[0][:20]) . &#39;xxbos i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of&#39; . # Examine example target variable which is the same plus added next word - this is what we want to predict &#39; &#39;.join(num.vocab[o] for o in y[0][:20]) . &#39;i caught up with this movie on xxup tv after 30 years or more . xxmaj several aspects of the&#39; . Training a text classifier . Fine tune language model . We can further simplify the text preparation for training our language model by combining the tokenisation, numericalisation and dataloader creation into one step by creating a TextBlock and then a dataloader. . # Create text dataloader for language model training dls_lm = DataBlock( blocks=TextBlock.from_folder(path, is_lm=True), get_items=get_imdb, splitter=RandomSplitter(0.1) ).dataloaders(path, path=path, bs=128, seq_len=80) . # Create a language model learner, by default will use x-entropy loss learn = language_model_learner( dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() # Train model learn.fit_one_cycle(1, 2e-2) # Save model encoder learn.save_encoder(&#39;finetuned&#39;) . Fine tune classifier model . To fine tune the classifier model we create the data loader in a slightly different way. . # Create text dataloader for classifier model training - using lm vocab dls_clas = DataBlock( blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path, path=path, bs=128, seq_len=72) . # Create classifier learner learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16() # Load encoder from language model learn = learn.load_encoder(&#39;finetuned&#39;) . When fine tuning the classifier, it is found to be best if we gradually unfreeze layers to train, and this is best done in manual steps. The first fit will just train the last layer. . # Train model - last layer only learn.fit_one_cycle(1, 2e-2) . # Unfreeze a few more layers and train some more with discriminative learning rates learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . # Unfreeze more layers and train more learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . # Unfreeze whole model and train more learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . On this IMDB dataset we can achieve a classification accuracy of around 95% using this approach. . Conclusion . In this article we have looked in more detail at how we can train a text classifier using the 3 step ULMFit fastai approach, and achieve a good level of accuracy. We also saw in more detail what the fastai library does under the hood to make this process much easier. .",
            "url": "https://www.livingdatalab.com/projects/natural-language-processing/2021/05/29/custom-text-classifier-movie-reviews.html",
            "relUrl": "/projects/natural-language-processing/2021/05/29/custom-text-classifier-movie-reviews.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post40": {
            "title": "Collaberative filtering from scratch",
            "content": "Introduction . In this article we will look to build a collaberitive filtering model from scratch, using pure Pytorch and some support from the Fastai deep learning library. We will also look at the theory and mathematics behind collaberative filtering. . Dataset . We will use the MovieLens dataset, and a special subset curated by fastai of the 100,000 movies. This consists of 2 separate tables for ratings and movies, which we will join together. . path = untar_data(URLs.ML_100k) # Load ratings table ratings = pd.read_csv(path/&#39;u.data&#39;, delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . movies = pd.read_csv(path/&#39;u.item&#39;, delimiter=&#39;|&#39;, encoding=&#39;latin-1&#39;, usecols=(0,1), names=(&#39;movie&#39;,&#39;title&#39;), header=None) movies.head() . movie title . 0 1 | Toy Story (1995) | . 1 2 | GoldenEye (1995) | . 2 3 | Four Rooms (1995) | . 3 4 | Get Shorty (1995) | . 4 5 | Copycat (1995) | . ratings = ratings.merge(movies) ratings.head() . user movie rating timestamp title . 0 196 | 242 | 3 | 881250949 | Kolya (1996) | . 1 63 | 242 | 3 | 875747190 | Kolya (1996) | . 2 226 | 242 | 5 | 883888671 | Kolya (1996) | . 3 154 | 242 | 3 | 879138235 | Kolya (1996) | . 4 306 | 242 | 5 | 876503793 | Kolya (1996) | . dls = CollabDataLoaders.from_df(ratings, item_name=&#39;title&#39;, bs=64) dls.show_batch() . user title rating . 0 542 | My Left Foot (1989) | 4 | . 1 422 | Event Horizon (1997) | 3 | . 2 311 | African Queen, The (1951) | 4 | . 3 595 | Face/Off (1997) | 4 | . 4 617 | Evil Dead II (1987) | 1 | . 5 158 | Jurassic Park (1993) | 5 | . 6 836 | Chasing Amy (1997) | 3 | . 7 474 | Emma (1996) | 3 | . 8 466 | Jackie Chan&#39;s First Strike (1996) | 3 | . 9 554 | Scream (1996) | 3 | . Theory . The key data here is the ratings i.e. the user-movie ratings, as we can see in the listing above. In collaberative filtering, an easier way to see this is as a user-item matrix, with movies as columns, users as rows, and cells as the ratings for each user-movie combination. . . We can see here some cells are not filled in which are ratings we do not know, these are the values we would like to predict so we can know for each user which movie they would like. . So how might we approach this? If we imagine there are some reasons that effect peoples preferences, lets call them factors such as genre, actors etc then that might give us a basis to figure out which users would like each movie. What if we could represent these factors as a set of numbers? then we could represent each user and movie as a unique set of these numbers (or vectors) representing how much of each of the factors that user or movie represented. . Then we could say, we want each of these user and movie factors vectors when multipled to equal a rating. This would give us a basis to learn these factors, as we have ratings we know, and we could use these to estimate the ratings we don&#39;t know. This approach of using movie vectors multipled by user vectors and summed up is known as the dot product and is the basis of matrix multiplication. . . So we can randomly initialise these user and movie vectors, and learn the correct values for these that predict the ratings we know, using gradient descent. . So to do the dot product we could look up the index of each user and movie, then multiply the vectors. But neural networks don&#39;t know how to look up using an index, they only multiply matrices together. However we can do a index looking up using matrix multiplication by using one-hot encoded vectors. . The matrix you index by multiplying by a one-hot encoded matrix, is called an embedding or embedding matrix. So our model will learn the values of these embedding matrices for the users and movies, using gradient descent. . It&#39;s actually very easy to create a collaberative filtering model using fastai&#39;s higher level methods - but we are going to explore doing this from scratch in this article. . learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) . learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.937713 | 0.953276 | 00:11 | . 1 | 0.838276 | 0.873933 | 00:11 | . 2 | 0.717332 | 0.832581 | 00:11 | . 3 | 0.592723 | 0.818247 | 00:11 | . 4 | 0.476174 | 0.818869 | 00:11 | . Collaberative filtering - Model 1 . We will now create our first collaberative filtering model from scratch. This will contain the embedding matrices for the users and movies, and will implement a method (in Pytorch this is normally the forward method) to do a dot product of these 2 matrices. . So the number of factors for each user and movie matrix will be determined when the model is initialised. . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return (users * movies).sum(dim=1) . So the input x to the model will be a tensor of whatever the batch size is multiplied by 2 - where the first column (x[:, 0]) contains the user IDs and the second column (x[:, 1]) contains the movie IDs. So the input essentially has 2 columns. . x,y = dls.one_batch() x.shape . torch.Size([64, 2]) . So we have defined our architecture and so can now create a learner to optimise the model. Because we are building the model from scratch we will use the Learner class to do this. We will use MSE as our loss function as this is a regression problem i.e. we are predicting a number, the rating. . n_users = len(dls.classes[&#39;user&#39;]) n_movies = len(dls.classes[&#39;title&#39;]) # Create model with 50 factors for users and movies each model = DotProduct(n_users, n_movies, 50) # Create Learner object learn = Learner(dls, model, loss_func=MSELossFlat()) . learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 1.336391 | 1.275613 | 00:09 | . 1 | 1.111210 | 1.126141 | 00:09 | . 2 | 0.988222 | 1.014545 | 00:09 | . 3 | 0.844100 | 0.912820 | 00:09 | . 4 | 0.813798 | 0.898948 | 00:09 | . Collaberative filtering - Model 2 . So how can we improve the model? we know the predictions - the ratings: should be between 0-5. Perhaps we can help our model by ensuring the predictions are forced between these valid values? We can use a sigmoid function to do this. . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return sigmoid_range((users * movies).sum(dim=1), *self.y_range) . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.985542 | 1.002896 | 00:10 | . 1 | 0.869398 | 0.914294 | 00:10 | . 2 | 0.673619 | 0.873486 | 00:10 | . 3 | 0.480611 | 0.878555 | 00:10 | . 4 | 0.381930 | 0.882388 | 00:10 | . Collaberative filtering - Model 3 . So while that didn&#39;t make a huge difference, there is more we can do to improve. At the moment by using our user and movie embedding matrices, this only gives us a sense of how a particular movie or user is described as specific values for these latent factors. What we don&#39;t have is a way to indicate something general about a particular movie or user such as this person is really fussy, or this movie is generally good or not good. . We can encode this general skew for each movie and user by including a bias value for each, which we can add after we have done the dot product. So lets add bias to our model. . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.user_bias = Embedding(n_users, 1) self.movie_factors = Embedding(n_movies, n_factors) self.movie_bias = Embedding(n_movies, 1) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) res = (users * movies).sum(dim=1, keepdim=True) res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) return sigmoid_range(res, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.941588 | 0.955934 | 00:10 | . 1 | 0.844541 | 0.865852 | 00:10 | . 2 | 0.603601 | 0.862635 | 00:10 | . 3 | 0.420309 | 0.883469 | 00:10 | . 4 | 0.293037 | 0.890913 | 00:10 | . So this started much better, but then got worse! Why is this? This looks like a case of overfitting. So we can&#39;t use data augmentation for this type of model, so we need some other way to stop the model fitting too much to the data i.e. some kind of regularization. One way to do this is with weight decay . Weight decay . So with weight decay, aka L2 regularization - adds an extra term to the loss function as the sum of all the weights squared. This will penalise our model for getting more complex than it needs to be i.e. overfitting, so this will encorage our model to have weights as small as possible the get the job done i.e. occams razor. . Why weights squared? The idea is the larger the model parameters are, the steeper the slope of the loss function. This can cause the model to focus too much on the data points in the training set. Adding weight decay will make training harder, but will force our model to be as simple as possible, less able to memorise the training data - and force it to generalise better. . Rather than calculate the sum of all weights squared, we take the derivative which is 2 x parameters and addd to our loss e.g. . parameters.grad += wd 2 parameters . Where wd is a factor we can control. . x = np.linspace(-2,2,100) a_s = [1,2,5,10,50] ys = [a * x**2 for a in a_s] _,ax = plt.subplots(figsize=(8,6)) for a,y in zip(a_s,ys): ax.plot(x,y, label=f&#39;a={a}&#39;) ax.set_ylim([0,5]) ax.legend(); . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.928223 | 0.957245 | 00:11 | . 1 | 0.886639 | 0.881928 | 00:10 | . 2 | 0.771433 | 0.832266 | 00:11 | . 3 | 0.597242 | 0.821840 | 00:11 | . 4 | 0.506455 | 0.822054 | 00:10 | . Manual embeddings . So we used a pre-made Embeddings class to make our embedding matrices, but did&#39;nt see how it works so lets make our own now. So we need a randomly initialised weight matrix for each. By default Pytorch tensors are not added as trainable parameters (think why, data are tensors also) so we need to create it in a particular way to make the embeddings trainable, using the nn.Parameter class. . # Create tensor as parameter function, with random initialisation def create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) # Create model with our manually created embeddings class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.movie_factors = create_params([n_movies, n_factors]) self.movie_bias = create_params([n_movies]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] movies = self.movie_factors[x[:,1]] res = (users*movies).sum(dim=1) res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] return sigmoid_range(res, *self.y_range) . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.923637 | 0.948116 | 00:12 | . 1 | 0.869177 | 0.879707 | 00:11 | . 2 | 0.731731 | 0.836616 | 00:12 | . 3 | 0.590497 | 0.825614 | 00:11 | . 4 | 0.484070 | 0.825161 | 00:11 | . Collaberative filtering - Model 4 . Our models developed so far are not deep learining models, as they dont have many layers. To turn this into a deep learning model we need to take the results of the embedding lookup and concatenate those activations together - this will then give us instead a matrix that we can then pass through linear layers with activation functions (non-linearities) as we would in a deep learning model. . As we are concatinating embeddings rather than taking their dot product, the embedding matrices can have different sizes. Fastai has a handy function for reccomending optimal embedding sizes from the data. . embs = get_emb_sz(dls) embs . [(944, 74), (1665, 102)] . class CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) . model = CollabNN(*embs) . learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) . epoch train_loss valid_loss time . 0 | 0.943013 | 0.951147 | 00:11 | . 1 | 0.913711 | 0.900089 | 00:11 | . 2 | 0.851407 | 0.886212 | 00:11 | . 3 | 0.816868 | 0.878591 | 00:11 | . 4 | 0.772557 | 0.881083 | 00:11 | . Fastai lets you create a deep learning version of the model like this with the higher level function calls by passing use_nn=True and lets you easily create more layers e.g. here with two hidden layers, of size 100 and 50, respectively. . learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 1.002377 | 0.995780 | 00:13 | . 1 | 0.879825 | 0.928848 | 00:13 | . 2 | 0.888932 | 0.899229 | 00:13 | . 3 | 0.821391 | 0.871980 | 00:13 | . 4 | 0.796728 | 0.869211 | 00:13 | . Conclusion . So we have built a collaberative filtering model from scratch, and saw how it can learn latent factors from the data itself. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/05/25/collaberative-filtering-from-scratch.html",
            "relUrl": "/deep-learning-theory/2021/05/25/collaberative-filtering-from-scratch.html",
            "date": " • May 25, 2021"
        }
        
    
  
    
        ,"post41": {
            "title": "State-of-the-art Deep Learning image model techniques in 2021",
            "content": "Introduction . In this article we are going to look at some of the most advanced techniques available in 2021 for training deep learning vision models. These go beyond the basics of mini-batch gradient descent, learning rates, pre-sizing, transfer learning, discriminative learning rates, and mixed-precision training. . Library and Dataset . I will be using the fastai deep learning library for code examples, as well as the fastai curated Imagenette dataset which is a specially curated subset of the well known ImageNet dataet of 1.3 million images from 1,000 categories. The Imagenette dataset consists of a much smaller set of images and just 10 categories. . We will define a baseline model here using the dataset to then compare the effect of each advanced technique. . . Normalisation . When training a model, its helpful to ensure the image data is normalised. This ensures that different images end up with data that is in the same range of values, which helps the model better focus more on the content on the images. So here by normalised we mean we want the image data values to have a mean of 0 and a standard deviation of 1. . The fastai library will automatically normalise images per batch, and this is suitable for models that we might train from scratch. When using transfer learning this default approach is not a good idea, because a pre-trained model has been trained on image data with a particular mean and standard deviation. So to use a pre-trained model with new images, we need to ensure these new images are normalised to the same mean and standard deviation that the original model data was trained with. . We can do this my specifying normalisation stats in fastai, which already knows the stats for many common datasets, including of course fastai’s own Imagenette dataset which makes it much easier. . We can also define a function get_dls() which will make it quicker to define different types of data loader i.e. with different batch or image sizes. . . After applying our normalisation, we can see the mean and standard deviation are approximatly 0 and 1 respectively on a test batch of images. . Lets now try this normalised data and train our model. . . While normalisation here hasn’t made a huge improvement over our baseline model, normalisation does make a bigger difference especially with bigger models and more data. . Progressive resizing . Progressive re-sizing is another technique pioneered by fastai. Essentially this involves training models on smaller versions of the images first, before continuing training on bigger images. This has 2 major benefits: . Model training time is much faster | Model accuracy ends up better than if we trained the model only on bigger images | . How can this be the case? lets remember that with convolutional deep learning models, early layers focus on recognising primitive features like lines and edges, and later layers more composite features such as eyes or fur. So if we change the image size during training, our earlier model will still have learnt many useful things applicable to bigger and higher resolution images. . In a way, this is a bit like training a model in one area then re-using that model on a similar area - which might sound familiar? As it should since this is very much what transfer learning is about as well, which works very well. So we should perhaps not be so surprised that this could work. . Another benefit of using lower resolution/smaller versions of the images first is that this is another kind of data augmentation - which should also help our models generalise better. . So lets use our get_dls() function that we defined earlier to define a data loader for our smaller lower resolution images and train the model for a few epochs. . . We will then define a new data loader for bigger images, and continue to train our model with these. . . So we can see we are already getting much better results than our baseline with just a few epochs, and much more quickly. It’s worth considering for the desired task, if transfer learning can in some cases harm performance. This might happen for example if the pre-trained model is trained on images already quite similar to the new ones you want to recognise - as in this case the model parameters are likely already quite close to what is needed, and progressive resizing could move the parameters further away from this and good results. If the use case for the pre-rained model is very different to what it was originally trained on i.e. very different sizes, shapes, styles etc - then progressive resizing here might actually help. . In either case, trying things experimentally would probably be the best way to determine which was the better approach. . Test time augmentation . Training time data augmentation is a common technique to help improve model training by providing different versions of the same images to improve the way the model generalises and with less data. Common techniques include random resize crop, squish, stretch, and image flip for example. . Test time augmentation (TTA) is an interesting approach of using augmentation when using the model for inference. Essentially at inference time for a given image, different augmentations of the same image will be predicted on by the model, then we can use either the average or maximum of these versions as a measure of model performance. This can give us a better idea of the models true performance, and often results in improvements in performance. . In the fastai library its quite easy to apply TTA. . . While this does not add any extra time to training, it does make inference slower. . Mixup . Mixup is a technique introduced in the paper mixup: Beyond Empirical Risk Minimization by Hongyi Zhang et al. It’s a powerful data augmentation technique that seeks to address the weaknesses of many previous methods such as crop-resize, squishing etc. One of the key drawbacks to previous approaches was needing some expert knowledge of when those techniques were applicable or nor as well as how to apply them. . For example, take the flip method that augments by flipping the image vertically or horizontally - should one apply that one way or the other? it will probably depend on the kind of images you have. Also flipping is limited i.e. you can just apply it one way or the other, there are no ‘degrees of flipping’ for example. Having ‘degrees of’ or gradation of augmentation can be very useful for giving the model a rich variety of images along the spectrum to allow it to better learn and generalise. . Mixup essentially takes two images and combines them, with a randomly selected weight of transparency for each image for the combined image. We will then take a weighted average (using the same random weights) applied to the labels of each image, to get the labels for the mixed image. . So the combined image will have labels that are in proportion to the amount of each original image. . . Here the third image is built from 0.3 of the first one and 0.7 of the second one. The one-hot encoded labels for the first and second images and final mixed image would be say: . Image 1: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] | Image 2: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0] | Mixed: [0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0] | . We can use this Mixup technique in the fastai library in the following way. . . This model is likely going to be harder and longer to train, for all the many examples ‘in between’ that this method will generate, but it should allow the model to generalise better. The beauty of this approach is that unlike many previous approaches this doesn’t require extra knowledge about the dataset to use - the ‘appropriateness’ of each image is present in the augmentation - so its the degrees of which we vary here really. This also opens this method to use in other areas beyond even vision models, to NLP for example. . Mixup also helps with another problem. A ‘perfect’ dataset with perfect labels say of only 1 and 0, pushes the model to train towards a sense of ‘perfection’ and absolute confidence, this is of course the ideal that the cross-entropy loss function does well to optimise for. By removing ‘perfection’ from our labels, we force our model to train to become less absolutely confident in its predictions, we train it to become more nuanced and subtle in its predictions that err towards partial than perfect probabilities for label prediction. . Label smoothing . Deep learning vision models train for perfection, this is especially due to the nature of the most common classification loss function cross-entropy loss. For example, because our labels are often perfect i.e. 1 or 0 despite how perfect the expression of that label is in the image, the model will keep pushing for the perfection of 1 or 0 i.e. even 0.999 will not be good enough. This can lead to overfitting, and is a consequence of this kind of training and loss function. In practice, images often do not conform to the perfection of the labels assigned them. . With label smoothing rather than use perfect labels of 1 and 0, we use a number a bit less than 1 and a number a bit more than zero. By doing this we encourage our model to become less confident, more robust (e.g. if there is mislabelled data). This model should generalise better. This technique was introduced in the paper Rethinking the Inception Architecture for Computer Vision by C Szegedy et al. . . We can use this technique in the fastai library in the following way. . . As with Mixup, you generally won’t see significant improvements with this technique until you train more epochs. . Conclusion . In this article we have covered 5 state-of-the-art techniques for training deep learning vision models using the fastai deep learning library, each of which can significantly help produce the best results currently possible for vision models in 2021. .",
            "url": "https://www.livingdatalab.com/deep-learning-theory/2021/05/22/state-of-the-art-deep-learning-image-model-2021.html",
            "relUrl": "/deep-learning-theory/2021/05/22/state-of-the-art-deep-learning-image-model-2021.html",
            "date": " • May 22, 2021"
        }
        
    
  
    
        ,"post42": {
            "title": "Automatic Recognition of Woodlands and Water from Satellite Imagery using AI and Deep Learning",
            "content": ". Introduction . Satellite imagery is being used together with AI and deep learning in many areas to produce stunning insights and discoveries. In this project I look at applying this approach to recognising buildings, woodlands &amp; water areas from satellite images. . Dataset . The dataset used for this project comes from the research paper LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands and Water from Aerial Imagery which gathered satellite imagery of different areas of Poland. The satellite images have 3 spectral bands so are RGB jpg images. The researchers chose to use 4 classes for identifying objects in these images: . Building | Woodland | Water | Background (i.e. everything else) | . . This is an image segmentation dataset, so the classes are expressed as colourmap shading by pixels for parts of the image that correspond to each class. These image colourmap/masks for the classes are represented as a png image, one of each of the satellite images. . . Methodology . For this project I used the fastai deep learning library which is based on Pytorch/Python. The dataset lends itself to the approach of image segmentation classification as the classes in the dataset are expressed as shaded regions, as opposed to say multi-label image classification using text labels. For this approach, the UNET deep learning architecture has prooven extremely good for image segmentation problems - which is what I chose to use here. . Prepare and load data . ## Set path for image files path = Path(DATA_PATH) ## Set the text for the classes codes = np.array([&quot;building&quot;, &quot;woodland&quot;, &quot;water&quot;, &quot;Background&quot;]) . ## Load image files from the path fnames = get_image_files(path/&quot;images&quot;) ## Define a function to get the label png file def label_func(fn): return path/&quot;labels&quot;/f&quot;{fn.stem}_m{&#39;.png&#39;}&quot; . ## Create a data loader for this image segmentation dataset dls = SegmentationDataLoaders.from_label_func( path, bs=BATCH_SIZE, fnames = fnames, label_func = label_func, codes = codes ) ## Show a batch of images dls.show_batch() . So we can see a nice feature of the fastai library is able to combine the original satellite image overlayed with the colourmap for the class labels with some transparency so we can see the image and labels together. . dls.show_batch(figsize=(10,10)) . dls.show_batch(figsize=(10,10)) . Training the UNET model . ## Create a UNET model using the resnet18 architecture learn = unet_learner(dls, resnet18) ## Train the model learn.fine_tune(3) ## Show the results learn.show_results() . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . epoch train_loss valid_loss time . 0 | 0.425658 | 0.249241 | 26:51 | . epoch train_loss valid_loss time . 0 | 0.207543 | 0.165862 | 27:07 | . 1 | 0.173056 | 0.227951 | 27:02 | . 2 | 0.128388 | 0.140451 | 27:00 | . So fastai&#39;s fine_tune() method will first freeze all but the last layer and train for 1 epoch, and then train for the specified number of epochs (3 in our case). Because image segmentation datasets are particularly big, these can take quite a while to train even on a GPU. In this case 1+3 epochs has taken around 2 hours of training time. . We can see though in this time both the training and validation loss have come down quite nicely, even after 4 epochs. Looking at our results we can see our UNET model has done extremely well when tested on validation images not previosuly seen by the model in the Target/Prediction pair examples above. . Lets see some more tests and results. . learn.show_results() . learn.show_results(max_n=4) . The model does seem to have generally done a good job at predicting the correct classes in the image for a wide range of different satellite image types and conditions. . Conclusion . In this project we have looked at a satellite image segmentation dataset and have achieved good results from only a limited amount of training. .",
            "url": "https://www.livingdatalab.com/projects/2021/05/15/satellite-recognition-buildings-woodland-water-ai.html",
            "relUrl": "/projects/2021/05/15/satellite-recognition-buildings-woodland-water-ai.html",
            "date": " • May 15, 2021"
        }
        
    
  
    
        ,"post43": {
            "title": "An Eye in the Sky - How AI and Satellite Imagery can help us better understand our changing world",
            "content": ". Introduction . Many of the greatest challenges the world faces today are global in nature, climate change being one of the clearest examples. While we also have huge amounts of data of different kinds, trying to make sense of all this data to help us make better decisions can be a significant challenge in itself when attempted by humans alone. AI is a powerful technology that holds huge potential for helping us use this data more easily, to help us make better decisions for the problems we face. . In the water industry where I work, satellite image data and AI holds great potential for helping solve a number of problems, such as the detection of leaks, water resource management to ensure on ongoing water supply accounting for changes in population and climate change, water quality monitoring, and flood protection. . Beyond the water industry, satellite images and AI are working together to provide critical insights in many diverse areas such as disaster response and recovery, the discovery of hidden archaeological sites, city infrastructure monitoring, combating illegal fishing, and predicting crop yields. . But how does this technology work? and can you understand the basics of how it works without any technical knowledge? The answer is I believe yes, and I will try to illustrate this by describing a recent project I completed using this approach. . Using AI to automatically recognise Woodlands and Water areas . In a recent project, I used satellite imagery from Poland to train an AI to automatically recognise areas in the images such as woodlands and water. So AI is just about throwing some data at it and some magic happens? Actually not quite! This is a common myth about how AI actually works. . . The key requirement for using AI is not just using any data, but something called labelled data. Labelled data is data that has been tagged with one or more labels that describe things inside the data. So in this project, the labels used for these satellite images were woodlands and water: if an image contains one of these things, the image would have a label or tag for that. This is how the AI learns, it looks at each satellite image, see’s which labels it has, and tries to learn what in the image indicates each label. So it’s not really magic how an AI learns at all, an AI just learns from examples of labelled things - that’s it basically. . Here are some more examples of the satellite images, now with labels. The labels are colours filled in, so for example water areas are coloured in pink and woodland areas in red. . . How do these images get their coloured labels? well some poor human has to painstakingly spend hours carefully colouring them all in with the right colours. But its well worth it, since we can train the AI to use these labelled satellite images (just 33 images) to learn to recognise these things in them, and once it can do this, we can then use the AI to recognise these things in new satellite images, as many times as we like. This is the real power of AI systems, which can learn to do things only humans could previously do, and then do them far more efficiently and quickly than a human could ever do, millions of times, without needing even a coffee break! . So how well does the AI learn to recognise these things? after running the training process a while, these are some of the results I got when I tested the AI on images it had never seen. Here the ‘Target’ on the left are the labels for images the AI has never seen, and the ‘Prediction’ on the right are what the AI thinks the label colour areas should be in the image. . . So I’d say the AI has done a pretty good job. You can see in these examples it seems to have recognised the correct water areas (in pink) and woodland areas (in red) pretty well? The AI was only trained for a limited time, most likely if I had trained it for longer it would have done even better. I could now use this AI on any new satellite images, and know it would do quite well at recognising woodland and water areas fairly accurately. Because the labels here are actually coloured dots on the image, we could add up all the dots for water or woodland on an image and get a fairly accurate measure for how much water or woodland there was there. . Just imagine what we could do with even this fairly simple AI. For example, we could use it to estimate the woodland and water areas of different parts of a country quite accurately, anywhere in the world. If we took different satellite photos of the same area over time, we could estimate how the water or woodland areas were changing over time, and by how much, all automatically. The possibilities are endless. . More about the technical details of this project can be found in this article . Conclusion . In this article I’ve introduced how satellite images and AI are a powerful new technology being used to provide valuable insights to a range of different challenges and tasks we face in the world today. By describing my own project using AI to recognise woodland and water areas in satellite images, I hope I have given you a better understanding of how this technology actually works, and of its huge potential for humanity. .",
            "url": "https://www.livingdatalab.com/opinion/2021/05/14/ai-satellite-images.html",
            "relUrl": "/opinion/2021/05/14/ai-satellite-images.html",
            "date": " • May 14, 2021"
        }
        
    
  
    
        ,"post44": {
            "title": "What AI can tell us about the hidden preferences of human beings",
            "content": ". Introduction . AI systems are increasingly being used in almost every area of human activity, from the online world, in streaming media, social media &amp; online shopping - to the offline world, in policing, healthcare and other public services as well as many different physical industries such as water, energy, agriculture and manufacturing. These applications of AI are having a huge impact, often beyond what is commonly known to the public, both positive and negative. . Most work in the field is focussed on trying to use AI to solve a particular problem at hand, and if the problem is ‘solved’ then little more thought is often done. Much less work goes on into understanding the fundamental nature of the AI created to solve a particular problem. To some extent this is of course because the main motivation is to solve the problem, but another reason is often because AI systems i.e. artificial neural networks, are often incredibly complicated, are not directly created by humans - and so can actually be very difficult to understand the inner workings of. Questions such as How is it doing it? Why is it doing it? What has it learnt? are questions often not addressed - as long as the AI system seems to ‘get the job done’. . It’s certainly my belief that this much neglected area is worth far more work than it often gets, not only to allow us to understand the AI system at a deeper level, but that it might also give us new insights and understandings into the area the AI system is being applied to. . In this article I’m going to look at one particular area of AI application called Recommendation Systems. For a recent project, I created an AI system for recommending new books to readers. I then extended the project to study how this AI book recommendation system itself was working, and discovered what it had learnt about the hidden preferences of book readers. . What are Recommendation Systems? . Recommendation systems are very common particularly in online services. For example, Amazon suggestions for alternative products, on Netflix suggestions for films, on Spotify for music, or on Facebook for the content you see on your newsfeed. All of these services and more use recommendation systems to suggest new content to people, but what are these systems and how do these actually work? . A very simple approach might be to recommend the most popular items to people. Of course, popular items would probably appeal to many - but not to everyone. With modern AI based recommendation systems we can do much better than this, to make more personalised suggestions that are unique to each person. There are two main approaches to this: content-based filtering and collaborative filtering. . . With content-based filtering, we look at the content a person has previously looked at (eg songs or films you have previously watched) as a basis to recommend new content. In this case, the AI system does the work here to understand what content is similar, for example what films are similar. This might be based on more obvious things such as the film genre, or which actors are in the film. However it can also be based on less obvious things that the AI can learn for itself about what makes films similar or different, things that are not given to the AI at all, but that the AI system can learn itself. These hidden aspects are called latent factors. . With collaborative filtering, we look at other people who are similar to us, and suggest items that they have liked, that we have not yet seen. Here the AI system does the work to understand which people are similar to us, as a basis to make suggestions. As a simple example, on a music service, we could find another listener who has listened to some of the same songs we have, find a song they have listened to that we have not, then recommend that to us. However, this simple strategy may not always be effective, just because 2 people like the same song, that does not always mean they would both like another song that one of them liked, let alone that both people are ‘similar’? What makes people similar and different, might be based on things like the genre of music they liked, which artists etc. . But what truly makes people similar or different in their preferences can also be based on less obvious things, more nuanced and subtle reasons, things people often do not perhaps even understand themselves, biases, etc that are hidden and unconscious, and yet drive and influence people’s choices and behaviour. These hidden biases and influences are things not given to the AI at all, how could they be? and yet, they are things AI systems can still learn about for itself, which are again here called latent factors. . Creating a book recommendation system . For my book recommendation system, I used the collaborative filtering approach. The data I used to create this system is the Book Crossing dataset which is data on peoples book ratings of around 27,000 different books, from the Book Crossing community website, gathered in September 2004. There are around 28,000 users of this website who are from all over the world, and from a range of different ages. The key data is a table of individual ratings of a person (identified by a unique user id), a book and a the value of the rating (a number between 0-10). . . These user ratings are not exhaustive, i.e. every user has not rated every book. Note also there is no extra information about the books such as categories, or about each person such as their ages. But we don’t actually need this extra data, we can create an AI collaborative filter based system (commonly called a ‘model’) that learns just from the table of users, books and ratings, to build up an understanding of the latent factors that uniquely describes each book, and each person. Once the model has been ‘trained’ on this data, and learnt these latent factors - the model can then use these latent factors to make recommendations for each person, about what new books they might like. . When the AI model learns, it doesn’t directly see the real ratings - it just tries to make guesses about what the ratings should be. We then compare the guesses to the actual ratings we know, and give the model back some measure of accuracy, which the model then uses to improve its guesses in the next cycle. This training process repeats for many cycles. . Going down the rabbit hole: what is our book recommendation system actually doing? . So we now have a book recommendation system that can suggest new books to people. But we can if we choose, take things further. Simply from the data of book ratings and the learning process, the system has had to understand something more, about the implicit reasons certain people prefer certain books over others - and indeed perhaps about general qualities that drive these choices. These qualities might correspond to categories we might recognise as more obvious book genres, but they might also correspond to other qualities that are not commonly recognised as ‘categories’ yet might still be factors that drive people to prefer different books. . How might we try and understand these latent factors that drive people’s preferences for books? . We actually have 2 types of latent factors, normal factors and bias factors. Bias factors represent a general bias towards a book, either positive or negative. This will mean for that book, regardless if it would be generally a good suggestion for a person - if it has a negative bias it will be far less likely to be suggested. Similarly, if a book has a very positive bias, it might be more likely to be suggested to you, even if you would not normally read that kind of book i.e. you would not normally read that genre. We can think of bias then as some kind of measure of ‘general popularity’ of a book. . Negative bias books . So these are the bottom 10 books with the most negative bias in the AI model: . Wild Animus | The Law of Love | Blood and Gold (Rice Anne Vampire Chronicles) | Gorky Park | The Cat Who Went into the Closet | Waiting to Exhale | Night Moves (Tom Clancy’s Net Force No. 3) | Ruthless.Com (Tom Clancy’s Power Plays) | Ground Zero and Beyond | Say Cheese and Die! | . Let us look at the 2 books with the most negative bias, ‘Wild Animus’ and ‘The Law of love’. So what is Wild Animus about? The synopsis reads: . “Sam Altman is an intense young man with an animal energy whose unleashed and increasingly unhinged imagination takes him first to Seattle and then farther north, to the remote Alaskan wilderness. …” . This book does have many many online reviews, on the whole which can be summarized by the review What the hell is Wild animus?. The review concludes with a quote from a goodreads reviewer: . “I’ll tell you the ending. A column of lava erupts from beneath his feet while he is dressed in a goat costume and wolves are in mid-air tearing him apart.” . On the whole it seems, Wild Animus seems to provoke a very unfavourable response from most reviewers! The next most negative bias book is ‘The law of love’, it’s synopsis reads: . “After one night of passion, Azucena, an astroanalyst in twenty-third-century Mexico City, is separated from her Twin Soul, Rodrigo, and journeys across the galaxy and through past lives to find her lost love, encountering a deadly enemy along the way…” . As it happens this book has as many positive reviews as negative online, in fact few reviews seem neutral at all. So this is not universally seen as a bad book, by humans who post reviews online anyway. Nevertheless, our AI model regards this as a book that should not really be suggested to anyone. Is that because the book seems to be so divisive? and perhaps there are other books that are ‘safer bets’? Either way, the computer says no. . Positive bias books . Let’s now look at the top 10 books with the most positive bias in the AI model: . The Lovely Bones: A Novel | Harry Potter and the Goblet of Fire | The Da Vinci Code | Harry Potter and the Prisoner of Azkaban | The Secret Life of Bees | Harry Potter and the Sorcerer’s Stone | Harry Potter and the Chamber of Secrets | Where the Heart Is | To Kill a Mockingbird | The Red Tent | . So looking in more detail at the 2 books with the most positive bias we have ‘The lovely bones’ and ‘Harry Potter and the Goblet of Fire’. So the synopsis of “The lovely bones” is as follows: . “It is the story of a teenage girl who, after being raped and murdered, watches from her personal Heaven as her family and friends struggle to move on with their lives while she comes to terms with her own death. “ . This book has a large number of very favourable reviews, in fact it was hard to find a very negative review of this book at all. The New York Time’s review perhaps sums up well the general sentiment felt by most reviewers of this book: . “…Sebold deals with almost unthinkable subjects with humor and intelligence and a kind of mysterious grace. The Lovely Bones takes the stuff of neighborhood tragedy – the unexplained disappearance of a child, the shattered family alone with its grief – and turns it into literature…” . So we can perhaps appreciate some of the reasons perhaps why the AI model thinks this is a good book to recommend to anyone, regardless of what their normal reading preferences might be. The second most positively biased book is “Harry Potter and the Goblet of fire”. Being one of a series of some of the most popular books of all time - this is perhaps not surprising at all that the AI model thinks this would be a good book to recommend to most people, regardless of their normal reading preferences. . Looking at other latent factors . So for the remaining latent factors, we actually have 50 of them for each of our 27,000 books - so quite a few! However we can use a process called dimensionality reduction to actually reduce these down, to the 2 most important latent factors for all books. We can then plot each book on a graph, with the measure that book has for each of these 2 key latent factors. . . A bigger view of this image of latent factors 1 &amp; 2 can be seen here . Here we can see 50 books plotted. On the horizontal axis that is a measure of how much of latent factor 1 each book has. On the vertical axis, that is a measure of how much of latent factor 2 each book has. . Let’s look into latent factor 1, which is the strongest latent factor used by the AI model to make book recommendations. . Books with high values for latent factor 1 . We can see in the bottom right corner of the chart ‘The lovely bones’. This has one of the highest measures of factor 1, because it is one of the furthest to the right. We also know from our look at bias factors, that this is the book with the strongest positive bias latent factor as well i.e. a generally popular book. Let’s also note it falls into the categories of ‘Crime, Thriller, Mystery’. . Looking at another book with a high factor 1, in the top right we have ‘Good in Bed’. The synopsis of the book is: . “It tells the story of an overweight Jewish female journalist, her love and work life and her emotional abuse issues with her father.” . It generally also has good reviews, and would fall into the category of ‘Women’s fiction’. Let’s look at a third book with a high factor 1, “The life of Pi”. The synopsis of this book is: . “After the tragic sinking of a cargo ship, a solitary lifeboat remains bobbing on the wild, blue Pacific. The only survivors from the wreck are a sixteen year-old boy named Pi, a hyena, a zebra (with a broken leg), a female orang-utan and a 450-pound Royal Bengal tiger.” . Again this book generated very good reviews, was very popular, and might fall into the category of ‘Contemporary fiction’. What are some common things to note about all of these books with a high latent factor 1? . They are all very popular and have great critical acclaim | 2 of these books turned into films, and the third is currently being adapted for film. | All 3 have a theme of a huge personal tragedy, which the protagonist is successful in overcoming and rising above by themselves | . So lets bear this in mind, while we look at books with the lowest latent factor 1. . Books with low values for latent factor 1 . Books with low values for factor one are on the far left of the chart. For example we have ‘A painted house’ the synopsis of this being: . “A Painted House is a moving story of one boy’s journey from innocence to experience, drawn from the personal experience of legendary legal thriller author John Grisham” . Let’s also note this would fall into the categories of ‘Contemporary fiction, mystery’. Looking at another book with a low factor 1 ‘The street lawyer’, the synopsis being: . “…about an attorney who leaves his high-priced firm to work for the less fortunate.” . This also seems to be another book by John Grisham, that would fall into categories such as ‘Thriller, Mystery’. Looking at Grisham’s work, how might we characterise his work more generally? He is well known for writing legal thrillers, and themes such as ‘the triumph of the underdog’, however A painted house seems not to quite fit these themes, an exception - so why is it here? A theme that might link both is ‘the triumph of working together’ in the case of the legal thrillers it’s the lawyer, the legal system his collaborators, in ‘a painted house’ its the family that needs to pull together to triumph, as explained in this review: . “…The primary theme is the importance of family: only by pulling together does the family achieve even moderate success” . In fact when we look at the other books with the lowest factor 1, on the far left of the chart, they pretty much are all John Grisham legal thriller books such as: . The Pelican brief | The Brethren | The Summons | The Firm | . So what is latent factor 1? . Let’s now consider what factor 1 might actually be about. Given most of these books, regardless of having a low or high value of factor 1, have all been popular and successful - so popularity I would argue has nothing to do with what factor 1 is really about. . Based on what we have learnt about these books so far, I would speculate that latent factor 1 might represent a measure of ‘The triumph of the group vs the triumph of the individual’ as a theme-axis. So, low values of factor 1 would correspond to ‘The triumph of the group’ type themes, and high values of factor 1 would correspond to ‘The triumph of the individual’ type themes for books. . Remember the AI model is given no information about book categories, authors, genres, themes etc. All the AI has to learn from is the ratings between users and books - that’s all. Not only has our AI model discovered this particular axis theme by itself from very limited information, but it has done so because the AI model has judged that this theme-axis, whatever it is, is one of the most useful for the purposes of making good book recommendations to people. . Discussion . So how do we make sense of our findings? We can’t conclusively say that my suggested ‘triumph of the group vs triumph of the individual’ theme-axis is generally true, or the key hidden factor for understanding generally why people prefer certain books over others. Firstly, it’s based on an inevitably limited data set of books, people and ratings. Perhaps the people who made those ratings are not representative of the general population? Secondly, we only randomly chose 50 books to plot for our latent factors. What if we randomly picked a different set of 50 books, would we see the same kind of themes for latent factor 1, or something else? If the ‘triumph of the group vs triumph of the individual’ theme axis does appear to be a key factor over many more books and people - why is this the case? and what does it suggest about human beings more generally? However these are questions that could be further investigated, researched, and better answered - with more time, and the conclusions of which could potentially be very interesting indeed. . What we can say is that from very limited information, looking at a limited number of books, and looking at some of its latent factors such as biases and the main key factor - this AI model seems to have discovered many relationships we could recognise as humans such as ‘generally popular books’ and ‘generally awful books’. The interpretation of the key latent factor as ‘triumph of the group vs triumph of the individual’ as a theme-axis is of course very speculative at this stage, and yet very intriguing! Would you come to a different conclusion looking at the books at either end of the axis of latent factor 1 on the chart? What do you think latent factor 1 on the horizontal axis is all about? What do you think latent factor 2 on the vertical axis is about? I’d love to hear your feedback and thoughts on this, so do feel free to comment below. . Conclusion . In this article I’ve tried to highlight a few key themes: that AI is being used everywhere, that little work is often done to understand how and why these AI systems work, and that we have so much to gain by actually trying to look inside and understand these AI systems better. I’d also argue this is becoming more and more important, given the growing impact of these increasingly powerful systems on our world and human life. . Looking inside these AI systems, even though not straightforward - gives us the chance to know more about what they are really doing and why, and can give us intriguing hints about the domains in which they are used, which I have tried to illustrate with my book recommendation project. . When these AI systems are used in the domain of human choices, the potential is there hidden within these systems to perhaps discover new insights, and pose new questions about ourselves and our choices, that we may have never even considered or knew to ask. Perhaps by looking a little deeper into AI, we can see a little deeper into ourselves. .",
            "url": "https://www.livingdatalab.com/opinion/2021/04/04/ai-human-preferences.html",
            "relUrl": "/opinion/2021/04/04/ai-human-preferences.html",
            "date": " • Apr 4, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Livingdatalab",
          "content": "Livingdatalab is the personal blog of Pranath Fernando, a Data Scientist in the water industry. . You can contact me via twitter or linkedin. .",
          "url": "https://www.livingdatalab.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  

  

  

  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.livingdatalab.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}