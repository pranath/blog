<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>State-of-the-art Deep Learning image model techniques in 2021 | livingdatalab</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="State-of-the-art Deep Learning image model techniques in 2021" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this article we are going to look at some of the most advanced techniques available in 2021 for training deep learning vision models." />
<meta property="og:description" content="In this article we are going to look at some of the most advanced techniques available in 2021 for training deep learning vision models." />
<link rel="canonical" href="https://www.livingdatalab.com/deep-learning-theory/2021/05/22/state-of-the-art-deep-learning-image-model-2021.html" />
<meta property="og:url" content="https://www.livingdatalab.com/deep-learning-theory/2021/05/22/state-of-the-art-deep-learning-image-model-2021.html" />
<meta property="og:site_name" content="livingdatalab" />
<meta property="og:image" content="https://www.livingdatalab.com/images/deep1.jpg" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-22T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://www.livingdatalab.com/deep-learning-theory/2021/05/22/state-of-the-art-deep-learning-image-model-2021.html","@type":"BlogPosting","headline":"State-of-the-art Deep Learning image model techniques in 2021","dateModified":"2021-05-22T00:00:00-05:00","datePublished":"2021-05-22T00:00:00-05:00","image":"https://www.livingdatalab.com/images/deep1.jpg","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.livingdatalab.com/deep-learning-theory/2021/05/22/state-of-the-art-deep-learning-image-model-2021.html"},"description":"In this article we are going to look at some of the most advanced techniques available in 2021 for training deep learning vision models.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://www.livingdatalab.com/feed.xml" title="livingdatalab" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-91568149-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">livingdatalab</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Livingdatalab</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">State-of-the-art Deep Learning image model techniques in 2021</h1><p class="page-description">In this article we are going to look at some of the most advanced techniques available in 2021 for training deep learning vision models.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-05-22T00:00:00-05:00" itemprop="datePublished">
        May 22, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#deep-learning-theory">deep-learning-theory</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#library-and-dataset">Library and Dataset</a></li>
<li class="toc-entry toc-h2"><a href="#normalisation">Normalisation</a></li>
<li class="toc-entry toc-h2"><a href="#progressive-resizing">Progressive resizing</a></li>
<li class="toc-entry toc-h2"><a href="#test-time-augmentation">Test time augmentation</a></li>
<li class="toc-entry toc-h2"><a href="#mixup">Mixup</a></li>
<li class="toc-entry toc-h2"><a href="#label-smoothing">Label smoothing</a></li>
<li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li>
</ul><h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>

<p>In this article we are going to look at some of the most advanced techniques available in 2021 for training deep learning vision models. These go beyond the basics of mini-batch gradient descent, learning rates, pre-sizing, transfer learning, discriminative learning rates, and mixed-precision training.</p>

<h2 id="library-and-dataset">
<a class="anchor" href="#library-and-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Library and Dataset</h2>

<p>I will be using the <a href="https://www.fast.ai" target="_blank">fastai</a> deep learning library for code examples, as well as the fastai curated <em>Imagenette</em> dataset which is a specially curated subset of the well known ImageNet dataet of 1.3 million images from 1,000 categories. The Imagenette dataset consists of a much smaller set of images and just 10 categories.</p>

<p>We will define a baseline model here using the dataset to then compare the effect of each advanced technique.</p>

<p><img src="/images/sota-vision/sota-vision1.png" alt="" title=" "></p>

<h2 id="normalisation">
<a class="anchor" href="#normalisation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Normalisation</h2>

<p>When training a model, its helpful to ensure the image data is normalised. This ensures that different images end up with data that is in the same range of values, which helps the model better focus more on the content on the images. So here by normalised we mean we want the image data values to have a mean of 0 and a standard deviation of 1.</p>

<p>The fastai library will automatically normalise images per batch, and this is suitable for models that we might train from scratch. When using transfer learning this default approach is not a good idea, because a pre-trained model has been trained on image data with a particular mean and standard deviation. So to use a pre-trained model with new images, we need to ensure these new images are normalised to the same mean and standard deviation that the original model data was trained with.</p>

<p>We can do this my specifying normalisation stats in fastai, which already knows the stats for many common datasets, including of course fastaiâ€™s own Imagenette dataset which makes it much easier.</p>

<p>We can also define a function <strong>get_dls()</strong> which will make it quicker to define different types of data loader i.e. with different batch or image sizes.</p>

<p><img src="/images/sota-vision/sota-vision2.png" alt="" title=" "></p>

<p>After applying our normalisation, we can see the mean and standard deviation are approximatly 0 and 1 respectively on a test batch of images.</p>

<p>Lets now try this normalised data and train our model.</p>

<p><img src="/images/sota-vision/sota-vision3.png" alt="" title=" "></p>

<p>While normalisation here hasnâ€™t made a huge improvement over our baseline model, normalisation does make a bigger difference especially with bigger models and more data.</p>

<h2 id="progressive-resizing">
<a class="anchor" href="#progressive-resizing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Progressive resizing</h2>

<p>Progressive re-sizing is another technique pioneered by fastai. Essentially this involves training models on smaller versions of the images first, before continuing training on bigger images. This has 2 major benefits:</p>

<ul>
  <li>Model training time is much faster</li>
  <li>Model accuracy ends up better than if we trained the model only on bigger images</li>
</ul>

<p>How can this be the case? lets remember that with convolutional deep learning models, early layers focus on recognising primitive features like lines and edges, and later layers more composite features such as eyes or fur. So if we change the image size during training, our earlier model will still have learnt many useful things applicable to bigger and higher resolution images.</p>

<p>In a way, this is a bit like training a model in one area then re-using that model on a similar area - which might sound familiar? As it should since this is very much what transfer learning is about as well, which works very well. So we should perhaps not be so surprised that this could work.</p>

<p>Another benefit of using lower resolution/smaller versions of the images first is that this is another kind of data augmentation - which should also help our models generalise better.</p>

<p>So lets use our <em>get_dls()</em> function that we defined earlier to define a data loader for our smaller lower resolution images and train the model for a few epochs.</p>

<p><img src="/images/sota-vision/sota-vision4.png" alt="" title=" "></p>

<p>We will then define a new data loader for bigger images, and continue to train our model with these.</p>

<p><img src="/images/sota-vision/sota-vision5.png" alt="" title=" "></p>

<p>So we can see we are already getting much better results than our baseline with just a few epochs, and much more quickly. Itâ€™s worth considering for the desired task, if transfer learning can in some cases harm performance. This might happen for example if the pre-trained model is trained on images already quite similar to the new ones you want to recognise - as in this case the model parameters are likely already quite close to what is needed, and progressive resizing could move the parameters further away from this and good results. If the use case for the pre-rained model is very different to what it was originally trained on i.e. very different sizes, shapes, styles etc - then progressive resizing here might actually help.</p>

<p>In either case, trying things experimentally would probably be the best way to determine which was the better approach.</p>

<h2 id="test-time-augmentation">
<a class="anchor" href="#test-time-augmentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Test time augmentation</h2>

<p>Training time data augmentation is a common technique to help improve model training by providing different versions of the same images to improve the way the model generalises and with less data. Common techniques include random resize crop, squish, stretch, and image flip for example.</p>

<p>Test time augmentation (TTA) is an interesting approach of using augmentation when using the model for inference. Essentially at inference time for a given image, different augmentations of the same image will be predicted on by the model, then we can use either the average or maximum of these versions as a measure of model performance. This can give us a better idea of the models true performance, and often results in improvements in performance.</p>

<p>In the fastai library its quite easy to apply TTA.</p>

<p><img src="/images/sota-vision/sota-vision6.png" alt="" title=" "></p>

<p>While this does not add any extra time to training, it does make inference slower.</p>

<h2 id="mixup">
<a class="anchor" href="#mixup" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mixup</h2>

<p>Mixup is a technique introduced in the paper <a href="https://arxiv.org/abs/1710.09412" target="_blank">mixup: Beyond Empirical Risk Minimization</a> by Hongyi Zhang et al. Itâ€™s a powerful data augmentation technique that seeks to address the weaknesses of many previous methods such as crop-resize, squishing etc. One of the key drawbacks to previous approaches was needing some expert knowledge of when those techniques were applicable or nor as well as how to apply them.</p>

<p>For example, take the flip method that augments by flipping the image vertically or horizontally - should one apply that one way or the other? it will probably depend on the kind of images you have. Also flipping is limited i.e. you can just apply it one way or the other, there are no â€˜degrees of flippingâ€™ for example. Having â€˜degrees ofâ€™ or gradation of augmentation can be very useful for giving the model a rich variety of images along the spectrum to allow it to better learn and generalise.</p>

<p>Mixup essentially takes two images and combines them, with a randomly selected weight of transparency for each image for the combined image. We will then take a weighted average (using the same random weights) applied to the labels of each image, to get the labels for the mixed image.</p>

<p>So the combined image will have labels that are in proportion to the amount of each original image.</p>

<p><img src="/images/sota-vision/sota-vision7.png" alt="" title=" "></p>

<p>Here the third image is built from 0.3 of the first one and 0.7 of the second one. The one-hot encoded labels for the first and second images and final mixed image would be say:</p>

<ul>
  <li>Image 1: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</li>
  <li>Image 2: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</li>
  <li>Mixed:   [0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0]</li>
</ul>

<p>We can use this Mixup technique in the fastai library in the following way.</p>

<p><img src="/images/sota-vision/sota-vision8.png" alt="" title=" "></p>

<p>This model is likely going to be harder and longer to train, for all the many examples â€˜in betweenâ€™ that this method will generate, but it should allow the model to generalise better. The beauty of this approach is that unlike many previous approaches this doesnâ€™t require extra knowledge about the dataset to use - the â€˜appropriatenessâ€™ of each image is present in the augmentation - so its the degrees of which we vary here really. This also opens this method to use in other areas beyond even vision models, to NLP for example.</p>

<p>Mixup also helps with another problem. A â€˜perfectâ€™ dataset with perfect labels say of only 1 and 0, pushes the model to train towards a sense of â€˜perfectionâ€™ and absolute confidence, this is of course the ideal that the cross-entropy loss function does well to optimise for. By removing â€˜perfectionâ€™ from our labels, we force our model to train to become less absolutely confident in its predictions, we train it to become more nuanced and subtle in its predictions that err towards partial than perfect probabilities for label prediction.</p>

<h2 id="label-smoothing">
<a class="anchor" href="#label-smoothing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Label smoothing</h2>

<p>Deep learning vision models train for perfection, this is especially due to the nature of the most common classification loss function cross-entropy loss. For example, because our labels are often perfect i.e. 1 or 0 despite how perfect the expression of that label is in the image, the model will keep pushing for the perfection of 1 or 0 i.e. even 0.999 will not be good enough. This can lead to overfitting, and is a consequence of this kind of training and loss function. In practice, images often do not conform to the perfection of the labels assigned them.</p>

<p>With <strong>label smoothing</strong> rather than use perfect labels of 1 and 0, we use a number a bit less than 1 and a number a bit more than zero. By doing this we encourage our model to become less confident, more robust (e.g. if there is mislabelled data). This model should generalise better. This technique was introduced in the paper <a href="https://arxiv.org/abs/1512.00567" target="_blank">Rethinking the Inception Architecture for Computer Vision</a> by C Szegedy et al. .</p>

<p>We can use this technique in the fastai library in the following way.</p>

<p><img src="/images/sota-vision/sota-vision9.png" alt="" title=" "></p>

<p>As with Mixup, you generally wonâ€™t see significant improvements with this technique until you train more epochs.</p>

<h2 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2>

<p>In this article we have covered 5 state-of-the-art techniques for training deep learning vision models using the fastai deep learning library, each of which can significantly help produce the best results currently possible for vision models in 2021.</p>

  </div>
  
  <a class="u-url" href="/deep-learning-theory/2021/05/22/state-of-the-art-deep-learning-image-model-2021.html" hidden></a>
</article><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="pranath/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>-->

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/pranath" title="pranath"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/pranath-fernando" title="pranath-fernando"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/livingdatalab" title="livingdatalab"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
<script type="text/javascript" src="/js/lightbox.js"></script>
<link rel="stylesheet" href="/css/lightbox.css">
</body>

</html>
